Script started on Sat 05 Mar 2022 06:59:57 PM EST
(base) demi@MS-7A71:~/CSI5180_A3$ conda act ivate CSI5138
(CSI5138) demi@MS-7A71:~/CSI5180_A3$ conda aactivate CSI5138M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[2Pexit
[KM[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cpython33 a3.py
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: But my bro from the 757 EJ Manuel is the 1st QB gone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.         0.57496187 0.4090901  0.
  0.4090901  0.4090901 ]
 [0.3174044  0.44610081 0.44610081 0.         0.3174044  0.44610081
  0.3174044  0.3174044 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: But my bro from the 757 EJ Manuel is the 1st QB gone
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['but', 'my', 'bro', 'from', 'the', 'ej', 'manuel', 'is', 'the', 'qb', 'gone']
cosine_similarity: 0.9798827171325684
train_input: [0.5193879933129156, 0.9798827], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Can believe EJ Manuel went as the 1st QB in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.         0.4472136  0.4472136  0.4472136  0.4472136
  0.        ]
 [0.33425073 0.46977774 0.33425073 0.33425073 0.33425073 0.33425073
  0.46977774]]
pairwise_similarity: [[1.         0.74740735]
 [0.74740735 1.        ]]
cosine_similarity: 0.7474073540060464
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Can believe EJ Manuel went as the 1st QB in the draft
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['can', 'believe', 'ej', 'manuel', 'went', 'as', 'the', 'qb', 'in', 'the', 'draft']
cosine_similarity: 0.990756094455719
train_input: [0.7474073540060464, 0.9907561], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: EJ MANUEL IS THE 1ST QB what
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.4090901  0.4090901  0.4090901 ]
 [0.5        0.         0.5        0.5        0.5       ]]
pairwise_similarity: [[1.         0.81818021]
 [0.81818021 1.        ]]
cosine_similarity: 0.8181802073667197
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: EJ MANUEL IS THE 1ST QB what
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['ej', 'manuel', 'is', 'the', 'qb', 'what']
cosine_similarity: 0.9716684818267822
train_input: [0.8181802073667197, 0.9716685], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: EJ da 1st QB off da board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.53309782 0.37930349 0.53309782
  0.37930349]
 [0.27867523 0.39166832 0.78333663 0.         0.27867523 0.
  0.27867523]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: EJ da 1st QB off da board
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['ej', 'da', 'qb', 'off', 'da', 'board']
cosine_similarity: 0.8780073523521423
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Manuel is the 1st QB to get drafted
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.         0.53309782 0.37930349 0.37930349]
 [0.44832087 0.         0.63009934 0.         0.44832087 0.44832087]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Manuel is the 1st QB to get drafted
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['manuel', 'is', 'the', 'qb', 'to', 'get', 'drafted']
cosine_similarity: 0.984354555606842
train_input: [0.5101490193104813, 0.98435456], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: My boy EJ Manuel being the 1st QB picked
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.57496187 0.4090901  0.4090901  0.
  0.4090901 ]
 [0.35464863 0.49844628 0.         0.35464863 0.35464863 0.49844628
  0.35464863]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: My boy EJ Manuel being the 1st QB picked
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['my', 'boy', 'ej', 'manuel', 'being', 'the', 'qb', 'picked']
cosine_similarity: 0.9718883633613586
train_input: [0.5803329846765685, 0.97188836], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Not surprised EJ Manuel was 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.4090901  0.4090901  0.4090901  0.
  0.        ]
 [0.35464863 0.         0.35464863 0.35464863 0.35464863 0.49844628
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Not surprised EJ Manuel was 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['not', 'surprised', 'ej', 'manuel', 'was', 'qb', 'taken']
cosine_similarity: 0.9453131556510925
train_input: [0.5803329846765685, 0.94531316], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: WOW EJ MANUEL FSU 1ST QB TAKEN
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.4090901  0.         0.4090901  0.4090901
  0.         0.        ]
 [0.3174044  0.         0.3174044  0.44610081 0.3174044  0.3174044
  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: WOW EJ MANUEL FSU 1ST QB TAKEN
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['wow', 'ej', 'manuel', 'fsu', 'qb', 'taken']
cosine_similarity: 0.913979172706604
train_input: [0.5193879933129156, 0.9139792], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Wow EJ Manuel 1st QB taken in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.4472136  0.4472136  0.4472136  0.4472136  0.
  0.        ]
 [0.33425073 0.33425073 0.33425073 0.33425073 0.33425073 0.46977774
  0.46977774]]
pairwise_similarity: [[1.         0.74740735]
 [0.74740735 1.        ]]
cosine_similarity: 0.7474073540060464
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Wow EJ Manuel 1st QB taken in the draft
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['wow', 'ej', 'manuel', 'qb', 'taken', 'in', 'the', 'draft']
cosine_similarity: 0.9821358323097229
train_input: [0.7474073540060464, 0.98213583], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: if EJ is the 1st QB off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.37930349 0.53309782 0.37930349]
 [0.44832087 0.63009934 0.         0.44832087 0.         0.44832087]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: if EJ is the 1st QB off the board
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['if', 'ej', 'is', 'the', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9835769534111023
train_input: [0.5101490193104813, 0.98357695], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Bills take EJ Manuel QBFlorida State 1st QB off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.         0.4090901  0.4090901  0.4090901
  0.         0.         0.57496187]
 [0.28986934 0.40740124 0.40740124 0.28986934 0.28986934 0.28986934
  0.40740124 0.40740124 0.        ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Bills take EJ Manuel QBFlorida State 1st QB off the board
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['bills', 'take', 'ej', 'manuel', 'state', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9512717723846436
train_input: [0.4743307064971939, 0.9512718], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Can NOT believe EJ Manuel was 1st qb taken in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.         0.         0.4472136  0.4472136  0.4472136
  0.4472136 ]
 [0.33425073 0.46977774 0.46977774 0.33425073 0.33425073 0.33425073
  0.33425073]]
pairwise_similarity: [[1.         0.74740735]
 [0.74740735 1.        ]]
cosine_similarity: 0.7474073540060464
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Can NOT believe EJ Manuel was 1st qb taken in the draft
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['can', 'not', 'believe', 'ej', 'manuel', 'was', 'qb', 'taken', 'in', 'the', 'draft']
cosine_similarity: 0.9887629151344299
train_input: [0.7474073540060464, 0.9887629], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Damon EJ 1st Qb off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.37930349 0.53309782 0.37930349
  0.53309782]
 [0.37930349 0.53309782 0.53309782 0.37930349 0.         0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Damon EJ 1st Qb off the board
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['damon', 'ej', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9750992655754089
train_input: [0.43161341897075145, 0.97509927], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: EJ Manuel not Geno Smith the 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.4472136  0.         0.4472136  0.4472136  0.
  0.4472136 ]
 [0.33425073 0.33425073 0.46977774 0.33425073 0.33425073 0.46977774
  0.33425073]]
pairwise_similarity: [[1.         0.74740735]
 [0.74740735 1.        ]]
cosine_similarity: 0.7474073540060464
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: EJ Manuel not Geno Smith the 1st QB taken
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['ej', 'manuel', 'not', 'geno', 'smith', 'the', 'qb', 'taken']
cosine_similarity: 0.9713899493217468
train_input: [0.7474073540060464, 0.97138995], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: EJ Manuel the 1st QB taken hmm I wonder who predicted that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.4472136  0.         0.4472136  0.         0.4472136
  0.4472136  0.        ]
 [0.30253071 0.30253071 0.42519636 0.30253071 0.42519636 0.30253071
  0.30253071 0.42519636]]
pairwise_similarity: [[1.         0.67647924]
 [0.67647924 1.        ]]
cosine_similarity: 0.6764792400877004
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: EJ Manuel the 1st QB taken hmm I wonder who predicted that
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['ej', 'manuel', 'the', 'qb', 'taken', 'hmm', 'i', 'wonder', 'who', 'predicted', 'that']
cosine_similarity: 0.9819480180740356
train_input: [0.6764792400877004, 0.981948], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: I was wrong EJ was the 1st QB off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.37930349 0.53309782 0.37930349 0.53309782
  0.        ]
 [0.37930349 0.53309782 0.37930349 0.         0.37930349 0.
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: I was wrong EJ was the 1st QB off the board
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['i', 'was', 'wrong', 'ej', 'was', 'the', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9742115139961243
train_input: [0.43161341897075145, 0.9742115], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Manuel over Geno as the 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.         0.4090901  0.4090901  0.4090901 ]
 [0.4090901  0.         0.57496187 0.4090901  0.4090901  0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Manuel over Geno as the 1st QB taken
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['manuel', 'over', 'geno', 'as', 'the', 'qb', 'taken']
cosine_similarity: 0.9673254489898682
train_input: [0.6694188517266485, 0.96732545], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Shock of the night EJ Manuel 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.4472136  0.4472136  0.         0.4472136  0.
  0.4472136 ]
 [0.33425073 0.33425073 0.33425073 0.46977774 0.33425073 0.46977774
  0.33425073]]
pairwise_similarity: [[1.         0.74740735]
 [0.74740735 1.        ]]
cosine_similarity: 0.7474073540060464
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Shock of the night EJ Manuel 1st QB taken
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['shock', 'of', 'the', 'night', 'ej', 'manuel', 'qb', 'taken']
cosine_similarity: 0.9824448823928833
train_input: [0.7474073540060464, 0.9824449], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Who wouldve thought EJ Manuel would be the 1st qb taken in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.         0.4472136  0.4472136  0.4472136  0.4472136
  0.         0.        ]
 [0.30253071 0.42519636 0.30253071 0.30253071 0.30253071 0.30253071
  0.42519636 0.42519636]]
pairwise_similarity: [[1.         0.67647924]
 [0.67647924 1.        ]]
cosine_similarity: 0.6764792400877004
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Who wouldve thought EJ Manuel would be the 1st qb taken in the draft
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['who', 'wouldve', 'thought', 'ej', 'manuel', 'would', 'be', 'the', 'qb', 'taken', 'in', 'the', 'draft']
cosine_similarity: 0.9816169142723083
train_input: [0.6764792400877004, 0.9816169], train_label: 1
TF_IDF_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Wow Ej Manuel is 1st qb taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.4472136  0.4472136  0.4472136  0.4472136  0.        ]
 [0.37863221 0.37863221 0.37863221 0.37863221 0.37863221 0.53215436]]
pairwise_similarity: [[1.         0.84664735]
 [0.84664735 1.        ]]
cosine_similarity: 0.8466473536503036
word_to_vector_cosine_similarity: sentence1: So EJ Manuel is the 1st QB taken, sentence2: Wow Ej Manuel is 1st qb taken
After tokenization, sentence1: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'taken'], sentence2: ['wow', 'ej', 'manuel', 'is', 'qb', 'taken']
cosine_similarity: 0.982524037361145
train_input: [0.8466473536503036, 0.98252404], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: 1st QB of the board EJ Manuel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.57496187 0.4090901  0.4090901  0.4090901 ]
 [0.4090901  0.57496187 0.         0.4090901  0.4090901  0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: 1st QB of the board EJ Manuel
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['qb', 'of', 'the', 'board', 'ej', 'manuel']
cosine_similarity: 0.9890094995498657
train_input: [0.6694188517266485, 0.9890095], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: 1st QB of the board is
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.49922133 0.49922133 0.35520009]
 [0.50154891 0.70490949 0.         0.         0.         0.50154891]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: 1st QB of the board is
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['qb', 'of', 'the', 'board', 'is']
cosine_similarity: 0.9790948629379272
train_input: [0.3563004293331381, 0.97909486], train_label: 0
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: Bills take EJ Manuel QBFlorida State 1st QB off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.         0.57496187 0.4090901  0.4090901
  0.4090901  0.         0.        ]
 [0.28986934 0.40740124 0.40740124 0.         0.28986934 0.28986934
  0.28986934 0.40740124 0.40740124]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: Bills take EJ Manuel QBFlorida State 1st QB off the board
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['bills', 'take', 'ej', 'manuel', 'state', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.969535231590271
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: EJ Manuel 1st QB off the board taken by the Bills
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.         0.57496187 0.4090901  0.4090901
  0.4090901  0.        ]
 [0.3174044  0.44610081 0.44610081 0.         0.3174044  0.3174044
  0.3174044  0.44610081]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: EJ Manuel 1st QB off the board taken by the Bills
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['ej', 'manuel', 'qb', 'off', 'the', 'board', 'taken', 'by', 'the', 'bills']
cosine_similarity: 0.9834752082824707
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: EJ Manuel selected as the 1st QB in the 2013 NFL Draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.         0.4472136  0.4472136  0.4472136  0.
  0.4472136  0.        ]
 [0.30253071 0.42519636 0.30253071 0.30253071 0.30253071 0.42519636
  0.30253071 0.42519636]]
pairwise_similarity: [[1.         0.67647924]
 [0.67647924 1.        ]]
cosine_similarity: 0.6764792400877004
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: EJ Manuel selected as the 1st QB in the 2013 NFL Draft
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['ej', 'manuel', 'selected', 'as', 'the', 'qb', 'in', 'the', 'nfl', 'draft']
cosine_similarity: 0.9844627976417542
train_input: [0.6764792400877004, 0.9844628], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: EJ Manuel will be the 1st QB taken in the 2013 NFL Draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.         0.4472136  0.4472136  0.4472136  0.
  0.4472136  0.        ]
 [0.30253071 0.42519636 0.30253071 0.30253071 0.30253071 0.42519636
  0.30253071 0.42519636]]
pairwise_similarity: [[1.         0.67647924]
 [0.67647924 1.        ]]
cosine_similarity: 0.6764792400877004
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: EJ Manuel will be the 1st QB taken in the 2013 NFL Draft
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['ej', 'manuel', 'will', 'be', 'the', 'qb', 'taken', 'in', 'the', 'nfl', 'draft']
cosine_similarity: 0.9903215765953064
train_input: [0.6764792400877004, 0.9903216], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: Ej Manuel 1st QB picked he on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.4090901  0.4090901  0.         0.4090901 ]
 [0.4090901  0.         0.4090901  0.4090901  0.57496187 0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: Ej Manuel 1st QB picked he on
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['ej', 'manuel', 'qb', 'picked', 'he', 'on']
cosine_similarity: 0.9582403302192688
train_input: [0.6694188517266485, 0.95824033], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: Lol and 1st QB off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.49922133 0.         0.49922133
  0.35520009]
 [0.40993715 0.57615236 0.         0.         0.57615236 0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: Lol and 1st QB off the board
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['lol', 'and', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9523199200630188
train_input: [0.29121941856368966, 0.9523199], train_label: 0
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: WOW EJ MANUEL FSU 1ST QB TAKEN
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.4090901  0.         0.4090901  0.4090901
  0.         0.        ]
 [0.3174044  0.         0.3174044  0.44610081 0.3174044  0.3174044
  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: WOW EJ MANUEL FSU 1ST QB TAKEN
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['wow', 'ej', 'manuel', 'fsu', 'qb', 'taken']
cosine_similarity: 0.9205012917518616
train_input: [0.5193879933129156, 0.9205013], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: t Ej Manuel 1st Qb off the board VA good Sh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.57496187 0.4090901  0.         0.4090901
  0.4090901  0.         0.        ]
 [0.28986934 0.40740124 0.         0.28986934 0.40740124 0.28986934
  0.28986934 0.40740124 0.40740124]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st QB in the draft, sentence2: t Ej Manuel 1st Qb off the board VA good Sh
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'in', 'the', 'draft'], sentence2: ['t', 'ej', 'manuel', 'qb', 'off', 'the', 'board', 'va', 'good', 'sh']
cosine_similarity: 0.9423741698265076
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: 1st QB of the board EJ Manuel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.35464863 0.49844628 0.49844628
  0.35464863]
 [0.4090901  0.57496187 0.4090901  0.4090901  0.         0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: 1st QB of the board EJ Manuel
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['qb', 'of', 'the', 'board', 'ej', 'manuel']
cosine_similarity: 0.9753871560096741
train_input: [0.5803329846765685, 0.97538716], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: 1st QB taken in the draft go noles
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.44665616 0.         0.44665616
  0.44665616 0.31779954 0.        ]
 [0.35520009 0.49922133 0.         0.         0.49922133 0.
  0.         0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: 1st QB taken in the draft go noles
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['qb', 'taken', 'in', 'the', 'draft', 'go', 'noles']
cosine_similarity: 0.9292157888412476
train_input: [0.22576484600261604, 0.9292158], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: And Geno is NOT the 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.44665616 0.44665616
  0.31779954 0.        ]
 [0.40993715 0.         0.57615236 0.         0.         0.
  0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: And Geno is NOT the 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['and', 'geno', 'is', 'not', 'the', 'qb', 'taken']
cosine_similarity: 0.9546006321907043
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: Did anyone call EJ being the 1st QB off the boards
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.         0.33471228 0.47042643 0.47042643
  0.47042643 0.33471228]
 [0.37930349 0.53309782 0.53309782 0.37930349 0.         0.
  0.         0.37930349]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: Did anyone call EJ being the 1st QB off the boards
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['did', 'anyone', 'call', 'ej', 'being', 'the', 'qb', 'off', 'the', 'boards']
cosine_similarity: 0.9374967217445374
train_input: [0.3808726084759436, 0.9374967], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: EJ Manuel is the 1st QB to get picked
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.37863221 0.53215436 0.37863221 0.37863221]
 [0.4472136  0.4472136  0.4472136  0.         0.4472136  0.4472136 ]]
pairwise_similarity: [[1.         0.84664735]
 [0.84664735 1.        ]]
cosine_similarity: 0.8466473536503036
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: EJ Manuel is the 1st QB to get picked
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['ej', 'manuel', 'is', 'the', 'qb', 'to', 'get', 'picked']
cosine_similarity: 0.978312075138092
train_input: [0.8466473536503036, 0.9783121], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: EJ really the 1st qb taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.47042643 0.47042643 0.33471228
  0.         0.        ]
 [0.37930349 0.37930349 0.         0.         0.         0.37930349
  0.53309782 0.53309782]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: EJ really the 1st qb taken
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['ej', 'really', 'the', 'qb', 'taken']
cosine_similarity: 0.9646804928779602
train_input: [0.3808726084759436, 0.9646805], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: Florida State s EJ Manuel was the 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.35464863 0.49844628 0.49844628
  0.35464863 0.         0.        ]
 [0.3174044  0.3174044  0.44610081 0.3174044  0.         0.
  0.3174044  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: Florida State s EJ Manuel was the 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['florida', 'state', 's', 'ej', 'manuel', 'was', 'the', 'qb', 'taken']
cosine_similarity: 0.963408350944519
train_input: [0.4502681446556265, 0.96340835], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: Lol and 1st QB off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.44665616 0.44665616
  0.44665616 0.31779954]
 [0.40993715 0.57615236 0.         0.57615236 0.         0.
  0.         0.40993715]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: Lol and 1st QB off the board
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['lol', 'and', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9253055453300476
train_input: [0.2605556710562624, 0.92530555], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: SO to my boy CBaire1 for EJ Manuel being the 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.         0.35464863 0.35464863 0.49844628
  0.49844628 0.35464863 0.        ]
 [0.3174044  0.44610081 0.44610081 0.3174044  0.3174044  0.
  0.         0.3174044  0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: SO to my boy CBaire1 for EJ Manuel being the 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['so', 'to', 'my', 'boy', 'for', 'ej', 'manuel', 'being', 'the', 'qb', 'taken']
cosine_similarity: 0.9358587265014648
train_input: [0.4502681446556265, 0.9358587], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: WOW EJ MANUEL THE 1ST QB TAKEN LMFAO
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.35464863 0.49844628 0.49844628
  0.35464863 0.         0.        ]
 [0.3174044  0.3174044  0.44610081 0.3174044  0.         0.
  0.3174044  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: EJ Manuel was the 1st QB picked overall, sentence2: WOW EJ MANUEL THE 1ST QB TAKEN LMFAO
After tokenization, sentence1: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall'], sentence2: ['wow', 'ej', 'manuel', 'the', 'qb', 'taken', 'lmfao']
cosine_similarity: 0.9604746103286743
train_input: [0.4502681446556265, 0.9604746], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: 1st QB taken on the 17th pick in NFLDraft different
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.         0.49844628 0.49844628 0.35464863
  0.         0.35464863 0.35464863]
 [0.44610081 0.3174044  0.44610081 0.         0.         0.3174044
  0.44610081 0.3174044  0.3174044 ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: 1st QB taken on the 17th pick in NFLDraft different
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['qb', 'taken', 'on', 'the', 'pick', 'in', 'nfldraft', 'different']
cosine_similarity: 0.941447377204895
train_input: [0.4502681446556265, 0.9414474], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: 757 stand up EJ MANUEL 1st QB IN THE 2013 DRAFT
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.         0.         0.35464863 0.35464863
  0.49844628 0.35464863 0.         0.49844628]
 [0.28986934 0.40740124 0.40740124 0.40740124 0.28986934 0.28986934
  0.         0.28986934 0.40740124 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: 757 stand up EJ MANUEL 1st QB IN THE 2013 DRAFT
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['stand', 'up', 'ej', 'manuel', 'qb', 'in', 'the', 'draft']
cosine_similarity: 0.9838992357254028
train_input: [0.41120705506761857, 0.98389924], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ MANUEL did go 1st QB
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.35464863 0.49844628 0.35464863
  0.49844628]
 [0.4090901  0.57496187 0.4090901  0.4090901  0.         0.4090901
  0.        ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ MANUEL did go 1st QB
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['ej', 'manuel', 'did', 'go', 'qb']
cosine_similarity: 0.9558732509613037
train_input: [0.5803329846765685, 0.95587325], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ Manuel 1st QB taken haha the bills being the bills
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.         0.37863221 0.         0.37863221 0.53215436
  0.37863221 0.37863221]
 [0.25926702 0.72878149 0.25926702 0.36439074 0.25926702 0.
  0.25926702 0.25926702]]
pairwise_similarity: [[1.         0.49083421]
 [0.49083421 1.        ]]
cosine_similarity: 0.49083421206610733
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ Manuel 1st QB taken haha the bills being the bills
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['ej', 'manuel', 'qb', 'taken', 'haha', 'the', 'bills', 'being', 'the', 'bills']
cosine_similarity: 0.9654847383499146
train_input: [0.49083421206610733, 0.96548474], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ Manuel the 1st QB taken hmm I wonder who predicted that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.         0.37863221 0.53215436 0.
  0.37863221 0.37863221 0.        ]
 [0.30253071 0.30253071 0.42519636 0.30253071 0.         0.42519636
  0.30253071 0.30253071 0.42519636]]
pairwise_similarity: [[1.         0.57273936]
 [0.57273936 1.        ]]
cosine_similarity: 0.57273935841962
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ Manuel the 1st QB taken hmm I wonder who predicted that
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['ej', 'manuel', 'the', 'qb', 'taken', 'hmm', 'i', 'wonder', 'who', 'predicted', 'that']
cosine_similarity: 0.9406880736351013
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ Manuel was the 1st QB picked overall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.35464863 0.49844628 0.         0.
  0.35464863 0.49844628]
 [0.35464863 0.35464863 0.35464863 0.         0.49844628 0.49844628
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ Manuel was the 1st QB picked overall
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['ej', 'manuel', 'was', 'the', 'qb', 'picked', 'overall']
cosine_similarity: 0.9773188829421997
train_input: [0.5031026124151313, 0.9773189], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Ej Manuel 1st QB picked in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.35464863 0.49844628 0.
  0.35464863 0.49844628]
 [0.35464863 0.49844628 0.35464863 0.35464863 0.         0.49844628
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Ej Manuel 1st QB picked in the draft
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['ej', 'manuel', 'qb', 'picked', 'in', 'the', 'draft']
cosine_similarity: 0.9908949732780457
train_input: [0.5031026124151313, 0.990895], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Is this the 1st QB pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.         0.31779954
  0.44665616]
 [0.50154891 0.         0.         0.         0.70490949 0.50154891
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Is this the 1st QB pick
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['is', 'this', 'the', 'qb', 'pick']
cosine_similarity: 0.9465616941452026
train_input: [0.31878402175377923, 0.9465617], train_label: 0
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Who had EJ Manuel being the 1st QB taken in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.         0.37863221 0.37863221 0.53215436 0.37863221
  0.37863221]
 [0.37863221 0.53215436 0.37863221 0.37863221 0.         0.37863221
  0.37863221]]
pairwise_similarity: [[1.         0.71681174]
 [0.71681174 1.        ]]
cosine_similarity: 0.7168117414430624
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Who had EJ Manuel being the 1st QB taken in the draft
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['who', 'had', 'ej', 'manuel', 'being', 'the', 'qb', 'taken', 'in', 'the', 'draft']
cosine_similarity: 0.9731476902961731
train_input: [0.7168117414430624, 0.9731477], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Who had EJ as 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.49844628 0.35464863 0.35464863]
 [0.5        0.5        0.         0.         0.5        0.5       ]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Who had EJ as 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['who', 'had', 'ej', 'as', 'qb', 'taken']
cosine_similarity: 0.9402015209197998
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Bills just took EJ Manuel as the 1st QB at pick 16
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.         0.35464863 0.         0.35464863
  0.49844628 0.         0.35464863 0.49844628 0.        ]
 [0.37729199 0.26844636 0.37729199 0.26844636 0.37729199 0.26844636
  0.         0.37729199 0.26844636 0.         0.37729199]]
pairwise_similarity: [[1.         0.38081653]
 [0.38081653 1.        ]]
cosine_similarity: 0.3808165329771113
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Bills just took EJ Manuel as the 1st QB at pick 16
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['bills', 'just', 'took', 'ej', 'manuel', 'as', 'the', 'qb', 'at', 'pick']
cosine_similarity: 0.9628381133079529
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ MANUEL 1st QB taken to Buffalo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.         0.37863221 0.37863221 0.53215436 0.37863221
  0.37863221]
 [0.37863221 0.53215436 0.37863221 0.37863221 0.         0.37863221
  0.37863221]]
pairwise_similarity: [[1.         0.71681174]
 [0.71681174 1.        ]]
cosine_similarity: 0.7168117414430624
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ MANUEL 1st QB taken to Buffalo
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['ej', 'manuel', 'qb', 'taken', 'to', 'buffalo']
cosine_similarity: 0.9660314321517944
train_input: [0.7168117414430624, 0.96603143], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ Manuel was the 1st QB taken I wasnt looking for that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.         0.37863221 0.53215436 0.37863221
  0.37863221 0.        ]
 [0.33425073 0.33425073 0.46977774 0.33425073 0.         0.33425073
  0.33425073 0.46977774]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ Manuel was the 1st QB taken I wasnt looking for that
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['ej', 'manuel', 'was', 'the', 'qb', 'taken', 'i', 'wasnt', 'looking', 'for', 'that']
cosine_similarity: 0.9506568312644958
train_input: [0.6327904583679949, 0.95065683], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ is the 1st QB taken in the draft justifying my previous sentiments of EJ Manuel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.         0.37863221 0.         0.37863221 0.53215436
  0.         0.37863221 0.         0.37863221]
 [0.25077445 0.35245474 0.50154891 0.35245474 0.25077445 0.
  0.35245474 0.25077445 0.35245474 0.25077445]]
pairwise_similarity: [[1.         0.56970771]
 [0.56970771 1.        ]]
cosine_similarity: 0.5697077090551435
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ is the 1st QB taken in the draft justifying my previous sentiments of EJ Manuel
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['ej', 'is', 'the', 'qb', 'taken', 'in', 'the', 'draft', 'justifying', 'my', 'previous', 'sentiments', 'of', 'ej', 'manuel']
cosine_similarity: 0.9601649641990662
train_input: [0.5697077090551435, 0.96016496], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ is the 1st qb in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.33471228 0.47042643 0.47042643 0.33471228
  0.47042643]
 [0.44832087 0.63009934 0.44832087 0.         0.         0.44832087
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: EJ is the 1st qb in the draft
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['ej', 'is', 'the', 'qb', 'in', 'the', 'draft']
cosine_similarity: 0.9764164686203003
train_input: [0.4501755023269897, 0.97641647], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Florida State s EJ Manuel was the 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.         0.37863221 0.53215436 0.37863221
  0.         0.37863221]
 [0.33425073 0.33425073 0.46977774 0.33425073 0.         0.33425073
  0.46977774 0.33425073]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Florida State s EJ Manuel was the 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['florida', 'state', 's', 'ej', 'manuel', 'was', 'the', 'qb', 'taken']
cosine_similarity: 0.9862903356552124
train_input: [0.6327904583679949, 0.98629034], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: I knew ej was gonna be the 1st qb drafted
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.33471228 0.         0.         0.47042643
  0.47042643 0.33471228 0.47042643]
 [0.33471228 0.47042643 0.33471228 0.47042643 0.47042643 0.
  0.         0.33471228 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: I knew ej was gonna be the 1st qb drafted
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['i', 'knew', 'ej', 'was', 'gonna', 'be', 'the', 'qb', 'drafted']
cosine_similarity: 0.9355122447013855
train_input: [0.3360969272762574, 0.93551224], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Latest the 1st QB was ever taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.47042643 0.47042643 0.33471228
  0.33471228]
 [0.44832087 0.         0.63009934 0.         0.         0.44832087
  0.44832087]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Latest the 1st QB was ever taken
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['latest', 'the', 'qb', 'was', 'ever', 'taken']
cosine_similarity: 0.9358633756637573
train_input: [0.4501755023269897, 0.9358634], train_label: 0
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Someone please explain to me how EJ Manuel is the 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.         0.37863221 0.53215436 0.37863221
  0.37863221]
 [0.37863221 0.37863221 0.53215436 0.37863221 0.         0.37863221
  0.37863221]]
pairwise_similarity: [[1.         0.71681174]
 [0.71681174 1.        ]]
cosine_similarity: 0.7168117414430624
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Someone please explain to me how EJ Manuel is the 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['someone', 'please', 'explain', 'to', 'me', 'how', 'ej', 'manuel', 'is', 'the', 'qb', 'taken']
cosine_similarity: 0.9185811281204224
TF_IDF_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Wait EJ Manuel was the 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.37863221 0.53215436 0.37863221 0.37863221
  0.        ]
 [0.37863221 0.37863221 0.37863221 0.         0.37863221 0.37863221
  0.53215436]]
pairwise_similarity: [[1.         0.71681174]
 [0.71681174 1.        ]]
cosine_similarity: 0.7168117414430624
word_to_vector_cosine_similarity: sentence1: EJ Manuel 1st QB taken in the NFLDraft, sentence2: Wait EJ Manuel was the 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'qb', 'taken', 'in', 'the', 'nfldraft'], sentence2: ['wait', 'ej', 'manuel', 'was', 'the', 'qb', 'taken']
cosine_similarity: 0.9747860431671143
train_input: [0.7168117414430624, 0.97478604], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: 1st QB in the draft go noles
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.44665616 0.44665616 0.
  0.31779954 0.44665616]
 [0.40993715 0.57615236 0.         0.         0.         0.57615236
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: 1st QB in the draft go noles
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['qb', 'in', 'the', 'draft', 'go', 'noles']
cosine_similarity: 0.9189452528953552
train_input: [0.2605556710562624, 0.91894525], train_label: 0
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: 1st QB off of the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.44665616 0.44665616 0.31779954
  0.44665616]
 [0.50154891 0.70490949 0.         0.         0.         0.50154891
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: 1st QB off of the board
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['qb', 'off', 'of', 'the', 'board']
cosine_similarity: 0.9361846446990967
train_input: [0.31878402175377923, 0.93618464], train_label: 0
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: EJ Manuel has been drafted as the 1ST QB TAKEN IN THE 2013 NFLDraft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.         0.         0.37863221 0.53215436 0.37863221
  0.         0.37863221 0.37863221]
 [0.30253071 0.42519636 0.42519636 0.30253071 0.         0.30253071
  0.42519636 0.30253071 0.30253071]]
pairwise_similarity: [[1.         0.57273936]
 [0.57273936 1.        ]]
cosine_similarity: 0.57273935841962
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: EJ Manuel has been drafted as the 1ST QB TAKEN IN THE 2013 NFLDraft
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['ej', 'manuel', 'has', 'been', 'drafted', 'as', 'the', 'qb', 'taken', 'in', 'the', 'nfldraft']
cosine_similarity: 0.9698389768600464
train_input: [0.57273935841962, 0.969839], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: EJ Manuel is the 1st qb taken huh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]
 [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 1.0000000000000002
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: EJ Manuel is the 1st qb taken huh
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh']
cosine_similarity: 1.0
train_input: [1.0000000000000002, 1.0], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: I was wrong EJ was the 1st QB off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.33471228 0.47042643 0.47042643 0.33471228
  0.47042643 0.        ]
 [0.37930349 0.53309782 0.37930349 0.         0.         0.37930349
  0.         0.53309782]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: I was wrong EJ was the 1st QB off the board
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['i', 'was', 'wrong', 'ej', 'was', 'the', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9673978090286255
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: Not surprised EJ Manuel was the 1st QB picked in draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.49844628 0.35464863 0.
  0.35464863 0.         0.49844628]
 [0.3174044  0.44610081 0.3174044  0.         0.3174044  0.44610081
  0.3174044  0.44610081 0.        ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: Not surprised EJ Manuel was the 1st QB picked in draft
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['not', 'surprised', 'ej', 'manuel', 'was', 'the', 'qb', 'picked', 'in', 'draft']
cosine_similarity: 0.9799820184707642
train_input: [0.4502681446556265, 0.979982], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: So EJ Manuel is the 1st QB off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.49844628 0.35464863 0.35464863
  0.49844628]
 [0.4090901  0.57496187 0.4090901  0.         0.4090901  0.4090901
  0.        ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: So EJ Manuel is the 1st QB off the board
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['so', 'ej', 'manuel', 'is', 'the', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9798713326454163
train_input: [0.5803329846765685, 0.97987133], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: Who had EJ as 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.49844628 0.35464863 0.35464863]
 [0.5        0.5        0.         0.         0.5        0.5       ]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: Who had EJ as 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['who', 'had', 'ej', 'as', 'qb', 'taken']
cosine_similarity: 0.9660431742668152
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: Wow EJ Manuel is 1st QB selected
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.35464863 0.
  0.49844628 0.        ]
 [0.35464863 0.35464863 0.         0.35464863 0.35464863 0.49844628
  0.         0.49844628]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: Wow EJ Manuel is 1st QB selected
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['wow', 'ej', 'manuel', 'is', 'qb', 'selected']
cosine_similarity: 0.9632297158241272
train_input: [0.5031026124151313, 0.9632297], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: to the buffalobills 1st QB taken 16 overall pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.         0.47042643 0.47042643 0.47042643
  0.         0.         0.33471228 0.33471228]
 [0.42567716 0.30287281 0.42567716 0.         0.         0.
  0.42567716 0.42567716 0.30287281 0.30287281]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: EJ Manuel is the 1st qb taken huh, sentence2: to the buffalobills 1st QB taken 16 overall pick
After tokenization, sentence1: ['ej', 'manuel', 'is', 'the', 'qb', 'taken', 'huh'], sentence2: ['to', 'the', 'qb', 'taken', 'overall', 'pick']
cosine_similarity: 0.9334264993667603
train_input: [0.30412574187549346, 0.9334265], train_label: 0
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: 1st QB to go in the 2013 NFL Draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.44665616 0.44665616
  0.         0.31779954 0.44665616]
 [0.35520009 0.49922133 0.         0.49922133 0.         0.
  0.49922133 0.35520009 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: 1st QB to go in the 2013 NFL Draft
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['qb', 'to', 'go', 'in', 'the', 'nfl', 'draft']
cosine_similarity: 0.9568864107131958
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: EJ Manual is the 1st qb taken by the bills
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.49844628 0.35464863 0.         0.49844628
  0.35464863 0.35464863]
 [0.35464863 0.49844628 0.         0.35464863 0.49844628 0.
  0.35464863 0.35464863]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: EJ Manual is the 1st qb taken by the bills
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['ej', 'manual', 'is', 'the', 'qb', 'taken', 'by', 'the', 'bills']
cosine_similarity: 0.9655860662460327
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: EJ Manuel 1st QB off the Draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.49844628 0.         0.35464863 0.35464863 0.35464863
  0.49844628]
 [0.4090901  0.         0.57496187 0.4090901  0.4090901  0.4090901
  0.        ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: EJ Manuel 1st QB off the Draft
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['ej', 'manuel', 'qb', 'off', 'the', 'draft']
cosine_similarity: 0.9537051916122437
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: EJ Manuel had himself as the 1st QB
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.49844628 0.35464863 0.35464863 0.35464863 0.49844628]
 [0.5        0.         0.5        0.5        0.5        0.        ]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: EJ Manuel had himself as the 1st QB
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['ej', 'manuel', 'had', 'himself', 'as', 'the', 'qb']
cosine_similarity: 0.9790579676628113
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: EJ Manuel the 1st QB to get drafted
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.49844628 0.         0.35464863 0.35464863 0.35464863
  0.49844628]
 [0.4090901  0.         0.57496187 0.4090901  0.4090901  0.4090901
  0.        ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: EJ Manuel the 1st QB to get drafted
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['ej', 'manuel', 'the', 'qb', 'to', 'get', 'drafted']
cosine_similarity: 0.9782963991165161
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: Geno wasnt the 1st QB picked
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.         0.44665616 0.
  0.31779954 0.44665616 0.        ]
 [0.35520009 0.         0.         0.49922133 0.         0.49922133
  0.35520009 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: Geno wasnt the 1st QB picked
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['geno', 'wasnt', 'the', 'qb', 'picked']
cosine_similarity: 0.9312388896942139
train_input: [0.22576484600261604, 0.9312389], train_label: 0
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: He s the 1st QB drafted too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.44665616 0.31779954
  0.44665616]
 [0.50154891 0.         0.70490949 0.         0.         0.50154891
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: He s the 1st QB drafted too
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['he', 's', 'the', 'qb', 'drafted', 'too']
cosine_similarity: 0.9802320599555969
train_input: [0.31878402175377923, 0.98023206], train_label: 0
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: He s the 1st QB taken in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.47042643 0.47042643 0.33471228
  0.33471228]
 [0.44832087 0.         0.63009934 0.         0.         0.44832087
  0.44832087]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: He s the 1st QB taken in the draft
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['he', 's', 'the', 'qb', 'taken', 'in', 'the', 'draft']
cosine_similarity: 0.9654948115348816
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: I wasnt expecting EJ Manuel to be the 1st QB to get drafted
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.49844628 0.         0.35464863 0.         0.35464863
  0.35464863 0.49844628 0.        ]
 [0.3174044  0.         0.44610081 0.3174044  0.44610081 0.3174044
  0.3174044  0.         0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: I wasnt expecting EJ Manuel to be the 1st QB to get drafted
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['i', 'wasnt', 'expecting', 'ej', 'manuel', 'to', 'be', 'the', 'qb', 'to', 'get', 'drafted']
cosine_similarity: 0.9867632985115051
train_input: [0.4502681446556265, 0.9867633], train_label: 1
TF_IDF_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: Is this the 1st QB pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.         0.31779954
  0.44665616]
 [0.50154891 0.         0.         0.         0.70490949 0.50154891
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Didnt See Ej Manuel As The 1st QB Taken, sentence2: Is this the 1st QB pick
After tokenization, sentence1: ['didnt', 'see', 'ej', 'manuel', 'as', 'the', 'qb', 'taken'], sentence2: ['is', 'this', 'the', 'qb', 'pick']
cosine_similarity: 0.9616664052009583
train_input: [0.31878402175377923, 0.9616664], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: 1st QB off the board 1st Nole off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.53309782 0.         0.37930349]
 [0.60369998 0.60369998 0.         0.         0.42423963 0.30184999]]
pairwise_similarity: [[1.         0.57246377]
 [0.57246377 1.        ]]
cosine_similarity: 0.5724637744551228
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: 1st QB off the board 1st Nole off the board
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['qb', 'off', 'the', 'board', 'nole', 'off', 'the', 'board']
cosine_similarity: 0.9781679511070251
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: 1st qb taken in draft ej manuel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.53309782 0.         0.37930349 0.
  0.37930349 0.        ]
 [0.33471228 0.         0.         0.47042643 0.33471228 0.47042643
  0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: 1st qb taken in draft ej manuel
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['qb', 'taken', 'in', 'draft', 'ej', 'manuel']
cosine_similarity: 0.9531083106994629
train_input: [0.38087260847594373, 0.9531083], train_label: 1
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: EJ MANUEL 1st QB taken to Buffalo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.         0.53309782 0.37930349 0.
  0.37930349 0.        ]
 [0.33471228 0.         0.47042643 0.         0.33471228 0.47042643
  0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: EJ MANUEL 1st QB taken to Buffalo
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['ej', 'manuel', 'qb', 'taken', 'to', 'buffalo']
cosine_similarity: 0.9650413990020752
train_input: [0.38087260847594373, 0.9650414], train_label: 1
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: EJ Manuel 1st QB selection in 2013 NFL Draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.53309782 0.         0.37930349
  0.         0.         0.37930349 0.        ]
 [0.27867523 0.39166832 0.         0.         0.39166832 0.27867523
  0.39166832 0.39166832 0.27867523 0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: EJ Manuel 1st QB selection in 2013 NFL Draft
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['ej', 'manuel', 'qb', 'selection', 'in', 'nfl', 'draft']
cosine_similarity: 0.9314568638801575
train_input: [0.31710746658027095, 0.93145686], train_label: 1
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: EJ Manuel selected as 1st QB in the Draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.53309782 0.         0.37930349 0.
  0.37930349 0.        ]
 [0.33471228 0.         0.         0.47042643 0.33471228 0.47042643
  0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: EJ Manuel selected as 1st QB in the Draft
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['ej', 'manuel', 'selected', 'as', 'qb', 'in', 'the', 'draft']
cosine_similarity: 0.9770198464393616
train_input: [0.38087260847594373, 0.97701985], train_label: 1
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: Manuel is the 1st QB to get drafted
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.49922133 0.
  0.35520009]
 [0.40993715 0.         0.         0.57615236 0.         0.57615236
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: Manuel is the 1st QB to get drafted
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['manuel', 'is', 'the', 'qb', 'to', 'get', 'drafted']
cosine_similarity: 0.9881831407546997
train_input: [0.29121941856368966, 0.98818314], train_label: 1
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: When s the 1st QB coming off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.53309782 0.37930349]
 [0.44832087 0.44832087 0.63009934 0.         0.         0.44832087]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: When s the 1st QB coming off the board
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['when', 's', 'the', 'qb', 'coming', 'off', 'the', 'board']
cosine_similarity: 0.9696500301361084
train_input: [0.5101490193104813, 0.96965003], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: Yoooooo EJ Manuel was the 1st QB picked
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.53309782 0.37930349 0.         0.
  0.37930349 0.        ]
 [0.33471228 0.         0.         0.33471228 0.47042643 0.47042643
  0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: Yoooooo EJ Manuel was the 1st QB picked
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['ej', 'manuel', 'was', 'the', 'qb', 'picked']
cosine_similarity: 0.9677544832229614
train_input: [0.38087260847594373, 0.9677545], train_label: 1
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: if EJ is the 1st QB off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.4090901  0.4090901 ]
 [0.5        0.5        0.         0.5        0.5       ]]
pairwise_similarity: [[1.         0.81818021]
 [0.81818021 1.        ]]
cosine_similarity: 0.8181802073667197
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: if EJ is the 1st QB off the board
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['if', 'ej', 'is', 'the', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9824110865592957
train_input: [0.8181802073667197, 0.9824111], train_label: 1
TF_IDF_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: who will be the 1st QB to go
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.49922133 0.35520009]
 [0.70710678 0.         0.         0.         0.70710678]]
pairwise_similarity: [[1.         0.50232878]
 [0.50232878 1.        ]]
cosine_similarity: 0.5023287782256717
word_to_vector_cosine_similarity: sentence1: Damon EJ 1st Qb off the board, sentence2: who will be the 1st QB to go
After tokenization, sentence1: ['damon', 'ej', 'qb', 'off', 'the', 'board'], sentence2: ['who', 'will', 'be', 'the', 'qb', 'to', 'go']
cosine_similarity: 0.9630178213119507
train_input: [0.5023287782256717, 0.9630178], train_label: 0
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: EJ Manuel had himself as the 1st QB
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.4090901  0.4090901  0.4090901 ]
 [0.5        0.         0.5        0.5        0.5       ]]
pairwise_similarity: [[1.         0.81818021]
 [0.81818021 1.        ]]
cosine_similarity: 0.8181802073667197
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: EJ Manuel had himself as the 1st QB
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['ej', 'manuel', 'had', 'himself', 'as', 'the', 'qb']
cosine_similarity: 0.9654470682144165
train_input: [0.8181802073667197, 0.96544707], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Ej Manuel 1st QB picked he on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.4090901  0.4090901  0.         0.4090901 ]
 [0.4090901  0.         0.4090901  0.4090901  0.57496187 0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Ej Manuel 1st QB picked he on
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['ej', 'manuel', 'qb', 'picked', 'he', 'on']
cosine_similarity: 0.9501029253005981
train_input: [0.6694188517266485, 0.9501029], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: So EJ Manuel ends up the 1st QB taken anyway
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.4090901  0.         0.4090901  0.4090901
  0.        ]
 [0.35464863 0.         0.35464863 0.49844628 0.35464863 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: So EJ Manuel ends up the 1st QB taken anyway
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['so', 'ej', 'manuel', 'ends', 'up', 'the', 'qb', 'taken', 'anyway']
cosine_similarity: 0.981882631778717
train_input: [0.5803329846765685, 0.98188263], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: So EJ manuel was the 1st qb off the board
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.57496187 0.4090901  0.4090901  0.4090901 ]
 [0.4090901  0.57496187 0.         0.4090901  0.4090901  0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: So EJ manuel was the 1st qb off the board
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['so', 'ej', 'manuel', 'was', 'the', 'qb', 'off', 'the', 'board']
cosine_similarity: 0.9892754554748535
train_input: [0.6694188517266485, 0.98927546], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: To EJ Manuel Being The 1st QB Taken In Da Draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.         0.4472136  0.4472136  0.4472136  0.4472136
  0.        ]
 [0.33425073 0.46977774 0.33425073 0.33425073 0.33425073 0.33425073
  0.46977774]]
pairwise_similarity: [[1.         0.74740735]
 [0.74740735 1.        ]]
cosine_similarity: 0.7474073540060464
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: To EJ Manuel Being The 1st QB Taken In Da Draft
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['to', 'ej', 'manuel', 'being', 'the', 'qb', 'taken', 'in', 'da', 'draft']
cosine_similarity: 0.9849883913993835
train_input: [0.7474073540060464, 0.9849884], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Very surprised to see Manuel as the 1st QB taken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.53309782 0.37930349 0.37930349 0.
  0.        ]
 [0.37930349 0.         0.         0.37930349 0.37930349 0.53309782
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Very surprised to see Manuel as the 1st QB taken
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['very', 'surprised', 'to', 'see', 'manuel', 'as', 'the', 'qb', 'taken']
cosine_similarity: 0.975680947303772
train_input: [0.43161341897075145, 0.97568095], train_label: 1
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Was that the 1st QB pick yall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.49922133 0.         0.35520009
  0.        ]
 [0.40993715 0.         0.         0.         0.57615236 0.40993715
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Was that the 1st QB pick yall
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['was', 'that', 'the', 'qb', 'pick', 'yall']
cosine_similarity: 0.9593448042869568
train_input: [0.29121941856368966, 0.9593448], train_label: 0
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Who woulda thought EJ would be the 1st QB taken in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.4090901  0.57496187 0.4090901  0.
  0.         0.        ]
 [0.3174044  0.3174044  0.3174044  0.         0.3174044  0.44610081
  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: Who woulda thought EJ would be the 1st QB taken in the draft
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['who', 'woulda', 'thought', 'ej', 'would', 'be', 'the', 'qb', 'taken', 'in', 'the', 'draft']
cosine_similarity: 0.965571939945221
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: no way ej manuel was the 1st qb picked
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.57496187 0.4090901  0.4090901  0.         0.4090901
  0.        ]
 [0.35464863 0.         0.35464863 0.35464863 0.49844628 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: no way ej manuel was the 1st qb picked
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['no', 'way', 'ej', 'manuel', 'was', 'the', 'qb', 'picked']
cosine_similarity: 0.972938597202301
TF_IDF_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: t Ej Manuel 1st Qb off the board VA good Sh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.57496187 0.4090901  0.         0.4090901
  0.4090901  0.         0.        ]
 [0.28986934 0.40740124 0.         0.28986934 0.40740124 0.28986934
  0.28986934 0.40740124 0.40740124]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: EJ Manuel the 1st QB to go in this draft, sentence2: t Ej Manuel 1st Qb off the board VA good Sh
After tokenization, sentence1: ['ej', 'manuel', 'the', 'qb', 'to', 'go', 'in', 'this', 'draft'], sentence2: ['t', 'ej', 'manuel', 'qb', 'off', 'the', 'board', 'va', 'good', 'sh']
cosine_similarity: 0.9552065134048462
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 Cent is still the man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.44665616]
 [0.50154891 0.50154891 0.         0.         0.70490949 0.
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 Cent is still the man
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['cent', 'is', 'still', 'the', 'man']
cosine_similarity: 0.9366231560707092
train_input: [0.31878402175377923, 0.93662316], train_label: 0
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 Cent just butchered that commercial
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.         0.44665616 0.
  0.44665616 0.44665616 0.44665616]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.         0.49922133
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 Cent just butchered that commercial
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['cent', 'just', 'butchered', 'that', 'commercial']
cosine_similarity: 0.9133018851280212
train_input: [0.22576484600261604, 0.9133019], train_label: 0
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 Cent s rap to start the NFL Draft was incredibly bad
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.35464863 0.         0.49844628
  0.35464863 0.         0.         0.49844628]
 [0.28986934 0.40740124 0.28986934 0.28986934 0.40740124 0.
  0.28986934 0.40740124 0.40740124 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 Cent s rap to start the NFL Draft was incredibly bad
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['cent', 's', 'rap', 'to', 'start', 'the', 'nfl', 'draft', 'was', 'incredibly', 'bad']
cosine_similarity: 0.971518874168396
train_input: [0.41120705506761857, 0.9715189], train_label: 1
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 cent is so lame now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.44665616]
 [0.50154891 0.50154891 0.         0.         0.70490949 0.
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 cent is so lame now
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['cent', 'is', 'so', 'lame', 'now']
cosine_similarity: 0.9103081822395325
train_input: [0.31878402175377923, 0.9103082], train_label: 0
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 cent with the NFL draft spot
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.35464863 0.49844628 0.35464863 0.
  0.49844628]
 [0.4090901  0.4090901  0.4090901  0.         0.4090901  0.57496187
  0.        ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: 50 cent with the NFL draft spot
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['cent', 'with', 'the', 'nfl', 'draft', 'spot']
cosine_similarity: 0.9898633360862732
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: I dont really get this 50 Cent song thing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.         0.         0.         0.44665616]
 [0.31779954 0.31779954 0.44665616 0.         0.         0.
  0.44665616 0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: I dont really get this 50 Cent song thing
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['i', 'dont', 'really', 'get', 'this', 'cent', 'song', 'thing']
cosine_similarity: 0.915172815322876
train_input: [0.20199309249791833, 0.9151728], train_label: 0
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: So 50 cent is still rapping
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.
  0.44665616]
 [0.50154891 0.50154891 0.         0.         0.         0.70490949
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: So 50 cent is still rapping
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['so', 'cent', 'is', 'still', 'rapping']
cosine_similarity: 0.9088029265403748
train_input: [0.31878402175377923, 0.9088029], train_label: 0
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: That Russell Wilson and 50 cent commercial is pretty damn sweet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.         0.44665616 0.44665616
  0.44665616 0.         0.         0.         0.44665616 0.        ]
 [0.26868528 0.26868528 0.37762778 0.37762778 0.         0.
  0.         0.37762778 0.37762778 0.37762778 0.         0.37762778]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: That Russell Wilson and 50 cent commercial is pretty damn sweet
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['that', 'russell', 'wilson', 'and', 'cent', 'commercial', 'is', 'pretty', 'damn', 'sweet']
cosine_similarity: 0.9293997287750244
train_input: [0.1707761131901165, 0.9293997], train_label: 0
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: That remade song by The Heavy and 50 Cent is tight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.         0.         0.         0.44665616]
 [0.31779954 0.31779954 0.         0.44665616 0.         0.
  0.44665616 0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: That remade song by The Heavy and 50 Cent is tight
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['that', 'remade', 'song', 'by', 'the', 'heavy', 'and', 'cent', 'is', 'tight']
cosine_similarity: 0.9420197010040283
train_input: [0.20199309249791833, 0.9420197], train_label: 1
TF_IDF_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: The Heavy s How You Like Me Now featuring 50 Cent
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.         0.         0.44665616
  0.         0.44665616 0.44665616]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.49922133 0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: 50 Cent killing this track for the NFL Draft, sentence2: The Heavy s How You Like Me Now featuring 50 Cent
After tokenization, sentence1: ['cent', 'killing', 'this', 'track', 'for', 'the', 'nfl', 'draft'], sentence2: ['the', 'heavy', 's', 'how', 'you', 'like', 'me', 'now', 'featuring', 'cent']
cosine_similarity: 0.9280744194984436
train_input: [0.22576484600261604, 0.9280744], train_label: 0
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Ahhhhhh original Disney movies on abc family
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.31779954 0.
  0.         0.44665616 0.44665616 0.44665616]
 [0.31779954 0.44665616 0.         0.44665616 0.31779954 0.44665616
  0.44665616 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Ahhhhhh original Disney movies on abc family
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['original', 'disney', 'movies', 'on', 'abc', 'family']
cosine_similarity: 0.9392385482788086
train_input: [0.20199309249791833, 0.93923855], train_label: 0
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Cinderella on abc family is so well remastered
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.31779954 0.44665616 0.44665616
  0.         0.44665616]
 [0.40993715 0.57615236 0.         0.40993715 0.         0.
  0.57615236 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Cinderella on abc family is so well remastered
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['cinderella', 'on', 'abc', 'family', 'is', 'so', 'well', 'remastered']
cosine_similarity: 0.9695733189582825
train_input: [0.2605556710562624, 0.9695733], train_label: 0
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Disney classics marathon on abc family
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.31779954 0.
  0.44665616 0.44665616 0.44665616]
 [0.35520009 0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Disney classics marathon on abc family
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['disney', 'classics', 'marathon', 'on', 'abc', 'family']
cosine_similarity: 0.9309393167495728
train_input: [0.22576484600261604, 0.9309393], train_label: 0
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Disney movies on abc family is the perfect thing after that long prom night
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.31779954 0.         0.
  0.         0.44665616 0.         0.44665616 0.         0.44665616
  0.        ]
 [0.25136004 0.         0.35327777 0.25136004 0.35327777 0.35327777
  0.35327777 0.         0.35327777 0.         0.35327777 0.
  0.35327777]]
pairwise_similarity: [[1.         0.15976421]
 [0.15976421 1.        ]]
cosine_similarity: 0.1597642092414444
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Disney movies on abc family is the perfect thing after that long prom night
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['disney', 'movies', 'on', 'abc', 'family', 'is', 'the', 'perfect', 'thing', 'after', 'that', 'long', 'prom', 'night']
cosine_similarity: 0.9774723649024963
train_input: [0.1597642092414444, 0.97747236], train_label: 0
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Harry Potter marathon next weekend on ABC family
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.31779954 0.         0.         0.44665616
  0.44665616 0.         0.44665616 0.        ]
 [0.31779954 0.         0.31779954 0.44665616 0.44665616 0.
  0.         0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Harry Potter marathon next weekend on ABC family
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['harry', 'potter', 'marathon', 'next', 'weekend', 'on', 'abc', 'family']
cosine_similarity: 0.9643780589103699
train_input: [0.20199309249791833, 0.96437806], train_label: 0
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Peter Pan then the lion king then Cinderella are on abc family tonight starting now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.49844628 0.35464863 0.         0.
  0.35464863 0.35464863 0.49844628 0.         0.        ]
 [0.26844636 0.37729199 0.         0.26844636 0.37729199 0.37729199
  0.26844636 0.26844636 0.         0.37729199 0.37729199]]
pairwise_similarity: [[1.         0.38081653]
 [0.38081653 1.        ]]
cosine_similarity: 0.3808165329771113
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Peter Pan then the lion king then Cinderella are on abc family tonight starting now
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['peter', 'pan', 'then', 'the', 'lion', 'king', 'then', 'cinderella', 'are', 'on', 'abc', 'family', 'tonight', 'starting', 'now']
cosine_similarity: 0.9894653558731079
train_input: [0.3808165329771113, 0.98946536], train_label: 0
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Seriously ABC Family is on point today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.31779954 0.44665616 0.44665616 0.
  0.44665616 0.         0.        ]
 [0.35520009 0.         0.35520009 0.         0.         0.49922133
  0.         0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: Seriously ABC Family is on point today
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['seriously', 'abc', 'family', 'is', 'on', 'point', 'today']
cosine_similarity: 0.9835419058799744
train_input: [0.22576484600261604, 0.9835419], train_label: 0
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: The Peter Pan cartoon on ABC family is an absolute classic
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.         0.         0.49844628 0.35464863
  0.35464863 0.35464863 0.49844628]
 [0.3174044  0.44610081 0.44610081 0.44610081 0.         0.3174044
  0.3174044  0.3174044  0.        ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: The Peter Pan cartoon on ABC family is an absolute classic
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['the', 'peter', 'pan', 'cartoon', 'on', 'abc', 'family', 'is', 'an', 'absolute', 'classic']
cosine_similarity: 0.9725306630134583
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: When a disney movie marathon is on abc family
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.31779954 0.         0.
  0.44665616 0.44665616 0.44665616]
 [0.35520009 0.         0.49922133 0.35520009 0.49922133 0.49922133
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: When a disney movie marathon is on abc family
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['when', 'a', 'disney', 'movie', 'marathon', 'is', 'on', 'abc', 'family']
cosine_similarity: 0.9794580340385437
train_input: [0.22576484600261604, 0.97945803], train_label: 0
TF_IDF_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: abc family has it going on today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.31779954 0.         0.44665616 0.44665616
  0.44665616 0.        ]
 [0.40993715 0.         0.40993715 0.57615236 0.         0.
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Peter Pan is coming on abc family right now, sentence2: abc family has it going on today
After tokenization, sentence1: ['peter', 'pan', 'is', 'coming', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['abc', 'family', 'has', 'it', 'going', 'on', 'today']
cosine_similarity: 0.9868602752685547
train_input: [0.2605556710562624, 0.9868603], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: 2009 Aaron Brooks we need ya
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.         0.57615236
  0.        ]
 [0.49922133 0.35520009 0.35520009 0.         0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: 2009 Aaron Brooks we need ya
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['aaron', 'brooks', 'we', 'need', 'ya']
cosine_similarity: 0.9448379874229431
train_input: [0.29121941856368966, 0.944838], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Aaron Brooks still got his ball handling skills I see
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.57615236
  0.         0.57615236]
 [0.31779954 0.44665616 0.31779954 0.44665616 0.44665616 0.
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Aaron Brooks still got his ball handling skills I see
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['aaron', 'brooks', 'still', 'got', 'his', 'ball', 'handling', 'skills', 'i', 'see']
cosine_similarity: 0.9564118981361389
train_input: [0.2605556710562624, 0.9564119], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Aaron Brooks was RAW before he went to China he been quiet since he came back
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.         0.         0.57615236]
 [0.29017021 0.29017021 0.4078241  0.4078241  0.         0.4078241
  0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Aaron Brooks was RAW before he went to China he been quiet since he came back
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['aaron', 'brooks', 'was', 'raw', 'before', 'he', 'went', 'to', 'china', 'he', 'been', 'quiet', 'since', 'he', 'came', 'back']
cosine_similarity: 0.9446485638618469
train_input: [0.23790309463326234, 0.94464856], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Aaron Brooks with some solid help D
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Aaron Brooks with some solid help D
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['aaron', 'brooks', 'with', 'some', 'solid', 'help', 'd']
cosine_similarity: 0.9393242597579956
train_input: [0.3360969272762575, 0.93932426], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Dont sleep on Aaron brooks tho my boy cold asf
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.         0.
  0.57615236 0.         0.         0.57615236]
 [0.26868528 0.37762778 0.37762778 0.26868528 0.37762778 0.37762778
  0.         0.37762778 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Dont sleep on Aaron brooks tho my boy cold asf
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['dont', 'sleep', 'on', 'aaron', 'brooks', 'tho', 'my', 'boy', 'cold', 'asf']
cosine_similarity: 0.9291395545005798
train_input: [0.2202881505618297, 0.92913955], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: I thought Aaron brooks got traded
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: I thought Aaron brooks got traded
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['i', 'thought', 'aaron', 'brooks', 'got', 'traded']
cosine_similarity: 0.9545679688453674
train_input: [0.29121941856368966, 0.95456797], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Is that Aaron Brooks damn cuzz where you been
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Is that Aaron Brooks damn cuzz where you been
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['is', 'that', 'aaron', 'brooks', 'damn', 'cuzz', 'where', 'you', 'been']
cosine_similarity: 0.9610159993171692
train_input: [0.3360969272762575, 0.961016], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: One of my favorite players is Aaron Brooks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: One of my favorite players is Aaron Brooks
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['one', 'of', 'my', 'favorite', 'players', 'is', 'aaron', 'brooks']
cosine_similarity: 0.9556439518928528
train_input: [0.3360969272762575, 0.95564395], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Yo Aaron Brooks with the WET
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]
 [0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: Yo Aaron Brooks with the WET
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['yo', 'aaron', 'brooks', 'with', 'the', 'wet']
cosine_similarity: 0.9598179459571838
train_input: [0.3360969272762575, 0.95981795], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: why Aaron brooks dont get more Play time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Wtf Aaron Brooks is back on Houston, sentence2: why Aaron brooks dont get more Play time
After tokenization, sentence1: ['wtf', 'aaron', 'brooks', 'is', 'back', 'on', 'houston'], sentence2: ['why', 'aaron', 'brooks', 'dont', 'get', 'more', 'play', 'time']
cosine_similarity: 0.9539039731025696
train_input: [0.29121941856368966, 0.953904], train_label: 0
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Aaron Dobson is going to be Tom Brady s new Favorite Weapon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.57615236 0.
  0.         0.57615236 0.         0.        ]
 [0.26868528 0.37762778 0.26868528 0.37762778 0.         0.37762778
  0.37762778 0.         0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Aaron Dobson is going to be Tom Brady s new Favorite Weapon
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['aaron', 'dobson', 'is', 'going', 'to', 'be', 'tom', 'brady', 's', 'new', 'favorite', 'weapon']
cosine_similarity: 0.9542675018310547
train_input: [0.2202881505618297, 0.9542675], train_label: 0
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Aaron Dobson once made this catch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.57615236 0.57615236]
 [0.50154891 0.70490949 0.50154891 0.         0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Aaron Dobson once made this catch
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['aaron', 'dobson', 'once', 'made', 'this', 'catch']
cosine_similarity: 0.9575048685073853
train_input: [0.4112070550676187, 0.95750487], train_label: 0
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Aaron Dobson went to RandyMoss university
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Aaron Dobson went to RandyMoss university
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['aaron', 'dobson', 'went', 'to', 'university']
cosine_similarity: 0.9376075267791748
train_input: [0.29121941856368966, 0.9376075], train_label: 0
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Congrats to Aaron Dobson who is now a Patriot
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.57615236 0.         0.57615236]
 [0.40993715 0.57615236 0.40993715 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Congrats to Aaron Dobson who is now a Patriot
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['congrats', 'to', 'aaron', 'dobson', 'who', 'is', 'now', 'a', 'patriot']
cosine_similarity: 0.9521700739860535
train_input: [0.3360969272762575, 0.9521701], train_label: 1
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: I love the Patriots pick there with Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.44832087 0.        ]
 [0.37930349 0.37930349 0.         0.53309782 0.37930349 0.53309782]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: I love the Patriots pick there with Aaron Dobson
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['i', 'love', 'the', 'patriots', 'pick', 'there', 'with', 'aaron', 'dobson']
cosine_similarity: 0.958203136920929
train_input: [0.5101490193104813, 0.95820314], train_label: 1
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: My angle of the greatest college catch ever from Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.40993715 0.57615236
  0.         0.57615236]
 [0.31779954 0.44665616 0.44665616 0.44665616 0.31779954 0.
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: My angle of the greatest college catch ever from Aaron Dobson
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['my', 'angle', 'of', 'the', 'greatest', 'college', 'catch', 'ever', 'from', 'aaron', 'dobson']
cosine_similarity: 0.9417769908905029
train_input: [0.2605556710562624, 0.941777], train_label: 0
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: New England Patriots select Aaron Dobson Marshall wide receiver
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.63009934 0.         0.
  0.44832087 0.         0.         0.        ]
 [0.25948224 0.25948224 0.36469323 0.         0.36469323 0.36469323
  0.25948224 0.36469323 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: New England Patriots select Aaron Dobson Marshall wide receiver
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['new', 'england', 'patriots', 'select', 'aaron', 'dobson', 'marshall', 'wide', 'receiver']
cosine_similarity: 0.9639101028442383
train_input: [0.3489939079552687, 0.9639101], train_label: 1
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Patriots select Marshall WR Aaron Dobson with 59th overall pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.63009934 0.         0.
  0.44832087 0.         0.         0.        ]
 [0.36469323 0.25948224 0.25948224 0.         0.36469323 0.36469323
  0.25948224 0.36469323 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Patriots select Marshall WR Aaron Dobson with 59th overall pick
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['patriots', 'select', 'marshall', 'wr', 'aaron', 'dobson', 'with', 'overall', 'pick']
cosine_similarity: 0.9662148952484131
train_input: [0.3489939079552687, 0.9662149], train_label: 1
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Tom Brady is going to make Aaron Dobson a beast
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.57615236 0.
  0.         0.57615236 0.        ]
 [0.29017021 0.4078241  0.4078241  0.29017021 0.         0.4078241
  0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: Tom Brady is going to make Aaron Dobson a beast
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['tom', 'brady', 'is', 'going', 'to', 'make', 'aaron', 'dobson', 'a', 'beast']
cosine_similarity: 0.9529412388801575
train_input: [0.23790309463326234, 0.95294124], train_label: 0
TF_IDF_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: With 59th overall pick the Patriots selected Marshall WR Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.63009934 0.         0.
  0.44832087 0.         0.         0.        ]
 [0.36469323 0.25948224 0.25948224 0.         0.36469323 0.36469323
  0.25948224 0.36469323 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: figures the patriots get Aaron Dobson, sentence2: With 59th overall pick the Patriots selected Marshall WR Aaron Dobson
After tokenization, sentence1: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson'], sentence2: ['with', 'overall', 'pick', 'the', 'patriots', 'selected', 'marshall', 'wr', 'aaron', 'dobson']
cosine_similarity: 0.9812136888504028
train_input: [0.3489939079552687, 0.9812137], train_label: 1
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson from Marshall is going to be the next Randy Moss
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42567716 0.30287281 0.30287281 0.         0.30287281 0.
  0.42567716 0.42567716 0.         0.42567716]
 [0.         0.33471228 0.33471228 0.47042643 0.33471228 0.47042643
  0.         0.         0.47042643 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson from Marshall is going to be the next Randy Moss
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['aaron', 'dobson', 'from', 'marshall', 'is', 'going', 'to', 'be', 'the', 'next', 'randy', 'moss']
cosine_similarity: 0.9102466106414795
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson from Marshall to the Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44610081 0.3174044  0.3174044  0.3174044  0.3174044  0.44610081
  0.44610081]
 [0.         0.5        0.5        0.5        0.5        0.
  0.        ]]
pairwise_similarity: [[1.        0.6348088]
 [0.6348088 1.       ]]
cosine_similarity: 0.6348087971775132
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson from Marshall to the Patriots
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['aaron', 'dobson', 'from', 'marshall', 'to', 'the', 'patriots']
cosine_similarity: 0.9557539820671082
train_input: [0.6348087971775132, 0.955754], train_label: 1
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson good pick for the Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44610081 0.3174044  0.3174044  0.         0.44610081 0.3174044
  0.3174044  0.44610081]
 [0.         0.4090901  0.4090901  0.57496187 0.         0.4090901
  0.4090901  0.        ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson good pick for the Patriots
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['aaron', 'dobson', 'good', 'pick', 'for', 'the', 'patriots']
cosine_similarity: 0.9464722275733948
train_input: [0.5193879933129156, 0.9464722], train_label: 1
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson is a fucking cutie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.         0.29017021 0.         0.4078241
  0.4078241  0.4078241  0.4078241 ]
 [0.         0.40993715 0.57615236 0.40993715 0.57615236 0.
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson is a fucking cutie
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['aaron', 'dobson', 'is', 'a', 'fucking', 'cutie']
cosine_similarity: 0.8377112150192261
train_input: [0.23790309463326234, 0.8377112], train_label: 0
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson still has the sickest catch Ive ever seen
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.         0.29017021 0.         0.4078241
  0.4078241  0.4078241  0.         0.         0.4078241 ]
 [0.         0.31779954 0.44665616 0.31779954 0.44665616 0.
  0.         0.         0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Aaron Dobson still has the sickest catch Ive ever seen
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['aaron', 'dobson', 'still', 'has', 'the', 'sickest', 'catch', 'ive', 'ever', 'seen']
cosine_similarity: 0.8319096565246582
train_input: [0.18443191662261305, 0.83190966], train_label: 0
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: I like Aaron Dobson to the Pats
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.29017021 0.         0.4078241  0.4078241
  0.         0.4078241  0.4078241 ]
 [0.         0.40993715 0.40993715 0.57615236 0.         0.
  0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: I like Aaron Dobson to the Pats
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['i', 'like', 'aaron', 'dobson', 'to', 'the', 'pats']
cosine_similarity: 0.9004127383232117
train_input: [0.23790309463326234, 0.90041274], train_label: 0
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Patriots pass up WRs Keenan Allen for WR Aaron Dobson from Marshall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.46977774 0.33425073 0.         0.33425073 0.         0.33425073
  0.         0.33425073 0.46977774 0.33425073 0.        ]
 [0.         0.27840869 0.39129369 0.27840869 0.39129369 0.27840869
  0.39129369 0.27840869 0.         0.27840869 0.39129369]]
pairwise_similarity: [[1.         0.46529153]
 [0.46529153 1.        ]]
cosine_similarity: 0.4652915323370137
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Patriots pass up WRs Keenan Allen for WR Aaron Dobson from Marshall
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['patriots', 'pass', 'up', 'wrs', 'keenan', 'allen', 'for', 'wr', 'aaron', 'dobson', 'from', 'marshall']
cosine_similarity: 0.9858222603797913
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Patriots take Aaron Dobson WR from Marshall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.46977774 0.33425073 0.33425073 0.33425073 0.33425073 0.46977774
  0.33425073]
 [0.         0.4472136  0.4472136  0.4472136  0.4472136  0.
  0.4472136 ]]
pairwise_similarity: [[1.         0.74740735]
 [0.74740735 1.        ]]
cosine_similarity: 0.7474073540060464
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: Patriots take Aaron Dobson WR from Marshall
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['patriots', 'take', 'aaron', 'dobson', 'wr', 'from', 'marshall']
cosine_similarity: 0.9931563138961792
train_input: [0.7474073540060464, 0.9931563], train_label: 1
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: South Charleston High grad Aaron Dobson picked by the New England Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42567716 0.30287281 0.         0.30287281 0.         0.
  0.         0.42567716 0.         0.30287281 0.42567716 0.
  0.         0.42567716]
 [0.         0.24377685 0.34261985 0.24377685 0.34261985 0.34261985
  0.34261985 0.         0.34261985 0.24377685 0.         0.34261985
  0.34261985 0.        ]]
pairwise_similarity: [[1.         0.22150013]
 [0.22150013 1.        ]]
cosine_similarity: 0.22150013430590973
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: South Charleston High grad Aaron Dobson picked by the New England Patriots
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['south', 'charleston', 'high', 'grad', 'aaron', 'dobson', 'picked', 'by', 'the', 'new', 'england', 'patriots']
cosine_similarity: 0.9111881256103516
train_input: [0.22150013430590973, 0.9111881], train_label: 1
TF_IDF_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: They pluck Aaron Dobson from the Herd
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.29017021 0.         0.4078241  0.4078241
  0.4078241  0.         0.4078241 ]
 [0.         0.40993715 0.40993715 0.57615236 0.         0.
  0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: 59th pick Patriots take Aaron Dobson wr Marshall, sentence2: They pluck Aaron Dobson from the Herd
After tokenization, sentence1: ['pick', 'patriots', 'take', 'aaron', 'dobson', 'wr', 'marshall'], sentence2: ['they', 'pluck', 'aaron', 'dobson', 'from', 'the', 'herd']
cosine_similarity: 0.8939279913902283
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Aaron Dobson follows in the footsteps of Troy Brown and Randy Moss
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.
  0.57615236 0.         0.57615236 0.        ]
 [0.26868528 0.37762778 0.26868528 0.37762778 0.37762778 0.37762778
  0.         0.37762778 0.         0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Aaron Dobson follows in the footsteps of Troy Brown and Randy Moss
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['aaron', 'dobson', 'follows', 'in', 'the', 'footsteps', 'of', 'troy', 'brown', 'and', 'randy', 'moss']
cosine_similarity: 0.9661895632743835
train_input: [0.2202881505618297, 0.96618956], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Aaron Dobson is a fucking cutie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.57615236 0.57615236]
 [0.40993715 0.57615236 0.40993715 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Aaron Dobson is a fucking cutie
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['aaron', 'dobson', 'is', 'a', 'fucking', 'cutie']
cosine_similarity: 0.9655281901359558
train_input: [0.3360969272762575, 0.9655282], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Aaron Dobson is going to be a name to hear for years to come
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.57615236
  0.57615236 0.        ]
 [0.31779954 0.44665616 0.31779954 0.44665616 0.44665616 0.
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Aaron Dobson is going to be a name to hear for years to come
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['aaron', 'dobson', 'is', 'going', 'to', 'be', 'a', 'name', 'to', 'hear', 'for', 'years', 'to', 'come']
cosine_similarity: 0.9572752118110657
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Aaron Dobson went to the New England Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236 0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Aaron Dobson went to the New England Patriots
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['aaron', 'dobson', 'went', 'to', 'the', 'new', 'england', 'patriots']
cosine_similarity: 0.9529394507408142
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Good pick by the Patriots taking Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.         0.44832087 0.63009934
  0.        ]
 [0.33471228 0.33471228 0.47042643 0.47042643 0.33471228 0.
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Good pick by the Patriots taking Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['good', 'pick', 'by', 'the', 'patriots', 'taking', 'aaron', 'dobson']
cosine_similarity: 0.9616589546203613
train_input: [0.4501755023269898, 0.96165895], train_label: 1
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Patriots 59th pick is WR Aaron Dobson MRSH
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.         0.         0.44832087
  0.63009934 0.        ]
 [0.42567716 0.30287281 0.30287281 0.42567716 0.42567716 0.30287281
  0.         0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Patriots 59th pick is WR Aaron Dobson MRSH
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['patriots', 'pick', 'is', 'wr', 'aaron', 'dobson']
cosine_similarity: 0.9129496216773987
train_input: [0.4073526042885674, 0.9129496], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Pats got Jamie Collins and WR Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.
  0.57615236 0.57615236 0.        ]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.4078241  0.4078241
  0.         0.         0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Pats got Jamie Collins and WR Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['pats', 'got', 'jamie', 'collins', 'and', 'wr', 'aaron', 'dobson']
cosine_similarity: 0.9241825938224792
train_input: [0.23790309463326234, 0.9241826], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: They pluck Aaron Dobson from the Herd
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: They pluck Aaron Dobson from the Herd
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['they', 'pluck', 'aaron', 'dobson', 'from', 'the', 'herd']
cosine_similarity: 0.9215731024742126
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Welcome to the patriotsnation Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.57615236 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Welcome to the patriotsnation Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson']
cosine_similarity: 0.9164438247680664
TF_IDF_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Wow congrats to Aaron Dobson getting drafted to Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.
  0.57615236 0.57615236 0.        ]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.4078241  0.4078241
  0.         0.         0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Aaron dobson is a smart pick, sentence2: Wow congrats to Aaron Dobson getting drafted to Patriots
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'smart', 'pick'], sentence2: ['wow', 'congrats', 'to', 'aaron', 'dobson', 'getting', 'drafted', 'to', 'patriots']
cosine_similarity: 0.9416457414627075
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Aaron Dobson from Marshall is going to be the next Randy Moss
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.         0.
  0.49922133 0.49922133 0.        ]
 [0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Aaron Dobson from Marshall is going to be the next Randy Moss
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['aaron', 'dobson', 'from', 'marshall', 'is', 'going', 'to', 'be', 'the', 'next', 'randy', 'moss']
cosine_similarity: 0.9721415042877197
train_input: [0.22576484600261604, 0.9721415], train_label: 0
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Aaron Dobson is a playmaker to say the least
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.        ]
 [0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Aaron Dobson is a playmaker to say the least
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['aaron', 'dobson', 'is', 'a', 'playmaker', 'to', 'say', 'the', 'least']
cosine_similarity: 0.9586099982261658
train_input: [0.29121941856368966, 0.95861], train_label: 0
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Aaron Dobson s a fucking dog tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133 0.        ]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Aaron Dobson s a fucking dog tho
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['aaron', 'dobson', 's', 'a', 'fucking', 'dog', 'tho']
cosine_similarity: 0.917268693447113
train_input: [0.2523342014336961, 0.9172687], train_label: 0
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Aaron Dobson to the patriots good to see
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.37930349 0.53309782]
 [0.44832087 0.44832087 0.63009934 0.         0.44832087 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Aaron Dobson to the patriots good to see
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['aaron', 'dobson', 'to', 'the', 'patriots', 'good', 'to', 'see']
cosine_similarity: 0.9771511554718018
train_input: [0.5101490193104813, 0.97715116], train_label: 1
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Hope Aaron Dobson will be a great WR for the Pats
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.         0.49922133 0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.         0.
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Hope Aaron Dobson will be a great WR for the Pats
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['hope', 'aaron', 'dobson', 'will', 'be', 'a', 'great', 'wr', 'for', 'the', 'pats']
cosine_similarity: 0.9742200970649719
train_input: [0.22576484600261604, 0.9742201], train_label: 1
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: My angle of the greatest college catch ever from Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.35520009 0.
  0.49922133 0.49922133 0.49922133]
 [0.31779954 0.44665616 0.44665616 0.44665616 0.31779954 0.44665616
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: My angle of the greatest college catch ever from Aaron Dobson
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['my', 'angle', 'of', 'the', 'greatest', 'college', 'catch', 'ever', 'from', 'aaron', 'dobson']
cosine_similarity: 0.9419066309928894
train_input: [0.22576484600261604, 0.94190663], train_label: 0
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Now I need to watch Aaron Dobson highlights
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.         0.49922133
  0.49922133 0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: Now I need to watch Aaron Dobson highlights
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['now', 'i', 'need', 'to', 'watch', 'aaron', 'dobson', 'highlights']
cosine_similarity: 0.9569830298423767
train_input: [0.2523342014336961, 0.95698303], train_label: 0
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: The Patriots have selected WR Aaron Dobson from Marshall with the 59th pick in the
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.4090901  0.57496187 0.         0.4090901
  0.4090901  0.         0.        ]
 [0.40740124 0.28986934 0.28986934 0.         0.40740124 0.28986934
  0.28986934 0.40740124 0.40740124]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: The Patriots have selected WR Aaron Dobson from Marshall with the 59th pick in the
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['the', 'patriots', 'have', 'selected', 'wr', 'aaron', 'dobson', 'from', 'marshall', 'with', 'the', 'pick', 'in', 'the']
cosine_similarity: 0.9761009812355042
train_input: [0.4743307064971939, 0.976101], train_label: 1
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: With the 59th overall pick the Patriots select Aaron Dobson WR Marshall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.4090901  0.57496187 0.         0.
  0.4090901  0.4090901  0.         0.        ]
 [0.37729199 0.26844636 0.26844636 0.         0.37729199 0.37729199
  0.26844636 0.26844636 0.37729199 0.37729199]]
pairwise_similarity: [[1.         0.43927499]
 [0.43927499 1.        ]]
cosine_similarity: 0.43927499031635536
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: With the 59th overall pick the Patriots select Aaron Dobson WR Marshall
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['with', 'the', 'overall', 'pick', 'the', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'marshall']
cosine_similarity: 0.9765158891677856
train_input: [0.43927499031635536, 0.9765159], train_label: 1
TF_IDF_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: figures the patriots get Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.37930349 0.53309782]
 [0.44832087 0.44832087 0.63009934 0.         0.44832087 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Love the Patriots Aaron Dobson pick, sentence2: figures the patriots get Aaron Dobson
After tokenization, sentence1: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick'], sentence2: ['figures', 'the', 'patriots', 'get', 'aaron', 'dobson']
cosine_similarity: 0.9767069220542908
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Aaron Dobson at 59 to New England
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37762778 0.26868528 0.26868528 0.         0.37762778
  0.         0.37762778 0.37762778 0.37762778 0.37762778]
 [0.49922133 0.         0.35520009 0.35520009 0.49922133 0.
  0.49922133 0.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Aaron Dobson at 59 to New England
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['aaron', 'dobson', 'at', 'to', 'new', 'england']
cosine_similarity: 0.9595122933387756
train_input: [0.1908740661302035, 0.9595123], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Aaron Dobson is a good young WR to develop with Tom Brady as his QB
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.         0.         0.27867523 0.
  0.39166832 0.39166832 0.39166832 0.         0.39166832 0.
  0.27867523 0.        ]
 [0.         0.25948224 0.36469323 0.36469323 0.25948224 0.36469323
  0.         0.         0.         0.36469323 0.         0.36469323
  0.25948224 0.36469323]]
pairwise_similarity: [[1.         0.21693382]
 [0.21693382 1.        ]]
cosine_similarity: 0.2169338170113969
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Aaron Dobson is a good young WR to develop with Tom Brady as his QB
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['aaron', 'dobson', 'is', 'a', 'good', 'young', 'wr', 'to', 'develop', 'with', 'tom', 'brady', 'as', 'his', 'qb']
cosine_similarity: 0.9666456580162048
train_input: [0.2169338170113969, 0.96664566], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Aaron Dobson taken with the 59th overall pick by the Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30253071 0.30253071 0.30253071 0.42519636 0.         0.30253071
  0.30253071 0.42519636 0.         0.42519636]
 [0.33425073 0.33425073 0.33425073 0.         0.46977774 0.33425073
  0.33425073 0.         0.46977774 0.        ]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739691
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Aaron Dobson taken with the 59th overall pick by the Patriots
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['aaron', 'dobson', 'taken', 'with', 'the', 'overall', 'pick', 'by', 'the', 'patriots']
cosine_similarity: 0.9840133786201477
train_input: [0.5056055588739691, 0.9840134], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Aaron Dobson to the Patriots may be a steal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.27867523 0.39166832 0.27867523 0.39166832
  0.39166832 0.         0.39166832]
 [0.         0.44832087 0.44832087 0.         0.44832087 0.
  0.         0.63009934 0.        ]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Aaron Dobson to the Patriots may be a steal
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['aaron', 'dobson', 'to', 'the', 'patriots', 'may', 'be', 'a', 'steal']
cosine_similarity: 0.9520398378372192
train_input: [0.3748077700589726, 0.95203984], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Follow the newest Patriot WR Aaron Dobson on Twitter
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.27867523 0.         0.39166832 0.
  0.         0.39166832 0.39166832 0.39166832 0.         0.27867523]
 [0.         0.30287281 0.30287281 0.42567716 0.         0.42567716
  0.42567716 0.         0.         0.         0.42567716 0.30287281]]
pairwise_similarity: [[1.         0.25320945]
 [0.25320945 1.        ]]
cosine_similarity: 0.2532094495161745
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Follow the newest Patriot WR Aaron Dobson on Twitter
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['follow', 'the', 'newest', 'patriot', 'wr', 'aaron', 'dobson', 'on', 'twitter']
cosine_similarity: 0.9631546139717102
train_input: [0.2532094495161745, 0.9631546], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Hey Aaron Dobson is from South Charleston
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.26868528 0.         0.37762778
  0.37762778 0.37762778 0.37762778 0.         0.37762778]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.
  0.         0.         0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Hey Aaron Dobson is from South Charleston
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['hey', 'aaron', 'dobson', 'is', 'from', 'south', 'charleston']
cosine_similarity: 0.9456927180290222
train_input: [0.1908740661302035, 0.9456927], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Love the Patriots Aaron Dobson pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.         0.40740124 0.28986934
  0.28986934 0.40740124 0.40740124]
 [0.         0.4090901  0.4090901  0.57496187 0.         0.4090901
  0.4090901  0.         0.        ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Love the Patriots Aaron Dobson pick
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick']
cosine_similarity: 0.9857572317123413
train_input: [0.4743307064971939, 0.98575723], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Patriots WR Aaron Dobson can do this
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.40740124 0.28986934 0.40740124
  0.40740124 0.28986934]
 [0.         0.5        0.5        0.         0.5        0.
  0.         0.5       ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Patriots WR Aaron Dobson can do this
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['patriots', 'wr', 'aaron', 'dobson', 'can', 'do', 'this']
cosine_similarity: 0.9613659977912903
train_input: [0.5797386715376657, 0.961366], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Patriots grab WR Aaron Dobson Todd McShay loves the pick Mel Kiper finds fault
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.30253071 0.30253071 0.         0.         0.
  0.         0.         0.42519636 0.         0.         0.30253071
  0.30253071 0.42519636 0.         0.30253071]
 [0.         0.2192505  0.2192505  0.30814893 0.30814893 0.30814893
  0.30814893 0.30814893 0.         0.30814893 0.30814893 0.2192505
  0.2192505  0.         0.30814893 0.2192505 ]]
pairwise_similarity: [[1.         0.33165005]
 [0.33165005 1.        ]]
cosine_similarity: 0.3316500504151153
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: Patriots grab WR Aaron Dobson Todd McShay loves the pick Mel Kiper finds fault
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['patriots', 'grab', 'wr', 'aaron', 'dobson', 'todd', 'mcshay', 'loves', 'the', 'pick', 'mel', 'kiper', 'finds', 'fault']
cosine_similarity: 0.955057680606842
train_input: [0.3316500504151153, 0.9550577], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: The Patriots select Aaron Dobson WRMarshall with pick 59
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.42519636 0.30253071 0.30253071 0.42519636 0.30253071
  0.30253071 0.30253071 0.42519636 0.        ]
 [0.46977774 0.         0.33425073 0.33425073 0.         0.33425073
  0.33425073 0.33425073 0.         0.46977774]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739691
word_to_vector_cosine_similarity: sentence1: With the 59th pick MY Patriots select Aaron Dobson WR from Marshall, sentence2: The Patriots select Aaron Dobson WRMarshall with pick 59
After tokenization, sentence1: ['with', 'the', 'pick', 'my', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'from', 'marshall'], sentence2: ['the', 'patriots', 'select', 'aaron', 'dobson', 'with', 'pick']
cosine_similarity: 0.9958639144897461
train_input: [0.5056055588739691, 0.9958639], train_label: 1
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Aaron Dobson out of Marshall is going to be a stud in the NFL
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.44665616 0.44665616 0.
  0.         0.44665616 0.44665616 0.        ]
 [0.31779954 0.31779954 0.44665616 0.         0.         0.44665616
  0.44665616 0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Aaron Dobson out of Marshall is going to be a stud in the NFL
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['aaron', 'dobson', 'out', 'of', 'marshall', 'is', 'going', 'to', 'be', 'a', 'stud', 'in', 'the', 'nfl']
cosine_similarity: 0.9635052680969238
train_input: [0.20199309249791833, 0.96350527], train_label: 0
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Glad the Pats drafted Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.         0.44665616 0.44665616
  0.44665616 0.         0.44665616]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.
  0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Glad the Pats drafted Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['glad', 'the', 'pats', 'drafted', 'aaron', 'dobson']
cosine_similarity: 0.9250505566596985
train_input: [0.22576484600261604, 0.92505056], train_label: 1
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Good pick by the Patriots with Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.37863221 0.53215436 0.37863221 0.37863221]
 [0.4472136  0.4472136  0.4472136  0.         0.4472136  0.4472136 ]]
pairwise_similarity: [[1.         0.84664735]
 [0.84664735 1.        ]]
cosine_similarity: 0.8466473536503036
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Good pick by the Patriots with Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['good', 'pick', 'by', 'the', 'patriots', 'with', 'aaron', 'dobson']
cosine_similarity: 0.9780744314193726
train_input: [0.8466473536503036, 0.97807443], train_label: 1
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Im really freaking happy that Aaron Dobson got drafted to the patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.         0.47042643 0.
  0.         0.         0.47042643 0.33471228 0.47042643 0.        ]
 [0.25948224 0.25948224 0.36469323 0.36469323 0.         0.36469323
  0.36469323 0.36469323 0.         0.25948224 0.         0.36469323]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Im really freaking happy that Aaron Dobson got drafted to the patriots
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['im', 'really', 'freaking', 'happy', 'that', 'aaron', 'dobson', 'got', 'drafted', 'to', 'the', 'patriots']
cosine_similarity: 0.9730539321899414
train_input: [0.2605556710562624, 0.97305393], train_label: 1
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: My angle of the greatest college catch ever from Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.         0.31779954 0.44665616
  0.         0.44665616 0.44665616 0.44665616]
 [0.31779954 0.44665616 0.44665616 0.44665616 0.31779954 0.
  0.44665616 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: My angle of the greatest college catch ever from Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['my', 'angle', 'of', 'the', 'greatest', 'college', 'catch', 'ever', 'from', 'aaron', 'dobson']
cosine_similarity: 0.9457884430885315
train_input: [0.20199309249791833, 0.94578844], train_label: 0
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Patriots New WR 63 210 Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.33471228 0.33471228 0.47042643 0.47042643
  0.         0.33471228 0.47042643 0.        ]
 [0.42567716 0.42567716 0.30287281 0.30287281 0.         0.
  0.42567716 0.30287281 0.         0.42567716]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Patriots New WR 63 210 Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['patriots', 'new', 'wr', 'aaron', 'dobson']
cosine_similarity: 0.8724336624145508
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Patriots finally get a wide receiver Aaron Dobson of Marshall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.47042643 0.47042643 0.
  0.33471228 0.47042643 0.         0.        ]
 [0.30287281 0.30287281 0.42567716 0.         0.         0.42567716
  0.30287281 0.         0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Patriots finally get a wide receiver Aaron Dobson of Marshall
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['patriots', 'finally', 'get', 'a', 'wide', 'receiver', 'aaron', 'dobson', 'of', 'marshall']
cosine_similarity: 0.9611767530441284
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Pats take Aaron Dobson ThunderingHerd baby
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.44665616 0.        ]
 [0.35520009 0.49922133 0.35520009 0.         0.         0.
  0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: Pats take Aaron Dobson ThunderingHerd baby
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['pats', 'take', 'aaron', 'dobson', 'baby']
cosine_similarity: 0.9543454051017761
train_input: [0.22576484600261604, 0.9543454], train_label: 1
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: That catch highlight is sick for Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.44665616 0.         0.44665616
  0.44665616 0.44665616 0.        ]
 [0.35520009 0.49922133 0.35520009 0.         0.49922133 0.
  0.         0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: That catch highlight is sick for Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['that', 'catch', 'highlight', 'is', 'sick', 'for', 'aaron', 'dobson']
cosine_similarity: 0.9597645998001099
train_input: [0.22576484600261604, 0.9597646], train_label: 0
TF_IDF_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: With 59th pick in the 2013 NFL Draft the Patriots select WR Aaron Dobson from Marshall University
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35464863 0.35464863 0.         0.49844628
  0.49844628 0.         0.         0.35464863 0.35464863 0.
  0.         0.        ]
 [0.3158336  0.3158336  0.22471821 0.22471821 0.3158336  0.
  0.         0.3158336  0.3158336  0.22471821 0.22471821 0.3158336
  0.3158336  0.3158336 ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.3187840217537792
word_to_vector_cosine_similarity: sentence1: AARON DOBSON to the patriots lets go good pick, sentence2: With 59th pick in the 2013 NFL Draft the Patriots select WR Aaron Dobson from Marshall University
After tokenization, sentence1: ['aaron', 'dobson', 'to', 'the', 'patriots', 'lets', 'go', 'good', 'pick'], sentence2: ['with', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall', 'university']
cosine_similarity: 0.9542545676231384
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: AARON DOBSON WELCOME TO THE Patriots IM VERY GLAD PROUD TO HAVE U ON OUR TEAM
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.27867523 0.         0.         0.39166832
  0.27867523 0.39166832 0.         0.39166832 0.         0.
  0.39166832]
 [0.         0.27867523 0.27867523 0.39166832 0.39166832 0.
  0.27867523 0.         0.39166832 0.         0.39166832 0.39166832
  0.        ]]
pairwise_similarity: [[1.         0.23297965]
 [0.23297965 1.        ]]
cosine_similarity: 0.2329796548048752
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: AARON DOBSON WELCOME TO THE Patriots IM VERY GLAD PROUD TO HAVE U ON OUR TEAM
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['aaron', 'dobson', 'welcome', 'to', 'the', 'patriots', 'im', 'very', 'glad', 'proud', 'to', 'have', 'u', 'on', 'our', 'team']
cosine_similarity: 0.9295842051506042
train_input: [0.2329796548048752, 0.9295842], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Aaron Dobson Good pick by the patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.         0.40740124 0.28986934
  0.28986934 0.40740124 0.40740124]
 [0.         0.4090901  0.4090901  0.57496187 0.         0.4090901
  0.4090901  0.         0.        ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Aaron Dobson Good pick by the patriots
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['aaron', 'dobson', 'good', 'pick', 'by', 'the', 'patriots']
cosine_similarity: 0.9905434846878052
train_input: [0.4743307064971939, 0.9905435], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Aaron Dobson is a playmaker to say the least
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.26868528 0.37762778 0.37762778 0.37762778
  0.         0.         0.37762778 0.37762778]
 [0.         0.40993715 0.40993715 0.         0.         0.
  0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Aaron Dobson is a playmaker to say the least
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['aaron', 'dobson', 'is', 'a', 'playmaker', 'to', 'say', 'the', 'least']
cosine_similarity: 0.9470338821411133
train_input: [0.2202881505618297, 0.9470339], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Aaron Dobson is going to be a legit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.26868528 0.         0.         0.37762778
  0.37762778 0.37762778 0.37762778 0.37762778]
 [0.         0.40993715 0.40993715 0.57615236 0.57615236 0.
  0.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Aaron Dobson is going to be a legit
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['aaron', 'dobson', 'is', 'going', 'to', 'be', 'a', 'legit']
cosine_similarity: 0.9259732961654663
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Aaron Dobson pats 2nd round pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.39166832 0.27867523 0.27867523 0.39166832 0.39166832
  0.         0.27867523 0.         0.39166832 0.39166832]
 [0.47042643 0.         0.33471228 0.33471228 0.         0.
  0.47042643 0.33471228 0.47042643 0.         0.        ]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Aaron Dobson pats 2nd round pick
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['aaron', 'dobson', 'pats', 'round', 'pick']
cosine_similarity: 0.9445081949234009
train_input: [0.27982806524328774, 0.9445082], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Big fan of Aaron Dobson great pick for Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.         0.28986934 0.         0.
  0.40740124 0.28986934 0.28986934 0.40740124 0.40740124]
 [0.         0.3174044  0.44610081 0.3174044  0.44610081 0.44610081
  0.         0.3174044  0.3174044  0.         0.        ]]
pairwise_similarity: [[1.         0.36802321]
 [0.36802321 1.        ]]
cosine_similarity: 0.36802320875611494
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Big fan of Aaron Dobson great pick for Patriots
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['big', 'fan', 'of', 'aaron', 'dobson', 'great', 'pick', 'for', 'patriots']
cosine_similarity: 0.9759911894798279
train_input: [0.36802320875611494, 0.9759912], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Patriots pick WR Aaron Dobson at No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.30253071 0.30253071 0.42519636 0.30253071 0.30253071
  0.42519636 0.30253071]
 [0.         0.4472136  0.4472136  0.         0.4472136  0.4472136
  0.         0.4472136 ]]
pairwise_similarity: [[1.         0.67647924]
 [0.67647924 1.        ]]
cosine_similarity: 0.6764792400877002
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: Patriots pick WR Aaron Dobson at No
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['patriots', 'pick', 'wr', 'aaron', 'dobson', 'at', 'no']
cosine_similarity: 0.9596064686775208
train_input: [0.6764792400877002, 0.95960647], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: South Charleston graduate Aaron Dobson goes to New England
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.26868528 0.         0.
  0.         0.37762778 0.         0.37762778 0.37762778 0.37762778
  0.         0.37762778]
 [0.         0.26868528 0.37762778 0.26868528 0.37762778 0.37762778
  0.37762778 0.         0.37762778 0.         0.         0.
  0.37762778 0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.14438355527738672
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: South Charleston graduate Aaron Dobson goes to New England
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['south', 'charleston', 'graduate', 'aaron', 'dobson', 'goes', 'to', 'new', 'england']
cosine_similarity: 0.9535421133041382
train_input: [0.14438355527738672, 0.9535421], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: That catch highlight is sick for Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.26868528 0.         0.37762778
  0.37762778 0.37762778 0.37762778 0.         0.37762778]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.
  0.         0.         0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: That catch highlight is sick for Aaron Dobson
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['that', 'catch', 'highlight', 'is', 'sick', 'for', 'aaron', 'dobson']
cosine_similarity: 0.9409894347190857
train_input: [0.1908740661302035, 0.94098943], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: The New England Patriots have selected Marshall WR Aaron Dobson at No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.30253071 0.30253071 0.         0.30253071 0.
  0.30253071 0.42519636 0.42519636 0.         0.30253071]
 [0.         0.30253071 0.30253071 0.42519636 0.30253071 0.42519636
  0.30253071 0.         0.         0.42519636 0.30253071]]
pairwise_similarity: [[1.         0.45762416]
 [0.45762416 1.        ]]
cosine_similarity: 0.4576241622696326
word_to_vector_cosine_similarity: sentence1: With the 59th pick the Patriots select WR Aaron Dobson from Marshall, sentence2: The New England Patriots have selected Marshall WR Aaron Dobson at No
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'marshall'], sentence2: ['the', 'new', 'england', 'patriots', 'have', 'selected', 'marshall', 'wr', 'aaron', 'dobson', 'at', 'no']
cosine_similarity: 0.9870604276657104
train_input: [0.4576241622696326, 0.9870604], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson is going to be stud with the Pats
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson is going to be stud with the Pats
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['aaron', 'dobson', 'is', 'going', 'to', 'be', 'stud', 'with', 'the', 'pats']
cosine_similarity: 0.9545962810516357
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson one of the best coming out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.57615236 0.57615236]
 [0.40993715 0.57615236 0.57615236 0.40993715 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson one of the best coming out
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['aaron', 'dobson', 'one', 'of', 'the', 'best', 'coming', 'out']
cosine_similarity: 0.9237104058265686
train_input: [0.3360969272762575, 0.9237104], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson s one handed catch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.57615236 0.57615236]
 [0.40993715 0.57615236 0.40993715 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson s one handed catch
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['aaron', 'dobson', 's', 'one', 'handed', 'catch']
cosine_similarity: 0.9323128461837769
train_input: [0.3360969272762575, 0.93231285], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson to the Pats great hands
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson to the Pats great hands
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['aaron', 'dobson', 'to', 'the', 'pats', 'great', 'hands']
cosine_similarity: 0.9636163115501404
train_input: [0.29121941856368966, 0.9636163], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson went to the New England Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236 0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Aaron Dobson went to the New England Patriots
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['aaron', 'dobson', 'went', 'to', 'the', 'new', 'england', 'patriots']
cosine_similarity: 0.9717932939529419
train_input: [0.2605556710562624, 0.9717933], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Good pick by the Patriots with Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Good pick by the Patriots with Aaron Dobson
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['good', 'pick', 'by', 'the', 'patriots', 'with', 'aaron', 'dobson']
cosine_similarity: 0.9604037404060364
train_input: [0.29121941856368966, 0.96040374], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: I love Aaron Dobson pick by the Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: I love Aaron Dobson pick by the Patriots
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['i', 'love', 'aaron', 'dobson', 'pick', 'by', 'the', 'patriots']
cosine_similarity: 0.9631106853485107
train_input: [0.29121941856368966, 0.9631107], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Patriots finally get a wide receiver Aaron Dobson of Marshall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.         0.57615236 0.        ]
 [0.29017021 0.29017021 0.4078241  0.4078241  0.4078241  0.
  0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Patriots finally get a wide receiver Aaron Dobson of Marshall
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['patriots', 'finally', 'get', 'a', 'wide', 'receiver', 'aaron', 'dobson', 'of', 'marshall']
cosine_similarity: 0.9395753145217896
train_input: [0.23790309463326234, 0.9395753], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Patriots select Marshall WR Aaron Dobson with their second pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.         0.         0.57615236 0.        ]
 [0.26868528 0.26868528 0.37762778 0.37762778 0.         0.37762778
  0.37762778 0.37762778 0.         0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: Patriots select Marshall WR Aaron Dobson with their second pick
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['patriots', 'select', 'marshall', 'wr', 'aaron', 'dobson', 'with', 'their', 'second', 'pick']
cosine_similarity: 0.9408015608787537
train_input: [0.2202881505618297, 0.94080156], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: With the 59th pick of the 2013 NFLDraft the Patriots select Aaron Dobson WR Marshall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.40993715 0.         0.
  0.         0.57615236 0.         0.         0.57615236 0.        ]
 [0.33310232 0.33310232 0.23700504 0.23700504 0.33310232 0.33310232
  0.33310232 0.         0.33310232 0.33310232 0.         0.33310232]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858148
word_to_vector_cosine_similarity: sentence1: Welcome to the patriotsnation Aaron Dobson, sentence2: With the 59th pick of the 2013 NFLDraft the Patriots select Aaron Dobson WR Marshall
After tokenization, sentence1: ['welcome', 'to', 'the', 'patriotsnation', 'aaron', 'dobson'], sentence2: ['with', 'the', 'pick', 'of', 'the', 'nfldraft', 'the', 'patriots', 'select', 'aaron', 'dobson', 'wr', 'marshall']
cosine_similarity: 0.9572778940200806
train_input: [0.19431434016858148, 0.9572779], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson WR welcome to the Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.49844628 0.35464863
  0.        ]
 [0.4090901  0.4090901  0.         0.4090901  0.         0.4090901
  0.57496187]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson WR welcome to the Patriots
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['aaron', 'dobson', 'wr', 'welcome', 'to', 'the', 'patriots']
cosine_similarity: 0.9790595769882202
train_input: [0.5803329846765685, 0.9790596], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson fills a definite need
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.44665616 0.         0.
  0.44665616 0.44665616 0.44665616]
 [0.35520009 0.49922133 0.35520009 0.         0.49922133 0.49922133
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson fills a definite need
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['aaron', 'dobson', 'fills', 'a', 'definite', 'need']
cosine_similarity: 0.8983396887779236
train_input: [0.22576484600261604, 0.8983397], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson is a good pick up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.         0.44665616]
 [0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson is a good pick up
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['aaron', 'dobson', 'is', 'a', 'good', 'pick', 'up']
cosine_similarity: 0.9321582913398743
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson is a great pick for the Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.         0.33471228 0.47042643
  0.         0.47042643]
 [0.37930349 0.37930349 0.         0.53309782 0.37930349 0.
  0.53309782 0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson is a great pick for the Patriots
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots']
cosine_similarity: 0.960284948348999
train_input: [0.3808726084759436, 0.96028495], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson is going to be Tom Brady s new Favorite Weapon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.44665616 0.         0.
  0.         0.44665616 0.44665616 0.         0.         0.44665616]
 [0.26868528 0.37762778 0.26868528 0.         0.37762778 0.37762778
  0.37762778 0.         0.         0.37762778 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Aaron Dobson is going to be Tom Brady s new Favorite Weapon
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['aaron', 'dobson', 'is', 'going', 'to', 'be', 'tom', 'brady', 's', 'new', 'favorite', 'weapon']
cosine_similarity: 0.9458929896354675
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: New England Patriots Select WR Aaron Dobson From The University Of Marshall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.47042643 0.         0.
  0.33471228 0.47042643 0.         0.         0.47042643 0.        ]
 [0.25948224 0.25948224 0.36469323 0.         0.36469323 0.36469323
  0.25948224 0.         0.36469323 0.36469323 0.         0.36469323]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562623
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: New England Patriots Select WR Aaron Dobson From The University Of Marshall
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['new', 'england', 'patriots', 'select', 'wr', 'aaron', 'dobson', 'from', 'the', 'university', 'of', 'marshall']
cosine_similarity: 0.9580249786376953
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Patriots select Marshall WR Aaron Dobson in effort to replace weswelker
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.47042643 0.         0.33471228
  0.47042643 0.         0.         0.47042643 0.         0.        ]
 [0.25948224 0.25948224 0.36469323 0.         0.36469323 0.25948224
  0.         0.36469323 0.36469323 0.         0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562623
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Patriots select Marshall WR Aaron Dobson in effort to replace weswelker
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['patriots', 'select', 'marshall', 'wr', 'aaron', 'dobson', 'in', 'effort', 'to', 'replace']
cosine_similarity: 0.9619641900062561
train_input: [0.2605556710562623, 0.9619642], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: That catch by Aaron Dobson from Marshall was sick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.44665616 0.         0.44665616
  0.44665616 0.         0.44665616]
 [0.35520009 0.49922133 0.35520009 0.         0.49922133 0.
  0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: That catch by Aaron Dobson from Marshall was sick
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['that', 'catch', 'by', 'aaron', 'dobson', 'from', 'marshall', 'was', 'sick']
cosine_similarity: 0.9277276396751404
train_input: [0.22576484600261604, 0.92772764], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Well Aaron Dobson welcome to New England
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.47042643 0.         0.47042643
  0.47042643 0.33471228]
 [0.37930349 0.37930349 0.53309782 0.         0.53309782 0.
  0.         0.37930349]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: Well Aaron Dobson welcome to New England
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['well', 'aaron', 'dobson', 'welcome', 'to', 'new', 'england']
cosine_similarity: 0.9734963178634644
train_input: [0.3808726084759436, 0.9734963], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: with a very savvy pick in Marshall WR Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.         0.         0.44665616 0.        ]
 [0.31779954 0.31779954 0.         0.44665616 0.         0.
  0.44665616 0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Welcome to the family Aaron Dobson Patriots PatriotsNation, sentence2: with a very savvy pick in Marshall WR Aaron Dobson
After tokenization, sentence1: ['welcome', 'to', 'the', 'family', 'aaron', 'dobson', 'patriots', 'patriotsnation'], sentence2: ['with', 'a', 'very', 'savvy', 'pick', 'in', 'marshall', 'wr', 'aaron', 'dobson']
cosine_similarity: 0.9427123069763184
train_input: [0.20199309249791833, 0.9427123], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Aaron Dobson is a freak of nature
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.         0.49922133
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Aaron Dobson is a freak of nature
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['aaron', 'dobson', 'is', 'a', 'freak', 'of', 'nature']
cosine_similarity: 0.9609355330467224
train_input: [0.29121941856368966, 0.96093553], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Aaron Dobson is going to be Tom Brady s new Favorite Weapon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.         0.49922133
  0.         0.49922133 0.49922133 0.         0.        ]
 [0.26868528 0.37762778 0.26868528 0.37762778 0.37762778 0.
  0.37762778 0.         0.         0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Aaron Dobson is going to be Tom Brady s new Favorite Weapon
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['aaron', 'dobson', 'is', 'going', 'to', 'be', 'tom', 'brady', 's', 'new', 'favorite', 'weapon']
cosine_similarity: 0.9890549182891846
train_input: [0.1908740661302035, 0.9890549], train_label: 1
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Aaron Dobson went to the New England Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.         0.37930349
  0.53309782 0.        ]
 [0.33471228 0.33471228 0.47042643 0.         0.47042643 0.33471228
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Aaron Dobson went to the New England Patriots
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['aaron', 'dobson', 'went', 'to', 'the', 'new', 'england', 'patriots']
cosine_similarity: 0.9817925691604614
train_input: [0.38087260847594373, 0.98179257], train_label: 1
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: And there it goes Aaron Dobson to the pats
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.         0.         0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: And there it goes Aaron Dobson to the pats
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['and', 'there', 'it', 'goes', 'aaron', 'dobson', 'to', 'the', 'pats']
cosine_similarity: 0.9785677194595337
train_input: [0.29121941856368966, 0.9785677], train_label: 1
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Damn Aaron Dobson is a beast i wanted him to the ravens so bad
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.35520009 0.49922133
  0.49922133 0.49922133 0.         0.        ]
 [0.29017021 0.4078241  0.4078241  0.4078241  0.29017021 0.
  0.         0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Damn Aaron Dobson is a beast i wanted him to the ravens so bad
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['damn', 'aaron', 'dobson', 'is', 'a', 'beast', 'i', 'wanted', 'him', 'to', 'the', 'ravens', 'so', 'bad']
cosine_similarity: 0.972137451171875
train_input: [0.20613696606828605, 0.97213745], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Love the Patriots Aaron Dobson pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.         0.4090901  0.4090901 ]
 [0.4090901  0.4090901  0.         0.57496187 0.4090901  0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Love the Patriots Aaron Dobson pick
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['love', 'the', 'patriots', 'aaron', 'dobson', 'pick']
cosine_similarity: 0.9773966073989868
train_input: [0.6694188517266485, 0.9773966], train_label: 1
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: My angle of the greatest college catch ever from Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.35520009 0.49922133
  0.         0.49922133 0.49922133]
 [0.31779954 0.44665616 0.44665616 0.44665616 0.31779954 0.
  0.44665616 0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: My angle of the greatest college catch ever from Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['my', 'angle', 'of', 'the', 'greatest', 'college', 'catch', 'ever', 'from', 'aaron', 'dobson']
cosine_similarity: 0.971297562122345
train_input: [0.22576484600261604, 0.97129756], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Oh hey Aaron Dobson welcome to the Patriots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.         0.         0.37930349
  0.53309782 0.        ]
 [0.33471228 0.33471228 0.         0.47042643 0.47042643 0.33471228
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Oh hey Aaron Dobson welcome to the Patriots
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['oh', 'hey', 'aaron', 'dobson', 'welcome', 'to', 'the', 'patriots']
cosine_similarity: 0.9677923321723938
train_input: [0.38087260847594373, 0.96779233], train_label: 1
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Patriots got Marshall WR Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.         0.37930349
  0.53309782 0.        ]
 [0.33471228 0.33471228 0.47042643 0.         0.47042643 0.33471228
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: Patriots got Marshall WR Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['patriots', 'got', 'marshall', 'wr', 'aaron', 'dobson']
cosine_similarity: 0.8661360740661621
TF_IDF_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: That onehanded catch is literally the only highlight you need to show for Aaron Dobson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.49922133 0.         0.
  0.         0.         0.49922133 0.49922133]
 [0.29017021 0.4078241  0.29017021 0.         0.4078241  0.4078241
  0.4078241  0.4078241  0.         0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Aaron Dobson is a great pick for the Patriots, sentence2: That onehanded catch is literally the only highlight you need to show for Aaron Dobson
After tokenization, sentence1: ['aaron', 'dobson', 'is', 'a', 'great', 'pick', 'for', 'the', 'patriots'], sentence2: ['that', 'catch', 'is', 'literally', 'the', 'only', 'highlight', 'you', 'need', 'to', 'show', 'for', 'aaron', 'dobson']
cosine_similarity: 0.9703759551048279
train_input: [0.20613696606828605, 0.97037596], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Aaron Rodgers is signed for 5 more years
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.4078241  0.4078241  0.4078241  0.29017021
  0.         0.4078241  0.        ]
 [0.         0.40993715 0.         0.         0.         0.40993715
  0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Aaron Rodgers is signed for 5 more years
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'is', 'signed', 'for', 'more', 'years']
cosine_similarity: 0.9664499759674072
train_input: [0.23790309463326234, 0.96645], train_label: 1
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Aaron Rodgers is signed for a long time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.29017021 0.         0.4078241  0.        ]
 [0.         0.35520009 0.         0.49922133 0.         0.
  0.35520009 0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Aaron Rodgers is signed for a long time
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'is', 'signed', 'for', 'a', 'long', 'time']
cosine_similarity: 0.9709814190864563
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Aaron Rodgers isnt worth all that money
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.4078241  0.         0.         0.4078241
  0.4078241  0.29017021 0.4078241  0.        ]
 [0.         0.35520009 0.         0.49922133 0.49922133 0.
  0.         0.35520009 0.         0.49922133]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Aaron Rodgers isnt worth all that money
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'isnt', 'worth', 'all', 'that', 'money']
cosine_similarity: 0.9313853979110718
train_input: [0.20613696606828605, 0.9313854], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Aaron Rodgers the highest paid player now bitches
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.         0.4078241  0.         0.4078241
  0.4078241  0.         0.         0.29017021 0.4078241 ]
 [0.         0.31779954 0.44665616 0.         0.44665616 0.
  0.         0.44665616 0.44665616 0.31779954 0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Aaron Rodgers the highest paid player now bitches
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'the', 'highest', 'paid', 'player', 'now', 'bitches']
cosine_similarity: 0.9677585959434509
train_input: [0.18443191662261305, 0.9677586], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Also drinks are on Aaron Rodgers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.         0.4078241  0.4078241  0.4078241
  0.29017021 0.4078241 ]
 [0.         0.50154891 0.70490949 0.         0.         0.
  0.50154891 0.        ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Also drinks are on Aaron Rodgers
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['also', 'drinks', 'are', 'on', 'aaron', 'rodgers']
cosine_similarity: 0.9502278566360474
train_input: [0.2910691023819054, 0.95022786], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Congrats to Aaron Rodgers new deal worth 5 years 110Mill
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42567716 0.         0.30287281 0.         0.         0.42567716
  0.30287281 0.42567716 0.30287281 0.42567716 0.         0.        ]
 [0.         0.39166832 0.27867523 0.39166832 0.39166832 0.
  0.27867523 0.         0.27867523 0.         0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.25320945]
 [0.25320945 1.        ]]
cosine_similarity: 0.2532094495161745
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Congrats to Aaron Rodgers new deal worth 5 years 110Mill
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['congrats', 'to', 'aaron', 'rodgers', 'new', 'deal', 'worth', 'years']
cosine_similarity: 0.9635968804359436
train_input: [0.2532094495161745, 0.9635969], train_label: 1
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Gren Bay Packers quarterback Aaron Rodgers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42567716 0.30287281 0.         0.42567716 0.         0.42567716
  0.30287281 0.         0.30287281 0.42567716]
 [0.         0.33471228 0.47042643 0.         0.47042643 0.
  0.33471228 0.47042643 0.33471228 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Gren Bay Packers quarterback Aaron Rodgers
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['gren', 'bay', 'packers', 'quarterback', 'aaron', 'rodgers']
cosine_similarity: 0.83511883020401
train_input: [0.3041257418754935, 0.83511883], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Haha Aaron Rodgers is game of thrones fan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.4078241  0.         0.         0.
  0.4078241  0.4078241  0.29017021 0.4078241  0.        ]
 [0.         0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.         0.         0.31779954 0.         0.44665616]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Haha Aaron Rodgers is game of thrones fan
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['haha', 'aaron', 'rodgers', 'is', 'game', 'of', 'thrones', 'fan']
cosine_similarity: 0.925762414932251
train_input: [0.18443191662261305, 0.9257624], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Hell of a pay day for Aaron Rodgers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.         0.4078241  0.         0.4078241
  0.4078241  0.         0.29017021 0.4078241 ]
 [0.         0.35520009 0.49922133 0.         0.49922133 0.
  0.         0.49922133 0.35520009 0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Hell of a pay day for Aaron Rodgers
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['hell', 'of', 'a', 'pay', 'day', 'for', 'aaron', 'rodgers']
cosine_similarity: 0.9610313177108765
train_input: [0.20613696606828605, 0.9610313], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Packers reach longterm contract extension with QB Aaron Rodgers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44610081 0.3174044  0.         0.3174044  0.         0.44610081
  0.3174044  0.         0.         0.3174044  0.44610081]
 [0.         0.28986934 0.40740124 0.28986934 0.40740124 0.
  0.28986934 0.40740124 0.40740124 0.28986934 0.        ]]
pairwise_similarity: [[1.         0.36802321]
 [0.36802321 1.        ]]
cosine_similarity: 0.36802320875611494
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers signs a new 110m extension with the Packers, sentence2: Packers reach longterm contract extension with QB Aaron Rodgers
After tokenization, sentence1: ['aaron', 'rodgers', 'signs', 'a', 'new', 'extension', 'with', 'the', 'packers'], sentence2: ['packers', 'reach', 'longterm', 'contract', 'extension', 'with', 'qb', 'aaron', 'rodgers']
cosine_similarity: 0.937308132648468
train_input: [0.36802320875611494, 0.93730813], train_label: 1
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers 5 yrs 110 million 40 million in first yr of contract
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40740124 0.28986934 0.28986934 0.40740124
  0.28986934 0.40740124 0.28986934 0.40740124 0.         0.        ]
 [0.36408901 0.36408901 0.         0.25905233 0.25905233 0.
  0.51810466 0.         0.25905233 0.         0.36408901 0.36408901]]
pairwise_similarity: [[1.         0.37545663]
 [0.37545663 1.        ]]
cosine_similarity: 0.37545663423725445
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers 5 yrs 110 million 40 million in first yr of contract
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['aaron', 'rodgers', 'yrs', 'million', 'million', 'in', 'first', 'yr', 'of', 'contract']
cosine_similarity: 0.9514540433883667
train_input: [0.37545663423725445, 0.95145404], train_label: 1
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers contract with Packers 5years 110M
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40740124 0.28986934 0.28986934 0.40740124
  0.40740124 0.28986934 0.28986934 0.40740124]
 [0.49844628 0.49844628 0.         0.35464863 0.35464863 0.
  0.         0.35464863 0.35464863 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers contract with Packers 5years 110M
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['aaron', 'rodgers', 'contract', 'with', 'packers']
cosine_similarity: 0.9523433446884155
train_input: [0.41120705506761857, 0.95234334], train_label: 1
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers is a happy man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.37762778 0.37762778 0.         0.
  0.37762778 0.37762778 0.26868528 0.37762778]
 [0.         0.40993715 0.         0.         0.57615236 0.57615236
  0.         0.         0.40993715 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers is a happy man
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['aaron', 'rodgers', 'is', 'a', 'happy', 'man']
cosine_similarity: 0.8938435912132263
train_input: [0.2202881505618297, 0.8938436], train_label: 0
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers is now the highestpaid NFL player
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.37762778 0.37762778 0.         0.37762778
  0.         0.37762778 0.         0.26868528 0.37762778]
 [0.         0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.         0.49922133 0.35520009 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers is now the highestpaid NFL player
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['aaron', 'rodgers', 'is', 'now', 'the', 'nfl', 'player']
cosine_similarity: 0.9464773535728455
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers just got crazy paid
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.37762778 0.         0.37762778 0.
  0.         0.37762778 0.37762778 0.         0.26868528 0.37762778]
 [0.         0.31779954 0.         0.44665616 0.         0.44665616
  0.44665616 0.         0.         0.44665616 0.31779954 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers just got crazy paid
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['aaron', 'rodgers', 'just', 'got', 'crazy', 'paid']
cosine_similarity: 0.9111427664756775
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers making 40 Million this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.39166832 0.27867523 0.39166832 0.39166832 0.
  0.27867523 0.39166832 0.27867523 0.         0.39166832]
 [0.47042643 0.         0.33471228 0.         0.         0.47042643
  0.33471228 0.         0.33471228 0.47042643 0.        ]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Aaron Rodgers making 40 Million this season
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['aaron', 'rodgers', 'making', 'million', 'this', 'season']
cosine_similarity: 0.9542562365531921
train_input: [0.27982806524328774, 0.95425624], train_label: 0
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Damn Aaron Rodgers is going to make 40m this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37762778 0.26868528 0.37762778 0.         0.37762778
  0.         0.         0.37762778 0.37762778 0.26868528 0.37762778
  0.        ]
 [0.4078241  0.         0.29017021 0.         0.4078241  0.
  0.4078241  0.4078241  0.         0.         0.29017021 0.
  0.4078241 ]]
pairwise_similarity: [[1.         0.15592893]
 [0.15592893 1.        ]]
cosine_similarity: 0.15592892548708362
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Damn Aaron Rodgers is going to make 40m this year
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['damn', 'aaron', 'rodgers', 'is', 'going', 'to', 'make', 'this', 'year']
cosine_similarity: 0.9000897407531738
train_input: [0.15592892548708362, 0.90008974], train_label: 0
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Did you miss Aaron Rodgers new contract announcement
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.         0.27867523 0.         0.39166832
  0.39166832 0.         0.         0.39166832 0.27867523 0.39166832]
 [0.         0.30287281 0.42567716 0.30287281 0.42567716 0.
  0.         0.42567716 0.42567716 0.         0.30287281 0.        ]]
pairwise_similarity: [[1.         0.25320945]
 [0.25320945 1.        ]]
cosine_similarity: 0.2532094495161745
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Did you miss Aaron Rodgers new contract announcement
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['did', 'you', 'miss', 'aaron', 'rodgers', 'new', 'contract', 'announcement']
cosine_similarity: 0.9316930770874023
train_input: [0.2532094495161745, 0.9316931], train_label: 0
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: The Packers signed Aaron Rodgers to a contract extension you guys
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.30253071 0.30253071 0.42519636 0.         0.
  0.42519636 0.30253071 0.30253071 0.30253071]
 [0.         0.33425073 0.33425073 0.         0.46977774 0.46977774
  0.         0.33425073 0.33425073 0.33425073]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739691
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: The Packers signed Aaron Rodgers to a contract extension you guys
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['the', 'packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension', 'you', 'guys']
cosine_similarity: 0.9777291417121887
train_input: [0.5056055588739691, 0.97772914], train_label: 0
TF_IDF_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Will Joe Flacco now hold out because Aaron Rodgers is the highest paid QB
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.37762778 0.37762778 0.         0.
  0.         0.         0.37762778 0.37762778 0.         0.
  0.26868528 0.37762778]
 [0.         0.26868528 0.         0.         0.37762778 0.37762778
  0.37762778 0.37762778 0.         0.         0.37762778 0.37762778
  0.26868528 0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.14438355527738672
word_to_vector_cosine_similarity: sentence1: Packers signed Aaron Rodgers to a 5yr110 million dollar contract, sentence2: Will Joe Flacco now hold out because Aaron Rodgers is the highest paid QB
After tokenization, sentence1: ['packers', 'signed', 'aaron', 'rodgers', 'to', 'a', 'million', 'dollar', 'contract'], sentence2: ['will', 'joe', 'flacco', 'now', 'hold', 'out', 'because', 'aaron', 'rodgers', 'is', 'the', 'highest', 'paid', 'qb']
cosine_similarity: 0.953343391418457
train_input: [0.14438355527738672, 0.9533434], train_label: 0
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: 40million guarantee for Aaron Rodgers next year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.44665616 0.44665616 0.
  0.31779954 0.44665616 0.        ]
 [0.49922133 0.35520009 0.         0.         0.         0.49922133
  0.35520009 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: 40million guarantee for Aaron Rodgers next year
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['guarantee', 'for', 'aaron', 'rodgers', 'next', 'year']
cosine_similarity: 0.9568817615509033
train_input: [0.22576484600261604, 0.95688176], train_label: 1
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers agrees to 5 years 110 million
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.         0.31779954 0.44665616 0.        ]
 [0.44665616 0.31779954 0.44665616 0.         0.         0.
  0.44665616 0.31779954 0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers agrees to 5 years 110 million
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'agrees', 'to', 'years', 'million']
cosine_similarity: 0.9634360671043396
train_input: [0.20199309249791833, 0.96343607], train_label: 1
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers be making 40 million in his new deal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.         0.         0.         0.31779954 0.44665616]
 [0.4078241  0.29017021 0.         0.         0.4078241  0.
  0.4078241  0.4078241  0.4078241  0.29017021 0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers be making 40 million in his new deal
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'be', 'making', 'million', 'in', 'his', 'new', 'deal']
cosine_similarity: 0.9481012225151062
train_input: [0.18443191662261305, 0.9481012], train_label: 1
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers has won the NFL lottery
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.         0.
  0.31779954 0.44665616 0.        ]
 [0.35520009 0.         0.         0.         0.49922133 0.49922133
  0.35520009 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers has won the NFL lottery
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'has', 'won', 'the', 'nfl', 'lottery']
cosine_similarity: 0.9705714583396912
train_input: [0.22576484600261604, 0.97057146], train_label: 0
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers is a bad bad man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.44665616 0.
  0.31779954 0.44665616]
 [0.29017021 0.         0.81564821 0.         0.         0.4078241
  0.29017021 0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers is a bad bad man
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'is', 'a', 'bad', 'bad', 'man']
cosine_similarity: 0.867920458316803
train_input: [0.18443191662261305, 0.86792046], train_label: 0
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers is highly overrated no need to spend that much money
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.         0.
  0.         0.         0.31779954 0.44665616 0.        ]
 [0.29017021 0.         0.         0.         0.4078241  0.4078241
  0.4078241  0.4078241  0.29017021 0.         0.4078241 ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers is highly overrated no need to spend that much money
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'is', 'highly', 'overrated', 'no', 'need', 'to', 'spend', 'that', 'much', 'money']
cosine_similarity: 0.8804025053977966
train_input: [0.18443191662261305, 0.8804025], train_label: 0
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers just signed his contract extension with the packers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.49844628 0.35464863 0.35464863 0.         0.
  0.35464863 0.         0.49844628]
 [0.3174044  0.         0.3174044  0.3174044  0.44610081 0.44610081
  0.3174044  0.44610081 0.        ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers just signed his contract extension with the packers
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'just', 'signed', 'his', 'contract', 'extension', 'with', 'the', 'packers']
cosine_similarity: 0.9831553101539612
train_input: [0.4502681446556265, 0.9831553], train_label: 1
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers will get paid 40 million this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.44665616 0.44665616 0.
  0.         0.31779954 0.         0.44665616]
 [0.44665616 0.31779954 0.         0.         0.         0.44665616
  0.44665616 0.31779954 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers will get paid 40 million this season
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'will', 'get', 'paid', 'million', 'this', 'season']
cosine_similarity: 0.9489089846611023
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers will make 40 millions in the 2013 season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.         0.31779954 0.         0.44665616]
 [0.4078241  0.4078241  0.29017021 0.         0.         0.
  0.4078241  0.4078241  0.29017021 0.4078241  0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Aaron Rodgers will make 40 millions in the 2013 season
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'will', 'make', 'millions', 'in', 'the', 'season']
cosine_similarity: 0.9438327550888062
TF_IDF_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Also drinks are on Aaron Rodgers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.         0.44665616 0.31779954
  0.44665616]
 [0.50154891 0.         0.         0.70490949 0.         0.50154891
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: announce the signing of Aaron Rodgers contract extension, sentence2: Also drinks are on Aaron Rodgers
After tokenization, sentence1: ['announce', 'the', 'signing', 'of', 'aaron', 'rodgers', 'contract', 'extension'], sentence2: ['also', 'drinks', 'are', 'on', 'aaron', 'rodgers']
cosine_similarity: 0.9248133301734924
train_input: [0.31878402175377923, 0.92481333], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: A source tells us Aaron Rodgers is now the highestpaid player in NFL history
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.         0.44665616
  0.         0.         0.44665616 0.31779954 0.         0.        ]
 [0.         0.26868528 0.         0.37762778 0.37762778 0.
  0.37762778 0.37762778 0.         0.26868528 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: A source tells us Aaron Rodgers is now the highestpaid player in NFL history
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['a', 'source', 'tells', 'us', 'aaron', 'rodgers', 'is', 'now', 'the', 'player', 'in', 'nfl', 'history']
cosine_similarity: 0.9381252527236938
train_input: [0.1707761131901165, 0.93812525], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers highest paid in the NFL
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.44665616 0.
  0.         0.44665616 0.31779954]
 [0.         0.35520009 0.         0.49922133 0.         0.49922133
  0.49922133 0.         0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers highest paid in the NFL
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['aaron', 'rodgers', 'highest', 'paid', 'in', 'the', 'nfl']
cosine_similarity: 0.9565259218215942
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers is getting paid 40m this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.         0.31779954 0.44665616 0.         0.44665616
  0.         0.44665616 0.31779954 0.        ]
 [0.         0.44665616 0.31779954 0.         0.44665616 0.
  0.44665616 0.         0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers is getting paid 40m this season
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['aaron', 'rodgers', 'is', 'getting', 'paid', 'this', 'season']
cosine_similarity: 0.9162260890007019
train_input: [0.20199309249791833, 0.9162261], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers is the new highest paid player in NFL history
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.         0.44665616
  0.         0.         0.         0.         0.44665616 0.31779954]
 [0.         0.26868528 0.         0.37762778 0.37762778 0.
  0.37762778 0.37762778 0.37762778 0.37762778 0.         0.26868528]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers is the new highest paid player in NFL history
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['aaron', 'rodgers', 'is', 'the', 'new', 'highest', 'paid', 'player', 'in', 'nfl', 'history']
cosine_similarity: 0.9392319917678833
train_input: [0.1707761131901165, 0.939232], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers jus put the ink to his new extension
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.33471228 0.33471228 0.         0.         0.47042643
  0.         0.47042643 0.33471228]
 [0.         0.33471228 0.33471228 0.47042643 0.47042643 0.
  0.47042643 0.         0.33471228]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers jus put the ink to his new extension
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['aaron', 'rodgers', 'jus', 'put', 'the', 'ink', 'to', 'his', 'new', 'extension']
cosine_similarity: 0.9155580401420593
train_input: [0.3360969272762574, 0.91555804], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers just one upd the over paid in the game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.         0.44665616
  0.         0.44665616 0.31779954 0.        ]
 [0.         0.31779954 0.         0.44665616 0.44665616 0.
  0.44665616 0.         0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Aaron Rodgers just one upd the over paid in the game
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['aaron', 'rodgers', 'just', 'one', 'upd', 'the', 'over', 'paid', 'in', 'the', 'game']
cosine_similarity: 0.916064441204071
train_input: [0.20199309249791833, 0.91606444], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: How in the fuck does Aaron Rodgers get an extension that big
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.33471228 0.         0.         0.33471228 0.
  0.47042643 0.47042643 0.33471228]
 [0.         0.33471228 0.47042643 0.47042643 0.33471228 0.47042643
  0.         0.         0.33471228]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: How in the fuck does Aaron Rodgers get an extension that big
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['how', 'in', 'the', 'fuck', 'does', 'aaron', 'rodgers', 'get', 'an', 'extension', 'that', 'big']
cosine_similarity: 0.8998965620994568
train_input: [0.3360969272762574, 0.89989656], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Plotting on Aaron Rodgers who wants in
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.31779954 0.        ]
 [0.         0.40993715 0.         0.         0.57615236 0.
  0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Plotting on Aaron Rodgers who wants in
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['plotting', 'on', 'aaron', 'rodgers', 'who', 'wants', 'in']
cosine_similarity: 0.9046048521995544
train_input: [0.2605556710562624, 0.90460485], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Randall Cobb will make 666k this year to Aaron Rodgers 40 million
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.         0.         0.33471228 0.         0.47042643
  0.         0.33471228 0.         0.47042643 0.33471228 0.        ]
 [0.         0.36469323 0.36469323 0.25948224 0.36469323 0.
  0.36469323 0.25948224 0.36469323 0.         0.25948224 0.36469323]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562623
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: Randall Cobb will make 666k this year to Aaron Rodgers 40 million
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['randall', 'cobb', 'will', 'make', 'this', 'year', 'to', 'aaron', 'rodgers', 'million']
cosine_similarity: 0.922370195388794
train_input: [0.2605556710562623, 0.9223702], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: aaron rodgers deserves every bit of that money
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.         0.44665616 0.44665616
  0.         0.44665616 0.31779954]
 [0.         0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133 0.         0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers received a 110 million extension, sentence2: aaron rodgers deserves every bit of that money
After tokenization, sentence1: ['aaron', 'rodgers', 'received', 'a', 'million', 'extension'], sentence2: ['aaron', 'rodgers', 'deserves', 'every', 'bit', 'of', 'that', 'money']
cosine_similarity: 0.9122880101203918
train_input: [0.22576484600261604, 0.912288], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers 40 million for 1 yr
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.40993715 0.57615236
  0.        ]
 [0.49922133 0.35520009 0.49922133 0.         0.35520009 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers 40 million for 1 yr
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'million', 'for', 'yr']
cosine_similarity: 0.974463939666748
train_input: [0.29121941856368966, 0.97446394], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers deal with the Packers is 5years 110 million highest salary in NFL history
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44832087 0.         0.         0.
  0.         0.         0.44832087 0.44832087 0.         0.63009934]
 [0.32412345 0.32412345 0.2306165  0.32412345 0.32412345 0.32412345
  0.32412345 0.32412345 0.2306165  0.2306165  0.32412345 0.        ]]
pairwise_similarity: [[1.         0.31017058]
 [0.31017058 1.        ]]
cosine_similarity: 0.31017057717950425
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers deal with the Packers is 5years 110 million highest salary in NFL history
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'deal', 'with', 'the', 'packers', 'is', 'million', 'highest', 'salary', 'in', 'nfl', 'history']
cosine_similarity: 0.9792000651359558
train_input: [0.31017057717950425, 0.97920007], train_label: 1
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers getting that discount double check
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.         0.57615236
  0.40993715 0.57615236]
 [0.31779954 0.44665616 0.44665616 0.44665616 0.44665616 0.
  0.31779954 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers getting that discount double check
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'getting', 'that', 'discount', 'double', 'check']
cosine_similarity: 0.9626869559288025
train_input: [0.2605556710562624, 0.96268696], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers highest paid QB now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.         0.40993715
  0.57615236]
 [0.35520009 0.49922133 0.         0.49922133 0.49922133 0.35520009
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers highest paid QB now
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'highest', 'paid', 'qb', 'now']
cosine_similarity: 0.953485906124115
train_input: [0.29121941856368966, 0.9534859], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers signed contract extension with the Packers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.44832087 0.44832087 0.
  0.63009934]
 [0.33471228 0.47042643 0.47042643 0.33471228 0.33471228 0.47042643
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers signed contract extension with the Packers
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'signed', 'contract', 'extension', 'with', 'the', 'packers']
cosine_similarity: 0.9552550911903381
train_input: [0.4501755023269898, 0.9552551], train_label: 1
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers will make 40million in year 1 of his new contract
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.         0.57615236
  0.40993715 0.         0.57615236]
 [0.4078241  0.29017021 0.4078241  0.4078241  0.4078241  0.
  0.29017021 0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Aaron Rodgers will make 40million in year 1 of his new contract
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['aaron', 'rodgers', 'will', 'make', 'in', 'year', 'of', 'his', 'new', 'contract']
cosine_similarity: 0.9817760586738586
train_input: [0.23790309463326234, 0.98177606], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Damn Aaron Rodgers is going to make 40m this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.         0.57615236
  0.40993715 0.         0.57615236]
 [0.4078241  0.29017021 0.4078241  0.4078241  0.4078241  0.
  0.29017021 0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Damn Aaron Rodgers is going to make 40m this year
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['damn', 'aaron', 'rodgers', 'is', 'going', 'to', 'make', 'this', 'year']
cosine_similarity: 0.9719029068946838
train_input: [0.23790309463326234, 0.9719029], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Green Bay Packers are nothing without Aaron Rodgers that contract extension was necessary
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.         0.
  0.44832087 0.44832087 0.63009934]
 [0.27867523 0.39166832 0.39166832 0.39166832 0.39166832 0.39166832
  0.27867523 0.27867523 0.        ]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: Green Bay Packers are nothing without Aaron Rodgers that contract extension was necessary
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['green', 'bay', 'packers', 'are', 'nothing', 'without', 'aaron', 'rodgers', 'that', 'contract', 'extension', 'was', 'necessary']
cosine_similarity: 0.976231038570404
train_input: [0.3748077700589726, 0.97623104], train_label: 1
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: The Green Bay Packers have signed quarterback Aaron Rodgers to a contract extension Gezwxm87
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.         0.
  0.44832087 0.         0.44832087 0.         0.63009934]
 [0.24377685 0.34261985 0.34261985 0.34261985 0.34261985 0.34261985
  0.24377685 0.34261985 0.24377685 0.34261985 0.        ]]
pairwise_similarity: [[1.         0.32787075]
 [0.32787075 1.        ]]
cosine_similarity: 0.3278707471841718
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: The Green Bay Packers have signed quarterback Aaron Rodgers to a contract extension Gezwxm87
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['the', 'green', 'bay', 'packers', 'have', 'signed', 'quarterback', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension']
cosine_similarity: 0.9746912717819214
train_input: [0.3278707471841718, 0.9746913], train_label: 1
TF_IDF_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: The Packers just made Aaron Rodgers the richest player in NFL History
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.44832087 0.
  0.         0.44832087 0.63009934]
 [0.27867523 0.39166832 0.39166832 0.39166832 0.27867523 0.39166832
  0.39166832 0.27867523 0.        ]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: 5 more years for Aaron Rodgers with the packers, sentence2: The Packers just made Aaron Rodgers the richest player in NFL History
After tokenization, sentence1: ['more', 'years', 'for', 'aaron', 'rodgers', 'with', 'the', 'packers'], sentence2: ['the', 'packers', 'just', 'made', 'aaron', 'rodgers', 'the', 'richest', 'player', 'in', 'nfl', 'history']
cosine_similarity: 0.9805247783660889
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers about to get 2325 million a year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53309782 0.37930349 0.53309782 0.         0.37930349
  0.37930349]
 [0.53309782 0.         0.37930349 0.         0.53309782 0.37930349
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers about to get 2325 million a year
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['aaron', 'rodgers', 'about', 'to', 'get', 'million', 'a', 'year']
cosine_similarity: 0.980709433555603
train_input: [0.43161341897075145, 0.98070943], train_label: 1
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers contract extension estimated at 110 million over 5 years or 22 million a year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.53309782 0.37930349 0.         0.
  0.         0.53309782 0.         0.37930349 0.37930349 0.        ]
 [0.29464404 0.29464404 0.         0.20964166 0.29464404 0.29464404
  0.29464404 0.         0.58928809 0.20964166 0.20964166 0.29464404]]
pairwise_similarity: [[1.         0.23855345]
 [0.23855345 1.        ]]
cosine_similarity: 0.23855344519607363
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers contract extension estimated at 110 million over 5 years or 22 million a year
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['aaron', 'rodgers', 'contract', 'extension', 'estimated', 'at', 'million', 'over', 'years', 'or', 'million', 'a', 'year']
cosine_similarity: 0.9419686198234558
train_input: [0.23855344519607363, 0.9419686], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers got paid very deserving
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.49922133 0.         0.
  0.35520009 0.49922133]
 [0.         0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.35520009 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers got paid very deserving
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['aaron', 'rodgers', 'got', 'paid', 'very', 'deserving']
cosine_similarity: 0.9540542960166931
train_input: [0.2523342014336961, 0.9540543], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers is supposed to make 40 million next year just as a football player
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.         0.37930349 0.         0.53309782 0.
  0.         0.         0.         0.37930349 0.         0.37930349]
 [0.         0.34261985 0.24377685 0.34261985 0.         0.34261985
  0.34261985 0.34261985 0.34261985 0.24377685 0.34261985 0.24377685]]
pairwise_similarity: [[1.         0.27739623]
 [0.27739623 1.        ]]
cosine_similarity: 0.27739622897624144
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers is supposed to make 40 million next year just as a football player
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['aaron', 'rodgers', 'is', 'supposed', 'to', 'make', 'million', 'next', 'year', 'just', 'as', 'a', 'football', 'player']
cosine_similarity: 0.9689950346946716
train_input: [0.27739622897624144, 0.96899503], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers to get 110M over 5 years
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.35520009 0.49922133 0.35520009 0.49922133
  0.        ]
 [0.57615236 0.         0.40993715 0.         0.40993715 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Aaron Rodgers to get 110M over 5 years
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['aaron', 'rodgers', 'to', 'get', 'over', 'years']
cosine_similarity: 0.9724113941192627
train_input: [0.29121941856368966, 0.9724114], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: How much they give Aaron Rodgers for that extension
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.49922133 0.35520009 0.49922133]
 [0.         0.50154891 0.70490949 0.         0.50154891 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: How much they give Aaron Rodgers for that extension
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['how', 'much', 'they', 'give', 'aaron', 'rodgers', 'for', 'that', 'extension']
cosine_similarity: 0.9498967528343201
train_input: [0.3563004293331381, 0.94989675], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Meet Aaron Rodgers the gold digger
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.49922133 0.         0.
  0.35520009 0.49922133]
 [0.         0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.35520009 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: Meet Aaron Rodgers the gold digger
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['meet', 'aaron', 'rodgers', 'the', 'gold', 'digger']
cosine_similarity: 0.9449751377105713
train_input: [0.2523342014336961, 0.94497514], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: QB Aaron Rodgers just signed a contract extension
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.49922133 0.
  0.         0.35520009 0.         0.49922133]
 [0.         0.29017021 0.4078241  0.4078241  0.         0.4078241
  0.4078241  0.29017021 0.4078241  0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: QB Aaron Rodgers just signed a contract extension
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['qb', 'aaron', 'rodgers', 'just', 'signed', 'a', 'contract', 'extension']
cosine_similarity: 0.9490333795547485
train_input: [0.20613696606828605, 0.9490334], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: So Aaron Rodgers brother is in the draft too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.49922133 0.35520009
  0.49922133]
 [0.         0.40993715 0.57615236 0.57615236 0.         0.40993715
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: So Aaron Rodgers brother is in the draft too
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['so', 'aaron', 'rodgers', 'brother', 'is', 'in', 'the', 'draft', 'too']
cosine_similarity: 0.9701074361801147
train_input: [0.29121941856368966, 0.97010744], train_label: 0
TF_IDF_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: The Green Bay Packers have signed quarterback Aaron Rodgers to a contract extension Gezwxm87
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.         0.49922133
  0.         0.         0.         0.         0.35520009 0.
  0.49922133]
 [0.         0.23700504 0.33310232 0.33310232 0.33310232 0.
  0.33310232 0.33310232 0.33310232 0.33310232 0.23700504 0.33310232
  0.        ]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: Aaron Rodgers getting 2325million a year, sentence2: The Green Bay Packers have signed quarterback Aaron Rodgers to a contract extension Gezwxm87
After tokenization, sentence1: ['aaron', 'rodgers', 'getting', 'a', 'year'], sentence2: ['the', 'green', 'bay', 'packers', 'have', 'signed', 'quarterback', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension']
cosine_similarity: 0.9588791728019714
train_input: [0.16836842163679844, 0.9588792], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers deserve a big contract
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.         0.30287281 0.         0.42567716
  0.42567716 0.30287281 0.42567716]
 [0.37930349 0.         0.53309782 0.37930349 0.53309782 0.
  0.         0.37930349 0.        ]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers deserve a big contract
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'deserve', 'a', 'big', 'contract']
cosine_similarity: 0.9744359254837036
train_input: [0.34464214103805474, 0.9744359], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers has penned a 5year 110M extension with the Green Bay Packers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.3174044  0.44610081 0.         0.44610081
  0.3174044  0.         0.3174044  0.         0.3174044  0.44610081]
 [0.37729199 0.37729199 0.26844636 0.         0.37729199 0.
  0.26844636 0.37729199 0.26844636 0.37729199 0.26844636 0.        ]]
pairwise_similarity: [[1.         0.34082422]
 [0.34082422 1.        ]]
cosine_similarity: 0.3408242166238352
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers has penned a 5year 110M extension with the Green Bay Packers
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'has', 'penned', 'a', 'extension', 'with', 'the', 'green', 'bay', 'packers']
cosine_similarity: 0.9801464080810547
train_input: [0.3408242166238352, 0.9801464], train_label: 1
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers is going to make 40M next year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.4078241  0.4078241  0.4078241  0.
  0.         0.4078241  0.29017021 0.4078241  0.        ]
 [0.44665616 0.31779954 0.         0.         0.         0.44665616
  0.44665616 0.         0.31779954 0.         0.44665616]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers is going to make 40M next year
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'is', 'going', 'to', 'make', 'next', 'year']
cosine_similarity: 0.9587581753730774
train_input: [0.18443191662261305, 0.9587582], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers just became the most selfish player in NFL history
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.4078241  0.4078241  0.         0.
  0.         0.4078241  0.         0.29017021 0.         0.4078241 ]
 [0.29017021 0.         0.         0.         0.4078241  0.4078241
  0.4078241  0.         0.4078241  0.29017021 0.4078241  0.        ]]
pairwise_similarity: [[1.        0.1683975]
 [0.1683975 1.       ]]
cosine_similarity: 0.16839750037215276
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers just became the most selfish player in NFL history
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'just', 'became', 'the', 'most', 'selfish', 'player', 'in', 'nfl', 'history']
cosine_similarity: 0.9566205739974976
train_input: [0.16839750037215276, 0.9566206], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers signs a 5 year contract extension for The Packers worth 110million
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33425073 0.46977774 0.33425073 0.33425073 0.33425073
  0.33425073 0.46977774 0.         0.         0.        ]
 [0.39129369 0.27840869 0.         0.27840869 0.27840869 0.27840869
  0.27840869 0.         0.39129369 0.39129369 0.39129369]]
pairwise_similarity: [[1.         0.46529153]
 [0.46529153 1.        ]]
cosine_similarity: 0.4652915323370137
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Aaron Rodgers signs a 5 year contract extension for The Packers worth 110million
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'signs', 'a', 'year', 'contract', 'extension', 'for', 'the', 'packers', 'worth']
cosine_similarity: 0.9919760823249817
train_input: [0.4652915323370137, 0.9919761], train_label: 1
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Congrats to Aaron Rodgers new deal worth 5 years 110Mill
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.4078241  0.         0.4078241  0.
  0.4078241  0.         0.4078241  0.29017021 0.4078241  0.
  0.        ]
 [0.37762778 0.26868528 0.         0.37762778 0.         0.37762778
  0.         0.37762778 0.         0.26868528 0.         0.37762778
  0.37762778]]
pairwise_similarity: [[1.         0.15592893]
 [0.15592893 1.        ]]
cosine_similarity: 0.15592892548708362
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Congrats to Aaron Rodgers new deal worth 5 years 110Mill
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['congrats', 'to', 'aaron', 'rodgers', 'new', 'deal', 'worth', 'years']
cosine_similarity: 0.975701630115509
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Congrats to Aaron Rodgers on becoming the highest paid player in NFL history
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.         0.4078241  0.4078241  0.
  0.         0.         0.4078241  0.         0.         0.29017021
  0.4078241 ]
 [0.26868528 0.         0.37762778 0.         0.         0.37762778
  0.37762778 0.37762778 0.         0.37762778 0.37762778 0.26868528
  0.        ]]
pairwise_similarity: [[1.         0.15592893]
 [0.15592893 1.        ]]
cosine_similarity: 0.15592892548708362
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Congrats to Aaron Rodgers on becoming the highest paid player in NFL history
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['congrats', 'to', 'aaron', 'rodgers', 'on', 'becoming', 'the', 'highest', 'paid', 'player', 'in', 'nfl', 'history']
cosine_similarity: 0.9808152318000793
train_input: [0.15592892548708362, 0.98081523], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: I bet youre thrilled with Aaron Rodgers new contract
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.         0.30287281 0.42567716 0.
  0.42567716 0.30287281 0.42567716 0.         0.        ]
 [0.30287281 0.         0.42567716 0.30287281 0.         0.42567716
  0.         0.30287281 0.         0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.27519581]
 [0.27519581 1.        ]]
cosine_similarity: 0.275195812175023
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: I bet youre thrilled with Aaron Rodgers new contract
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['i', 'bet', 'youre', 'thrilled', 'with', 'aaron', 'rodgers', 'new', 'contract']
cosine_similarity: 0.9612564444541931
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Over the next 2 years not one will get a higher contract than Aaron Rodgers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.30287281 0.42567716 0.         0.42567716
  0.30287281 0.42567716 0.        ]
 [0.37930349 0.         0.37930349 0.         0.53309782 0.
  0.37930349 0.         0.53309782]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Over the next 2 years not one will get a higher contract than Aaron Rodgers
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['over', 'the', 'next', 'years', 'not', 'one', 'will', 'get', 'a', 'higher', 'contract', 'than', 'aaron', 'rodgers']
cosine_similarity: 0.9716905951499939
train_input: [0.34464214103805474, 0.9716906], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Rushing to Lambeau because Aaron Rodgers signed a contract extension on day 2 of the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.46977774 0.33425073 0.         0.         0.33425073
  0.         0.46977774 0.33425073 0.         0.33425073]
 [0.27840869 0.         0.27840869 0.39129369 0.39129369 0.27840869
  0.39129369 0.         0.27840869 0.39129369 0.27840869]]
pairwise_similarity: [[1.         0.46529153]
 [0.46529153 1.        ]]
cosine_similarity: 0.4652915323370137
word_to_vector_cosine_similarity: sentence1: The Packers announced they have signed Aaron Rodgers to a contract extension, sentence2: Rushing to Lambeau because Aaron Rodgers signed a contract extension on day 2 of the draft
After tokenization, sentence1: ['the', 'packers', 'announced', 'they', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension'], sentence2: ['rushing', 'to', 'lambeau', 'because', 'aaron', 'rodgers', 'signed', 'a', 'contract', 'extension', 'on', 'day', 'of', 'the', 'draft']
cosine_similarity: 0.9908921718597412
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: 5 year 110M contract for AARON RODGERS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.         0.57615236 0.40993715
  0.        ]
 [0.49922133 0.35520009 0.         0.49922133 0.         0.35520009
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: 5 year 110M contract for AARON RODGERS
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['year', 'contract', 'for', 'aaron', 'rodgers']
cosine_similarity: 0.8785780072212219
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers Bouta make 40 mill this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.         0.57615236 0.
  0.40993715 0.        ]
 [0.44665616 0.31779954 0.         0.44665616 0.         0.44665616
  0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers Bouta make 40 mill this season
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['aaron', 'rodgers', 'bouta', 'make', 'mill', 'this', 'season']
cosine_similarity: 0.9213298559188843
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers got an extension today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.57615236 0.40993715
  0.        ]
 [0.35520009 0.         0.49922133 0.49922133 0.         0.35520009
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers got an extension today
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['aaron', 'rodgers', 'got', 'an', 'extension', 'today']
cosine_similarity: 0.9104442000389099
train_input: [0.29121941856368966, 0.9104442], train_label: 0
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers got paid very deserving
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.57615236 0.
  0.40993715]
 [0.35520009 0.         0.49922133 0.49922133 0.         0.49922133
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers got paid very deserving
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['aaron', 'rodgers', 'got', 'paid', 'very', 'deserving']
cosine_similarity: 0.914517343044281
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers is now the Highest Paid Player in NFL History the Packers extended his contract
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.         0.
  0.57615236 0.         0.         0.         0.         0.40993715]
 [0.23700504 0.         0.33310232 0.33310232 0.33310232 0.33310232
  0.         0.33310232 0.33310232 0.33310232 0.33310232 0.23700504]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858148
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers is now the Highest Paid Player in NFL History the Packers extended his contract
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['aaron', 'rodgers', 'is', 'now', 'the', 'highest', 'paid', 'player', 'in', 'nfl', 'history', 'the', 'packers', 'extended', 'his', 'contract']
cosine_similarity: 0.8809432983398438
train_input: [0.19431434016858148, 0.8809433], train_label: 0
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers officially the highest paid nfl player of all time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.57615236 0.         0.
  0.         0.         0.40993715 0.        ]
 [0.26868528 0.         0.37762778 0.         0.37762778 0.37762778
  0.37762778 0.37762778 0.26868528 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Aaron Rodgers officially the highest paid nfl player of all time
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['aaron', 'rodgers', 'officially', 'the', 'highest', 'paid', 'nfl', 'player', 'of', 'all', 'time']
cosine_similarity: 0.8856894373893738
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: God I fucking love Aaron Rodgers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.         0.57615236
  0.40993715]
 [0.35520009 0.         0.49922133 0.49922133 0.49922133 0.
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: God I fucking love Aaron Rodgers
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['god', 'i', 'fucking', 'love', 'aaron', 'rodgers']
cosine_similarity: 0.9422633647918701
train_input: [0.29121941856368966, 0.94226336], train_label: 0
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: I kno stephenasmith is happy abt aaron rodgers new contract
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.         0.
  0.57615236 0.         0.40993715 0.        ]
 [0.26868528 0.37762778 0.         0.37762778 0.37762778 0.37762778
  0.         0.37762778 0.26868528 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: I kno stephenasmith is happy abt aaron rodgers new contract
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['i', 'kno', 'is', 'happy', 'abt', 'aaron', 'rodgers', 'new', 'contract']
cosine_similarity: 0.9276009202003479
train_input: [0.2202881505618297, 0.9276009], train_label: 0
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: No duh Aaron Rodgers got a contract extension
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.         0.
  0.57615236 0.40993715]
 [0.31779954 0.         0.44665616 0.44665616 0.44665616 0.44665616
  0.         0.31779954]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: No duh Aaron Rodgers got a contract extension
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension']
cosine_similarity: 0.9233032464981079
train_input: [0.2605556710562624, 0.92330325], train_label: 0
TF_IDF_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Pretty sure that Aaron Rodgers is gonna be able to buy a small country
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.         0.
  0.57615236 0.         0.40993715 0.         0.        ]
 [0.25136004 0.35327777 0.         0.35327777 0.35327777 0.35327777
  0.         0.35327777 0.25136004 0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: AARON RODGERS is a BAAAAD MAAAN, sentence2: Pretty sure that Aaron Rodgers is gonna be able to buy a small country
After tokenization, sentence1: ['aaron', 'rodgers', 'is', 'a', 'baaaad', 'maaan'], sentence2: ['pretty', 'sure', 'that', 'aaron', 'rodgers', 'is', 'gonna', 'be', 'able', 'to', 'buy', 'a', 'small', 'country']
cosine_similarity: 0.923719584941864
train_input: [0.20608363501393823, 0.9237196], train_label: 0
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers about to get major paper
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.49922133 0.49922133
  0.35520009]
 [0.40993715 0.         0.57615236 0.57615236 0.         0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers about to get major paper
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['aaron', 'rodgers', 'about', 'to', 'get', 'major', 'paper']
cosine_similarity: 0.9066455364227295
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers in the highest payed player in NFL History
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.         0.         0.4090901  0.
  0.57496187 0.4090901 ]
 [0.3174044  0.3174044  0.44610081 0.44610081 0.3174044  0.44610081
  0.         0.3174044 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers in the highest payed player in NFL History
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['aaron', 'rodgers', 'in', 'the', 'highest', 'payed', 'player', 'in', 'nfl', 'history']
cosine_similarity: 0.9508726596832275
train_input: [0.5193879933129156, 0.95087266], train_label: 1
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers is cool as fuck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.49922133 0.49922133
  0.35520009]
 [0.40993715 0.57615236 0.57615236 0.         0.         0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers is cool as fuck
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['aaron', 'rodgers', 'is', 'cool', 'as', 'fuck']
cosine_similarity: 0.8850055932998657
train_input: [0.29121941856368966, 0.8850056], train_label: 0
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers is gonna be MAKING BANK
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.         0.49922133
  0.49922133 0.35520009]
 [0.35520009 0.49922133 0.49922133 0.         0.49922133 0.
  0.         0.35520009]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers is gonna be MAKING BANK
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['aaron', 'rodgers', 'is', 'gonna', 'be', 'making', 'bank']
cosine_similarity: 0.9238632917404175
train_input: [0.2523342014336961, 0.9238633], train_label: 1
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers is now highest paid player in NFL history
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.         0.         0.53309782
  0.         0.53309782 0.37930349]
 [0.30287281 0.30287281 0.42567716 0.42567716 0.42567716 0.
  0.42567716 0.         0.30287281]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers is now highest paid player in NFL history
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['aaron', 'rodgers', 'is', 'now', 'highest', 'paid', 'player', 'in', 'nfl', 'history']
cosine_similarity: 0.9538747668266296
train_input: [0.34464214103805474, 0.95387477], train_label: 1
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers is set to earn 40m this season alone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.35520009 0.         0.        ]
 [0.44665616 0.31779954 0.44665616 0.         0.         0.
  0.31779954 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers is set to earn 40m this season alone
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['aaron', 'rodgers', 'is', 'set', 'to', 'earn', 'this', 'season', 'alone']
cosine_similarity: 0.899419903755188
train_input: [0.22576484600261604, 0.8994199], train_label: 1
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers trajectory is what many people point in regards to potential
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.         0.
  0.49922133 0.         0.35520009 0.        ]
 [0.29017021 0.         0.         0.4078241  0.4078241  0.4078241
  0.         0.4078241  0.29017021 0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: Aaron Rodgers trajectory is what many people point in regards to potential
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['aaron', 'rodgers', 'trajectory', 'is', 'what', 'many', 'people', 'point', 'in', 'regards', 'to', 'potential']
cosine_similarity: 0.8931298851966858
train_input: [0.20613696606828605, 0.8931299], train_label: 0
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: As expected Aaron Rodgers just got PAID
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.         0.
  0.49922133 0.49922133 0.35520009]
 [0.31779954 0.44665616 0.44665616 0.         0.44665616 0.44665616
  0.         0.         0.31779954]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: As expected Aaron Rodgers just got PAID
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['as', 'expected', 'aaron', 'rodgers', 'just', 'got', 'paid']
cosine_similarity: 0.9460623264312744
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: How s Christian Ponder taking the news of Aaron Rodgers bigmoney contract extension
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.         0.49922133
  0.         0.49922133 0.         0.49922133 0.35520009 0.        ]
 [0.25136004 0.35327777 0.35327777 0.35327777 0.35327777 0.
  0.35327777 0.         0.35327777 0.         0.25136004 0.35327777]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: How s Christian Ponder taking the news of Aaron Rodgers bigmoney contract extension
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['how', 's', 'christian', 'ponder', 'taking', 'the', 'news', 'of', 'aaron', 'rodgers', 'bigmoney', 'contract', 'extension']
cosine_similarity: 0.9367853403091431
train_input: [0.17856621555757476, 0.93678534], train_label: 1
TF_IDF_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: I wonder what the Aaron Rodgers contract extension is worth
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.49922133 0.49922133
  0.35520009 0.         0.        ]
 [0.31779954 0.44665616 0.44665616 0.         0.         0.
  0.31779954 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Highest payed QB is Aaron Rodgers, sentence2: I wonder what the Aaron Rodgers contract extension is worth
After tokenization, sentence1: ['highest', 'payed', 'qb', 'is', 'aaron', 'rodgers'], sentence2: ['i', 'wonder', 'what', 'the', 'aaron', 'rodgers', 'contract', 'extension', 'is', 'worth']
cosine_similarity: 0.929175853729248
train_input: [0.22576484600261604, 0.92917585], train_label: 0
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers contract is 5 years and 110M
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.33471228 0.47042643 0.47042643 0.47042643
  0.33471228 0.        ]
 [0.53309782 0.37930349 0.37930349 0.         0.         0.
  0.37930349 0.53309782]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers contract is 5 years and 110M
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'contract', 'is', 'years', 'and']
cosine_similarity: 0.9303191900253296
train_input: [0.3808726084759436, 0.9303192], train_label: 0
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers deal is 5years 110 million
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.31779954 0.44665616 0.         0.44665616
  0.44665616 0.44665616 0.         0.31779954]
 [0.44665616 0.44665616 0.31779954 0.         0.44665616 0.
  0.         0.         0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers deal is 5years 110 million
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'deal', 'is', 'million']
cosine_similarity: 0.9386808276176453
train_input: [0.20199309249791833, 0.9386808], train_label: 0
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers is happy today or so I think
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.44665616 0.
  0.31779954 0.         0.        ]
 [0.35520009 0.         0.         0.         0.         0.49922133
  0.35520009 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers is happy today or so I think
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'is', 'happy', 'today', 'or', 'so', 'i', 'think']
cosine_similarity: 0.8963494300842285
train_input: [0.22576484600261604, 0.89634943], train_label: 0
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers new contract just made him the richest payed player in NFL history
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.47042643 0.47042643 0.
  0.         0.         0.         0.         0.         0.
  0.33471228]
 [0.24377685 0.24377685 0.         0.         0.         0.34261985
  0.34261985 0.34261985 0.34261985 0.34261985 0.34261985 0.34261985
  0.24377685]]
pairwise_similarity: [[1.         0.24478531]
 [0.24478531 1.        ]]
cosine_similarity: 0.2447853117345521
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers new contract just made him the richest payed player in NFL history
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'new', 'contract', 'just', 'made', 'him', 'the', 'richest', 'payed', 'player', 'in', 'nfl', 'history']
cosine_similarity: 0.9328854084014893
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers signs long awaited contract extension with Green Bay Packers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.         0.35464863 0.49844628 0.35464863
  0.49844628 0.         0.         0.         0.35464863 0.        ]
 [0.25116439 0.35300279 0.35300279 0.25116439 0.         0.25116439
  0.         0.35300279 0.35300279 0.35300279 0.25116439 0.35300279]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.35630042933313805
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers signs long awaited contract extension with Green Bay Packers
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'signs', 'long', 'awaited', 'contract', 'extension', 'with', 'green', 'bay', 'packers']
cosine_similarity: 0.9104115962982178
train_input: [0.35630042933313805, 0.9104116], train_label: 1
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers will make more money next this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.44665616 0.
  0.         0.31779954 0.        ]
 [0.35520009 0.         0.         0.         0.         0.49922133
  0.49922133 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Aaron Rodgers will make more money next this year
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['aaron', 'rodgers', 'will', 'make', 'more', 'money', 'next', 'this', 'year']
cosine_similarity: 0.9170417785644531
train_input: [0.22576484600261604, 0.9170418], train_label: 0
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: How is Aaron Rodgers not trending
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.44665616 0.31779954
  0.        ]
 [0.50154891 0.         0.         0.         0.         0.50154891
  0.70490949]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: How is Aaron Rodgers not trending
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['how', 'is', 'aaron', 'rodgers', 'not', 'trending']
cosine_similarity: 0.9133588075637817
train_input: [0.31878402175377923, 0.9133588], train_label: 0
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Is there a discount double check clause in Aaron Rodgers new contract
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.         0.33471228 0.         0.
  0.47042643 0.47042643 0.47042643 0.         0.33471228]
 [0.27867523 0.39166832 0.39166832 0.27867523 0.39166832 0.39166832
  0.         0.         0.         0.39166832 0.27867523]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Is there a discount double check clause in Aaron Rodgers new contract
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['is', 'there', 'a', 'discount', 'double', 'check', 'clause', 'in', 'aaron', 'rodgers', 'new', 'contract']
cosine_similarity: 0.9389417171478271
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Packers QB Aaron Rodgers gets extension in Green Bay
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.47042643 0.47042643 0.33471228 0.
  0.47042643 0.         0.         0.         0.33471228]
 [0.27867523 0.39166832 0.         0.         0.27867523 0.39166832
  0.         0.39166832 0.39166832 0.39166832 0.27867523]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: Packers QB Aaron Rodgers gets extension in Green Bay
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['packers', 'qb', 'aaron', 'rodgers', 'gets', 'extension', 'in', 'green', 'bay']
cosine_similarity: 0.9266260862350464
train_input: [0.27982806524328774, 0.9266261], train_label: 1
TF_IDF_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: according to espnnfcnblog the Packers have signed Aaron Rodgers to a contract extension
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.49844628 0.         0.35464863
  0.49844628 0.         0.35464863 0.        ]
 [0.28986934 0.40740124 0.28986934 0.         0.40740124 0.28986934
  0.         0.40740124 0.28986934 0.40740124]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: No duh Aaron Rodgers got a contract extension, sentence2: according to espnnfcnblog the Packers have signed Aaron Rodgers to a contract extension
After tokenization, sentence1: ['no', 'duh', 'aaron', 'rodgers', 'got', 'a', 'contract', 'extension'], sentence2: ['according', 'to', 'the', 'packers', 'have', 'signed', 'aaron', 'rodgers', 'to', 'a', 'contract', 'extension']
cosine_similarity: 0.9558167457580566
train_input: [0.41120705506761857, 0.95581675], train_label: 1
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: 1 game left you would be absolutely shiting it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.53404633]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: 1 game left you would be absolutely shiting it
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['game', 'left', 'you', 'would', 'be', 'absolutely', 'shiting', 'it']
cosine_similarity: 0.9795407056808472
train_input: [0.1443835552773867, 0.9795407], train_label: 0
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: Follow butler182 he s absolutely lovely
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.53404633 0.
  0.53404633]
 [0.37997836 0.         0.53404633 0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: Follow butler182 he s absolutely lovely
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['follow', 'he', 's', 'absolutely', 'lovely']
cosine_similarity: 0.9304397106170654
train_input: [0.1443835552773867, 0.9304397], train_label: 0
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I absolutely cant stand research papers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633]
 [0.37997836 0.         0.         0.53404633 0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I absolutely cant stand research papers
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['i', 'absolutely', 'cant', 'stand', 'research', 'papers']
cosine_similarity: 0.9594194293022156
train_input: [0.1443835552773867, 0.9594194], train_label: 0
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I absolutely hate the abbeyview bus
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.53404633 0.
  0.53404633]
 [0.53404633 0.37997836 0.         0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I absolutely hate the abbeyview bus
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['i', 'absolutely', 'hate', 'the', 'bus']
cosine_similarity: 0.9745603203773499
train_input: [0.1443835552773867, 0.9745603], train_label: 0
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I absolutely have no motivation to study for this modern final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.         0.53404633]
 [0.33517574 0.         0.47107781 0.         0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I absolutely have no motivation to study for this modern final
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['i', 'absolutely', 'have', 'no', 'motivation', 'to', 'study', 'for', 'this', 'modern', 'final']
cosine_similarity: 0.9601150155067444
train_input: [0.1273595297947935, 0.960115], train_label: 0
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I have absolutely no desire to do anything today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.53404633 0.        ]
 [0.44943642 0.         0.6316672  0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I have absolutely no desire to do anything today
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['i', 'have', 'absolutely', 'no', 'desire', 'to', 'do', 'anything', 'today']
cosine_similarity: 0.9679030179977417
train_input: [0.17077611319011649, 0.967903], train_label: 0
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I have absolutely no idea what Perry is talking about
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633]
 [0.37997836 0.         0.         0.53404633 0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: I have absolutely no idea what Perry is talking about
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['i', 'have', 'absolutely', 'no', 'idea', 'what', 'perry', 'is', 'talking', 'about']
cosine_similarity: 0.984489917755127
train_input: [0.1443835552773867, 0.9844899], train_label: 0
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: Second singing lesson was absolutely class
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.         0.53404633]
 [0.33517574 0.         0.47107781 0.         0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: Second singing lesson was absolutely class
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['second', 'singing', 'lesson', 'was', 'absolutely', 'class']
cosine_similarity: 0.9472333788871765
train_input: [0.1273595297947935, 0.9472334], train_label: 0
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: Teaching for the afternoon was absolutely wicked
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.         0.53404633
  0.        ]
 [0.37997836 0.53404633 0.         0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: Teaching for the afternoon was absolutely wicked
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['teaching', 'for', 'the', 'afternoon', 'was', 'absolutely', 'wicked']
cosine_similarity: 0.9556878209114075
train_input: [0.1443835552773867, 0.9556878], train_label: 0
TF_IDF_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: i absolutely hate how you make fun of me all the time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.53404633 0.        ]
 [0.33517574 0.         0.47107781 0.         0.47107781 0.47107781
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: My brother is the only guy I tell absolutely everything to, sentence2: i absolutely hate how you make fun of me all the time
After tokenization, sentence1: ['my', 'brother', 'is', 'the', 'only', 'guy', 'i', 'tell', 'absolutely', 'everything', 'to'], sentence2: ['i', 'absolutely', 'hate', 'how', 'you', 'make', 'fun', 'of', 'me', 'all', 'the', 'time']
cosine_similarity: 0.991775631904602
train_input: [0.1273595297947935, 0.99177563], train_label: 0
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace Sanders a Tampa native goes to JAX
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.         0.57615236
  0.40993715 0.        ]
 [0.31779954 0.44665616 0.44665616 0.         0.44665616 0.
  0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace Sanders a Tampa native goes to JAX
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['ace', 'sanders', 'a', 'tampa', 'native', 'goes', 'to', 'jax']
cosine_similarity: 0.9401242136955261
train_input: [0.2605556710562624, 0.9401242], train_label: 0
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace Sanders has one of the best punt returns in college football
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.57615236 0.57615236
  0.         0.         0.40993715]
 [0.29017021 0.4078241  0.4078241  0.4078241  0.         0.
  0.4078241  0.4078241  0.29017021]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace Sanders has one of the best punt returns in college football
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['ace', 'sanders', 'has', 'one', 'of', 'the', 'best', 'punt', 'returns', 'in', 'college', 'football']
cosine_similarity: 0.9464325308799744
train_input: [0.23790309463326234, 0.94643253], train_label: 0
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace Sanders is a football player
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.57615236 0.         0.40993715]
 [0.40993715 0.57615236 0.         0.         0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace Sanders is a football player
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['ace', 'sanders', 'is', 'a', 'football', 'player']
cosine_similarity: 0.9326333403587341
train_input: [0.3360969272762575, 0.93263334], train_label: 0
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace Sanders is one hell of a pickup for the Jags in Round 4
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.57615236 0.
  0.         0.40993715]
 [0.31779954 0.44665616 0.44665616 0.         0.         0.44665616
  0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace Sanders is one hell of a pickup for the Jags in Round 4
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['ace', 'sanders', 'is', 'one', 'hell', 'of', 'a', 'pickup', 'for', 'the', 'jags', 'in', 'round']
cosine_similarity: 0.9745445251464844
train_input: [0.2605556710562624, 0.9745445], train_label: 1
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace sanders is one of the most athletic players in the SEC
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.57615236 0.         0.40993715
  0.        ]
 [0.35520009 0.49922133 0.         0.         0.49922133 0.35520009
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Ace sanders is one of the most athletic players in the SEC
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['ace', 'sanders', 'is', 'one', 'of', 'the', 'most', 'athletic', 'players', 'in', 'the', 'sec']
cosine_similarity: 0.9462786912918091
train_input: [0.29121941856368966, 0.9462787], train_label: 0
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: GREAT PICK Jacksonville getting Ace Sanders from USC
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.63009934 0.44832087
  0.44832087 0.        ]
 [0.30287281 0.42567716 0.42567716 0.42567716 0.         0.30287281
  0.30287281 0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: GREAT PICK Jacksonville getting Ace Sanders from USC
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['great', 'pick', 'jacksonville', 'getting', 'ace', 'sanders', 'from', 'usc']
cosine_similarity: 0.9286542534828186
train_input: [0.4073526042885674, 0.92865425], train_label: 1
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: My boy Ace Sanders to the Jaguars
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.57615236 0.40993715]
 [0.40993715 0.57615236 0.57615236 0.         0.         0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: My boy Ace Sanders to the Jaguars
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['my', 'boy', 'ace', 'sanders', 'to', 'the', 'jaguars']
cosine_similarity: 0.984742283821106
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Really hope the ChicagoBears have an Ace Sanders up their sleeve
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.57615236 0.
  0.40993715 0.        ]
 [0.31779954 0.44665616 0.44665616 0.         0.         0.44665616
  0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Really hope the ChicagoBears have an Ace Sanders up their sleeve
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['really', 'hope', 'the', 'chicagobears', 'have', 'an', 'ace', 'sanders', 'up', 'their', 'sleeve']
cosine_similarity: 0.9751799702644348
train_input: [0.2605556710562624, 0.97518], train_label: 0
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: The Jaguars have drafted Ace Sanders and the Houston Texans drafted DJ Swearinger
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.         0.57615236
  0.57615236 0.40993715 0.         0.        ]
 [0.2248583  0.3160305  0.632061   0.3160305  0.3160305  0.
  0.         0.2248583  0.3160305  0.3160305 ]]
pairwise_similarity: [[1.         0.18435554]
 [0.18435554 1.        ]]
cosine_similarity: 0.18435554192630063
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: The Jaguars have drafted Ace Sanders and the Houston Texans drafted DJ Swearinger
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['the', 'jaguars', 'have', 'drafted', 'ace', 'sanders', 'and', 'the', 'houston', 'texans', 'drafted', 'dj', 'swearinger']
cosine_similarity: 0.9060748815536499
train_input: [0.18435554192630063, 0.9060749], train_label: 0
TF_IDF_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Wasnt Ace Sanders supposed to go in the 7th round
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.57615236 0.         0.40993715
  0.         0.        ]
 [0.44665616 0.31779954 0.         0.         0.44665616 0.31779954
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I LOVE the Ace Sanders pick, sentence2: Wasnt Ace Sanders supposed to go in the 7th round
After tokenization, sentence1: ['i', 'love', 'the', 'ace', 'sanders', 'pick'], sentence2: ['wasnt', 'ace', 'sanders', 'supposed', 'to', 'go', 'in', 'the', 'round']
cosine_similarity: 0.9708906412124634
train_input: [0.2605556710562624, 0.97089064], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Also in South Africa actual money was spent on transportredoing airports etc
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.57615236 0.
  0.40993715 0.         0.        ]
 [0.4078241  0.29017021 0.4078241  0.         0.         0.4078241
  0.29017021 0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Also in South Africa actual money was spent on transportredoing airports etc
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['also', 'in', 'south', 'africa', 'actual', 'money', 'was', 'spent', 'on', 'airports', 'etc']
cosine_similarity: 0.9513804912567139
train_input: [0.23790309463326234, 0.9513805], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: I guess kids in Africa dont have it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633]
 [0.37997836 0.         0.         0.53404633 0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: I guess kids in Africa dont have it
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['i', 'guess', 'kids', 'in', 'africa', 'dont', 'have', 'it']
cosine_similarity: 0.9526703357696533
train_input: [0.1443835552773867, 0.95267034], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: I think South Africa is finally getting the picture
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.57615236 0.         0.         0.
  0.40993715 0.        ]
 [0.31779954 0.         0.         0.44665616 0.44665616 0.44665616
  0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: I think South Africa is finally getting the picture
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['i', 'think', 'south', 'africa', 'is', 'finally', 'getting', 'the', 'picture']
cosine_similarity: 0.9507110714912415
train_input: [0.2605556710562624, 0.9507111], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Is there an idealware for Africa
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.53404633]
 [0.57973867 0.         0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Is there an idealware for Africa
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['is', 'there', 'an', 'for', 'africa']
cosine_similarity: 0.940258800983429
train_input: [0.22028815056182965, 0.9402588], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Meet 1 of South Africa s Top Winemakers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.57615236 0.         0.40993715 0.        ]
 [0.40993715 0.         0.         0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Meet 1 of South Africa s Top Winemakers
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['meet', 'of', 'south', 'africa', 's', 'top', 'winemakers']
cosine_similarity: 0.9363332390785217
train_input: [0.3360969272762575, 0.93633324], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Miss Africa Utah Traditional outfit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.53404633
  0.         0.        ]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Miss Africa Utah Traditional outfit
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['miss', 'africa', 'utah', 'traditional', 'outfit']
cosine_similarity: 0.9283924102783203
train_input: [0.1273595297947935, 0.9283924], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Oh I was right daddy owns a few houses in South Africa
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.57615236 0.         0.         0.
  0.         0.         0.40993715]
 [0.29017021 0.         0.         0.4078241  0.4078241  0.4078241
  0.4078241  0.4078241  0.29017021]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Oh I was right daddy owns a few houses in South Africa
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['oh', 'i', 'was', 'right', 'daddy', 'owns', 'a', 'few', 'houses', 'in', 'south', 'africa']
cosine_similarity: 0.9663766026496887
train_input: [0.23790309463326234, 0.9663766], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Shout out to the prince of Africa AliDoee
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.         0.
  0.53404633]
 [0.37997836 0.53404633 0.         0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: Shout out to the prince of Africa AliDoee
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['shout', 'out', 'to', 'the', 'prince', 'of', 'africa']
cosine_similarity: 0.955331027507782
train_input: [0.1443835552773867, 0.955331], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: We are Africans in Africa
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.53404633]
 [0.57973867 0.81480247 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: We are Africans in Africa
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['we', 'are', 'africans', 'in', 'africa']
cosine_similarity: 0.9434396028518677
train_input: [0.22028815056182965, 0.9434396], train_label: 0
TF_IDF_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: bmthofficial are you ever going to preform in south Africa
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.57615236 0.         0.
  0.40993715]
 [0.35520009 0.         0.49922133 0.         0.49922133 0.49922133
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: COME TO SOUTH AFRICA ASSHOLE, sentence2: bmthofficial are you ever going to preform in south Africa
After tokenization, sentence1: ['come', 'to', 'south', 'africa', 'asshole'], sentence2: ['are', 'you', 'ever', 'going', 'to', 'preform', 'in', 'south', 'africa']
cosine_similarity: 0.9732812643051147
train_input: [0.29121941856368966, 0.97328126], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: AAAAAAAAAAAAAAHAHAHA the spirit of Al Davis lives on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.49922133 0.         0.49922133
  0.49922133 0.        ]
 [0.49922133 0.35520009 0.35520009 0.         0.49922133 0.
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: AAAAAAAAAAAAAAHAHAHA the spirit of Al Davis lives on
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['the', 'spirit', 'of', 'al', 'davis', 'lives', 'on']
cosine_similarity: 0.9592717885971069
train_input: [0.2523342014336961, 0.9592718], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: AL DAVIS IS BACK FROM THE DEAD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]
 [0.50154891 0.50154891 0.70490949 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: AL DAVIS IS BACK FROM THE DEAD
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['al', 'davis', 'is', 'back', 'from', 'the', 'dead']
cosine_similarity: 0.96404629945755
train_input: [0.3563004293331381, 0.9640463], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Al Davis is smiling down on the Raiders tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Al Davis is smiling down on the Raiders tonight
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['al', 'davis', 'is', 'smiling', 'down', 'on', 'the', 'raiders', 'tonight']
cosine_similarity: 0.9631280899047852
train_input: [0.2523342014336961, 0.9631281], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Al Davis might die but the Raaaiiiiders stay the same
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.         0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Al Davis might die but the Raaaiiiiders stay the same
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['al', 'davis', 'might', 'die', 'but', 'the', 'stay', 'the', 'same']
cosine_similarity: 0.9632412195205688
train_input: [0.2523342014336961, 0.9632412], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Al Davis put them in the biggest hole
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.49922133 0.49922133
  0.49922133]
 [0.40993715 0.57615236 0.40993715 0.57615236 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Al Davis put them in the biggest hole
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['al', 'davis', 'put', 'them', 'in', 'the', 'biggest', 'hole']
cosine_similarity: 0.9732681512832642
train_input: [0.29121941856368966, 0.97326815], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Al Davis s clone is making picks for the Raiders
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.49922133 0.49922133 0.
  0.49922133 0.         0.        ]
 [0.31779954 0.44665616 0.31779954 0.         0.         0.44665616
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Al Davis s clone is making picks for the Raiders
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['al', 'davis', 's', 'clone', 'is', 'making', 'picks', 'for', 'the', 'raiders']
cosine_similarity: 0.9835977554321289
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: DJ Hayden is an Al Davis CB
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.         0.49922133
  0.49922133 0.49922133]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.49922133 0.
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: DJ Hayden is an Al Davis CB
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['dj', 'hayden', 'is', 'an', 'al', 'davis', 'cb']
cosine_similarity: 0.9348213076591492
train_input: [0.2523342014336961, 0.9348213], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Proof the afterlife exists as Al Davis is still picking for the Raiders
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.         0.49922133 0.49922133
  0.49922133 0.         0.         0.        ]
 [0.4078241  0.29017021 0.29017021 0.4078241  0.         0.
  0.         0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: Proof the afterlife exists as Al Davis is still picking for the Raiders
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['proof', 'the', 'afterlife', 'exists', 'as', 'al', 'davis', 'is', 'still', 'picking', 'for', 'the', 'raiders']
cosine_similarity: 0.9741020202636719
train_input: [0.20613696606828605, 0.974102], train_label: 1
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: WO Al Davis s corpse waddling round Oakland the Raiders making good moves lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.49922133 0.
  0.49922133 0.         0.         0.         0.49922133 0.
  0.         0.         0.        ]
 [0.21440614 0.30134034 0.21440614 0.30134034 0.         0.30134034
  0.         0.30134034 0.30134034 0.30134034 0.         0.30134034
  0.30134034 0.30134034 0.30134034]]
pairwise_similarity: [[1.         0.15231416]
 [0.15231416 1.        ]]
cosine_similarity: 0.15231415519375907
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: WO Al Davis s corpse waddling round Oakland the Raiders making good moves lol
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['wo', 'al', 'davis', 's', 'corpse', 'waddling', 'round', 'oakland', 'the', 'raiders', 'making', 'good', 'moves', 'lol']
cosine_similarity: 0.9639059901237488
train_input: [0.15231415519375907, 0.963906], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: lol Jerry Jones is the new Al Davis
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.         0.         0.
  0.49922133 0.         0.49922133]
 [0.31779954 0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Hologram Al Davis should make the pick, sentence2: lol Jerry Jones is the new Al Davis
After tokenization, sentence1: ['hologram', 'al', 'davis', 'should', 'make', 'the', 'pick'], sentence2: ['lol', 'jerry', 'jones', 'is', 'the', 'new', 'al', 'davis']
cosine_similarity: 0.9667425155639648
train_input: [0.22576484600261604, 0.9667425], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: Amber is not as great as the judges think
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.         0.47107781
  0.         0.47107781]
 [0.37997836 0.         0.         0.53404633 0.53404633 0.
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: Amber is not as great as the judges think
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['amber', 'is', 'not', 'as', 'great', 'as', 'the', 'judges', 'think']
cosine_similarity: 0.9587147235870361
train_input: [0.1273595297947935, 0.9587147], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: Anything Amber does is current
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.47107781]
 [0.44943642 0.         0.6316672  0.         0.6316672  0.
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: Anything Amber does is current
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['anything', 'amber', 'does', 'is', 'current']
cosine_similarity: 0.9641311764717102
train_input: [0.15064018498706508, 0.9641312], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: How about getting Amber s name right
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.47107781]
 [0.44943642 0.         0.         0.6316672  0.         0.6316672
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: How about getting Amber s name right
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['how', 'about', 'getting', 'amber', 's', 'name', 'right']
cosine_similarity: 0.970919132232666
train_input: [0.15064018498706508, 0.97091913], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: I am dying to hear AMBER SING WHITNEY
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.         0.47107781
  0.         0.47107781 0.        ]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.
  0.47107781 0.         0.47107781]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: I am dying to hear AMBER SING WHITNEY
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['i', 'am', 'dying', 'to', 'hear', 'amber', 'sing', 'whitney']
cosine_similarity: 0.9136134386062622
train_input: [0.11234277891542777, 0.91361344], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: I just dont see why American votes for amber she s ugly and boring
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.         0.47107781 0.
  0.         0.47107781 0.47107781 0.         0.        ]
 [0.27894255 0.39204401 0.         0.39204401 0.         0.39204401
  0.39204401 0.         0.         0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536716
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: I just dont see why American votes for amber she s ugly and boring
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['i', 'just', 'dont', 'see', 'why', 'american', 'votes', 'for', 'amber', 'she', 's', 'ugly', 'and', 'boring']
cosine_similarity: 0.9640426635742188
train_input: [0.09349477497536716, 0.96404266], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: I thought that was only Amber
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.47107781 0.         0.47107781]
 [0.57973867 0.         0.         0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: I thought that was only Amber
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['i', 'thought', 'that', 'was', 'only', 'amber']
cosine_similarity: 0.9676687717437744
train_input: [0.19431434016858146, 0.9676688], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: The judges need to listen to Amber s last performance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.         0.47107781
  0.         0.         0.47107781]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.
  0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: The judges need to listen to Amber s last performance
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['the', 'judges', 'need', 'to', 'listen', 'to', 'amber', 's', 'last', 'performance']
cosine_similarity: 0.9352811574935913
train_input: [0.11234277891542777, 0.93528116], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: Well I couldnt disagree more with the judges on Amber s second song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.         0.47107781
  0.         0.         0.47107781]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.
  0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: Well I couldnt disagree more with the judges on Amber s second song
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['well', 'i', 'couldnt', 'disagree', 'more', 'with', 'the', 'judges', 'on', 'amber', 's', 'second', 'song']
cosine_similarity: 0.9591951370239258
train_input: [0.11234277891542777, 0.95919514], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: they still showing that Amber Cole Video wow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.         0.47107781 0.47107781 0.
  0.47107781 0.         0.        ]
 [0.33517574 0.         0.47107781 0.         0.         0.47107781
  0.         0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: they still showing that Amber Cole Video wow
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['they', 'still', 'showing', 'that', 'amber', 'cole', 'video', 'wow']
cosine_similarity: 0.9707391858100891
train_input: [0.11234277891542777, 0.9707392], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: why she choose amber then
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.         0.47107781 0.47107781 0.47107781]
 [0.57973867 0.         0.81480247 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Amber and Angie look totally different, sentence2: why she choose amber then
After tokenization, sentence1: ['amber', 'and', 'angie', 'look', 'totally', 'different'], sentence2: ['why', 'she', 'choose', 'amber', 'then']
cosine_similarity: 0.9680083394050598
train_input: [0.19431434016858146, 0.96800834], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: American Idol is just one retarded ass show now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.        ]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: American Idol is just one retarded ass show now
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['american', 'idol', 'is', 'just', 'one', 'retarded', 'ass', 'show', 'now']
cosine_similarity: 0.9704256057739258
train_input: [0.3563004293331381, 0.9704256], train_label: 1
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: For the first time in my life Im going to be hitting the American Idol texts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891 0.
  0.         0.         0.        ]
 [0.         0.26868528 0.37762778 0.37762778 0.26868528 0.37762778
  0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.26951761]
 [0.26951761 1.        ]]
cosine_similarity: 0.26951761324603224
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: For the first time in my life Im going to be hitting the American Idol texts
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['for', 'the', 'first', 'time', 'in', 'my', 'life', 'im', 'going', 'to', 'be', 'hitting', 'the', 'american', 'idol', 'texts']
cosine_similarity: 0.9741009473800659
train_input: [0.26951761324603224, 0.97410095], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: Is it just me or is American Idol really bad this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.        ]
 [0.         0.31779954 0.44665616 0.31779954 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: Is it just me or is American Idol really bad this year
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['is', 'it', 'just', 'me', 'or', 'is', 'american', 'idol', 'really', 'bad', 'this', 'year']
cosine_similarity: 0.9835506081581116
train_input: [0.31878402175377923, 0.9835506], train_label: 1
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: Jordan just may be the next American idol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.        ]
 [0.         0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: Jordan just may be the next American idol
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['jordan', 'just', 'may', 'be', 'the', 'next', 'american', 'idol']
cosine_similarity: 0.9781185984611511
train_input: [0.4112070550676187, 0.9781186], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: Nicki Minaj is so straight up and honest on American idol I love it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.         0.        ]
 [0.         0.29017021 0.4078241  0.29017021 0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: Nicki Minaj is so straight up and honest on American idol I love it
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['nicki', 'minaj', 'is', 'so', 'straight', 'up', 'and', 'honest', 'on', 'american', 'idol', 'i', 'love', 'it']
cosine_similarity: 0.982647180557251
train_input: [0.2910691023819054, 0.9826472], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: So Im watching American idol on live tv
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.         0.
  0.        ]
 [0.         0.31779954 0.31779954 0.44665616 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: So Im watching American idol on live tv
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['so', 'im', 'watching', 'american', 'idol', 'on', 'live', 'tv']
cosine_similarity: 0.9610201716423035
train_input: [0.31878402175377923, 0.9610202], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: The drummers for the duet on American Idol was killing it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891 0.        ]
 [0.         0.35520009 0.49922133 0.49922133 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: The drummers for the duet on American Idol was killing it
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['the', 'drummers', 'for', 'the', 'duet', 'on', 'american', 'idol', 'was', 'killing', 'it']
cosine_similarity: 0.9821044206619263
train_input: [0.3563004293331381, 0.9821044], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: The judges on American Idol are a joke
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.        ]
 [0.         0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: The judges on American Idol are a joke
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['the', 'judges', 'on', 'american', 'idol', 'are', 'a', 'joke']
cosine_similarity: 0.9699543118476868
train_input: [0.4112070550676187, 0.9699543], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: Who hired NICKIMINAJ for American Idol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.        ]
 [0.         0.40993715 0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: Who hired NICKIMINAJ for American Idol
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['who', 'hired', 'nickiminaj', 'for', 'american', 'idol']
cosine_similarity: 0.9125602841377258
train_input: [0.4112070550676187, 0.9125603], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: so I suppose the audiance will choose the american idol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891 0.        ]
 [0.         0.35520009 0.49922133 0.49922133 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Why is American idol still even on the air, sentence2: so I suppose the audiance will choose the american idol
After tokenization, sentence1: ['why', 'is', 'american', 'idol', 'still', 'even', 'on', 'the', 'air'], sentence2: ['so', 'i', 'suppose', 'the', 'audiance', 'will', 'choose', 'the', 'american', 'idol']
cosine_similarity: 0.9745285511016846
train_input: [0.3563004293331381, 0.97452855], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Allow amir khan wanna see him get knocked out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.4078241  0.4078241  0.4078241  0.29017021
  0.4078241  0.         0.         0.4078241 ]
 [0.49922133 0.35520009 0.         0.         0.         0.35520009
  0.         0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Allow amir khan wanna see him get knocked out
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['allow', 'amir', 'khan', 'wanna', 'see', 'him', 'get', 'knocked', 'out']
cosine_similarity: 0.965458869934082
train_input: [0.20613696606828605, 0.96545887], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Amir Khan is a world class boxer until he get s punched in the mouth
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.         0.         0.4078241  0.4078241  0.4078241
  0.29017021 0.4078241  0.         0.         0.4078241  0.        ]
 [0.29017021 0.4078241  0.4078241  0.         0.         0.
  0.29017021 0.         0.4078241  0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.        0.1683975]
 [0.1683975 1.       ]]
cosine_similarity: 0.16839750037215276
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Amir Khan is a world class boxer until he get s punched in the mouth
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['amir', 'khan', 'is', 'a', 'world', 'class', 'boxer', 'until', 'he', 'get', 's', 'punched', 'in', 'the', 'mouth']
cosine_similarity: 0.9734483361244202
train_input: [0.16839750037215276, 0.97344834], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Amir Khan just got dropped by Julio Diaz
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.         0.         0.4078241  0.4078241
  0.         0.         0.         0.29017021 0.4078241  0.4078241 ]
 [0.29017021 0.         0.4078241  0.4078241  0.         0.
  0.4078241  0.4078241  0.4078241  0.29017021 0.         0.        ]]
pairwise_similarity: [[1.        0.1683975]
 [0.1683975 1.       ]]
cosine_similarity: 0.16839750037215276
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Amir Khan just got dropped by Julio Diaz
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['amir', 'khan', 'just', 'got', 'dropped', 'by', 'julio', 'diaz']
cosine_similarity: 0.918846070766449
train_input: [0.16839750037215276, 0.9188461], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Amir khan s glass chin strikes again in the 4th round
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.         0.4078241  0.4078241  0.4078241
  0.         0.29017021 0.4078241  0.         0.         0.4078241 ]
 [0.4078241  0.29017021 0.4078241  0.         0.         0.
  0.4078241  0.29017021 0.         0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.        0.1683975]
 [0.1683975 1.       ]]
cosine_similarity: 0.16839750037215276
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Amir khan s glass chin strikes again in the 4th round
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['amir', 'khan', 's', 'glass', 'chin', 'strikes', 'again', 'in', 'the', 'round']
cosine_similarity: 0.9566377401351929
train_input: [0.16839750037215276, 0.95663774], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Diaz knocks down Amir Khan with a left hook in 4th Round
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.         0.29017021 0.4078241  0.         0.         0.
  0.4078241 ]
 [0.37762778 0.26868528 0.         0.37762778 0.         0.
  0.37762778 0.26868528 0.         0.37762778 0.37762778 0.37762778
  0.        ]]
pairwise_similarity: [[1.         0.15592893]
 [0.15592893 1.        ]]
cosine_similarity: 0.15592892548708362
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Diaz knocks down Amir Khan with a left hook in 4th Round
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['diaz', 'knocks', 'down', 'amir', 'khan', 'with', 'a', 'left', 'hook', 'in', 'round']
cosine_similarity: 0.9584503769874573
train_input: [0.15592892548708362, 0.9584504], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Great first two rounds by Amir Khan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.4078241  0.4078241  0.         0.29017021
  0.4078241  0.         0.4078241 ]
 [0.40993715 0.         0.         0.         0.57615236 0.40993715
  0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Great first two rounds by Amir Khan
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['great', 'first', 'two', 'rounds', 'by', 'amir', 'khan']
cosine_similarity: 0.964633584022522
train_input: [0.23790309463326234, 0.9646336], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Hope Amir Khan gets banged out tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.         0.4078241  0.4078241  0.4078241  0.
  0.         0.29017021 0.4078241  0.         0.4078241 ]
 [0.31779954 0.44665616 0.         0.         0.         0.44665616
  0.44665616 0.31779954 0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Hope Amir Khan gets banged out tonight
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['hope', 'amir', 'khan', 'gets', 'banged', 'out', 'tonight']
cosine_similarity: 0.9583526849746704
train_input: [0.18443191662261305, 0.9583527], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Is Amir Khan just a lighter Audley Harrison
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.         0.4078241  0.4078241  0.4078241  0.
  0.         0.29017021 0.4078241  0.         0.4078241 ]
 [0.31779954 0.44665616 0.         0.         0.         0.44665616
  0.44665616 0.31779954 0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: Is Amir Khan just a lighter Audley Harrison
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['is', 'amir', 'khan', 'just', 'a', 'lighter', 'audley', 'harrison']
cosine_similarity: 0.9290744662284851
train_input: [0.18443191662261305, 0.92907447], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: My TL right now is all about Amir Khan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.4078241  0.4078241  0.29017021 0.4078241
  0.         0.         0.4078241 ]
 [0.40993715 0.         0.         0.         0.40993715 0.
  0.57615236 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: My TL right now is all about Amir Khan
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['my', 'tl', 'right', 'now', 'is', 'all', 'about', 'amir', 'khan']
cosine_similarity: 0.9642756581306458
train_input: [0.23790309463326234, 0.96427566], train_label: 0
TF_IDF_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: what time is amir khan playing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.4078241  0.4078241  0.29017021 0.4078241
  0.         0.         0.4078241 ]
 [0.40993715 0.         0.         0.         0.40993715 0.
  0.57615236 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Amir Khan Is Winning This Fight Easily Despite The Knock Down, sentence2: what time is amir khan playing
After tokenization, sentence1: ['amir', 'khan', 'is', 'winning', 'this', 'fight', 'easily', 'despite', 'the', 'knock', 'down'], sentence2: ['what', 'time', 'is', 'amir', 'khan', 'playing']
cosine_similarity: 0.9618861079216003
train_input: [0.23790309463326234, 0.9618861], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: Anyone know of any alternatives to LiveView when using an Android phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.         0.81480247 0.
  0.        ]
 [0.4261596  0.30321606 0.4261596  0.4261596  0.         0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: Anyone know of any alternatives to LiveView when using an Android phone
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['anyone', 'know', 'of', 'any', 'alternatives', 'to', 'when', 'using', 'an', 'android', 'phone']
cosine_similarity: 0.9467961192131042
train_input: [0.17578607839334617, 0.9467961], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: Anyone using Line2 on their iPhone or Android devices
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247 0.        ]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.         0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: Anyone using Line2 on their iPhone or Android devices
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['anyone', 'using', 'on', 'their', 'iphone', 'or', 'android', 'devices']
cosine_similarity: 0.9294341206550598
train_input: [0.19431434016858146, 0.9294341], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: Better off with my android
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.81480247]
 [0.57973867 0.81480247 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: Better off with my android
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['better', 'off', 'with', 'my', 'android']
cosine_similarity: 0.9462121725082397
train_input: [0.3360969272762574, 0.9462122], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: Creating 3 android screens with the functionalities as at
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.81480247 0.        ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: Creating 3 android screens with the functionalities as at
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['creating', 'android', 'screens', 'with', 'the', 'as', 'at']
cosine_similarity: 0.9059579372406006
train_input: [0.22028815056182965, 0.90595794], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: EXOK HQ wallpapers for PC Android and iPhone devices from Ivy Club are now available
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.
  0.         0.         0.81480247 0.         0.        ]
 [0.23076793 0.32433627 0.32433627 0.32433627 0.32433627 0.32433627
  0.32433627 0.32433627 0.         0.32433627 0.32433627]]
pairwise_similarity: [[1.         0.13378509]
 [0.13378509 1.        ]]
cosine_similarity: 0.1337850929463124
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: EXOK HQ wallpapers for PC Android and iPhone devices from Ivy Club are now available
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['exok', 'hq', 'wallpapers', 'for', 'pc', 'android', 'and', 'iphone', 'devices', 'from', 'ivy', 'club', 'are', 'now', 'available']
cosine_similarity: 0.8997479677200317
train_input: [0.1337850929463124, 0.89974797], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: I dont want an iPhone anymore but I dont want an Android
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247 0.        ]
 [0.21951095 0.30851498 0.61702996 0.30851498 0.         0.61702996]]
pairwise_similarity: [[1.         0.12725899]
 [0.12725899 1.        ]]
cosine_similarity: 0.12725898700975247
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: I dont want an iPhone anymore but I dont want an Android
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['i', 'dont', 'want', 'an', 'iphone', 'anymore', 'but', 'i', 'dont', 'want', 'an', 'android']
cosine_similarity: 0.9422153830528259
train_input: [0.12725898700975247, 0.9422154], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: Link to IVY Club s Wallpaper for PC Android iPhone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.81480247
  0.         0.        ]
 [0.27894255 0.39204401 0.39204401 0.39204401 0.39204401 0.
  0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.16171378]
 [0.16171378 1.        ]]
cosine_similarity: 0.16171378066252898
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: Link to IVY Club s Wallpaper for PC Android iPhone
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['link', 'to', 'ivy', 'club', 's', 'wallpaper', 'for', 'pc', 'android', 'iphone']
cosine_similarity: 0.9241742491722107
train_input: [0.16171378066252898, 0.92417425], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: Tennis in the Face goes for the Grand Slam now on Android and
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247 0.
  0.        ]
 [0.30321606 0.4261596  0.4261596  0.4261596  0.         0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: Tennis in the Face goes for the Grand Slam now on Android and
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['tennis', 'in', 'the', 'face', 'goes', 'for', 'the', 'grand', 'slam', 'now', 'on', 'android', 'and']
cosine_similarity: 0.9263190031051636
train_input: [0.17578607839334617, 0.926319], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: Trade Forex with the All New iPhone iPad and Android App from Vantage FX
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.
  0.81480247 0.         0.         0.        ]
 [0.24395573 0.34287126 0.34287126 0.34287126 0.34287126 0.34287126
  0.         0.34287126 0.34287126 0.34287126]]
pairwise_similarity: [[1.         0.14143057]
 [0.14143057 1.        ]]
cosine_similarity: 0.14143056792554487
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: Trade Forex with the All New iPhone iPad and Android App from Vantage FX
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['trade', 'forex', 'with', 'the', 'all', 'new', 'iphone', 'ipad', 'and', 'android', 'app', 'from', 'vantage', 'fx']
cosine_similarity: 0.908851146697998
train_input: [0.14143056792554487, 0.90885115], train_label: 0
TF_IDF_cosine_similarity: sentence1: no more on android lolol, sentence2: do you know how to install Android ICS or JB on Nokia n9
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.81480247
  0.         0.        ]
 [0.27894255 0.39204401 0.39204401 0.39204401 0.39204401 0.
  0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.16171378]
 [0.16171378 1.        ]]
cosine_similarity: 0.16171378066252898
word_to_vector_cosine_similarity: sentence1: no more on android lolol, sentence2: do you know how to install Android ICS or JB on Nokia n9
After tokenization, sentence1: ['no', 'more', 'on', 'android', 'lolol'], sentence2: ['do', 'you', 'know', 'how', 'to', 'install', 'android', 'ics', 'or', 'jb', 'on', 'nokia']
cosine_similarity: 0.9595538973808289
train_input: [0.16171378066252898, 0.9595539], train_label: 0
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid has ate the first pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.35520009 0.49922133
  0.49922133]
 [0.40993715 0.57615236 0.         0.57615236 0.40993715 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid has ate the first pick
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['andy', 'reid', 'has', 'ate', 'the', 'first', 'pick']
cosine_similarity: 0.960789144039154
train_input: [0.29121941856368966, 0.96078914], train_label: 0
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid is about to draft a KFC buffet with the first overall pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.         0.
  0.         0.35520009 0.49922133 0.49922133]
 [0.29017021 0.         0.4078241  0.4078241  0.4078241  0.4078241
  0.4078241  0.29017021 0.         0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid is about to draft a KFC buffet with the first overall pick
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['andy', 'reid', 'is', 'about', 'to', 'draft', 'a', 'kfc', 'buffet', 'with', 'the', 'first', 'overall', 'pick']
cosine_similarity: 0.9666514992713928
train_input: [0.20613696606828605, 0.9666515], train_label: 0
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid s stellar draft performance continues
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.         0.35520009
  0.         0.49922133 0.49922133]
 [0.31779954 0.         0.44665616 0.44665616 0.44665616 0.31779954
  0.44665616 0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid s stellar draft performance continues
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['andy', 'reid', 's', 'stellar', 'draft', 'performance', 'continues']
cosine_similarity: 0.8917222619056702
train_input: [0.22576484600261604, 0.89172226], train_label: 0
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid taking his time making the Eagles sweat their pick out a little longer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.         0.         0.         0.
  0.         0.37930349 0.         0.         0.37930349 0.53309782]
 [0.24377685 0.         0.34261985 0.34261985 0.34261985 0.34261985
  0.34261985 0.24377685 0.34261985 0.34261985 0.24377685 0.        ]]
pairwise_similarity: [[1.         0.27739623]
 [0.27739623 1.        ]]
cosine_similarity: 0.27739622897624144
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid taking his time making the Eagles sweat their pick out a little longer
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['andy', 'reid', 'taking', 'his', 'time', 'making', 'the', 'eagles', 'sweat', 'their', 'pick', 'out', 'a', 'little', 'longer']
cosine_similarity: 0.9664373397827148
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid thought he was going to the Pro Bowl tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.         0.35520009
  0.         0.49922133 0.         0.49922133]
 [0.29017021 0.         0.4078241  0.4078241  0.4078241  0.29017021
  0.4078241  0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Andy Reid thought he was going to the Pro Bowl tonight
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['andy', 'reid', 'thought', 'he', 'was', 'going', 'to', 'the', 'pro', 'bowl', 'tonight']
cosine_similarity: 0.9674245119094849
train_input: [0.20613696606828605, 0.9674245], train_label: 0
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Chiefs got Andy Reid and they bringing in Alex Smith
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.         0.         0.
  0.35520009 0.         0.49922133 0.49922133]
 [0.4078241  0.29017021 0.         0.4078241  0.4078241  0.4078241
  0.29017021 0.4078241  0.         0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Chiefs got Andy Reid and they bringing in Alex Smith
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['chiefs', 'got', 'andy', 'reid', 'and', 'they', 'bringing', 'in', 'alex', 'smith']
cosine_similarity: 0.9661960005760193
train_input: [0.20613696606828605, 0.966196], train_label: 0
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: If Andy Reid was smart hed pick up the homie lutzenkirchen here
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.         0.
  0.35520009 0.         0.49922133 0.49922133]
 [0.29017021 0.         0.4078241  0.4078241  0.4078241  0.4078241
  0.29017021 0.4078241  0.         0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: If Andy Reid was smart hed pick up the homie lutzenkirchen here
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['if', 'andy', 'reid', 'was', 'smart', 'hed', 'pick', 'up', 'the', 'homie', 'here']
cosine_similarity: 0.9717152118682861
train_input: [0.20613696606828605, 0.9717152], train_label: 0
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Is Andy Reid waiting on the Bills to trade him a bucket of KFC
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.         0.35520009
  0.49922133 0.         0.49922133 0.        ]
 [0.29017021 0.         0.4078241  0.4078241  0.4078241  0.29017021
  0.         0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Is Andy Reid waiting on the Bills to trade him a bucket of KFC
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['is', 'andy', 'reid', 'waiting', 'on', 'the', 'bills', 'to', 'trade', 'him', 'a', 'bucket', 'of', 'kfc']
cosine_similarity: 0.9666840434074402
train_input: [0.20613696606828605, 0.96668404], train_label: 0
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Where has Andy Reid gone though
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.35520009 0.49922133 0.49922133]
 [0.50154891 0.         0.70490949 0.50154891 0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: Where has Andy Reid gone though
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['where', 'has', 'andy', 'reid', 'gone', 'though']
cosine_similarity: 0.9732884764671326
train_input: [0.3563004293331381, 0.9732885], train_label: 0
TF_IDF_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: it would be like Andy Reid to let the clock expire
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.         0.
  0.35520009 0.49922133 0.49922133]
 [0.31779954 0.         0.44665616 0.44665616 0.44665616 0.44665616
  0.31779954 0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Unfortunately Andy Reid is aware that the time is his, sentence2: it would be like Andy Reid to let the clock expire
After tokenization, sentence1: ['unfortunately', 'andy', 'reid', 'is', 'aware', 'that', 'the', 'time', 'is', 'his'], sentence2: ['it', 'would', 'be', 'like', 'andy', 'reid', 'to', 'let', 'the', 'clock', 'expire']
cosine_similarity: 0.976749837398529
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Angie you are fantastic I love hearing you sing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.53404633 0.
  0.53404633 0.        ]
 [0.33517574 0.         0.47107781 0.47107781 0.         0.47107781
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Angie you are fantastic I love hearing you sing
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['angie', 'you', 'are', 'fantastic', 'i', 'love', 'hearing', 'you', 'sing']
cosine_similarity: 0.9686126708984375
train_input: [0.1273595297947935, 0.9686127], train_label: 0
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: By my fav Candice Angie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]
 [0.44943642 0.6316672  0.         0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: By my fav Candice Angie
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['by', 'my', 'fav', 'candice', 'angie']
cosine_similarity: 0.9184028506278992
train_input: [0.17077611319011649, 0.91840285], train_label: 0
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Can Angie be my best friend though
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]
 [0.44943642 0.6316672  0.         0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Can Angie be my best friend though
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['can', 'angie', 'be', 'my', 'best', 'friend', 'though']
cosine_similarity: 0.9576384425163269
train_input: [0.17077611319011649, 0.95763844], train_label: 0
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Candice Angie are about to do my favorite song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.53404633
  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Candice Angie are about to do my favorite song
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['candice', 'angie', 'are', 'about', 'to', 'do', 'my', 'favorite', 'song']
cosine_similarity: 0.9546242356300354
train_input: [0.1443835552773867, 0.95462424], train_label: 0
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: I like Angie more than Amber especially after that performance of Halo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.         0.53404633
  0.         0.53404633 0.        ]
 [0.4261596  0.30321606 0.         0.4261596  0.4261596  0.
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: I like Angie more than Amber especially after that performance of Halo
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['i', 'like', 'angie', 'more', 'than', 'amber', 'especially', 'after', 'that', 'performance', 'of', 'halo']
cosine_similarity: 0.9475551843643188
train_input: [0.11521554337793122, 0.9475552], train_label: 0
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Nic you just called amber angie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.53404633 0.        ]
 [0.47107781 0.33517574 0.47107781 0.         0.47107781 0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Nic you just called amber angie
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['nic', 'you', 'just', 'called', 'amber', 'angie']
cosine_similarity: 0.9700741767883301
train_input: [0.1273595297947935, 0.9700742], train_label: 0
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Oh my gosh Angie Miller
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.53404633
  0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Oh my gosh Angie Miller
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['oh', 'my', 'gosh', 'angie', 'miller']
cosine_similarity: 0.9299483299255371
train_input: [0.1443835552773867, 0.92994833], train_label: 0
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Ummm NICKIMINAJ just called Amber Angie on AmericanIdol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37997836 0.         0.53404633 0.
  0.53404633 0.53404633 0.         0.        ]
 [0.39204401 0.39204401 0.27894255 0.39204401 0.         0.39204401
  0.         0.         0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: Ummm NICKIMINAJ just called Amber Angie on AmericanIdol
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['ummm', 'nickiminaj', 'just', 'called', 'amber', 'angie', 'on', 'americanidol']
cosine_similarity: 0.9286195039749146
train_input: [0.1059921313509325, 0.9286195], train_label: 0
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: angie was so much better than amber
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]
 [0.6316672  0.44943642 0.6316672  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: angie was so much better than amber
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['angie', 'was', 'so', 'much', 'better', 'than', 'amber']
cosine_similarity: 0.9660047292709351
train_input: [0.17077611319011649, 0.9660047], train_label: 0
TF_IDF_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: that Angie and Candice duet though
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]
 [0.44943642 0.6316672  0.         0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I dont mind angie and kree, sentence2: that Angie and Candice duet though
After tokenization, sentence1: ['i', 'dont', 'mind', 'angie', 'and', 'kree'], sentence2: ['that', 'angie', 'and', 'candice', 'duet', 'though']
cosine_similarity: 0.9724990129470825
train_input: [0.17077611319011649, 0.972499], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez had himself a game tonight by the way
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.44665616 0.         0.31779954
  0.44665616 0.         0.        ]
 [0.         0.35520009 0.         0.         0.49922133 0.35520009
  0.         0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez had himself a game tonight by the way
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['anibal', 'sanchez', 'had', 'himself', 'a', 'game', 'tonight', 'by', 'the', 'way']
cosine_similarity: 0.8854877352714539
train_input: [0.22576484600261604, 0.88548774], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez has more strikeouts then the tigers have runs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.33471228 0.47042643 0.47042643 0.         0.33471228
  0.33471228 0.        ]
 [0.         0.37930349 0.         0.         0.53309782 0.37930349
  0.37930349 0.53309782]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez has more strikeouts then the tigers have runs
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['anibal', 'sanchez', 'has', 'more', 'strikeouts', 'then', 'the', 'tigers', 'have', 'runs']
cosine_similarity: 0.899772584438324
train_input: [0.3808726084759436, 0.8997726], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez is a strikeout machine tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.44665616 0.         0.31779954
  0.         0.44665616 0.        ]
 [0.         0.35520009 0.         0.         0.49922133 0.35520009
  0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez is a strikeout machine tonight
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight']
cosine_similarity: 0.9171978831291199
train_input: [0.22576484600261604, 0.9171979], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez is kind of good
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.44665616 0.         0.
  0.31779954 0.44665616]
 [0.         0.40993715 0.         0.         0.57615236 0.57615236
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez is kind of good
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['anibal', 'sanchez', 'is', 'kind', 'of', 'good']
cosine_similarity: 0.8527467250823975
train_input: [0.2605556710562624, 0.8527467], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez leaves the 8th inning with career high 17 Ks and franchise record
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.33471228 0.47042643 0.47042643 0.
  0.         0.         0.         0.         0.         0.
  0.33471228 0.47042643]
 [0.2306165  0.32412345 0.2306165  0.         0.         0.32412345
  0.32412345 0.32412345 0.32412345 0.32412345 0.32412345 0.32412345
  0.2306165  0.        ]]
pairwise_similarity: [[1.         0.23157053]
 [0.23157053 1.        ]]
cosine_similarity: 0.23157052612297288
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez leaves the 8th inning with career high 17 Ks and franchise record
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['anibal', 'sanchez', 'leaves', 'the', 'inning', 'with', 'career', 'high', 'ks', 'and', 'franchise', 'record']
cosine_similarity: 0.8999365568161011
train_input: [0.23157052612297288, 0.89993656], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez setting a franchise record for strikeouts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.33471228 0.47042643 0.47042643 0.         0.
  0.33471228 0.         0.33471228]
 [0.         0.33471228 0.         0.         0.47042643 0.47042643
  0.33471228 0.47042643 0.33471228]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: Anibal Sanchez setting a franchise record for strikeouts
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['anibal', 'sanchez', 'setting', 'a', 'franchise', 'record', 'for', 'strikeouts']
cosine_similarity: 0.8877872228622437
train_input: [0.3360969272762574, 0.8877872], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: New strikeout record for Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.44665616 0.         0.
  0.31779954 0.         0.44665616]
 [0.         0.35520009 0.         0.         0.49922133 0.49922133
  0.35520009 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: New strikeout record for Anibal Sanchez
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['new', 'strikeout', 'record', 'for', 'anibal', 'sanchez']
cosine_similarity: 0.8898441195487976
train_input: [0.22576484600261604, 0.8898441], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: anibal sanchez tigers record 17 Wow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.47042643 0.         0.33471228
  0.47042643 0.         0.        ]
 [0.33471228 0.33471228 0.         0.         0.47042643 0.33471228
  0.         0.47042643 0.47042643]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.33609692727625745
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: anibal sanchez tigers record 17 Wow
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['anibal', 'sanchez', 'tigers', 'record', 'wow']
cosine_similarity: 0.9087653756141663
train_input: [0.33609692727625745, 0.9087654], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: i started anibal sanchez tonight in my other league
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.44665616 0.         0.31779954
  0.         0.44665616 0.        ]
 [0.         0.35520009 0.         0.         0.49922133 0.35520009
  0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: i started anibal sanchez tonight in my other league
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['i', 'started', 'anibal', 'sanchez', 'tonight', 'in', 'my', 'other', 'league']
cosine_similarity: 0.8661032915115356
train_input: [0.22576484600261604, 0.8661033], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: or I could have anibal Sanchez drop 17 k s on the braves
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.47042643 0.         0.
  0.33471228 0.47042643]
 [0.37930349 0.37930349 0.         0.         0.53309782 0.53309782
  0.37930349 0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez 17 strikeouts bad ass, sentence2: or I could have anibal Sanchez drop 17 k s on the braves
After tokenization, sentence1: ['anibal', 'sanchez', 'strikeouts', 'bad', 'ass'], sentence2: ['or', 'i', 'could', 'have', 'anibal', 'sanchez', 'drop', 'k', 's', 'on', 'the', 'braves']
cosine_similarity: 0.8460105657577515
train_input: [0.38087260847594373, 0.84601057], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: 14 strikeouts through seven for Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53309782 0.37930349 0.53309782 0.37930349 0.
  0.37930349]
 [0.53309782 0.         0.37930349 0.         0.37930349 0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: 14 strikeouts through seven for Anibal Sanchez
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['strikeouts', 'through', 'seven', 'for', 'anibal', 'sanchez']
cosine_similarity: 0.9644617438316345
train_input: [0.43161341897075145, 0.96446174], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez has 14 strikeouts in only 7 innings for the Tigers tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53309782 0.37930349 0.53309782 0.         0.37930349
  0.37930349 0.         0.        ]
 [0.42567716 0.         0.30287281 0.         0.42567716 0.30287281
  0.30287281 0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez has 14 strikeouts in only 7 innings for the Tigers tonight
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'has', 'strikeouts', 'in', 'only', 'innings', 'for', 'the', 'tigers', 'tonight']
cosine_similarity: 0.9511115550994873
train_input: [0.34464214103805474, 0.95111156], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez sets the franchise record with 17 Ks in a 9inning game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.37930349 0.         0.53309782 0.
  0.         0.         0.37930349 0.         0.53309782]
 [0.25948224 0.36469323 0.25948224 0.36469323 0.         0.36469323
  0.36469323 0.36469323 0.25948224 0.36469323 0.        ]]
pairwise_similarity: [[1.         0.29526756]
 [0.29526756 1.        ]]
cosine_similarity: 0.2952675553824053
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez sets the franchise record with 17 Ks in a 9inning game
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'sets', 'the', 'franchise', 'record', 'with', 'ks', 'in', 'a', 'game']
cosine_similarity: 0.9307876229286194
train_input: [0.2952675553824053, 0.9307876], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez struck out 17 in eight marvelous innings vs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.         0.         0.37930349
  0.53309782 0.         0.        ]
 [0.30287281 0.30287281 0.         0.42567716 0.42567716 0.30287281
  0.         0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez struck out 17 in eight marvelous innings vs
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'struck', 'out', 'in', 'eight', 'marvelous', 'innings', 'vs']
cosine_similarity: 0.9439484477043152
train_input: [0.34464214103805474, 0.94394845], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez struck out 17 tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.37930349 0.53309782 0.
  0.        ]
 [0.37930349 0.37930349 0.         0.37930349 0.         0.53309782
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez struck out 17 tonight
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'struck', 'out', 'tonight']
cosine_similarity: 0.9513858556747437
train_input: [0.43161341897075145, 0.95138586], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez wins tonight s award for Happiest Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.49922133 0.         0.35520009
  0.49922133 0.         0.        ]
 [0.         0.27840869 0.39129369 0.         0.39129369 0.55681737
  0.         0.39129369 0.39129369]]
pairwise_similarity: [[1.         0.29667237]
 [0.29667237 1.        ]]
cosine_similarity: 0.29667236689709053
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: Anibal Sanchez wins tonight s award for Happiest Sanchez
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'wins', 'tonight', 's', 'award', 'for', 'happiest', 'sanchez']
cosine_similarity: 0.9260345101356506
train_input: [0.29667236689709053, 0.9260345], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: What a performance by Anibal Sanchez tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.         0.35520009 0.49922133
  0.        ]
 [0.         0.40993715 0.         0.57615236 0.40993715 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: What a performance by Anibal Sanchez tonight
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['what', 'a', 'performance', 'by', 'anibal', 'sanchez', 'tonight']
cosine_similarity: 0.924997091293335
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: With that 16th strikeout Anibal Sanchez set the Comerica Park record for Strikeouts in a game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53309782 0.37930349 0.         0.53309782 0.
  0.         0.         0.37930349 0.         0.         0.37930349]
 [0.34261985 0.         0.24377685 0.34261985 0.         0.34261985
  0.34261985 0.34261985 0.24377685 0.34261985 0.34261985 0.24377685]]
pairwise_similarity: [[1.         0.27739623]
 [0.27739623 1.        ]]
cosine_similarity: 0.27739622897624144
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: With that 16th strikeout Anibal Sanchez set the Comerica Park record for Strikeouts in a game
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['with', 'that', 'strikeout', 'anibal', 'sanchez', 'set', 'the', 'comerica', 'park', 'record', 'for', 'strikeouts', 'in', 'a', 'game']
cosine_similarity: 0.9367504119873047
train_input: [0.27739622897624144, 0.9367504], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: anibal Sanchez has 17 K s in 8 innings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.         0.37930349 0.53309782]
 [0.44832087 0.44832087 0.         0.63009934 0.44832087 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: anibal Sanchez has 17 K s in 8 innings
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'has', 'k', 's', 'in', 'innings']
cosine_similarity: 0.9050077199935913
train_input: [0.5101490193104813, 0.9050077], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: having Anibal Sanchez and his 17 Ks on your fantasy team
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.         0.
  0.37930349 0.53309782 0.        ]
 [0.30287281 0.30287281 0.42567716 0.         0.42567716 0.42567716
  0.30287281 0.         0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: 17 fucking strikeouts for Anibal Sanchez, sentence2: having Anibal Sanchez and his 17 Ks on your fantasy team
After tokenization, sentence1: ['fucking', 'strikeouts', 'for', 'anibal', 'sanchez'], sentence2: ['having', 'anibal', 'sanchez', 'and', 'his', 'ks', 'on', 'your', 'fantasy', 'team']
cosine_similarity: 0.916124165058136
train_input: [0.34464214103805474, 0.91612417], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez has just set the club record for strikeouts in a game with 17
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.28986934 0.28986934 0.         0.         0.         0.
  0.28986934 0.         0.28986934 0.81480247]
 [0.26844636 0.26844636 0.37729199 0.37729199 0.37729199 0.37729199
  0.26844636 0.37729199 0.26844636 0.        ]]
pairwise_similarity: [[1.         0.31125747]
 [0.31125747 1.        ]]
cosine_similarity: 0.3112574675270537
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez has just set the club record for strikeouts in a game with 17
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['anibal', 'sanchez', 'has', 'just', 'set', 'the', 'club', 'record', 'for', 'strikeouts', 'in', 'a', 'game', 'with']
cosine_similarity: 0.9261141419410706
train_input: [0.3112574675270537, 0.92611414], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez is an absolute stud
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.         0.26868528 0.26868528 0.37762778 0.
  0.75525556]
 [0.         0.57615236 0.40993715 0.40993715 0.         0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez is an absolute stud
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['anibal', 'sanchez', 'is', 'an', 'absolute', 'stud']
cosine_similarity: 0.8667621612548828
train_input: [0.2202881505618297, 0.86676216], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez is killing it tonight son
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44554752 0.31701073 0.         0.31701073 0.         0.44554752
  0.63402146]
 [0.         0.37930349 0.53309782 0.37930349 0.53309782 0.
  0.37930349]]
pairwise_similarity: [[1.         0.48097311]
 [0.48097311 1.        ]]
cosine_similarity: 0.4809731079601443
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez is killing it tonight son
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['anibal', 'sanchez', 'is', 'killing', 'it', 'tonight', 'son']
cosine_similarity: 0.9079931378364563
train_input: [0.4809731079601443, 0.90799314], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez is throwing a gem
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.26868528 0.37762778 0.
  0.75525556]
 [0.         0.40993715 0.57615236 0.40993715 0.         0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez is throwing a gem
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['anibal', 'sanchez', 'is', 'throwing', 'a', 'gem']
cosine_similarity: 0.9116469025611877
train_input: [0.2202881505618297, 0.9116469], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez sets the franchise record with 17 Ks in a 9inning game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27867523 0.         0.27867523 0.         0.         0.
  0.         0.27867523 0.         0.39166832 0.78333663]
 [0.25948224 0.36469323 0.25948224 0.36469323 0.36469323 0.36469323
  0.36469323 0.25948224 0.36469323 0.         0.        ]]
pairwise_similarity: [[1.         0.21693382]
 [0.21693382 1.        ]]
cosine_similarity: 0.2169338170113969
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Anibal Sanchez sets the franchise record with 17 Ks in a 9inning game
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['anibal', 'sanchez', 'sets', 'the', 'franchise', 'record', 'with', 'ks', 'in', 'a', 'game']
cosine_similarity: 0.9338923096656799
train_input: [0.2169338170113969, 0.9338923], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Braves cant hit Anibal Sanchez throws
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.         0.26868528 0.37762778
  0.         0.75525556]
 [0.         0.35520009 0.49922133 0.49922133 0.35520009 0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Braves cant hit Anibal Sanchez throws
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['braves', 'cant', 'hit', 'anibal', 'sanchez', 'throws']
cosine_similarity: 0.9222545027732849
train_input: [0.1908740661302035, 0.9222545], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: How bout anibal Sanchez with 17 strikeouts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.28986934 0.28986934 0.         0.28986934 0.28986934 0.81480247]
 [0.4090901  0.4090901  0.57496187 0.4090901  0.4090901  0.        ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: How bout anibal Sanchez with 17 strikeouts
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['how', 'bout', 'anibal', 'sanchez', 'with', 'strikeouts']
cosine_similarity: 0.9437060952186584
train_input: [0.4743307064971939, 0.9437061], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Make that 14 strikeouts of Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.39166832 0.27867523 0.         0.27867523 0.27867523
  0.78333663]
 [0.53309782 0.         0.37930349 0.53309782 0.37930349 0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Make that 14 strikeouts of Anibal Sanchez
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['make', 'that', 'strikeouts', 'of', 'anibal', 'sanchez']
cosine_similarity: 0.929664134979248
train_input: [0.31710746658027095, 0.92966413], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: The Marlins traded Anibal Sanchez to the Tigers last year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.26868528 0.37762778 0.
  0.75525556 0.         0.        ]
 [0.         0.31779954 0.44665616 0.31779954 0.         0.44665616
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: The Marlins traded Anibal Sanchez to the Tigers last year
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['the', 'marlins', 'traded', 'anibal', 'sanchez', 'to', 'the', 'tigers', 'last', 'year']
cosine_similarity: 0.9611746072769165
train_input: [0.1707761131901165, 0.9611746], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Tigers 10 Braves 0 Anibal Sanchez strikes out 17 in 8 innings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.27867523 0.27867523 0.         0.         0.27867523
  0.39166832 0.         0.         0.78333663]
 [0.39166832 0.27867523 0.27867523 0.39166832 0.39166832 0.27867523
  0.         0.39166832 0.39166832 0.        ]]
pairwise_similarity: [[1.         0.23297965]
 [0.23297965 1.        ]]
cosine_similarity: 0.2329796548048752
word_to_vector_cosine_similarity: sentence1: 17 strikeouts tonight for Anibal Sanchez tonight, sentence2: Tigers 10 Braves 0 Anibal Sanchez strikes out 17 in 8 innings
After tokenization, sentence1: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight'], sentence2: ['tigers', 'braves', 'anibal', 'sanchez', 'strikes', 'out', 'in', 'innings']
cosine_similarity: 0.9486512541770935
train_input: [0.2329796548048752, 0.94865125], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: 17 strikeouts tonight for Anibal Sanchez tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.47042643 0.47042643 0.33471228
  0.         0.        ]
 [0.27867523 0.27867523 0.         0.         0.         0.27867523
  0.39166832 0.78333663]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: 17 strikeouts tonight for Anibal Sanchez tonight
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['strikeouts', 'tonight', 'for', 'anibal', 'sanchez', 'tonight']
cosine_similarity: 0.922100305557251
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez has 17 strikeouts through 8 innings for the Tigers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.         0.47042643 0.47042643
  0.33471228 0.         0.        ]
 [0.33471228 0.33471228 0.         0.47042643 0.         0.
  0.33471228 0.47042643 0.47042643]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez has 17 strikeouts through 8 innings for the Tigers
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'has', 'strikeouts', 'through', 'innings', 'for', 'the', 'tigers']
cosine_similarity: 0.9208234548568726
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez has set the Tigers franchise record with 17 Ks tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829
  0.         0.         0.        ]
 [0.2895694  0.2895694  0.2895694  0.2895694  0.2895694  0.2895694
  0.40697968 0.40697968 0.40697968]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062739
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez has set the Tigers franchise record with 17 Ks tonight
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'has', 'set', 'the', 'tigers', 'franchise', 'record', 'with', 'ks', 'tonight']
cosine_similarity: 0.9542861580848694
train_input: [0.7092972666062739, 0.95428616], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez is not playing tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.31779954 0.        ]
 [0.         0.40993715 0.         0.         0.57615236 0.
  0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez is not playing tonight
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'is', 'not', 'playing', 'tonight']
cosine_similarity: 0.9023782014846802
train_input: [0.2605556710562624, 0.9023782], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez just struck out 17 Braves in 8 innings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.47042643 0.         0.
  0.47042643 0.47042643 0.33471228 0.        ]
 [0.30287281 0.30287281 0.42567716 0.         0.42567716 0.42567716
  0.         0.         0.30287281 0.42567716]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez just struck out 17 Braves in 8 innings
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'just', 'struck', 'out', 'braves', 'in', 'innings']
cosine_similarity: 0.8950074911117554
train_input: [0.30412574187549346, 0.8950075], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez with a franchise record of 17 strikeouts in a 9 inning game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.37863221 0.         0.         0.53215436
  0.37863221 0.37863221 0.        ]
 [0.30253071 0.30253071 0.30253071 0.42519636 0.42519636 0.
  0.30253071 0.30253071 0.42519636]]
pairwise_similarity: [[1.         0.57273936]
 [0.57273936 1.        ]]
cosine_similarity: 0.57273935841962
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez with a franchise record of 17 strikeouts in a 9 inning game
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'with', 'a', 'franchise', 'record', 'of', 'strikeouts', 'in', 'a', 'inning', 'game']
cosine_similarity: 0.9684620499610901
train_input: [0.57273935841962, 0.96846205], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez with a new franchise record for the Tigers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49844628 0.35464863 0.35464863 0.49844628 0.         0.35464863
  0.35464863 0.        ]
 [0.         0.35464863 0.35464863 0.         0.49844628 0.35464863
  0.35464863 0.49844628]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Anibal Sanchez with a new franchise record for the Tigers
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['anibal', 'sanchez', 'with', 'a', 'new', 'franchise', 'record', 'for', 'the', 'tigers']
cosine_similarity: 0.9696139693260193
train_input: [0.5031026124151313, 0.96961397], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: KKKKKKKKKKKKKKKKK Anibal Sanchez with 17 K s Well done
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.         0.47042643 0.47042643
  0.33471228]
 [0.44832087 0.44832087 0.         0.63009934 0.         0.
  0.44832087]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: KKKKKKKKKKKKKKKKK Anibal Sanchez with 17 K s Well done
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['kkkkkkkkkkkkkkkkk', 'anibal', 'sanchez', 'with', 'k', 's', 'well', 'done']
cosine_similarity: 0.8827331066131592
train_input: [0.4501755023269897, 0.8827331], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Tigers Anibal Sanchez with Club record 17 Ks in 100 Win vs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37863221 0.37863221 0.         0.53215436 0.37863221
  0.37863221 0.37863221 0.         0.         0.        ]
 [0.36439074 0.25926702 0.25926702 0.36439074 0.         0.25926702
  0.25926702 0.25926702 0.36439074 0.36439074 0.36439074]]
pairwise_similarity: [[1.         0.49083421]
 [0.49083421 1.        ]]
cosine_similarity: 0.49083421206610733
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Tigers Anibal Sanchez with Club record 17 Ks in 100 Win vs
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['tigers', 'anibal', 'sanchez', 'with', 'club', 'record', 'ks', 'in', 'win', 'vs']
cosine_similarity: 0.9548404812812805
train_input: [0.49083421206610733, 0.9548405], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Wait Anibal Sanchez got 17Ks tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.         0.31779954 0.44665616 0.         0.44665616
  0.44665616 0.31779954 0.         0.        ]
 [0.         0.44665616 0.31779954 0.         0.44665616 0.
  0.         0.31779954 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: 17 Ks a franchise record for Anibal Sanchez, sentence2: Wait Anibal Sanchez got 17Ks tonight
After tokenization, sentence1: ['ks', 'a', 'franchise', 'record', 'for', 'anibal', 'sanchez'], sentence2: ['wait', 'anibal', 'sanchez', 'got', 'tonight']
cosine_similarity: 0.8790916204452515
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez 17 k s through 8
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.33471228 0.47042643 0.47042643]
 [0.57735027 0.57735027 0.         0.57735027 0.         0.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376658
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez 17 k s through 8
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['anibal', 'sanchez', 'k', 's', 'through']
cosine_similarity: 0.8684630393981934
train_input: [0.5797386715376658, 0.86846304], train_label: 1
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez has 14 K s in 7 innings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44665616 0.31779954 0.         0.44665616 0.31779954
  0.44665616 0.44665616]
 [0.57615236 0.         0.40993715 0.57615236 0.         0.40993715
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez has 14 K s in 7 innings
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['anibal', 'sanchez', 'has', 'k', 's', 'in', 'innings']
cosine_similarity: 0.9098352193832397
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez has 17 strikeouts through 8 innings for the Tigers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.49844628 0.35464863 0.
  0.35464863 0.49844628]
 [0.35464863 0.35464863 0.49844628 0.         0.35464863 0.49844628
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez has 17 strikeouts through 8 innings for the Tigers
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['anibal', 'sanchez', 'has', 'strikeouts', 'through', 'innings', 'for', 'the', 'tigers']
cosine_similarity: 0.9264532923698425
train_input: [0.5031026124151313, 0.9264533], train_label: 1
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez is beast mode tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.         0.44665616 0.31779954
  0.44665616 0.         0.44665616]
 [0.         0.35520009 0.49922133 0.49922133 0.         0.35520009
  0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez is beast mode tonight
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['anibal', 'sanchez', 'is', 'beast', 'mode', 'tonight']
cosine_similarity: 0.9058760404586792
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez struck the Braves out 17 times
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.47042643 0.33471228 0.
  0.47042643 0.         0.47042643]
 [0.33471228 0.33471228 0.47042643 0.         0.33471228 0.47042643
  0.         0.47042643 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez struck the Braves out 17 times
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['anibal', 'sanchez', 'struck', 'the', 'braves', 'out', 'times']
cosine_similarity: 0.9362758994102478
train_input: [0.3360969272762574, 0.9362759], train_label: 1
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez wins tonight s award for Happiest Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.         0.44665616 0.31779954
  0.44665616 0.         0.         0.44665616]
 [0.         0.27840869 0.39129369 0.39129369 0.         0.55681737
  0.         0.39129369 0.39129369 0.        ]]
pairwise_similarity: [[1.         0.26543445]
 [0.26543445 1.        ]]
cosine_similarity: 0.265434454961717
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez wins tonight s award for Happiest Sanchez
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['anibal', 'sanchez', 'wins', 'tonight', 's', 'award', 'for', 'happiest', 'sanchez']
cosine_similarity: 0.9206578731536865
train_input: [0.265434454961717, 0.9206579], train_label: 0
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez with 16 K s on the night
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44665616 0.31779954 0.         0.44665616 0.31779954
  0.44665616 0.44665616]
 [0.57615236 0.         0.40993715 0.57615236 0.         0.40993715
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Anibal Sanchez with 16 K s on the night
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['anibal', 'sanchez', 'with', 'k', 's', 'on', 'the', 'night']
cosine_similarity: 0.8610621690750122
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: I forgot how good of a pitcher Anibal Sanchez is
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.         0.         0.44665616
  0.31779954 0.44665616 0.44665616]
 [0.         0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.35520009 0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: I forgot how good of a pitcher Anibal Sanchez is
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['i', 'forgot', 'how', 'good', 'of', 'a', 'pitcher', 'anibal', 'sanchez', 'is']
cosine_similarity: 0.862294614315033
train_input: [0.22576484600261604, 0.8622946], train_label: 0
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Pretty sure Anibal Sanchez took a shit on the Braves
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.         0.44665616 0.31779954
  0.         0.         0.44665616 0.         0.44665616]
 [0.         0.29017021 0.4078241  0.4078241  0.         0.29017021
  0.4078241  0.4078241  0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: Pretty sure Anibal Sanchez took a shit on the Braves
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['pretty', 'sure', 'anibal', 'sanchez', 'took', 'a', 'shit', 'on', 'the', 'braves']
cosine_similarity: 0.8891533613204956
train_input: [0.18443191662261305, 0.88915336], train_label: 0
TF_IDF_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: What a night by Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.44665616 0.31779954 0.44665616
  0.44665616]
 [0.         0.50154891 0.70490949 0.         0.50154891 0.
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: anibal sanchez tigers record 17 Wow, sentence2: What a night by Anibal Sanchez
After tokenization, sentence1: ['anibal', 'sanchez', 'tigers', 'record', 'wow'], sentence2: ['what', 'a', 'night', 'by', 'anibal', 'sanchez']
cosine_similarity: 0.886215329170227
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez has 17 K for the Tigers a record for K in a game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.         0.37930349
  0.         0.53309782]
 [0.33471228 0.33471228 0.47042643 0.         0.47042643 0.33471228
  0.47042643 0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez has 17 K for the Tigers a record for K in a game
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'record', 'for', 'k', 'in', 'a', 'game']
cosine_similarity: 0.973421037197113
train_input: [0.38087260847594373, 0.97342104], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez is knocking on the door with 17
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.         0.37930349
  0.53309782]
 [0.37930349 0.37930349 0.53309782 0.         0.53309782 0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez is knocking on the door with 17
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['anibal', 'sanchez', 'is', 'knocking', 'on', 'the', 'door', 'with']
cosine_similarity: 0.931644082069397
train_input: [0.43161341897075145, 0.9316441], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez just set tigers alltime record for strikeouts in a 9inn
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.         0.35520009 0.49922133 0.
  0.         0.35520009 0.         0.         0.         0.49922133]
 [0.         0.35327777 0.35327777 0.25136004 0.         0.35327777
  0.35327777 0.25136004 0.35327777 0.35327777 0.35327777 0.        ]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez just set tigers alltime record for strikeouts in a 9inn
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['anibal', 'sanchez', 'just', 'set', 'tigers', 'alltime', 'record', 'for', 'strikeouts', 'in', 'a']
cosine_similarity: 0.9529638886451721
train_input: [0.17856621555757476, 0.9529639], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez sets franchise record in strikeouts for a 9 inning game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.         0.49922133
  0.         0.35520009 0.         0.         0.49922133]
 [0.         0.26868528 0.37762778 0.37762778 0.37762778 0.
  0.37762778 0.26868528 0.37762778 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez sets franchise record in strikeouts for a 9 inning game
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['anibal', 'sanchez', 'sets', 'franchise', 'record', 'in', 'strikeouts', 'for', 'a', 'inning', 'game']
cosine_similarity: 0.9372848868370056
train_input: [0.1908740661302035, 0.9372849], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez w careerhigh 17 K s vs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.37930349 0.53309782
  0.        ]
 [0.37930349 0.37930349 0.53309782 0.         0.37930349 0.
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Anibal Sanchez w careerhigh 17 K s vs
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['anibal', 'sanchez', 'w', 'k', 's', 'vs']
cosine_similarity: 0.9140368700027466
train_input: [0.43161341897075145, 0.91403687], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Holy shit Anibal Sanchez has struck out 17 hitters threw 8 innings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.         0.         0.4090901  0.4090901
  0.         0.         0.         0.57496187]
 [0.26844636 0.26844636 0.37729199 0.37729199 0.26844636 0.26844636
  0.37729199 0.37729199 0.37729199 0.        ]]
pairwise_similarity: [[1.         0.43927499]
 [0.43927499 1.        ]]
cosine_similarity: 0.43927499031635536
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Holy shit Anibal Sanchez has struck out 17 hitters threw 8 innings
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['holy', 'shit', 'anibal', 'sanchez', 'has', 'struck', 'out', 'hitters', 'threw', 'innings']
cosine_similarity: 0.8668143153190613
train_input: [0.43927499031635536, 0.8668143], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: I forgot to start Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.49922133 0.35520009 0.
  0.49922133]
 [0.         0.40993715 0.57615236 0.         0.40993715 0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: I forgot to start Anibal Sanchez
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['i', 'forgot', 'to', 'start', 'anibal', 'sanchez']
cosine_similarity: 0.919715940952301
train_input: [0.29121941856368966, 0.91971594], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: So the Tigers Anibal Sanchez has 17 Ks through eight innings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.4090901  0.         0.4090901  0.
  0.57496187]
 [0.35464863 0.35464863 0.35464863 0.49844628 0.35464863 0.49844628
  0.        ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: So the Tigers Anibal Sanchez has 17 Ks through eight innings
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['so', 'the', 'tigers', 'anibal', 'sanchez', 'has', 'ks', 'through', 'eight', 'innings']
cosine_similarity: 0.9393047094345093
train_input: [0.5803329846765685, 0.9393047], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: This Anibal Sanchez kid is dealing against the braves
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.49922133 0.
  0.35520009 0.49922133]
 [0.         0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.35520009 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: This Anibal Sanchez kid is dealing against the braves
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['this', 'anibal', 'sanchez', 'kid', 'is', 'dealing', 'against', 'the', 'braves']
cosine_similarity: 0.9210281372070312
train_input: [0.2523342014336961, 0.92102814], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Wow Anibal Sanchez with 17 strikeouts for the Tigers through 8 innings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.4090901  0.4090901  0.         0.
  0.57496187 0.        ]
 [0.3174044  0.3174044  0.3174044  0.3174044  0.44610081 0.44610081
  0.         0.44610081]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez with 17 K s in 8 innings tonight, sentence2: Wow Anibal Sanchez with 17 strikeouts for the Tigers through 8 innings
After tokenization, sentence1: ['anibal', 'sanchez', 'with', 'k', 's', 'in', 'innings', 'tonight'], sentence2: ['wow', 'anibal', 'sanchez', 'with', 'strikeouts', 'for', 'the', 'tigers', 'through', 'innings']
cosine_similarity: 0.9372853636741638
train_input: [0.5193879933129156, 0.93728536], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: ANIBAL SANCHEZ WITH 17 KS SETS THE NEW TIGERS FRANCHISE RECORD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.         0.49922133 0.
  0.         0.35520009 0.         0.49922133 0.         0.49922133]
 [0.35327777 0.25136004 0.35327777 0.35327777 0.         0.35327777
  0.35327777 0.25136004 0.35327777 0.         0.35327777 0.        ]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: ANIBAL SANCHEZ WITH 17 KS SETS THE NEW TIGERS FRANCHISE RECORD
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['anibal', 'sanchez', 'with', 'ks', 'sets', 'the', 'new', 'tigers', 'franchise', 'record']
cosine_similarity: 0.9549855589866638
train_input: [0.17856621555757476, 0.95498556], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez has 17 strikeouts against the Braves tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.         0.53309782 0.37930349 0.53309782
  0.         0.37930349]
 [0.47042643 0.33471228 0.47042643 0.         0.33471228 0.
  0.47042643 0.33471228]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez has 17 strikeouts against the Braves tonight
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['anibal', 'sanchez', 'has', 'strikeouts', 'against', 'the', 'braves', 'tonight']
cosine_similarity: 0.9459820985794067
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez has 17 strikeouts vs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.35520009 0.49922133 0.
  0.49922133 0.        ]
 [0.49922133 0.35520009 0.         0.35520009 0.         0.49922133
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez has 17 strikeouts vs
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['anibal', 'sanchez', 'has', 'strikeouts', 'vs']
cosine_similarity: 0.8752533793449402
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez has just set the club record for strikeouts in a game with 17
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.         0.         0.49922133
  0.         0.35520009 0.         0.49922133 0.         0.49922133]
 [0.35327777 0.25136004 0.35327777 0.35327777 0.35327777 0.
  0.35327777 0.25136004 0.35327777 0.         0.35327777 0.        ]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez has just set the club record for strikeouts in a game with 17
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['anibal', 'sanchez', 'has', 'just', 'set', 'the', 'club', 'record', 'for', 'strikeouts', 'in', 'a', 'game', 'with']
cosine_similarity: 0.9604111313819885
train_input: [0.17856621555757476, 0.96041113], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez has this game set on EZMODE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.35520009 0.
  0.49922133 0.49922133]
 [0.35520009 0.49922133 0.49922133 0.         0.35520009 0.49922133
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez has this game set on EZMODE
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['anibal', 'sanchez', 'has', 'this', 'game', 'set', 'on']
cosine_similarity: 0.9731058478355408
train_input: [0.2523342014336961, 0.97310585], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez is throwing a Jem too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.35520009 0.49922133 0.
  0.49922133]
 [0.40993715 0.57615236 0.         0.40993715 0.         0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez is throwing a Jem too
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['anibal', 'sanchez', 'is', 'throwing', 'a', 'jem', 'too']
cosine_similarity: 0.955337643623352
train_input: [0.29121941856368966, 0.95533764], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez leaves the 8th inning with career high 17 Ks and franchise record
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35520009 0.         0.         0.
  0.         0.         0.         0.49922133 0.         0.35520009
  0.49922133 0.49922133]
 [0.3160305  0.3160305  0.2248583  0.3160305  0.3160305  0.3160305
  0.3160305  0.3160305  0.3160305  0.         0.3160305  0.2248583
  0.         0.        ]]
pairwise_similarity: [[1.         0.15973938]
 [0.15973938 1.        ]]
cosine_similarity: 0.15973937686327605
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Anibal Sanchez leaves the 8th inning with career high 17 Ks and franchise record
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['anibal', 'sanchez', 'leaves', 'the', 'inning', 'with', 'career', 'high', 'ks', 'and', 'franchise', 'record']
cosine_similarity: 0.9598295092582703
train_input: [0.15973937686327605, 0.9598295], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Have a ball game Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.35520009 0.49922133
  0.49922133]
 [0.40993715 0.57615236 0.57615236 0.         0.40993715 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Have a ball game Anibal Sanchez
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['have', 'a', 'ball', 'game', 'anibal', 'sanchez']
cosine_similarity: 0.9738472700119019
train_input: [0.29121941856368966, 0.97384727], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Tigers Anibal Sanchez with Club record 17 Ks in 100 Win vs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35520009 0.         0.         0.49922133
  0.         0.35520009 0.49922133 0.         0.49922133 0.
  0.        ]
 [0.33310232 0.33310232 0.23700504 0.33310232 0.33310232 0.
  0.33310232 0.23700504 0.         0.33310232 0.         0.33310232
  0.33310232]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: Tigers Anibal Sanchez with Club record 17 Ks in 100 Win vs
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['tigers', 'anibal', 'sanchez', 'with', 'club', 'record', 'ks', 'in', 'win', 'vs']
cosine_similarity: 0.9450878500938416
train_input: [0.16836842163679844, 0.94508785], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: anibal sanchez has 15ks and is still out in the game in the 8th
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35520009 0.         0.49922133 0.35520009
  0.49922133 0.49922133]
 [0.49922133 0.49922133 0.35520009 0.49922133 0.         0.35520009
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez is a strikeout machine tonight, sentence2: anibal sanchez has 15ks and is still out in the game in the 8th
After tokenization, sentence1: ['anibal', 'sanchez', 'is', 'a', 'strikeout', 'machine', 'tonight'], sentence2: ['anibal', 'sanchez', 'has', 'and', 'is', 'still', 'out', 'in', 'the', 'game', 'in', 'the']
cosine_similarity: 0.9360443949699402
train_input: [0.2523342014336961, 0.9360444], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: 12 K s for Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4078241  0.29017021 0.4078241  0.4078241  0.4078241
  0.29017021 0.4078241 ]
 [0.70490949 0.         0.50154891 0.         0.         0.
  0.50154891 0.        ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: 12 K s for Anibal Sanchez
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['k', 's', 'for', 'anibal', 'sanchez']
cosine_similarity: 0.936622142791748
train_input: [0.2910691023819054, 0.93662214], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez has 14 K s in the 7th inning
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4078241  0.         0.29017021 0.4078241  0.4078241
  0.         0.4078241  0.29017021 0.4078241 ]
 [0.49922133 0.         0.49922133 0.35520009 0.         0.
  0.49922133 0.         0.35520009 0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez has 14 K s in the 7th inning
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['anibal', 'sanchez', 'has', 'k', 's', 'in', 'the', 'inning']
cosine_similarity: 0.98076993227005
train_input: [0.20613696606828605, 0.98076993], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez has 17 strikeouts tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.42567716 0.42567716 0.42567716 0.30287281
  0.         0.42567716 0.        ]
 [0.37930349 0.37930349 0.         0.         0.         0.37930349
  0.53309782 0.         0.53309782]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez has 17 strikeouts tonight
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['anibal', 'sanchez', 'has', 'strikeouts', 'tonight']
cosine_similarity: 0.8523162007331848
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez has 17K in 8 IP so far
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.         0.29017021 0.         0.4078241  0.4078241
  0.         0.4078241  0.29017021 0.4078241 ]
 [0.         0.49922133 0.35520009 0.49922133 0.         0.
  0.49922133 0.         0.35520009 0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez has 17K in 8 IP so far
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['anibal', 'sanchez', 'has', 'in', 'ip', 'so', 'far']
cosine_similarity: 0.9615933895111084
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez is ON FIRE right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.4078241  0.4078241  0.4078241  0.
  0.29017021 0.4078241 ]
 [0.         0.50154891 0.         0.         0.         0.70490949
  0.50154891 0.        ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez is ON FIRE right now
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['anibal', 'sanchez', 'is', 'on', 'fire', 'right', 'now']
cosine_similarity: 0.9348668456077576
train_input: [0.2910691023819054, 0.93486685], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez sets the franchise record with 17 Ks in a 9inning game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35409974 0.         0.35409974 0.35409974 0.35409974 0.
  0.35409974 0.35409974 0.         0.49767483]
 [0.2895694  0.40697968 0.2895694  0.2895694  0.2895694  0.40697968
  0.2895694  0.2895694  0.40697968 0.        ]]
pairwise_similarity: [[1.         0.61521869]
 [0.61521869 1.        ]]
cosine_similarity: 0.615218690896332
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez sets the franchise record with 17 Ks in a 9inning game
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['anibal', 'sanchez', 'sets', 'the', 'franchise', 'record', 'with', 'ks', 'in', 'a', 'game']
cosine_similarity: 0.9810590744018555
train_input: [0.615218690896332, 0.9810591], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez with 17 strikeouts in 8 innings tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.42567716 0.42567716 0.         0.42567716
  0.30287281 0.         0.42567716 0.        ]
 [0.33471228 0.33471228 0.         0.         0.47042643 0.
  0.33471228 0.47042643 0.         0.47042643]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Anibal Sanchez with 17 strikeouts in 8 innings tonight
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['anibal', 'sanchez', 'with', 'strikeouts', 'in', 'innings', 'tonight']
cosine_similarity: 0.893623411655426
train_input: [0.3041257418754935, 0.8936234], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Okay Anibal Sanchez for NO GAMES
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.4078241  0.4078241  0.         0.
  0.4078241  0.29017021 0.4078241 ]
 [0.         0.40993715 0.         0.         0.57615236 0.57615236
  0.         0.40993715 0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Okay Anibal Sanchez for NO GAMES
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['okay', 'anibal', 'sanchez', 'for', 'no', 'games']
cosine_similarity: 0.9567158818244934
train_input: [0.23790309463326234, 0.9567159], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Tigers Anibal Sanchez records 17 strikeouts in 8 scoreless innings in 100 win vs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.3174044  0.3174044  0.44610081 0.44610081 0.
  0.44610081 0.         0.3174044  0.         0.         0.3174044
  0.         0.        ]
 [0.33287178 0.23684101 0.23684101 0.         0.         0.33287178
  0.         0.33287178 0.23684101 0.33287178 0.33287178 0.23684101
  0.33287178 0.33287178]]
pairwise_similarity: [[1.         0.30069751]
 [0.30069751 1.        ]]
cosine_similarity: 0.30069751492635505
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Tigers Anibal Sanchez records 17 strikeouts in 8 scoreless innings in 100 win vs
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['tigers', 'anibal', 'sanchez', 'records', 'strikeouts', 'in', 'scoreless', 'innings', 'in', 'win', 'vs']
cosine_similarity: 0.8720272183418274
train_input: [0.30069751492635505, 0.8720272], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Wow Anibal Sanchez has 15 strikeouts vs the braves
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4078241  0.29017021 0.         0.4078241  0.4078241
  0.4078241  0.29017021 0.         0.4078241  0.         0.        ]
 [0.4078241  0.         0.29017021 0.4078241  0.         0.
  0.         0.29017021 0.4078241  0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.1683975]
 [0.1683975 1.       ]]
cosine_similarity: 0.16839750037215276
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez has 17 K for the Tigers a franchise record for K in a game, sentence2: Wow Anibal Sanchez has 15 strikeouts vs the braves
After tokenization, sentence1: ['anibal', 'sanchez', 'has', 'k', 'for', 'the', 'tigers', 'a', 'franchise', 'record', 'for', 'k', 'in', 'a', 'game'], sentence2: ['wow', 'anibal', 'sanchez', 'has', 'strikeouts', 'vs', 'the', 'braves']
cosine_similarity: 0.9051010608673096
train_input: [0.16839750037215276, 0.90510106], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: A masterful performance from the Tigers Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.35520009 0.49922133
  0.         0.49922133]
 [0.         0.35520009 0.49922133 0.49922133 0.35520009 0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: A masterful performance from the Tigers Anibal Sanchez
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['a', 'masterful', 'performance', 'from', 'the', 'tigers', 'anibal', 'sanchez']
cosine_similarity: 0.9342346787452698
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Anibal Sanchez 17 k s through 8
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.37930349 0.53309782 0.53309782]
 [0.57735027 0.57735027 0.57735027 0.         0.        ]]
pairwise_similarity: [[1.         0.65697292]
 [0.65697292 1.        ]]
cosine_similarity: 0.6569729210330906
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Anibal Sanchez 17 k s through 8
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['anibal', 'sanchez', 'k', 's', 'through']
cosine_similarity: 0.8278528451919556
train_input: [0.6569729210330906, 0.82785285], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Anibal Sanchez K s 17 thus far
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.37930349 0.53309782 0.53309782]
 [0.44832087 0.44832087 0.63009934 0.44832087 0.         0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Anibal Sanchez K s 17 thus far
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['anibal', 'sanchez', 'k', 's', 'thus', 'far']
cosine_similarity: 0.8325281143188477
train_input: [0.5101490193104813, 0.8325281], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Anibal Sanchez having the game of his life
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.         0.35520009
  0.49922133 0.49922133]
 [0.         0.35520009 0.49922133 0.49922133 0.49922133 0.35520009
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Anibal Sanchez having the game of his life
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['anibal', 'sanchez', 'having', 'the', 'game', 'of', 'his', 'life']
cosine_similarity: 0.9239519238471985
train_input: [0.2523342014336961, 0.9239519], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Anibal Sanchez to the Tigers batters
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.35520009 0.49922133 0.
  0.49922133]
 [0.         0.40993715 0.57615236 0.40993715 0.         0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Anibal Sanchez to the Tigers batters
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['anibal', 'sanchez', 'to', 'the', 'tigers', 'batters']
cosine_similarity: 0.9446569085121155
train_input: [0.29121941856368966, 0.9446569], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Detroit Tigers Anibal Sanchez is extra dirty tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.37930349 0.         0.         0.         0.37930349
  0.53309782 0.         0.37930349]
 [0.         0.30287281 0.42567716 0.42567716 0.42567716 0.30287281
  0.         0.42567716 0.30287281]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Detroit Tigers Anibal Sanchez is extra dirty tonight
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['detroit', 'tigers', 'anibal', 'sanchez', 'is', 'extra', 'dirty', 'tonight']
cosine_similarity: 0.9632117748260498
train_input: [0.34464214103805474, 0.9632118], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Just saw that Anibal Sanchez has 17 Ks Hot damn
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.         0.         0.
  0.37930349 0.         0.53309782 0.53309782]
 [0.27867523 0.27867523 0.39166832 0.39166832 0.39166832 0.39166832
  0.27867523 0.39166832 0.         0.        ]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Just saw that Anibal Sanchez has 17 Ks Hot damn
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['just', 'saw', 'that', 'anibal', 'sanchez', 'has', 'ks', 'hot', 'damn']
cosine_similarity: 0.924974799156189
train_input: [0.31710746658027095, 0.9249748], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: KKKKKKKKKKKKKKKKK Anibal Sanchez with 17 K s Well done
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.37930349 0.53309782 0.53309782]
 [0.44832087 0.44832087 0.63009934 0.44832087 0.         0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: KKKKKKKKKKKKKKKKK Anibal Sanchez with 17 K s Well done
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['kkkkkkkkkkkkkkkkk', 'anibal', 'sanchez', 'with', 'k', 's', 'well', 'done']
cosine_similarity: 0.8388130068778992
train_input: [0.5101490193104813, 0.838813], train_label: 1
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Make that 14 strikeouts of Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.35520009 0.         0.35520009 0.
  0.49922133 0.49922133]
 [0.49922133 0.         0.35520009 0.49922133 0.35520009 0.49922133
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: Make that 14 strikeouts of Anibal Sanchez
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['make', 'that', 'strikeouts', 'of', 'anibal', 'sanchez']
cosine_similarity: 0.9368993043899536
train_input: [0.2523342014336961, 0.9368993], train_label: 0
TF_IDF_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: That s whiff number FOURTEEN for Anibal Sanchez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.35520009 0.49922133
  0.49922133 0.        ]
 [0.         0.35520009 0.49922133 0.49922133 0.35520009 0.
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Anibal Sanchez struck out 17 tonight, sentence2: That s whiff number FOURTEEN for Anibal Sanchez
After tokenization, sentence1: ['anibal', 'sanchez', 'struck', 'out', 'tonight'], sentence2: ['that', 's', 'whiff', 'number', 'fourteen', 'for', 'anibal', 'sanchez']
cosine_similarity: 0.9149363040924072
train_input: [0.2523342014336961, 0.9149363], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: 17 Ks for Anibal Sanchez through 8 tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.         0.37930349 0.53309782
  0.        ]
 [0.37930349 0.37930349 0.         0.53309782 0.37930349 0.
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: 17 Ks for Anibal Sanchez through 8 tonight
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['ks', 'for', 'anibal', 'sanchez', 'through', 'tonight']
cosine_similarity: 0.9104571342468262
train_input: [0.43161341897075145, 0.91045713], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez 17 strikeouts tonight against the Braves
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.         0.57496187 0.4090901  0.4090901
  0.        ]
 [0.35464863 0.35464863 0.49844628 0.         0.35464863 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez 17 strikeouts tonight against the Braves
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['anibal', 'sanchez', 'strikeouts', 'tonight', 'against', 'the', 'braves']
cosine_similarity: 0.9671039581298828
train_input: [0.5803329846765685, 0.96710396], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez K s 17 thus far
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.37930349 0.53309782]
 [0.44832087 0.44832087 0.63009934 0.         0.44832087 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez K s 17 thus far
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['anibal', 'sanchez', 'k', 's', 'thus', 'far']
cosine_similarity: 0.803583562374115
train_input: [0.5101490193104813, 0.80358356], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez give them a chance brother
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.49922133 0.35520009
  0.49922133]
 [0.         0.40993715 0.57615236 0.57615236 0.         0.40993715
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez give them a chance brother
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['anibal', 'sanchez', 'give', 'them', 'a', 'chance', 'brother']
cosine_similarity: 0.8299175500869751
train_input: [0.29121941856368966, 0.82991755], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez has struck out 17 batters in 8 innings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.         0.4090901  0.4090901  0.57496187
  0.        ]
 [0.35464863 0.35464863 0.49844628 0.35464863 0.35464863 0.
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez has struck out 17 batters in 8 innings
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['anibal', 'sanchez', 'has', 'struck', 'out', 'batters', 'in', 'innings']
cosine_similarity: 0.9436255097389221
train_input: [0.5803329846765685, 0.9436255], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez thinks this a video game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.49922133 0.35520009 0.49922133
  0.         0.        ]
 [0.         0.35520009 0.49922133 0.         0.35520009 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez thinks this a video game
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['anibal', 'sanchez', 'thinks', 'this', 'a', 'video', 'game']
cosine_similarity: 0.8230345249176025
train_input: [0.2523342014336961, 0.8230345], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez with the dominant performance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.49922133 0.         0.35520009
  0.49922133]
 [0.         0.40993715 0.57615236 0.         0.57615236 0.40993715
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Anibal Sanchez with the dominant performance
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['anibal', 'sanchez', 'with', 'the', 'dominant', 'performance']
cosine_similarity: 0.9031786918640137
train_input: [0.29121941856368966, 0.9031787], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Wait youre telling me anibal sanchez had 17 K s against the braves
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.37930349 0.53309782
  0.         0.         0.        ]
 [0.30287281 0.30287281 0.42567716 0.         0.30287281 0.
  0.42567716 0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: Wait youre telling me anibal sanchez had 17 K s against the braves
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['wait', 'youre', 'telling', 'me', 'anibal', 'sanchez', 'had', 'k', 's', 'against', 'the', 'braves']
cosine_similarity: 0.8261352777481079
train_input: [0.34464214103805474, 0.8261353], train_label: 1
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: anibal sanchez has 15ks and is still out in the game in the 8th
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.35520009 0.         0.49922133
  0.35520009 0.49922133]
 [0.49922133 0.         0.49922133 0.35520009 0.49922133 0.
  0.35520009 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: anibal sanchez has 15ks and is still out in the game in the 8th
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['anibal', 'sanchez', 'has', 'and', 'is', 'still', 'out', 'in', 'the', 'game', 'in', 'the']
cosine_similarity: 0.8372599482536316
train_input: [0.2523342014336961, 0.83725995], train_label: 0
TF_IDF_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: starting pitcher Anibal Sanchez has 17 strikeouts so far
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.         0.57496187 0.         0.4090901
  0.         0.4090901 ]
 [0.3174044  0.3174044  0.44610081 0.         0.44610081 0.3174044
  0.44610081 0.3174044 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: 17 strikeouts for Anibal Sanchez through 8 innings, sentence2: starting pitcher Anibal Sanchez has 17 strikeouts so far
After tokenization, sentence1: ['strikeouts', 'for', 'anibal', 'sanchez', 'through', 'innings'], sentence2: ['starting', 'pitcher', 'anibal', 'sanchez', 'has', 'strikeouts', 'so', 'far']
cosine_similarity: 0.94418865442276
train_input: [0.5193879933129156, 0.94418865], train_label: 1
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: Ariana and Mac duet is better than Ariana and Justin duet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.         0.47107781 0.
  0.         0.47107781 0.47107781]
 [0.47368202 0.33287178 0.         0.66574355 0.         0.33287178
  0.33287178 0.         0.        ]]
pairwise_similarity: [[1.         0.15876672]
 [0.15876672 1.        ]]
cosine_similarity: 0.1587667239811707
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: Ariana and Mac duet is better than Ariana and Justin duet
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['ariana', 'and', 'mac', 'duet', 'is', 'better', 'than', 'ariana', 'and', 'justin', 'duet']
cosine_similarity: 0.9194510579109192
train_input: [0.1587667239811707, 0.91945106], train_label: 0
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: But I dont like Ariana that much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.47107781]
 [0.44943642 0.         0.6316672  0.         0.6316672  0.
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: But I dont like Ariana that much
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['but', 'i', 'dont', 'like', 'ariana', 'that', 'much']
cosine_similarity: 0.848014235496521
train_input: [0.15064018498706508, 0.84801424], train_label: 0
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: I cant wait for Ariana to get huge
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.47107781 0.47107781
  0.        ]
 [0.44943642 0.         0.         0.6316672  0.         0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: I cant wait for Ariana to get huge
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['i', 'cant', 'wait', 'for', 'ariana', 'to', 'get', 'huge']
cosine_similarity: 0.8686341047286987
train_input: [0.15064018498706508, 0.8686341], train_label: 0
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: OMFG HAS SCOOTER REALLY SIGNED ARIANA
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.53309782 0.         0.         0.37930349
  0.37930349]
 [0.37930349 0.         0.         0.53309782 0.53309782 0.37930349
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: OMFG HAS SCOOTER REALLY SIGNED ARIANA
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['omfg', 'has', 'scooter', 'really', 'signed', 'ariana']
cosine_similarity: 0.961132287979126
train_input: [0.43161341897075145, 0.9611323], train_label: 1
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: Omg Scooter signed Ariana asdfghjkl omfg
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.53309782 0.         0.
  0.37930349 0.37930349]
 [0.33471228 0.47042643 0.         0.         0.47042643 0.47042643
  0.33471228 0.33471228]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: Omg Scooter signed Ariana asdfghjkl omfg
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['omg', 'scooter', 'signed', 'ariana', 'asdfghjkl', 'omfg']
cosine_similarity: 0.8995437026023865
train_input: [0.38087260847594373, 0.8995437], train_label: 1
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: SBJSVSJWJW ARIANA AND JUSTIN MY BABIES
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.47107781 0.         0.
  0.47107781 0.47107781]
 [0.37997836 0.53404633 0.         0.         0.53404633 0.53404633
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: SBJSVSJWJW ARIANA AND JUSTIN MY BABIES
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['ariana', 'and', 'justin', 'my', 'babies']
cosine_similarity: 0.8942785859107971
train_input: [0.1273595297947935, 0.8942786], train_label: 0
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: Should it be a scrapbook for ariana grande or the fergies
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.35520009 0.49922133 0.
  0.49922133]
 [0.40993715 0.         0.57615236 0.40993715 0.         0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: Should it be a scrapbook for ariana grande or the fergies
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['should', 'it', 'be', 'a', 'scrapbook', 'for', 'ariana', 'grande', 'or', 'the', 'fergies']
cosine_similarity: 0.9199471473693848
train_input: [0.29121941856368966, 0.91994715], train_label: 0
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: What if Justin and Ariana did a song together
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.47107781 0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: What if Justin and Ariana did a song together
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['what', 'if', 'justin', 'and', 'ariana', 'did', 'a', 'song', 'together']
cosine_similarity: 0.9131160378456116
train_input: [0.1273595297947935, 0.91311604], train_label: 0
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: do you think ariana is creeping right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.47107781 0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: do you think ariana is creeping right now
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['do', 'you', 'think', 'ariana', 'is', 'creeping', 'right', 'now']
cosine_similarity: 0.8612126111984253
train_input: [0.1273595297947935, 0.8612126], train_label: 0
TF_IDF_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: my tl is about Justin and ariana
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.47107781 0.47107781
  0.        ]
 [0.44943642 0.         0.         0.6316672  0.         0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: So scooter Braun has signed Ariana grande, sentence2: my tl is about Justin and ariana
After tokenization, sentence1: ['so', 'scooter', 'braun', 'has', 'signed', 'ariana', 'grande'], sentence2: ['my', 'tl', 'is', 'about', 'justin', 'and', 'ariana']
cosine_similarity: 0.9006656408309937
train_input: [0.15064018498706508, 0.90066564], train_label: 0
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Another solid pick for the Ravens in Arthur Brown
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Another solid pick for the Ravens in Arthur Brown
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['another', 'solid', 'pick', 'for', 'the', 'ravens', 'in', 'arthur', 'brown']
cosine_similarity: 0.9572576284408569
train_input: [0.29121941856368966, 0.9572576], train_label: 1
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Arthur Brown will be The God of Hell Fire somewhere else
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Arthur Brown will be The God of Hell Fire somewhere else
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['arthur', 'brown', 'will', 'be', 'the', 'god', 'of', 'hell', 'fire', 'somewhere', 'else']
cosine_similarity: 0.9295198917388916
train_input: [0.3360969272762575, 0.9295199], train_label: 0
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Arthur brown is a first round talent
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.57615236]
 [0.40993715 0.40993715 0.         0.57615236 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Arthur brown is a first round talent
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['arthur', 'brown', 'is', 'a', 'first', 'round', 'talent']
cosine_similarity: 0.935291588306427
train_input: [0.3360969272762575, 0.9352916], train_label: 0
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Bet on LB Arthur Brown from KState
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.57615236
  0.57615236]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Bet on LB Arthur Brown from KState
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['bet', 'on', 'lb', 'arthur', 'brown', 'from', 'kstate']
cosine_similarity: 0.9611523151397705
train_input: [0.29121941856368966, 0.9611523], train_label: 0
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: God I hate how good a pick Arthur Brown is
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.
  0.57615236 0.57615236]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.44665616
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: God I hate how good a pick Arthur Brown is
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['god', 'i', 'hate', 'how', 'good', 'a', 'pick', 'arthur', 'brown', 'is']
cosine_similarity: 0.9341893196105957
train_input: [0.2605556710562624, 0.9341893], train_label: 0
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Love the Ravens pick of Arthur Brown remember following him and his brothers recruitment
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.
  0.         0.57615236 0.         0.         0.57615236]
 [0.25136004 0.35327777 0.25136004 0.35327777 0.35327777 0.35327777
  0.35327777 0.         0.35327777 0.35327777 0.        ]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Love the Ravens pick of Arthur Brown remember following him and his brothers recruitment
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['love', 'the', 'ravens', 'pick', 'of', 'arthur', 'brown', 'remember', 'following', 'him', 'and', 'his', 'brothers', 'recruitment']
cosine_similarity: 0.9516767263412476
train_input: [0.20608363501393823, 0.9516767], train_label: 1
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Loved the Arthur Brown pick to the Ravens
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Loved the Arthur Brown pick to the Ravens
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['loved', 'the', 'arthur', 'brown', 'pick', 'to', 'the', 'ravens']
cosine_similarity: 0.9746453166007996
train_input: [0.29121941856368966, 0.9746453], train_label: 1
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Or Arthur Brown oh man cant believe the talent left
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.
  0.57615236 0.         0.57615236]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.4078241  0.4078241
  0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Or Arthur Brown oh man cant believe the talent left
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['or', 'arthur', 'brown', 'oh', 'man', 'cant', 'believe', 'the', 'talent', 'left']
cosine_similarity: 0.9373725056648254
train_input: [0.23790309463326234, 0.9373725], train_label: 0
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Please let Arthur Brown be available at 50 for the Bears
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.40993715 0.
  0.57615236 0.57615236]
 [0.44665616 0.31779954 0.44665616 0.44665616 0.31779954 0.44665616
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: Please let Arthur Brown be available at 50 for the Bears
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['please', 'let', 'arthur', 'brown', 'be', 'available', 'at', 'for', 'the', 'bears']
cosine_similarity: 0.960932195186615
train_input: [0.2605556710562624, 0.9609322], train_label: 0
TF_IDF_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: please please please let Arthur brown drop to 50
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.57615236
  0.57615236]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: WELCOME TO THE RAVENSNATION ARTHUR BROWN, sentence2: please please please let Arthur brown drop to 50
After tokenization, sentence1: ['welcome', 'to', 'the', 'ravensnation', 'arthur', 'brown'], sentence2: ['please', 'please', 'please', 'let', 'arthur', 'brown', 'drop', 'to']
cosine_similarity: 0.9310309290885925
train_input: [0.29121941856368966, 0.9310309], train_label: 0
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: Any real hockey fan immediately switches to the WildAvs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.         0.         0.         0.         0.
  0.         0.70710678]
 [0.         0.40824829 0.40824829 0.40824829 0.40824829 0.40824829
  0.40824829 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: Any real hockey fan immediately switches to the WildAvs
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['any', 'real', 'hockey', 'fan', 'immediately', 'switches', 'to', 'the']
cosine_similarity: 0.9519435167312622
train_input: [0.0, 0.9519435], train_label: 0
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: Avs couldnt even gain the zone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.81480247 0.        ]
 [0.44943642 0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: Avs couldnt even gain the zone
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['avs', 'couldnt', 'even', 'gain', 'the', 'zone']
cosine_similarity: 0.9415464997291565
train_input: [0.2605556710562624, 0.9415465], train_label: 0
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: Come on avs be somebody
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.81480247]
 [0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: Come on avs be somebody
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['come', 'on', 'avs', 'be', 'somebody']
cosine_similarity: 0.9534119963645935
train_input: [0.2605556710562624, 0.953412], train_label: 1
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: Half the league are AVs fans right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.81480247]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: Half the league are AVs fans right now
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['half', 'the', 'league', 'are', 'avs', 'fans', 'right', 'now']
cosine_similarity: 0.9555320143699646
train_input: [0.19431434016858146, 0.955532], train_label: 0
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: I miss the 2000 avs with patrick roy forsberg sakic
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.         0.         0.
  0.         0.81480247]
 [0.39204401 0.27894255 0.39204401 0.39204401 0.39204401 0.39204401
  0.39204401 0.        ]]
pairwise_similarity: [[1.         0.16171378]
 [0.16171378 1.        ]]
cosine_similarity: 0.16171378066252898
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: I miss the 2000 avs with patrick roy forsberg sakic
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['i', 'miss', 'the', 'avs', 'with', 'patrick', 'roy', 'forsberg', 'sakic']
cosine_similarity: 0.8916110992431641
train_input: [0.16171378066252898, 0.8916111], train_label: 0
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: I want avs to tue this game and make the Wild really nervous
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.
  0.         0.         0.81480247]
 [0.25969799 0.36499647 0.36499647 0.36499647 0.36499647 0.36499647
  0.36499647 0.36499647 0.        ]]
pairwise_similarity: [[1.         0.15055697]
 [0.15055697 1.        ]]
cosine_similarity: 0.15055696960204948
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: I want avs to tue this game and make the Wild really nervous
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['i', 'want', 'avs', 'to', 'tue', 'this', 'game', 'and', 'make', 'the', 'wild', 'really', 'nervous']
cosine_similarity: 0.9511000514030457
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: I would be too if I played for the Avs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.81480247]
 [0.57973867 0.81480247 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: I would be too if I played for the Avs
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['i', 'would', 'be', 'too', 'if', 'i', 'played', 'for', 'the', 'avs']
cosine_similarity: 0.952051043510437
train_input: [0.3360969272762574, 0.95205104], train_label: 0
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: haha Im totally watching wildavs now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.         0.         0.         0.         0.
  0.70710678]
 [0.         0.4472136  0.4472136  0.4472136  0.4472136  0.4472136
  0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: haha Im totally watching wildavs now
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['haha', 'im', 'totally', 'watching', 'now']
cosine_similarity: 0.8241454362869263
train_input: [0.0, 0.82414544], train_label: 0
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: if the avs can beat Minnesota
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.81480247]
 [0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: if the avs can beat Minnesota
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['if', 'the', 'avs', 'can', 'beat', 'minnesota']
cosine_similarity: 0.9561329483985901
train_input: [0.2605556710562624, 0.95613295], train_label: 0
TF_IDF_cosine_similarity: sentence1: Please give the Avs a win, sentence2: the one time I want the Avs to win
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.         0.         0.70710678]
 [0.40993715 0.57615236 0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: Please give the Avs a win, sentence2: the one time I want the Avs to win
After tokenization, sentence1: ['please', 'give', 'the', 'avs', 'a', 'win'], sentence2: ['the', 'one', 'time', 'i', 'want', 'the', 'avs', 'to', 'win']
cosine_similarity: 0.9766988754272461
train_input: [0.5797386715376657, 0.9766989], train_label: 1
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: All the how to videos that you need for BBQ
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.57735027 0.         0.57735027 0.        ]
 [0.57735027 0.         0.         0.57735027 0.         0.57735027]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: All the how to videos that you need for BBQ
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['all', 'the', 'how', 'to', 'videos', 'that', 'you', 'need', 'for', 'bbq']
cosine_similarity: 0.9346791505813599
train_input: [0.0, 0.93467915], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: BBQ to celebrate my birthday haha 2 days late
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57735027 0.         0.         0.
  0.         0.57735027 0.57735027]
 [0.40824829 0.40824829 0.         0.40824829 0.40824829 0.40824829
  0.40824829 0.         0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: BBQ to celebrate my birthday haha 2 days late
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['bbq', 'to', 'celebrate', 'my', 'birthday', 'haha', 'days', 'late']
cosine_similarity: 0.8799739480018616
train_input: [0.0, 0.87997395], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: Bout to take a shower than hit up this BBQ
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.         0.57735027 0.
  0.57735027]
 [0.5        0.         0.5        0.5        0.         0.5
  0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: Bout to take a shower than hit up this BBQ
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['bout', 'to', 'take', 'a', 'shower', 'than', 'hit', 'up', 'this', 'bbq']
cosine_similarity: 0.924994170665741
train_input: [0.0, 0.9249942], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: I got BBQ sauce on my uniform
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.57735027 0.         0.57735027
  0.        ]
 [0.5        0.         0.5        0.         0.5        0.
  0.5       ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: I got BBQ sauce on my uniform
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['i', 'got', 'bbq', 'sauce', 'on', 'my', 'uniform']
cosine_similarity: 0.9104520678520203
train_input: [0.0, 0.91045207], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: I guess it s a BBQbrolaws playing the game here today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.         0.57735027 0.
  0.57735027 0.        ]
 [0.4472136  0.         0.4472136  0.4472136  0.         0.4472136
  0.         0.4472136 ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: I guess it s a BBQbrolaws playing the game here today
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['i', 'guess', 'it', 's', 'a', 'playing', 'the', 'game', 'here', 'today']
cosine_similarity: 0.9502550363540649
train_input: [0.0, 0.95025504], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: I need somebody to BBQ
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.57735027 0.         0.         0.57735027]
 [0.57735027 0.         0.         0.57735027 0.57735027 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: I need somebody to BBQ
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['i', 'need', 'somebody', 'to', 'bbq']
cosine_similarity: 0.8939036130905151
train_input: [0.0, 0.8939036], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: I want to go swimming today so i guess ima hit up nacwc bbq
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.         0.         0.57735027
  0.         0.57735027 0.         0.         0.        ]
 [0.35355339 0.         0.35355339 0.35355339 0.35355339 0.
  0.35355339 0.         0.35355339 0.35355339 0.35355339]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: I want to go swimming today so i guess ima hit up nacwc bbq
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['i', 'want', 'to', 'go', 'swimming', 'today', 'so', 'i', 'guess', 'ima', 'hit', 'up', 'bbq']
cosine_similarity: 0.8977055549621582
train_input: [0.0, 0.89770555], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: got to plan another bbq soon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.57735027 0.         0.
  0.57735027]
 [0.5        0.         0.5        0.         0.5        0.5
  0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: got to plan another bbq soon
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['got', 'to', 'plan', 'another', 'bbq', 'soon']
cosine_similarity: 0.9325404167175293
train_input: [0.0, 0.9325404], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: home made rub and BBQ sauce
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.57735027 0.         0.
  0.57735027]
 [0.5        0.         0.5        0.         0.5        0.5
  0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: home made rub and BBQ sauce
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['home', 'made', 'rub', 'and', 'bbq', 'sauce']
cosine_similarity: 0.8616551756858826
train_input: [0.0, 0.8616552], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: made my famous bbq chicken
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.         0.57735027 0.57735027]
 [0.57735027 0.         0.57735027 0.57735027 0.         0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sticker for blackstrapbbq on the metro, sentence2: made my famous bbq chicken
After tokenization, sentence1: ['sticker', 'for', 'on', 'the', 'metro'], sentence2: ['made', 'my', 'famous', 'bbq', 'chicken']
cosine_similarity: 0.8560106754302979
train_input: [0.0, 0.8560107], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: Bale is a great player one must admit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.6316672  0.         0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: Bale is a great player one must admit
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['bale', 'is', 'a', 'great', 'player', 'one', 'must', 'admit']
cosine_similarity: 0.9478165507316589
train_input: [0.17077611319011649, 0.94781655], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: Bale just scored a disgusting goal but wigan just scored
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.        ]
 [0.         0.21440614 0.30134034 0.21440614 0.60268068 0.60268068
  0.30134034]]
pairwise_similarity: [[1.         0.21507033]
 [0.21507033 1.        ]]
cosine_similarity: 0.21507032570577075
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: Bale just scored a disgusting goal but wigan just scored
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['bale', 'just', 'scored', 'a', 'disgusting', 'goal', 'but', 'wigan', 'just', 'scored']
cosine_similarity: 0.9399311542510986
train_input: [0.21507032570577075, 0.93993115], train_label: 1
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: Goal if the season there from Gareth Bale
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.        ]
 [0.         0.40993715 0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: Goal if the season there from Gareth Bale
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['goal', 'if', 'the', 'season', 'there', 'from', 'gareth', 'bale']
cosine_similarity: 0.9555843472480774
train_input: [0.4112070550676187, 0.95558435], train_label: 1
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: Have to say I loved Bale s goal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.        ]
 [0.         0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: Have to say I loved Bale s goal
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['have', 'to', 'say', 'i', 'loved', 'bale', 's', 'goal']
cosine_similarity: 0.9414970874786377
train_input: [0.4112070550676187, 0.9414971], train_label: 1
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: I am one Gareth Bale ankle roll away from Scotch and The Smiths
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.         0.44943642 0.         0.6316672
  0.         0.         0.        ]
 [0.         0.39204401 0.39204401 0.27894255 0.39204401 0.
  0.39204401 0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: I am one Gareth Bale ankle roll away from Scotch and The Smiths
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['i', 'am', 'one', 'gareth', 'bale', 'ankle', 'roll', 'away', 'from', 'scotch', 'and', 'the', 'smiths']
cosine_similarity: 0.9189667701721191
train_input: [0.12536693798731732, 0.91896677], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: I think Bale s inherited P
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.6316672  0.         0.        ]
 [0.         0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: I think Bale s inherited P
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['i', 'think', 'bale', 's', 'inherited', 'p']
cosine_similarity: 0.9240344166755676
train_input: [0.20199309249791833, 0.9240344], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: If youre an American soccer fan Gareth Bale is kind of a frenemy isnt he
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.
  0.6316672  0.         0.         0.         0.        ]
 [0.         0.34287126 0.24395573 0.34287126 0.34287126 0.34287126
  0.         0.34287126 0.34287126 0.34287126 0.34287126]]
pairwise_similarity: [[1.         0.10964259]
 [0.10964259 1.        ]]
cosine_similarity: 0.10964258683453854
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: If youre an American soccer fan Gareth Bale is kind of a frenemy isnt he
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['if', 'youre', 'an', 'american', 'soccer', 'fan', 'gareth', 'bale', 'is', 'kind', 'of', 'a', 'frenemy', 'isnt', 'he']
cosine_similarity: 0.9402426481246948
train_input: [0.10964258683453854, 0.94024265], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: go Bale make a wonderful goal again bro
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.        ]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: go Bale make a wonderful goal again bro
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['go', 'bale', 'make', 'a', 'wonderful', 'goal', 'again', 'bro']
cosine_similarity: 0.9417780637741089
train_input: [0.3563004293331381, 0.94177806], train_label: 1
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: had spurs win 21 bale first scorer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.6316672  0.         0.
  0.        ]
 [0.47107781 0.         0.33517574 0.         0.47107781 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: had spurs win 21 bale first scorer
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['had', 'spurs', 'win', 'bale', 'first', 'scorer']
cosine_similarity: 0.9355267882347107
train_input: [0.15064018498706508, 0.9355268], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: team bale for champions league next season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.6316672  0.         0.
  0.        ]
 [0.         0.33517574 0.47107781 0.         0.47107781 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Bale s goal is amazing, sentence2: team bale for champions league next season
After tokenization, sentence1: ['bale', 's', 'goal', 'is', 'amazing'], sentence2: ['team', 'bale', 'for', 'champions', 'league', 'next', 'season']
cosine_similarity: 0.9411962032318115
train_input: [0.15064018498706508, 0.9411962], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Id rather have Dortmund win CL than Barca or bayern
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.         0.
  0.53404633 0.         0.53404633]
 [0.30321606 0.4261596  0.4261596  0.         0.4261596  0.4261596
  0.         0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Id rather have Dortmund win CL than Barca or bayern
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['id', 'rather', 'have', 'dortmund', 'win', 'cl', 'than', 'barca', 'or', 'bayern']
cosine_similarity: 0.9611742496490479
train_input: [0.11521554337793122, 0.96117425], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: LETS GO BARCA EVERYONE BELIEVES YOU COULD DO IT
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]
 [0.44943642 0.6316672  0.         0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: LETS GO BARCA EVERYONE BELIEVES YOU COULD DO IT
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['lets', 'go', 'barca', 'everyone', 'believes', 'you', 'could', 'do', 'it']
cosine_similarity: 0.9776498675346375
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: LMAO Barca to turn a 4 goal deficit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.         0.53404633
  0.         0.53404633]
 [0.33517574 0.         0.47107781 0.47107781 0.47107781 0.
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: LMAO Barca to turn a 4 goal deficit
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['lmao', 'barca', 'to', 'turn', 'a', 'goal', 'deficit']
cosine_similarity: 0.9667292833328247
train_input: [0.1273595297947935, 0.9667293], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Let s see Barca score 5goals tomorrow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.53404633 0.
  0.         0.53404633]
 [0.47107781 0.33517574 0.         0.47107781 0.         0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Let s see Barca score 5goals tomorrow
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['let', 's', 'see', 'barca', 'score', 'tomorrow']
cosine_similarity: 0.9599084854125977
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Let s see barca againts bayern tomorrow after iron man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.         0.
  0.53404633 0.         0.         0.53404633]
 [0.39204401 0.27894255 0.39204401 0.         0.39204401 0.39204401
  0.         0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Let s see barca againts bayern tomorrow after iron man
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['let', 's', 'see', 'barca', 'againts', 'bayern', 'tomorrow', 'after', 'iron', 'man']
cosine_similarity: 0.961694061756134
train_input: [0.1059921313509325, 0.96169406], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: My bets on Barca still
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.53404633]
 [0.57973867 0.81480247 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: My bets on Barca still
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['my', 'bets', 'on', 'barca', 'still']
cosine_similarity: 0.9649374485015869
train_input: [0.22028815056182965, 0.96493745], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Okay Im taking back my good luck to Barca
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.         0.53404633
  0.         0.         0.53404633]
 [0.30321606 0.         0.4261596  0.4261596  0.4261596  0.
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Okay Im taking back my good luck to Barca
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['okay', 'im', 'taking', 'back', 'my', 'good', 'luck', 'to', 'barca']
cosine_similarity: 0.9511163830757141
train_input: [0.11521554337793122, 0.9511164], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: The barca that dives cheats
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]
 [0.44943642 0.6316672  0.         0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: The barca that dives cheats
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['the', 'barca', 'that', 'dives', 'cheats']
cosine_similarity: 0.9320107102394104
train_input: [0.17077611319011649, 0.9320107], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Will Barca overcome the impressive 40
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.53404633 0.
  0.53404633]
 [0.53404633 0.37997836 0.         0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: Will Barca overcome the impressive 40
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['will', 'barca', 'overcome', 'the', 'impressive']
cosine_similarity: 0.9426644444465637
train_input: [0.1443835552773867, 0.94266444], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: YOURE A BARCA FAN LE 7ATIT LEWANDOWSKI
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.         0.
  0.53404633 0.53404633 0.        ]
 [0.4261596  0.30321606 0.         0.4261596  0.4261596  0.4261596
  0.         0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Wonder if Barca will make a comeback, sentence2: YOURE A BARCA FAN LE 7ATIT LEWANDOWSKI
After tokenization, sentence1: ['wonder', 'if', 'barca', 'will', 'make', 'a', 'comeback'], sentence2: ['youre', 'a', 'barca', 'fan', 'le', 'lewandowski']
cosine_similarity: 0.8613075613975525
train_input: [0.11521554337793122, 0.86130756], train_label: 0
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Barkevious Mingo baby Ill take it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.31779954 0.44665616]
 [0.57615236 0.40993715 0.         0.57615236 0.         0.
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Barkevious Mingo baby Ill take it
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['barkevious', 'mingo', 'baby', 'ill', 'take', 'it']
cosine_similarity: 0.9561463594436646
train_input: [0.2605556710562624, 0.95614636], train_label: 0
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Barkevious Mingo is such a goofy name
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.44665616 0.31779954
  0.44665616]
 [0.50154891 0.         0.70490949 0.         0.         0.50154891
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Barkevious Mingo is such a goofy name
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['barkevious', 'mingo', 'is', 'such', 'a', 'goofy', 'name']
cosine_similarity: 0.9184694290161133
train_input: [0.31878402175377923, 0.9184694], train_label: 0
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Barkevious Mingo is the first person named Barkevious to do something positive with his life
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.44665616 0.31779954
  0.         0.         0.44665616 0.        ]
 [0.55681737 0.         0.39129369 0.         0.         0.27840869
  0.39129369 0.39129369 0.         0.39129369]]
pairwise_similarity: [[1.         0.26543445]
 [0.26543445 1.        ]]
cosine_similarity: 0.265434454961717
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Barkevious Mingo is the first person named Barkevious to do something positive with his life
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['barkevious', 'mingo', 'is', 'the', 'first', 'person', 'named', 'barkevious', 'to', 'do', 'something', 'positive', 'with', 'his', 'life']
cosine_similarity: 0.9494747519493103
train_input: [0.265434454961717, 0.94947475], train_label: 0
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Barkevious Mingo won name of the year in 2009
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.44665616 0.44665616 0.31779954
  0.44665616 0.         0.        ]
 [0.49922133 0.35520009 0.         0.         0.         0.35520009
  0.         0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Barkevious Mingo won name of the year in 2009
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['barkevious', 'mingo', 'won', 'name', 'of', 'the', 'year', 'in']
cosine_similarity: 0.9591752290725708
train_input: [0.22576484600261604, 0.9591752], train_label: 0
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Honestly Barkevious Mingo is the best name of any human being ever
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.         0.44665616
  0.44665616 0.31779954 0.44665616]
 [0.35520009 0.49922133 0.         0.49922133 0.49922133 0.
  0.         0.35520009 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: Honestly Barkevious Mingo is the best name of any human being ever
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['honestly', 'barkevious', 'mingo', 'is', 'the', 'best', 'name', 'of', 'any', 'human', 'being', 'ever']
cosine_similarity: 0.9220370054244995
train_input: [0.22576484600261604, 0.922037], train_label: 0
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: I got Barkevious Mingo from a one night stand before
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.44665616 0.31779954
  0.         0.44665616 0.        ]
 [0.35520009 0.         0.49922133 0.         0.         0.35520009
  0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: I got Barkevious Mingo from a one night stand before
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['i', 'got', 'barkevious', 'mingo', 'from', 'a', 'one', 'night', 'stand', 'before']
cosine_similarity: 0.9437630772590637
train_input: [0.22576484600261604, 0.9437631], train_label: 0
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: I think Barkevious Mingo s mother had the good stuff in her epidural
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.         0.44665616 0.44665616
  0.31779954 0.         0.44665616 0.         0.        ]
 [0.29017021 0.         0.4078241  0.4078241  0.         0.
  0.29017021 0.4078241  0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: I think Barkevious Mingo s mother had the good stuff in her epidural
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['i', 'think', 'barkevious', 'mingo', 's', 'mother', 'had', 'the', 'good', 'stuff', 'in', 'her', 'epidural']
cosine_similarity: 0.9352515935897827
train_input: [0.18443191662261305, 0.9352516], train_label: 0
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: My favorite name of the NFLDraft is Barkevious Mingo the Defensive End out of LSU
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.         0.         0.47042643
  0.33471228 0.33471228 0.         0.47042643]
 [0.30287281 0.         0.42567716 0.42567716 0.42567716 0.
  0.30287281 0.30287281 0.42567716 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: My favorite name of the NFLDraft is Barkevious Mingo the Defensive End out of LSU
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['my', 'favorite', 'name', 'of', 'the', 'nfldraft', 'is', 'barkevious', 'mingo', 'the', 'defensive', 'end', 'out', 'of', 'lsu']
cosine_similarity: 0.9620376825332642
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: My twitter name was Barkevious Mingo for a while
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.31779954 0.44665616
  0.        ]
 [0.50154891 0.         0.         0.         0.50154891 0.
  0.70490949]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: My twitter name was Barkevious Mingo for a while
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['my', 'twitter', 'name', 'was', 'barkevious', 'mingo', 'for', 'a', 'while']
cosine_similarity: 0.9291301965713501
train_input: [0.31878402175377923, 0.9291302], train_label: 0
TF_IDF_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: This dude s name was Barkevious Mingo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.44665616 0.31779954
  0.44665616]
 [0.50154891 0.         0.70490949 0.         0.         0.50154891
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: I like the Browns pick from LSU Barkevious Mingo, sentence2: This dude s name was Barkevious Mingo
After tokenization, sentence1: ['i', 'like', 'the', 'browns', 'pick', 'from', 'lsu', 'barkevious', 'mingo'], sentence2: ['this', 'dude', 's', 'name', 'was', 'barkevious', 'mingo']
cosine_similarity: 0.9478015303611755
train_input: [0.31878402175377923, 0.94780153], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: A little Salty the Giants passed up on Barrett Jones though
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.         0.49922133
  0.49922133 0.         0.49922133]
 [0.31779954 0.44665616 0.31779954 0.44665616 0.44665616 0.
  0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: A little Salty the Giants passed up on Barrett Jones though
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['a', 'little', 'salty', 'the', 'giants', 'passed', 'up', 'on', 'barrett', 'jones', 'though']
cosine_similarity: 0.9787203669548035
train_input: [0.22576484600261604, 0.97872037], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Barrett Jones goes to St Louis the pick before Dallas
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.37930349 0.         0.37930349
  0.53309782 0.         0.53309782]
 [0.30287281 0.42567716 0.42567716 0.30287281 0.42567716 0.30287281
  0.         0.42567716 0.        ]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Barrett Jones goes to St Louis the pick before Dallas
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['barrett', 'jones', 'goes', 'to', 'st', 'louis', 'the', 'pick', 'before', 'dallas']
cosine_similarity: 0.9827157258987427
train_input: [0.34464214103805474, 0.9827157], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Barrett Jones is the 37th SEC player drafted
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.35520009 0.49922133 0.
  0.49922133 0.         0.49922133]
 [0.44665616 0.31779954 0.44665616 0.31779954 0.         0.44665616
  0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Barrett Jones is the 37th SEC player drafted
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['barrett', 'jones', 'is', 'the', 'sec', 'player', 'drafted']
cosine_similarity: 0.9617747664451599
train_input: [0.22576484600261604, 0.96177477], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Barrett Jones to the Rams RAW VIDEO
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.37930349 0.         0.53309782
  0.        ]
 [0.37930349 0.37930349 0.         0.37930349 0.53309782 0.
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Barrett Jones to the Rams RAW VIDEO
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['barrett', 'jones', 'to', 'the', 'rams', 'raw', 'video']
cosine_similarity: 0.9827560782432556
train_input: [0.43161341897075145, 0.9827561], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Im a huge Barrett Jones fan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.35520009 0.49922133
  0.49922133 0.49922133]
 [0.35520009 0.49922133 0.49922133 0.49922133 0.35520009 0.
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Im a huge Barrett Jones fan
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['im', 'a', 'huge', 'barrett', 'jones', 'fan']
cosine_similarity: 0.9643681049346924
train_input: [0.2523342014336961, 0.9643681], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Love the Rams grabbing Barrett Jones
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.37930349 0.         0.53309782 0.37930349
  0.53309782]
 [0.37930349 0.53309782 0.37930349 0.53309782 0.         0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Love the Rams grabbing Barrett Jones
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['love', 'the', 'rams', 'grabbing', 'barrett', 'jones']
cosine_similarity: 0.9665846824645996
train_input: [0.43161341897075145, 0.9665847], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: St Louis just got the steal of the draft so far with Barrett Jones
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.35520009 0.
  0.         0.49922133 0.49922133 0.         0.         0.49922133]
 [0.25136004 0.35327777 0.35327777 0.35327777 0.25136004 0.35327777
  0.35327777 0.         0.         0.35327777 0.35327777 0.        ]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: St Louis just got the steal of the draft so far with Barrett Jones
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['st', 'louis', 'just', 'got', 'the', 'steal', 'of', 'the', 'draft', 'so', 'far', 'with', 'barrett', 'jones']
cosine_similarity: 0.9783441424369812
train_input: [0.17856621555757476, 0.97834414], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: We all love the Barrett Jones pick cuz weve heard of him
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.37930349 0.         0.37930349
  0.53309782 0.53309782 0.        ]
 [0.30287281 0.42567716 0.42567716 0.30287281 0.42567716 0.30287281
  0.         0.         0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: We all love the Barrett Jones pick cuz weve heard of him
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['we', 'all', 'love', 'the', 'barrett', 'jones', 'pick', 'cuz', 'weve', 'heard', 'of', 'him']
cosine_similarity: 0.9536048173904419
train_input: [0.34464214103805474, 0.9536048], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Well there goes Barrett Jones to the Rams
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.37930349 0.53309782 0.37930349 0.53309782]
 [0.44832087 0.63009934 0.44832087 0.         0.44832087 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Well there goes Barrett Jones to the Rams
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['well', 'there', 'goes', 'barrett', 'jones', 'to', 'the', 'rams']
cosine_similarity: 0.9862827658653259
TF_IDF_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Where are the Barrett jones tweets
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.        ]
 [0.50154891 0.50154891 0.         0.         0.         0.70490949]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Barrett jones is a sweeeeeet pick for the rams, sentence2: Where are the Barrett jones tweets
After tokenization, sentence1: ['barrett', 'jones', 'is', 'a', 'sweeeeeet', 'pick', 'for', 'the', 'rams'], sentence2: ['where', 'are', 'the', 'barrett', 'jones', 'tweets']
cosine_similarity: 0.9706204533576965
train_input: [0.3563004293331381, 0.97062045], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Awwwww yeah barry sanders on the cover of madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.35464863 0.49844628 0.35464863
  0.49844628 0.        ]
 [0.49844628 0.35464863 0.35464863 0.35464863 0.         0.35464863
  0.         0.49844628]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Awwwww yeah barry sanders on the cover of madden
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['awwwww', 'yeah', 'barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9828360676765442
train_input: [0.5031026124151313, 0.98283607], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Barry Sanders got the Madden cover and Denard Robinson got the NCAA cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.         0.35464863 0.
  0.49844628 0.         0.35464863 0.49844628]
 [0.21912062 0.43824124 0.30796639 0.61593278 0.21912062 0.30796639
  0.         0.30796639 0.21912062 0.        ]]
pairwise_similarity: [[1.         0.38855414]
 [0.38855414 1.        ]]
cosine_similarity: 0.38855414466577193
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Barry Sanders got the Madden cover and Denard Robinson got the NCAA cover
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'got', 'the', 'madden', 'cover', 'and', 'denard', 'robinson', 'got', 'the', 'ncaa', 'cover']
cosine_similarity: 0.9952933192253113
train_input: [0.38855414466577193, 0.9952933], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Barry Sanders on the new Madden cover not bad
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.35464863 0.         0.49844628
  0.35464863 0.49844628]
 [0.49844628 0.35464863 0.35464863 0.35464863 0.49844628 0.
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Barry Sanders on the new Madden cover not bad
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'on', 'the', 'new', 'madden', 'cover', 'not', 'bad']
cosine_similarity: 0.9805851578712463
train_input: [0.5031026124151313, 0.98058516], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Barry Sanders on the new cover of Madden 14
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.35464863 0.         0.49844628
  0.35464863 0.49844628]
 [0.49844628 0.35464863 0.35464863 0.35464863 0.49844628 0.
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Barry Sanders on the new cover of Madden 14
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'on', 'the', 'new', 'cover', 'of', 'madden']
cosine_similarity: 0.9759629368782043
train_input: [0.5031026124151313, 0.97596294], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Barry sanders gonna be on the Next madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.33471228 0.47042643 0.33471228
  0.47042643]
 [0.44832087 0.         0.63009934 0.44832087 0.         0.44832087
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Barry sanders gonna be on the Next madden
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'gonna', 'be', 'on', 'the', 'next', 'madden']
cosine_similarity: 0.9811370968818665
train_input: [0.4501755023269897, 0.9811371], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Denard Robinson on the NCAA cover and Barry Sanders on the madden 25 cover dayumm
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.         0.         0.35464863
  0.         0.49844628 0.         0.35464863 0.49844628]
 [0.3421187  0.24342027 0.48684054 0.3421187  0.3421187  0.24342027
  0.3421187  0.         0.3421187  0.24342027 0.        ]]
pairwise_similarity: [[1.         0.43164333]
 [0.43164333 1.        ]]
cosine_similarity: 0.43164332907309677
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Denard Robinson on the NCAA cover and Barry Sanders on the madden 25 cover dayumm
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['denard', 'robinson', 'on', 'the', 'ncaa', 'cover', 'and', 'barry', 'sanders', 'on', 'the', 'madden', 'cover', 'dayumm']
cosine_similarity: 0.9866642951965332
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: First Calvin put the Madden curse to rest and now Barry Sanders on on the cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.         0.35464863 0.49844628
  0.         0.35464863 0.49844628]
 [0.3174044  0.44610081 0.3174044  0.44610081 0.3174044  0.
  0.44610081 0.3174044  0.        ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: First Calvin put the Madden curse to rest and now Barry Sanders on on the cover
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['first', 'calvin', 'put', 'the', 'madden', 'curse', 'to', 'rest', 'and', 'now', 'barry', 'sanders', 'on', 'on', 'the', 'cover']
cosine_similarity: 0.9690462350845337
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Hell ya Barry Sanders is on the new cover of madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.35464863 0.         0.49844628
  0.35464863 0.49844628 0.        ]
 [0.3174044  0.3174044  0.44610081 0.3174044  0.44610081 0.
  0.3174044  0.         0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: Hell ya Barry Sanders is on the new cover of madden
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['hell', 'ya', 'barry', 'sanders', 'is', 'on', 'the', 'new', 'cover', 'of', 'madden']
cosine_similarity: 0.9701412916183472
train_input: [0.4502681446556265, 0.9701413], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: So Barry Sanders is on the cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.47042643 0.33471228 0.47042643]
 [0.57735027 0.57735027 0.         0.         0.57735027 0.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376658
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: So Barry Sanders is on the cover
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['so', 'barry', 'sanders', 'is', 'on', 'the', 'cover']
cosine_similarity: 0.970487654209137
train_input: [0.5797386715376658, 0.97048765], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: on the cover of NCAA 14 now Barry Sanders on the cove of Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.         0.35464863 0.35464863 0.
  0.49844628 0.35464863 0.49844628]
 [0.44610081 0.3174044  0.44610081 0.3174044  0.3174044  0.44610081
  0.         0.3174044  0.        ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Barry Sanders really won the Madden cover, sentence2: on the cover of NCAA 14 now Barry Sanders on the cove of Madden
After tokenization, sentence1: ['barry', 'sanders', 'really', 'won', 'the', 'madden', 'cover'], sentence2: ['on', 'the', 'cover', 'of', 'ncaa', 'now', 'barry', 'sanders', 'on', 'the', 'cove', 'of', 'madden']
cosine_similarity: 0.9636586904525757
train_input: [0.4502681446556265, 0.9636587], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry Sanders gets the Madden25 Cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.         0.37930349
  0.53309782]
 [0.37930349 0.37930349 0.53309782 0.         0.53309782 0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry Sanders gets the Madden25 Cover
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'gets', 'the', 'cover']
cosine_similarity: 0.9757918119430542
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry Sanders wins the Madden cover for 14
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.4090901  0.4090901  0.4090901  0.57496187
  0.        ]
 [0.49844628 0.35464863 0.35464863 0.35464863 0.35464863 0.
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry Sanders wins the Madden cover for 14
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'wins', 'the', 'madden', 'cover', 'for']
cosine_similarity: 0.9789696931838989
train_input: [0.5803329846765685, 0.9789697], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry Sanders wins the madden 14 cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.4090901  0.4090901  0.4090901  0.57496187
  0.        ]
 [0.49844628 0.35464863 0.35464863 0.35464863 0.35464863 0.
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry Sanders wins the madden 14 cover
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'wins', 'the', 'madden', 'cover']
cosine_similarity: 0.9680759906768799
train_input: [0.5803329846765685, 0.968076], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry sanders is now on the cover of EAMaddenNFL 25th anniversary game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37930349 0.37930349 0.         0.
  0.53309782 0.37930349 0.53309782]
 [0.42567716 0.42567716 0.30287281 0.30287281 0.42567716 0.42567716
  0.         0.30287281 0.        ]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry sanders is now on the cover of EAMaddenNFL 25th anniversary game
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'is', 'now', 'on', 'the', 'cover', 'of', 'anniversary', 'game']
cosine_similarity: 0.9889048337936401
train_input: [0.34464214103805474, 0.98890483], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry sanders wins madden cover over ap
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.4090901  0.4090901  0.4090901  0.57496187
  0.        ]
 [0.49844628 0.35464863 0.35464863 0.35464863 0.35464863 0.
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Barry sanders wins madden cover over ap
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'wins', 'madden', 'cover', 'over', 'ap']
cosine_similarity: 0.9677318334579468
train_input: [0.5803329846765685, 0.96773183], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: It s awesome that Barry Sanders is on the Madden cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.4090901  0.4090901  0.4090901  0.57496187]
 [0.57496187 0.4090901  0.4090901  0.4090901  0.4090901  0.        ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: It s awesome that Barry Sanders is on the Madden cover
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['it', 's', 'awesome', 'that', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover']
cosine_similarity: 0.9925521016120911
train_input: [0.6694188517266485, 0.9925521], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: SO to Barry Sanders for winning the cover vote
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.37930349 0.         0.53309782
  0.        ]
 [0.37930349 0.37930349 0.         0.37930349 0.53309782 0.
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: SO to Barry Sanders for winning the cover vote
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['so', 'to', 'barry', 'sanders', 'for', 'winning', 'the', 'cover', 'vote']
cosine_similarity: 0.972055196762085
train_input: [0.43161341897075145, 0.9720552], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Ughhhhhhh barry sanders beat ap in the madden cover vote
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.         0.4090901  0.4090901  0.4090901
  0.         0.         0.57496187]
 [0.40740124 0.28986934 0.40740124 0.28986934 0.28986934 0.28986934
  0.40740124 0.40740124 0.        ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Ughhhhhhh barry sanders beat ap in the madden cover vote
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'beat', 'ap', 'in', 'the', 'madden', 'cover', 'vote']
cosine_similarity: 0.981559693813324
train_input: [0.4743307064971939, 0.9815597], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Yesss Barry Sanders got the cover of Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.         0.4090901  0.4090901  0.57496187
  0.        ]
 [0.35464863 0.35464863 0.49844628 0.35464863 0.35464863 0.
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: Yesss Barry Sanders got the cover of Madden
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['yesss', 'barry', 'sanders', 'got', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9766035079956055
train_input: [0.5803329846765685, 0.9766035], train_label: 1
TF_IDF_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: so Barry Sanders is the new face of madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.         0.37930349 0.         0.37930349
  0.53309782]
 [0.37930349 0.         0.53309782 0.37930349 0.53309782 0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Wait Barry Sanders is on the madden cover, sentence2: so Barry Sanders is the new face of madden
After tokenization, sentence1: ['wait', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover'], sentence2: ['so', 'barry', 'sanders', 'is', 'the', 'new', 'face', 'of', 'madden']
cosine_similarity: 0.9893022775650024
train_input: [0.43161341897075145, 0.9893023], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Adrian Peterson lost the Madden cover to Barry Sanders
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.         0.4090901  0.4090901  0.         0.4090901
  0.         0.4090901 ]
 [0.         0.44610081 0.3174044  0.3174044  0.44610081 0.3174044
  0.44610081 0.3174044 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Adrian Peterson lost the Madden cover to Barry Sanders
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['adrian', 'peterson', 'lost', 'the', 'madden', 'cover', 'to', 'barry', 'sanders']
cosine_similarity: 0.9747812151908875
train_input: [0.5193879933129156, 0.9747812], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: BARRY SANDERS WILL BE ON THE NEW MADDEN GAME COVER
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.4090901  0.         0.4090901  0.
  0.4090901 ]
 [0.         0.35464863 0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: BARRY SANDERS WILL BE ON THE NEW MADDEN GAME COVER
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'will', 'be', 'on', 'the', 'new', 'madden', 'game', 'cover']
cosine_similarity: 0.9820584058761597
train_input: [0.5803329846765685, 0.9820584], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders beat AP out for the cover of Madden 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.         0.         0.4090901  0.         0.4090901
  0.4090901  0.4090901 ]
 [0.         0.44610081 0.44610081 0.3174044  0.44610081 0.3174044
  0.3174044  0.3174044 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders beat AP out for the cover of Madden 25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'beat', 'ap', 'out', 'for', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.98360276222229
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders on the cover of Madden ayeee
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.         0.4090901  0.4090901  0.4090901  0.4090901 ]
 [0.         0.57496187 0.4090901  0.4090901  0.4090901  0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders on the cover of Madden ayeee
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9941120743751526
train_input: [0.6694188517266485, 0.9941121], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders shouldnt be on the cover of madden not should the name be Madden 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.         0.4090901  0.4090901  0.4090901  0.4090901
  0.        ]
 [0.         0.42471719 0.30218978 0.30218978 0.60437955 0.30218978
  0.42471719]]
pairwise_similarity: [[1.         0.61811423]
 [0.61811423 1.        ]]
cosine_similarity: 0.6181142335061833
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders shouldnt be on the cover of madden not should the name be Madden 25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'shouldnt', 'be', 'on', 'the', 'cover', 'of', 'madden', 'not', 'should', 'the', 'name', 'be', 'madden']
cosine_similarity: 0.9726148247718811
train_input: [0.6181142335061833, 0.9726148], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry sanders is already on the cover of NCAA 13
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53309782 0.37930349 0.37930349 0.53309782 0.
  0.37930349]
 [0.53309782 0.         0.37930349 0.37930349 0.         0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry sanders is already on the cover of NCAA 13
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'is', 'already', 'on', 'the', 'cover', 'of', 'ncaa']
cosine_similarity: 0.9894083738327026
train_input: [0.43161341897075145, 0.9894084], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Nic Barry Sanders on the cover of Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.4090901  0.4090901  0.         0.4090901 ]
 [0.         0.4090901  0.4090901  0.4090901  0.57496187 0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Nic Barry Sanders on the cover of Madden
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['nic', 'barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9893775582313538
train_input: [0.6694188517266485, 0.98937756], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: They really put Barry Sanders on the Madden cover smh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.4090901  0.4090901  0.         0.4090901
  0.        ]
 [0.         0.35464863 0.35464863 0.35464863 0.49844628 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: They really put Barry Sanders on the Madden cover smh
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['they', 'really', 'put', 'barry', 'sanders', 'on', 'the', 'madden', 'cover', 'smh']
cosine_similarity: 0.9684767127037048
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: nice to see Barry Sanders win the Madden cover vote
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.4090901  0.4090901  0.         0.4090901
  0.         0.        ]
 [0.         0.3174044  0.3174044  0.3174044  0.44610081 0.3174044
  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: nice to see Barry Sanders win the Madden cover vote
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['nice', 'to', 'see', 'barry', 'sanders', 'win', 'the', 'madden', 'cover', 'vote']
cosine_similarity: 0.9675605297088623
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: the oldskooler Barry Sanders get the madden25 cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.37930349 0.37930349 0.53309782 0.         0.
  0.37930349]
 [0.         0.37930349 0.37930349 0.         0.53309782 0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: the oldskooler Barry Sanders get the madden25 cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['the', 'barry', 'sanders', 'get', 'the', 'cover']
cosine_similarity: 0.9875573515892029
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders finally on the cover of madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.49844628 0.35464863 0.49844628
  0.35464863]
 [0.4090901  0.4090901  0.57496187 0.         0.4090901  0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders finally on the cover of madden
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'finally', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9934983849525452
train_input: [0.5803329846765685, 0.9934984], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders will be on the next cover of Madden 2014
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863]
 [0.57496187 0.4090901  0.4090901  0.         0.4090901  0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders will be on the next cover of Madden 2014
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'will', 'be', 'on', 'the', 'next', 'cover', 'of', 'madden']
cosine_similarity: 0.9963213801383972
train_input: [0.5803329846765685, 0.9963214], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry sanders beat AP in the cover vote
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.         0.33471228 0.47042643 0.47042643
  0.47042643 0.33471228 0.        ]
 [0.47042643 0.33471228 0.47042643 0.33471228 0.         0.
  0.         0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry sanders beat AP in the cover vote
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'beat', 'ap', 'in', 'the', 'cover', 'vote']
cosine_similarity: 0.9760072827339172
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Buying madden 25 because Barry Sanders will be on the cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.         0.35464863 0.49844628 0.35464863
  0.49844628 0.35464863]
 [0.49844628 0.35464863 0.49844628 0.35464863 0.         0.35464863
  0.         0.35464863]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Buying madden 25 because Barry Sanders will be on the cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['buying', 'madden', 'because', 'barry', 'sanders', 'will', 'be', 'on', 'the', 'cover']
cosine_similarity: 0.993104875087738
train_input: [0.5031026124151313, 0.9931049], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Glad Barry Sanders is on the cover of madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.49844628 0.35464863 0.49844628
  0.35464863]
 [0.4090901  0.4090901  0.57496187 0.         0.4090901  0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Glad Barry Sanders is on the cover of madden
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['glad', 'barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.994040310382843
train_input: [0.5803329846765685, 0.9940403], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Love seeing my boy Barry Sanders on the madden cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.49844628 0.         0.35464863
  0.49844628 0.35464863 0.        ]
 [0.3174044  0.44610081 0.3174044  0.         0.44610081 0.3174044
  0.         0.3174044  0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Love seeing my boy Barry Sanders on the madden cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['love', 'seeing', 'my', 'boy', 'barry', 'sanders', 'on', 'the', 'madden', 'cover']
cosine_similarity: 0.9872007966041565
train_input: [0.4502681446556265, 0.9872008], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Madden 25 Barry Sanders Cover Feat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.         0.49844628 0.35464863
  0.49844628 0.35464863]
 [0.49844628 0.35464863 0.35464863 0.49844628 0.         0.35464863
  0.         0.35464863]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Madden 25 Barry Sanders Cover Feat
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['madden', 'barry', 'sanders', 'cover', 'feat']
cosine_similarity: 0.8848893642425537
train_input: [0.5031026124151313, 0.88488936], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: barry sanders is on the cover of the new madden this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.53215436 0.37863221 0.37863221 0.37863221
  0.        ]
 [0.37863221 0.37863221 0.         0.37863221 0.37863221 0.37863221
  0.53215436]]
pairwise_similarity: [[1.         0.71681174]
 [0.71681174 1.        ]]
cosine_similarity: 0.7168117414430624
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: barry sanders is on the cover of the new madden this year
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'the', 'new', 'madden', 'this', 'year']
cosine_similarity: 0.9923490285873413
train_input: [0.7168117414430624, 0.992349], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: barry sanders won tha madden 25 cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863 0.         0.        ]
 [0.44610081 0.3174044  0.3174044  0.         0.3174044  0.
  0.3174044  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: barry sanders won tha madden 25 cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'won', 'tha', 'madden', 'cover']
cosine_similarity: 0.9442646503448486
train_input: [0.4502681446556265, 0.94426465], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: so how does the madden curse work now with barry sanders on the cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.         0.49844628 0.35464863
  0.49844628 0.35464863 0.        ]
 [0.3174044  0.3174044  0.44610081 0.44610081 0.         0.3174044
  0.         0.3174044  0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: so how does the madden curse work now with barry sanders on the cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['so', 'how', 'does', 'the', 'madden', 'curse', 'work', 'now', 'with', 'barry', 'sanders', 'on', 'the', 'cover']
cosine_similarity: 0.9895122051239014
train_input: [0.4502681446556265, 0.9895122], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry Sanders is front cover for Madden 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863]
 [0.57496187 0.4090901  0.4090901  0.         0.4090901  0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry Sanders is front cover for Madden 25
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'is', 'front', 'cover', 'for', 'madden']
cosine_similarity: 0.9851406216621399
train_input: [0.5803329846765685, 0.9851406], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry Sanders on the cover of 25 year madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863 0.        ]
 [0.49844628 0.35464863 0.35464863 0.         0.35464863 0.
  0.35464863 0.49844628]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry Sanders on the cover of 25 year madden
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'of', 'year', 'madden']
cosine_similarity: 0.9775335788726807
train_input: [0.5031026124151313, 0.9775336], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry Sanders on the cover of NCAA and Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.         0.49844628
  0.35464863]
 [0.4090901  0.4090901  0.         0.4090901  0.57496187 0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry Sanders on the cover of NCAA and Madden
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'of', 'ncaa', 'and', 'madden']
cosine_similarity: 0.9746699929237366
train_input: [0.5803329846765685, 0.97467], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry sanders made the cover of the next madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.49844628 0.35464863]
 [0.5        0.5        0.         0.5        0.         0.5       ]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry sanders made the cover of the next madden
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'made', 'the', 'cover', 'of', 'the', 'next', 'madden']
cosine_similarity: 0.9811742305755615
train_input: [0.7092972666062737, 0.98117423], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry sanders on the cover this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.47042643 0.47042643 0.33471228
  0.        ]
 [0.44832087 0.44832087 0.         0.         0.         0.44832087
  0.63009934]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Barry sanders on the cover this year
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'this', 'year']
cosine_similarity: 0.9731112718582153
train_input: [0.4501755023269897, 0.9731113], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Congrats to Barry Sanders on his Madden cover vote win
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863 0.         0.        ]
 [0.3174044  0.44610081 0.3174044  0.         0.3174044  0.
  0.3174044  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Congrats to Barry Sanders on his Madden cover vote win
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['congrats', 'to', 'barry', 'sanders', 'on', 'his', 'madden', 'cover', 'vote', 'win']
cosine_similarity: 0.9658651947975159
train_input: [0.4502681446556265, 0.9658652], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: I think Adrian Peterson is happy Barry Sanders won the Madden cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.49844628 0.         0.35464863
  0.         0.49844628 0.35464863 0.         0.        ]
 [0.37729199 0.26844636 0.26844636 0.         0.37729199 0.26844636
  0.37729199 0.         0.26844636 0.37729199 0.37729199]]
pairwise_similarity: [[1.         0.38081653]
 [0.38081653 1.        ]]
cosine_similarity: 0.3808165329771113
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: I think Adrian Peterson is happy Barry Sanders won the Madden cover
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['i', 'think', 'adrian', 'peterson', 'is', 'happy', 'barry', 'sanders', 'won', 'the', 'madden', 'cover']
cosine_similarity: 0.9876769185066223
train_input: [0.3808165329771113, 0.9876769], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Love the Madden cover for this year with Barry Sanders on it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.         0.35464863 0.49844628
  0.35464863 0.        ]
 [0.35464863 0.35464863 0.         0.49844628 0.35464863 0.
  0.35464863 0.49844628]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Love the Madden cover for this year with Barry Sanders on it
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['love', 'the', 'madden', 'cover', 'for', 'this', 'year', 'with', 'barry', 'sanders', 'on', 'it']
cosine_similarity: 0.971554160118103
train_input: [0.5031026124151313, 0.97155416], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Very cool to see Barry Sanders made the cover of the 25th edition of Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.         0.35464863 0.         0.49844628
  0.35464863 0.49844628 0.35464863]
 [0.44610081 0.3174044  0.44610081 0.3174044  0.44610081 0.
  0.3174044  0.         0.3174044 ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Very cool to see Barry Sanders made the cover of the 25th edition of Madden
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['very', 'cool', 'to', 'see', 'barry', 'sanders', 'made', 'the', 'cover', 'of', 'the', 'edition', 'of', 'madden']
cosine_similarity: 0.96836918592453
train_input: [0.4502681446556265, 0.9683692], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Well Barry Sanders is on the madden 25 cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863]
 [0.57496187 0.4090901  0.4090901  0.         0.4090901  0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders really got the madden cover, sentence2: Well Barry Sanders is on the madden 25 cover
After tokenization, sentence1: ['barry', 'sanders', 'really', 'got', 'the', 'madden', 'cover'], sentence2: ['well', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover']
cosine_similarity: 0.9865334630012512
train_input: [0.5803329846765685, 0.98653346], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Actually really happy Barry Sanders is on the cover of Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.         0.63009934 0.
  0.         0.44832087]
 [0.42567716 0.30287281 0.30287281 0.42567716 0.         0.42567716
  0.42567716 0.30287281]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Actually really happy Barry Sanders is on the cover of Madden
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['actually', 'really', 'happy', 'barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9930296540260315
train_input: [0.4073526042885674, 0.99302965], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders gonna be on the Madden 25 cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.         0.63009934 0.
  0.44832087]
 [0.47042643 0.33471228 0.33471228 0.47042643 0.         0.47042643
  0.33471228]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders gonna be on the Madden 25 cover
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['barry', 'sanders', 'gonna', 'be', 'on', 'the', 'madden', 'cover']
cosine_similarity: 0.9873144626617432
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders got the madden cover YEAAAAAAAAAAH
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.63009934 0.         0.44832087
  0.        ]
 [0.33471228 0.33471228 0.47042643 0.         0.47042643 0.33471228
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders got the madden cover YEAAAAAAAAAAH
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['barry', 'sanders', 'got', 'the', 'madden', 'cover', 'yeaaaaaaaaaah']
cosine_similarity: 0.9551264643669128
train_input: [0.4501755023269898, 0.95512646], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders is gonna be on the cover of this years Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.63009934 0.         0.44832087
  0.        ]
 [0.33471228 0.33471228 0.47042643 0.         0.47042643 0.33471228
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders is gonna be on the cover of this years Madden
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'cover', 'of', 'this', 'years', 'madden']
cosine_similarity: 0.9873841404914856
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders was just named the cover athlete for Madden 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44832087 0.44832087 0.         0.63009934
  0.         0.         0.44832087]
 [0.39166832 0.39166832 0.27867523 0.27867523 0.39166832 0.
  0.39166832 0.39166832 0.27867523]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders was just named the cover athlete for Madden 25
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['barry', 'sanders', 'was', 'just', 'named', 'the', 'cover', 'athlete', 'for', 'madden']
cosine_similarity: 0.9832497239112854
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders won the Madden cover after thr season AP had
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.63009934 0.         0.44832087
  0.         0.         0.        ]
 [0.39166832 0.27867523 0.27867523 0.         0.39166832 0.27867523
  0.39166832 0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Barry Sanders won the Madden cover after thr season AP had
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['barry', 'sanders', 'won', 'the', 'madden', 'cover', 'after', 'thr', 'season', 'ap', 'had']
cosine_similarity: 0.96160888671875
train_input: [0.3748077700589726, 0.9616089], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Gotta bump wale Barry Sanders for him winning the Madden cover vote
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.44832087 0.         0.63009934 0.
  0.44832087 0.         0.         0.        ]
 [0.25948224 0.36469323 0.25948224 0.36469323 0.         0.36469323
  0.25948224 0.36469323 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Gotta bump wale Barry Sanders for him winning the Madden cover vote
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['gotta', 'bump', 'wale', 'barry', 'sanders', 'for', 'him', 'winning', 'the', 'madden', 'cover', 'vote']
cosine_similarity: 0.9627278447151184
train_input: [0.3489939079552687, 0.96272784], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: How is Barry sanders gonna be on the cover of madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.63009934 0.         0.44832087]
 [0.37930349 0.37930349 0.53309782 0.         0.53309782 0.37930349]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: How is Barry sanders gonna be on the cover of madden
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['how', 'is', 'barry', 'sanders', 'gonna', 'be', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9907364845275879
train_input: [0.5101490193104813, 0.9907365], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: That s what s up Barry sanders on the cover of madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.44832087]
 [0.44832087 0.44832087 0.         0.63009934 0.44832087]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: That s what s up Barry sanders on the cover of madden
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['that', 's', 'what', 's', 'up', 'barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.979180634021759
train_input: [0.6029748160380572, 0.97918063], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Why tf Barry Sanders on the Madden cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.44832087 0.        ]
 [0.37930349 0.37930349 0.         0.53309782 0.37930349 0.53309782]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Barry Sanders on the cover is legit, sentence2: Why tf Barry Sanders on the Madden cover
After tokenization, sentence1: ['barry', 'sanders', 'on', 'the', 'cover', 'is', 'legit'], sentence2: ['why', 'tf', 'barry', 'sanders', 'on', 'the', 'madden', 'cover']
cosine_similarity: 0.9845114350318909
train_input: [0.5101490193104813, 0.98451144], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry Sanders named cover athlete of Madden NFL 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31661852 0.         0.31661852 0.63323704 0.31661852 0.
  0.         0.31661852 0.44499628]
 [0.30253071 0.42519636 0.30253071 0.30253071 0.30253071 0.42519636
  0.42519636 0.30253071 0.        ]]
pairwise_similarity: [[1.         0.57472096]
 [0.57472096 1.        ]]
cosine_similarity: 0.5747209632337569
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry Sanders named cover athlete of Madden NFL 25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['barry', 'sanders', 'named', 'cover', 'athlete', 'of', 'madden', 'nfl']
cosine_similarity: 0.9447324275970459
train_input: [0.5747209632337569, 0.9447324], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry Sanders on the cover of Madden 2014 Damn 25 years
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31661852 0.31661852 0.63323704 0.         0.31661852
  0.31661852 0.44499628 0.        ]
 [0.42519636 0.30253071 0.30253071 0.30253071 0.42519636 0.30253071
  0.30253071 0.         0.42519636]]
pairwise_similarity: [[1.         0.57472096]
 [0.57472096 1.        ]]
cosine_similarity: 0.5747209632337569
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry Sanders on the cover of Madden 2014 Damn 25 years
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden', 'damn', 'years']
cosine_similarity: 0.9898281693458557
train_input: [0.5747209632337569, 0.98982817], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry Sanders on the cover of Madden hell yeah
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42471719 0.30218978 0.60437955 0.         0.30218978 0.30218978
  0.42471719 0.        ]
 [0.         0.35464863 0.35464863 0.49844628 0.35464863 0.35464863
  0.         0.49844628]]
pairwise_similarity: [[1.         0.53585595]
 [0.53585595 1.        ]]
cosine_similarity: 0.5358559548726151
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry Sanders on the cover of Madden hell yeah
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden', 'hell', 'yeah']
cosine_similarity: 0.9850003123283386
train_input: [0.5358559548726151, 0.9850003], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry Sanders on the cover of Madden this yr
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42471719 0.30218978 0.60437955 0.30218978 0.30218978 0.42471719
  0.        ]
 [0.         0.4090901  0.4090901  0.4090901  0.4090901  0.
  0.57496187]]
pairwise_similarity: [[1.         0.61811423]
 [0.61811423 1.        ]]
cosine_similarity: 0.6181142335061833
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry Sanders on the cover of Madden this yr
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden', 'this', 'yr']
cosine_similarity: 0.9906362891197205
train_input: [0.6181142335061833, 0.9906363], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry sanders on the cover of madden my favorite player
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42471719 0.30218978 0.60437955 0.         0.30218978 0.
  0.30218978 0.42471719]
 [0.         0.35464863 0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.53585595]
 [0.53585595 1.        ]]
cosine_similarity: 0.5358559548726151
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry sanders on the cover of madden my favorite player
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden', 'my', 'favorite', 'player']
cosine_similarity: 0.9925139546394348
train_input: [0.5358559548726151, 0.99251395], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry sanders won to contest for the Madden 25 cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31661852 0.31661852 0.         0.63323704 0.31661852 0.31661852
  0.44499628 0.        ]
 [0.33425073 0.33425073 0.46977774 0.33425073 0.33425073 0.33425073
  0.         0.46977774]]
pairwise_similarity: [[1.         0.63497983]
 [0.63497983 1.        ]]
cosine_similarity: 0.6349798322956081
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Barry sanders won to contest for the Madden 25 cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['barry', 'sanders', 'won', 'to', 'contest', 'for', 'the', 'madden', 'cover']
cosine_similarity: 0.9829881191253662
train_input: [0.6349798322956081, 0.9829881], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Did Barry sanders just win the cover vote
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42471719 0.30218978 0.60437955 0.         0.         0.42471719
  0.30218978 0.30218978 0.        ]
 [0.         0.3174044  0.3174044  0.44610081 0.44610081 0.
  0.3174044  0.3174044  0.44610081]]
pairwise_similarity: [[1.         0.47958182]
 [0.47958182 1.        ]]
cosine_similarity: 0.47958182018755263
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: Did Barry sanders just win the cover vote
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['did', 'barry', 'sanders', 'just', 'win', 'the', 'cover', 'vote']
cosine_similarity: 0.9846038818359375
train_input: [0.47958182018755263, 0.9846039], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: So Barry Sanders is on cover of Madden 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31661852 0.31661852 0.63323704 0.31661852 0.31661852 0.44499628]
 [0.4472136  0.4472136  0.4472136  0.4472136  0.4472136  0.        ]]
pairwise_similarity: [[1.         0.84957665]
 [0.84957665 1.        ]]
cosine_similarity: 0.849576645041241
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: So Barry Sanders is on cover of Madden 25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['so', 'barry', 'sanders', 'is', 'on', 'cover', 'of', 'madden']
cosine_similarity: 0.9906535148620605
train_input: [0.849576645041241, 0.9906535], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: congrats on winning the cover vote Barry sanders is the best of the best
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42471719 0.30218978 0.         0.         0.60437955 0.42471719
  0.30218978 0.30218978 0.        ]
 [0.         0.25116439 0.70600557 0.35300279 0.25116439 0.
  0.25116439 0.25116439 0.35300279]]
pairwise_similarity: [[1.         0.37949655]
 [0.37949655 1.        ]]
cosine_similarity: 0.3794965521353535
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: congrats on winning the cover vote Barry sanders is the best of the best
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['congrats', 'on', 'winning', 'the', 'cover', 'vote', 'barry', 'sanders', 'is', 'the', 'best', 'of', 'the', 'best']
cosine_similarity: 0.9858049750328064
train_input: [0.3794965521353535, 0.985805], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: tell your Dad Barry Sanders is gonna be on the cover of the next Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42471719 0.30218978 0.60437955 0.         0.         0.30218978
  0.30218978 0.         0.42471719]
 [0.         0.3174044  0.3174044  0.44610081 0.44610081 0.3174044
  0.3174044  0.44610081 0.        ]]
pairwise_similarity: [[1.         0.47958182]
 [0.47958182 1.        ]]
cosine_similarity: 0.47958182018755263
word_to_vector_cosine_similarity: sentence1: Barry Sanders is on the cover of Madden 25 cover vote, sentence2: tell your Dad Barry Sanders is gonna be on the cover of the next Madden
After tokenization, sentence1: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'cover', 'vote'], sentence2: ['tell', 'your', 'dad', 'barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'cover', 'of', 'the', 'next', 'madden']
cosine_similarity: 0.9750639200210571
train_input: [0.47958182018755263, 0.9750639], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Barry Sanders got the cover for Madden25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.63009934 0.         0.44832087]
 [0.37930349 0.37930349 0.53309782 0.         0.53309782 0.37930349]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Barry Sanders got the cover for Madden25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'got', 'the', 'cover', 'for']
cosine_similarity: 0.9858812689781189
train_input: [0.5101490193104813, 0.98588127], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Barry Sanders is on the cover of Madden 14 back to back years lions on the cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.5        0.5        0.         0.5        0.5
  0.        ]
 [0.39092014 0.2781429  0.55628581 0.39092014 0.2781429  0.2781429
  0.39092014]]
pairwise_similarity: [[1.         0.69535726]
 [0.69535726 1.        ]]
cosine_similarity: 0.6953572570566552
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Barry Sanders is on the cover of Madden 14 back to back years lions on the cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'is', 'on', 'the', 'cover', 'of', 'madden', 'back', 'to', 'back', 'years', 'lions', 'on', 'the', 'cover']
cosine_similarity: 0.989622950553894
train_input: [0.6953572570566552, 0.98962295], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Barry Sanders on the cover Madden next year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.5        0.5        0.        ]
 [0.4090901  0.4090901  0.4090901  0.4090901  0.57496187]]
pairwise_similarity: [[1.         0.81818021]
 [0.81818021 1.        ]]
cosine_similarity: 0.8181802073667197
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Barry Sanders on the cover Madden next year
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'madden', 'next', 'year']
cosine_similarity: 0.9860741496086121
train_input: [0.8181802073667197, 0.98607415], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Barry sanders on the cover of Madden NFL 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.5        0.5        0.5        0.         0.5       ]
 [0.49844628 0.35464863 0.35464863 0.35464863 0.49844628 0.35464863]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Barry sanders on the cover of Madden NFL 25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden', 'nfl']
cosine_similarity: 0.9891045093536377
train_input: [0.7092972666062737, 0.9891045], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: I like Barry Sanders on Madden cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.         0.5        0.5       ]
 [0.4090901  0.4090901  0.57496187 0.4090901  0.4090901 ]]
pairwise_similarity: [[1.         0.81818021]
 [0.81818021 1.        ]]
cosine_similarity: 0.8181802073667197
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: I like Barry Sanders on Madden cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['i', 'like', 'barry', 'sanders', 'on', 'madden', 'cover']
cosine_similarity: 0.9785517454147339
train_input: [0.8181802073667197, 0.97855175], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Love the Madden cover for this year with Barry Sanders on it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.         0.5        0.5        0.        ]
 [0.35464863 0.35464863 0.49844628 0.35464863 0.35464863 0.49844628]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Love the Madden cover for this year with Barry Sanders on it
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['love', 'the', 'madden', 'cover', 'for', 'this', 'year', 'with', 'barry', 'sanders', 'on', 'it']
cosine_similarity: 0.9840290546417236
train_input: [0.7092972666062737, 0.98402905], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Niggas beasted putting Barry Sanders on the next Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.63009934 0.44832087 0.         0.
  0.44832087]
 [0.33471228 0.47042643 0.         0.33471228 0.47042643 0.47042643
  0.33471228]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Niggas beasted putting Barry Sanders on the next Madden
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['niggas', 'beasted', 'putting', 'barry', 'sanders', 'on', 'the', 'next', 'madden']
cosine_similarity: 0.9541639685630798
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Thats pretty fresh Barry Sanders is gonna be on the cover of Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.         0.         0.5        0.
  0.5        0.        ]
 [0.28986934 0.28986934 0.40740124 0.40740124 0.28986934 0.40740124
  0.28986934 0.40740124]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Thats pretty fresh Barry Sanders is gonna be on the cover of Madden
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['thats', 'pretty', 'fresh', 'barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9801725745201111
train_input: [0.5797386715376657, 0.9801726], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Why is Barry Sanders going to be on the cover of Madden 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.5        0.5        0.         0.5        0.5       ]
 [0.49844628 0.35464863 0.35464863 0.49844628 0.35464863 0.35464863]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: Why is Barry Sanders going to be on the cover of Madden 25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['why', 'is', 'barry', 'sanders', 'going', 'to', 'be', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9873078465461731
train_input: [0.7092972666062737, 0.98730785], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: nice to see Barry Sanders win the Madden cover vote
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.5        0.         0.5        0.
  0.        ]
 [0.3174044  0.3174044  0.3174044  0.44610081 0.3174044  0.44610081
  0.44610081]]
pairwise_similarity: [[1.        0.6348088]
 [0.6348088 1.       ]]
cosine_similarity: 0.6348087971775132
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the on the cover of Madden, sentence2: nice to see Barry Sanders win the Madden cover vote
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'on', 'the', 'cover', 'of', 'madden'], sentence2: ['nice', 'to', 'see', 'barry', 'sanders', 'win', 'the', 'madden', 'cover', 'vote']
cosine_similarity: 0.9738093018531799
train_input: [0.6348087971775132, 0.9738093], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Ayyee Barry Sanders is gonna be on the cover of Madden 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37863221 0.37863221 0.37863221 0.37863221
  0.53215436 0.37863221]
 [0.46977774 0.46977774 0.33425073 0.33425073 0.33425073 0.33425073
  0.         0.33425073]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Ayyee Barry Sanders is gonna be on the cover of Madden 25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['ayyee', 'barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9963395595550537
train_input: [0.6327904583679949, 0.99633956], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: BARRY SANDERS On That Madden Cover Boi
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863]
 [0.4090901  0.57496187 0.4090901  0.         0.4090901  0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: BARRY SANDERS On That Madden Cover Boi
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'on', 'that', 'madden', 'cover', 'boi']
cosine_similarity: 0.9767444133758545
train_input: [0.5803329846765685, 0.9767444], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders got the Madden 25 vote
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.47042643 0.47042643 0.         0.33471228
  0.47042643 0.33471228 0.        ]
 [0.47042643 0.33471228 0.         0.         0.47042643 0.33471228
  0.         0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders got the Madden 25 vote
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'got', 'the', 'madden', 'vote']
cosine_similarity: 0.9699676632881165
train_input: [0.3360969272762574, 0.96996766], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders got the cover of Madden babyyyyyyy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.49844628 0.         0.35464863
  0.49844628 0.35464863]
 [0.49844628 0.35464863 0.35464863 0.         0.49844628 0.35464863
  0.         0.35464863]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders got the cover of Madden babyyyyyyy
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'got', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9887986779212952
train_input: [0.5031026124151313, 0.9887987], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders to grace the cover of
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.         0.47042643 0.47042643
  0.33471228]
 [0.44832087 0.44832087 0.         0.63009934 0.         0.
  0.44832087]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders to grace the cover of
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'to', 'grace', 'the', 'cover', 'of']
cosine_similarity: 0.9735368490219116
train_input: [0.4501755023269897, 0.97353685], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders won the Madden25 cover athlete
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.33471228 0.47042643 0.47042643 0.
  0.47042643 0.33471228 0.        ]
 [0.47042643 0.33471228 0.33471228 0.         0.         0.47042643
  0.         0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry Sanders won the Madden25 cover athlete
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'won', 'the', 'cover', 'athlete']
cosine_similarity: 0.9664636254310608
train_input: [0.3360969272762574, 0.9664636], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry sanders on the cover of madden is awesome
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.49844628 0.35464863 0.49844628
  0.35464863]
 [0.57496187 0.4090901  0.4090901  0.         0.4090901  0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Barry sanders on the cover of madden is awesome
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden', 'is', 'awesome']
cosine_similarity: 0.9925822615623474
train_input: [0.5803329846765685, 0.99258226], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Congratulation to Barry Sanders for being voted to be on the Madden 14 cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.         0.35464863 0.49844628 0.35464863
  0.49844628 0.35464863 0.        ]
 [0.44610081 0.3174044  0.44610081 0.3174044  0.         0.3174044
  0.         0.3174044  0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Congratulation to Barry Sanders for being voted to be on the Madden 14 cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['congratulation', 'to', 'barry', 'sanders', 'for', 'being', 'voted', 'to', 'be', 'on', 'the', 'madden', 'cover']
cosine_similarity: 0.98237144947052
train_input: [0.4502681446556265, 0.98237145], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Denard makes NCAA cover Barry Sanders makes Madden cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.49844628 0.35464863 0.
  0.         0.49844628 0.35464863]
 [0.23031454 0.46062909 0.32369906 0.         0.23031454 0.64739811
  0.32369906 0.         0.23031454]]
pairwise_similarity: [[1.         0.40840369]
 [0.40840369 1.        ]]
cosine_similarity: 0.40840369224375
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: Denard makes NCAA cover Barry Sanders makes Madden cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['denard', 'makes', 'ncaa', 'cover', 'barry', 'sanders', 'makes', 'madden', 'cover']
cosine_similarity: 0.9516524076461792
train_input: [0.40840369224375, 0.9516524], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: How did Barry sanders beat ap in the madden 25 cover vote
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35464863 0.         0.35464863 0.
  0.49844628 0.35464863 0.49844628 0.35464863 0.        ]
 [0.37729199 0.37729199 0.26844636 0.37729199 0.26844636 0.37729199
  0.         0.26844636 0.         0.26844636 0.37729199]]
pairwise_similarity: [[1.         0.38081653]
 [0.38081653 1.        ]]
cosine_similarity: 0.3808165329771113
word_to_vector_cosine_similarity: sentence1: Barry Sanders is gonna be on the new madden cover, sentence2: How did Barry sanders beat ap in the madden 25 cover vote
After tokenization, sentence1: ['barry', 'sanders', 'is', 'gonna', 'be', 'on', 'the', 'new', 'madden', 'cover'], sentence2: ['how', 'did', 'barry', 'sanders', 'beat', 'ap', 'in', 'the', 'madden', 'cover', 'vote']
cosine_similarity: 0.9888992309570312
train_input: [0.3808165329771113, 0.98889923], train_label: 0
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders beat out Adrian Peterson for the cover of the new Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.         0.4090901  0.         0.4090901  0.4090901
  0.         0.         0.4090901 ]
 [0.         0.40740124 0.28986934 0.40740124 0.28986934 0.28986934
  0.40740124 0.40740124 0.28986934]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders beat out Adrian Peterson for the cover of the new Madden
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'beat', 'out', 'adrian', 'peterson', 'for', 'the', 'cover', 'of', 'the', 'new', 'madden']
cosine_similarity: 0.9881519675254822
train_input: [0.4743307064971939, 0.98815197], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders for madden 14 cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57496187 0.4090901  0.4090901  0.4090901  0.4090901 ]
 [0.57496187 0.         0.4090901  0.4090901  0.4090901  0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders for madden 14 cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'for', 'madden', 'cover']
cosine_similarity: 0.9679915308952332
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders gon be on the Madden 25 cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.         0.4090901  0.4090901  0.         0.4090901
  0.4090901 ]
 [0.         0.49844628 0.35464863 0.35464863 0.49844628 0.35464863
  0.35464863]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders gon be on the Madden 25 cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'gon', 'be', 'on', 'the', 'madden', 'cover']
cosine_similarity: 0.9767645597457886
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders in the cover of madden 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.         0.4090901  0.4090901  0.4090901  0.4090901 ]
 [0.         0.57496187 0.4090901  0.4090901  0.4090901  0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders in the cover of madden 25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'in', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.995617687702179
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders the new face of Madden 25
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.         0.37930349 0.53309782 0.         0.37930349
  0.         0.37930349]
 [0.         0.47042643 0.33471228 0.         0.47042643 0.33471228
  0.47042643 0.33471228]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders the new face of Madden 25
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'the', 'new', 'face', 'of', 'madden']
cosine_similarity: 0.9922337532043457
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders will be on a madden cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.4090901  0.4090901  0.4090901 ]
 [0.         0.5        0.5        0.5        0.5       ]]
pairwise_similarity: [[1.         0.81818021]
 [0.81818021 1.        ]]
cosine_similarity: 0.8181802073667197
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Barry Sanders will be on a madden cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['barry', 'sanders', 'will', 'be', 'on', 'a', 'madden', 'cover']
cosine_similarity: 0.9747236967086792
train_input: [0.8181802073667197, 0.9747237], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Glad Barry Sanders won madden cover
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.4090901  0.         0.4090901  0.4090901
  0.        ]
 [0.         0.35464863 0.35464863 0.49844628 0.35464863 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Glad Barry Sanders won madden cover
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['glad', 'barry', 'sanders', 'won', 'madden', 'cover']
cosine_similarity: 0.9635424613952637
train_input: [0.5803329846765685, 0.96354246], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Nic Barry Sanders on the cover of Madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.4090901  0.4090901  0.         0.4090901 ]
 [0.         0.4090901  0.4090901  0.4090901  0.57496187 0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Nic Barry Sanders on the cover of Madden
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['nic', 'barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9893775582313538
train_input: [0.6694188517266485, 0.98937756], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Pretty ridiculous that Barry Sanders is on the Madden cover and not a current player
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.4090901  0.         0.4090901  0.
  0.         0.         0.4090901 ]
 [0.         0.28986934 0.28986934 0.40740124 0.28986934 0.40740124
  0.40740124 0.40740124 0.28986934]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: Pretty ridiculous that Barry Sanders is on the Madden cover and not a current player
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['pretty', 'ridiculous', 'that', 'barry', 'sanders', 'is', 'on', 'the', 'madden', 'cover', 'and', 'not', 'a', 'current', 'player']
cosine_similarity: 0.976434588432312
train_input: [0.4743307064971939, 0.9764346], train_label: 1
TF_IDF_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: That s what s up Barry sanders on the cover of madden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.4090901  0.4090901  0.4090901 ]
 [0.         0.5        0.5        0.5        0.5       ]]
pairwise_similarity: [[1.         0.81818021]
 [0.81818021 1.        ]]
cosine_similarity: 0.8181802073667197
word_to_vector_cosine_similarity: sentence1: Barry Sanders is the cover of Madden 2014, sentence2: That s what s up Barry sanders on the cover of madden
After tokenization, sentence1: ['barry', 'sanders', 'is', 'the', 'cover', 'of', 'madden'], sentence2: ['that', 's', 'what', 's', 'up', 'barry', 'sanders', 'on', 'the', 'cover', 'of', 'madden']
cosine_similarity: 0.9714859127998352
train_input: [0.8181802073667197, 0.9714859], train_label: 1
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Belcher didnt appear to be prepared to fight Bisping
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.27894255 0.         0.39204401 0.         0.78408803
  0.         0.39204401 0.        ]
 [0.4261596  0.30321606 0.4261596  0.         0.4261596  0.
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.08457986]
 [0.08457986 1.        ]]
cosine_similarity: 0.0845798608014702
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Belcher didnt appear to be prepared to fight Bisping
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['belcher', 'didnt', 'appear', 'to', 'be', 'prepared', 'to', 'fight', 'bisping']
cosine_similarity: 0.9172645211219788
train_input: [0.0845798608014702, 0.9172645], train_label: 0
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Belcher s eye just got fucked up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.46977774 0.66850146 0.         0.         0.46977774
  0.        ]
 [0.35520009 0.         0.35520009 0.49922133 0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.35617766]
 [0.35617766 1.        ]]
cosine_similarity: 0.3561776636856882
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Belcher s eye just got fucked up
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['belcher', 's', 'eye', 'just', 'got', 'fucked', 'up']
cosine_similarity: 0.9693101644515991
train_input: [0.3561776636856882, 0.96931016], train_label: 1
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Belcher was still on the mat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27894255 0.39204401 0.78408803 0.39204401 0.        ]
 [0.57973867 0.         0.         0.         0.81480247]]
pairwise_similarity: [[1.         0.16171378]
 [0.16171378 1.        ]]
cosine_similarity: 0.16171378066252898
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Belcher was still on the mat
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['belcher', 'was', 'still', 'on', 'the', 'mat']
cosine_similarity: 0.9653878211975098
train_input: [0.16171378066252898, 0.9653878], train_label: 0
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Bisping was too much for Belcher tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27894255 0.         0.39204401 0.78408803 0.39204401 0.        ]
 [0.44943642 0.6316672  0.         0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Bisping was too much for Belcher tonight
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['bisping', 'was', 'too', 'much', 'for', 'belcher', 'tonight']
cosine_similarity: 0.9379673600196838
train_input: [0.12536693798731732, 0.93796736], train_label: 0
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Here s hoping that Belcher shuts Bisping the eff up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27894255 0.         0.39204401 0.         0.78408803 0.
  0.39204401 0.        ]
 [0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.         0.47107781]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536716
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Here s hoping that Belcher shuts Bisping the eff up
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['here', 's', 'hoping', 'that', 'belcher', 'shuts', 'bisping', 'the', 'eff', 'up']
cosine_similarity: 0.9578109979629517
train_input: [0.09349477497536716, 0.957811], train_label: 0
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Hopefully Belcher is OK that s one of the nastier eye pokes Ive seen
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.46977774 0.66850146 0.         0.46977774 0.
  0.         0.         0.         0.        ]
 [0.26868528 0.         0.26868528 0.37762778 0.         0.37762778
  0.37762778 0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.26942475]
 [0.26942475 1.        ]]
cosine_similarity: 0.26942474918647663
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Hopefully Belcher is OK that s one of the nastier eye pokes Ive seen
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['hopefully', 'belcher', 'is', 'ok', 'that', 's', 'one', 'of', 'the', 'nastier', 'eye', 'pokes', 'ive', 'seen']
cosine_similarity: 0.9731428027153015
train_input: [0.26942474918647663, 0.9731428], train_label: 1
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: I really hope Belcher is okay
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27894255 0.39204401 0.78408803 0.         0.39204401 0.
  0.        ]
 [0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: I really hope Belcher is okay
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['i', 'really', 'hope', 'belcher', 'is', 'okay']
cosine_similarity: 0.9401090741157532
train_input: [0.1059921313509325, 0.9401091], train_label: 0
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Unfortunate ending to the Bisping Belcher fight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27894255 0.         0.39204401 0.         0.78408803 0.
  0.39204401 0.        ]
 [0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.         0.47107781]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536716
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Unfortunate ending to the Bisping Belcher fight
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['unfortunate', 'ending', 'to', 'the', 'bisping', 'belcher', 'fight']
cosine_similarity: 0.9112984538078308
train_input: [0.09349477497536716, 0.91129845], train_label: 0
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Very unfortunate to see Belcher incur another eye injury
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.46977774 0.66850146 0.         0.46977774 0.
  0.        ]
 [0.35520009 0.         0.35520009 0.49922133 0.         0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.35617766]
 [0.35617766 1.        ]]
cosine_similarity: 0.3561776636856882
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: Very unfortunate to see Belcher incur another eye injury
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['very', 'unfortunate', 'to', 'see', 'belcher', 'incur', 'another', 'eye', 'injury']
cosine_similarity: 0.9457908272743225
train_input: [0.3561776636856882, 0.9457908], train_label: 1
TF_IDF_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: micheal bisping beat Alan belcher
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.27894255 0.         0.39204401 0.78408803
  0.39204401 0.        ]
 [0.47107781 0.47107781 0.33517574 0.47107781 0.         0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536716
word_to_vector_cosine_similarity: sentence1: Belcher s eye is bleeding and that was his injured eye, sentence2: micheal bisping beat Alan belcher
After tokenization, sentence1: ['belcher', 's', 'eye', 'is', 'bleeding', 'and', 'that', 'was', 'his', 'injured', 'eye'], sentence2: ['micheal', 'bisping', 'beat', 'alan', 'belcher']
cosine_similarity: 0.6935026049613953
train_input: [0.09349477497536716, 0.6935026], train_label: 0
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino Look Just Like Dat Damn Blue Clown Wit No Neck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.33471228 0.         0.47042643 0.         0.
  0.         0.47042643 0.         0.33471228 0.33471228 0.
  0.        ]
 [0.         0.24377685 0.34261985 0.         0.34261985 0.34261985
  0.34261985 0.         0.34261985 0.24377685 0.24377685 0.34261985
  0.34261985]]
pairwise_similarity: [[1.         0.24478531]
 [0.24478531 1.        ]]
cosine_similarity: 0.24478531173455215
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino Look Just Like Dat Damn Blue Clown Wit No Neck
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['benzino', 'look', 'just', 'like', 'dat', 'damn', 'blue', 'clown', 'wit', 'no', 'neck']
cosine_similarity: 0.967613160610199
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino is ugghe s short and have wrinkles
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.30321606 0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.         0.        ]
 [0.         0.37997836 0.         0.         0.         0.
  0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino is ugghe s short and have wrinkles
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['benzino', 'is', 's', 'short', 'and', 'have', 'wrinkles']
cosine_similarity: 0.928449273109436
train_input: [0.11521554337793122, 0.9284493], train_label: 0
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino just gave Stevie J a REAL NIGGA talk
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.30321606 0.4261596  0.         0.4261596  0.
  0.4261596  0.4261596  0.         0.         0.         0.        ]
 [0.         0.27894255 0.         0.39204401 0.         0.39204401
  0.         0.         0.39204401 0.39204401 0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.08457986]
 [0.08457986 1.        ]]
cosine_similarity: 0.0845798608014702
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino just gave Stevie J a REAL NIGGA talk
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['benzino', 'just', 'gave', 'stevie', 'j', 'a', 'real', 'nigga', 'talk']
cosine_similarity: 0.9355512261390686
train_input: [0.0845798608014702, 0.9355512], train_label: 0
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino probably sells Cuban Avon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.         0.30321606 0.4261596  0.         0.4261596
  0.4261596  0.4261596  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.
  0.         0.         0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino probably sells Cuban Avon
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['benzino', 'probably', 'sells', 'cuban', 'avon']
cosine_similarity: 0.821171224117279
train_input: [0.10163066979112656, 0.8211712], train_label: 0
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino reminds of the University of Georgia Mascot BullDogs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.30321606 0.4261596  0.         0.         0.4261596
  0.4261596  0.4261596  0.         0.         0.        ]
 [0.         0.30321606 0.         0.4261596  0.4261596  0.
  0.         0.         0.4261596  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.09193998]
 [0.09193998 1.        ]]
cosine_similarity: 0.09193998174078082
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Benzino reminds of the University of Georgia Mascot BullDogs
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['benzino', 'reminds', 'of', 'the', 'university', 'of', 'georgia', 'mascot', 'bulldogs']
cosine_similarity: 0.8300761580467224
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Fawwk why Benzino dont have a neck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.30321606 0.4261596  0.         0.         0.4261596
  0.4261596  0.4261596  0.        ]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.
  0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Fawwk why Benzino dont have a neck
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['fawwk', 'why', 'benzino', 'dont', 'have', 'a', 'neck']
cosine_similarity: 0.9615758061408997
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: I like the name benzino its so cool
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.44665616 0.31779954
  0.44665616]
 [0.         0.50154891 0.         0.70490949 0.         0.50154891
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: I like the name benzino its so cool
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['i', 'like', 'the', 'name', 'benzino', 'its', 'so', 'cool']
cosine_similarity: 0.9391627907752991
train_input: [0.31878402175377923, 0.9391628], train_label: 0
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Imagine Benzino with his shirt off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.30321606 0.4261596  0.4261596  0.         0.4261596
  0.4261596  0.        ]
 [0.         0.44943642 0.         0.         0.6316672  0.
  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: Imagine Benzino with his shirt off
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['imagine', 'benzino', 'with', 'his', 'shirt', 'off']
cosine_similarity: 0.9541031122207642
train_input: [0.1362763414390864, 0.9541031], train_label: 0
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: When Benzino is the voice of reason dog
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.30321606 0.4261596  0.         0.4261596  0.4261596
  0.4261596  0.         0.        ]
 [0.         0.37997836 0.         0.53404633 0.         0.
  0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: When Benzino is the voice of reason dog
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['when', 'benzino', 'is', 'the', 'voice', 'of', 'reason', 'dog']
cosine_similarity: 0.9186828136444092
train_input: [0.11521554337793122, 0.9186828], train_label: 0
TF_IDF_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: i like benzino his name cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.44665616 0.31779954
  0.44665616]
 [0.         0.50154891 0.         0.70490949 0.         0.50154891
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Benzino Look Like Aa Bobble Head, sentence2: i like benzino his name cute
After tokenization, sentence1: ['benzino', 'look', 'like', 'aa', 'bobble', 'head'], sentence2: ['i', 'like', 'benzino', 'his', 'name', 'cute']
cosine_similarity: 0.9591430425643921
train_input: [0.31878402175377923, 0.95914304], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: And give the ball to Patrick Beverley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.         0.50154891 0.50154891]
 [0.         0.70490949 0.50154891 0.50154891]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: And give the ball to Patrick Beverley
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['and', 'give', 'the', 'ball', 'to', 'patrick', 'beverley']
cosine_similarity: 0.942836344242096
train_input: [0.5031026124151314, 0.94283634], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Beverley need to stop flopping
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672  0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Beverley need to stop flopping
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['beverley', 'need', 'to', 'stop', 'flopping']
cosine_similarity: 0.868236780166626
train_input: [0.17077611319011649, 0.8682368], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Beverley out here frustrating that man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672 ]
 [0.         0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Beverley out here frustrating that man
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['beverley', 'out', 'here', 'frustrating', 'that', 'man']
cosine_similarity: 0.9103403687477112
train_input: [0.20199309249791833, 0.91034037], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Give Beverley a fucking Oscar for that performance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672  0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Give Beverley a fucking Oscar for that performance
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['give', 'beverley', 'a', 'fucking', 'oscar', 'for', 'that', 'performance']
cosine_similarity: 0.8960220217704773
train_input: [0.17077611319011649, 0.896022], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Hey beverley youre a cunt
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672  0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Hey beverley youre a cunt
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['hey', 'beverley', 'youre', 'a', 'cunt']
cosine_similarity: 0.8587867021560669
train_input: [0.17077611319011649, 0.8587867], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Patrick Beverley is about to be a little lighter in the wallet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891 0.        ]
 [0.         0.35520009 0.49922133 0.49922133 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Patrick Beverley is about to be a little lighter in the wallet
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['patrick', 'beverley', 'is', 'about', 'to', 'be', 'a', 'little', 'lighter', 'in', 'the', 'wallet']
cosine_similarity: 0.9262098670005798
train_input: [0.3563004293331381, 0.92620987], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Patrick Beverley please punch Russell Westbrook in the face
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.        ]
 [0.         0.31779954 0.44665616 0.31779954 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Patrick Beverley please punch Russell Westbrook in the face
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['patrick', 'beverley', 'please', 'punch', 'russell', 'westbrook', 'in', 'the', 'face']
cosine_similarity: 0.9475549459457397
train_input: [0.31878402175377923, 0.94755495], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Westbrook used off arm Beverley acted
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.         0.44943642 0.6316672  0.
  0.        ]
 [0.         0.47107781 0.47107781 0.33517574 0.         0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Westbrook used off arm Beverley acted
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['westbrook', 'used', 'off', 'arm', 'beverley', 'acted']
cosine_similarity: 0.8683479428291321
train_input: [0.15064018498706508, 0.86834794], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Who is this Patrick Beverley character
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891]
 [0.         0.50154891 0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: Who is this Patrick Beverley character
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['who', 'is', 'this', 'patrick', 'beverley', 'character']
cosine_similarity: 0.9510025978088379
train_input: [0.5031026124151314, 0.9510026], train_label: 0
TF_IDF_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: You just love ANYTHING Beverley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672 ]
 [0.         0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Patrick Beverley is at 764, sentence2: You just love ANYTHING Beverley
After tokenization, sentence1: ['patrick', 'beverley', 'is', 'at'], sentence2: ['you', 'just', 'love', 'anything', 'beverley']
cosine_similarity: 0.8884508013725281
train_input: [0.20199309249791833, 0.8884508], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Encourages stupid fans to threaten Beverly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.53404633 0.53404633
  0.         0.        ]
 [0.33517574 0.         0.47107781 0.47107781 0.         0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Encourages stupid fans to threaten Beverly
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['encourages', 'stupid', 'fans', 'to', 'threaten', 'beverly']
cosine_similarity: 0.8803315758705139
train_input: [0.1273595297947935, 0.8803316], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: I hope Patrick Beverly gets hit by a bus
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.         0.
  0.57615236 0.40993715]
 [0.31779954 0.44665616 0.         0.44665616 0.44665616 0.44665616
  0.         0.31779954]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: I hope Patrick Beverly gets hit by a bus
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['i', 'hope', 'patrick', 'beverly', 'gets', 'hit', 'by', 'a', 'bus']
cosine_similarity: 0.9256260395050049
train_input: [0.2605556710562624, 0.92562604], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: It aint the fact that Patrick Beverly hurt Westbrook in the play
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.         0.         0.57615236
  0.40993715 0.         0.        ]
 [0.4078241  0.29017021 0.         0.4078241  0.4078241  0.
  0.29017021 0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: It aint the fact that Patrick Beverly hurt Westbrook in the play
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['it', 'aint', 'the', 'fact', 'that', 'patrick', 'beverly', 'hurt', 'westbrook', 'in', 'the', 'play']
cosine_similarity: 0.9241970181465149
train_input: [0.23790309463326234, 0.924197], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Na Beverly from Houston should be fined for hurting westbrook
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.         0.53404633
  0.         0.53404633 0.        ]
 [0.30321606 0.         0.4261596  0.4261596  0.4261596  0.
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Na Beverly from Houston should be fined for hurting westbrook
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['na', 'beverly', 'from', 'houston', 'should', 'be', 'fined', 'for', 'hurting', 'westbrook']
cosine_similarity: 0.8873991966247559
train_input: [0.11521554337793122, 0.8873992], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: On an injured knee Westbrook still punkd Beverly at will
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.53404633
  0.         0.        ]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: On an injured knee Westbrook still punkd Beverly at will
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['on', 'an', 'injured', 'knee', 'westbrook', 'still', 'punkd', 'beverly', 'at', 'will']
cosine_similarity: 0.8972160816192627
train_input: [0.1273595297947935, 0.8972161], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Smh anybody know Where Patrick Beverly located
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.57615236 0.         0.
  0.40993715 0.        ]
 [0.44665616 0.31779954 0.         0.         0.44665616 0.44665616
  0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Smh anybody know Where Patrick Beverly located
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['smh', 'anybody', 'know', 'where', 'patrick', 'beverly', 'located']
cosine_similarity: 0.8994295001029968
train_input: [0.2605556710562624, 0.8994295], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Westbrook is an idiot if he s truly angry at Beverly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.53404633 0.53404633
  0.         0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.         0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Westbrook is an idiot if he s truly angry at Beverly
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['westbrook', 'is', 'an', 'idiot', 'if', 'he', 's', 'truly', 'angry', 'at', 'beverly']
cosine_similarity: 0.9063594341278076
train_input: [0.1273595297947935, 0.90635943], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Westbrook is gonna pop on Beverly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.53404633 0.
  0.        ]
 [0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: Westbrook is gonna pop on Beverly
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['westbrook', 'is', 'gonna', 'pop', 'on', 'beverly']
cosine_similarity: 0.9261751174926758
train_input: [0.1443835552773867, 0.9261751], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: excuse me dawg I meant Beverly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.53404633 0.
  0.53404633]
 [0.37997836 0.         0.53404633 0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: excuse me dawg I meant Beverly
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['excuse', 'me', 'dawg', 'i', 'meant', 'beverly']
cosine_similarity: 0.8329051733016968
train_input: [0.1443835552773867, 0.8329052], train_label: 0
TF_IDF_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: it s a quick play he calls to Beverly goes for the ball
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.53404633 0.         0.        ]
 [0.4261596  0.30321606 0.4261596  0.         0.4261596  0.
  0.         0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: from that incidental clip by Patrick Beverly, sentence2: it s a quick play he calls to Beverly goes for the ball
After tokenization, sentence1: ['from', 'that', 'incidental', 'clip', 'by', 'patrick', 'beverly'], sentence2: ['it', 's', 'a', 'quick', 'play', 'he', 'calls', 'to', 'beverly', 'goes', 'for', 'the', 'ball']
cosine_similarity: 0.9113991260528564
train_input: [0.11521554337793122, 0.9113991], train_label: 0
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Big Country drops Cheick Kongo with a huge overhand
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.         0.
  0.49922133 0.49922133 0.         0.49922133]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.4078241  0.4078241
  0.         0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Big Country drops Cheick Kongo with a huge overhand
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['big', 'country', 'drops', 'cheick', 'kongo', 'with', 'a', 'huge', 'overhand']
cosine_similarity: 0.9242061376571655
train_input: [0.20613696606828605, 0.92420614], train_label: 0
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Big Country just put Kongo out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Big Country just put Kongo out
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['big', 'country', 'just', 'put', 'kongo', 'out']
cosine_similarity: 0.9540175199508667
train_input: [0.29121941856368966, 0.9540175], train_label: 0
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Man that big country ko was epic
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.         0.37930349 0.53309782
  0.53309782]
 [0.37930349 0.37930349 0.53309782 0.53309782 0.37930349 0.
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Man that big country ko was epic
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['man', 'that', 'big', 'country', 'ko', 'was', 'epic']
cosine_similarity: 0.9578085541725159
train_input: [0.43161341897075145, 0.95780855], train_label: 1
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Roy Big Country Nelson cant be stopped
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.4090901  0.4090901  0.        ]
 [0.4090901  0.4090901  0.         0.4090901  0.4090901  0.57496187]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Roy Big Country Nelson cant be stopped
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['roy', 'big', 'country', 'nelson', 'cant', 'be', 'stopped']
cosine_similarity: 0.9735776782035828
train_input: [0.6694188517266485, 0.9735777], train_label: 1
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Roy Big Country Nelson is a man s man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.4472136  0.4472136  0.4472136  0.4472136 ]
 [0.35355339 0.35355339 0.70710678 0.35355339 0.35355339]]
pairwise_similarity: [[1.        0.9486833]
 [0.9486833 1.       ]]
cosine_similarity: 0.9486832980505138
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Roy Big Country Nelson is a man s man
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['roy', 'big', 'country', 'nelson', 'is', 'a', 'man', 's', 'man']
cosine_similarity: 0.9801763296127319
train_input: [0.9486832980505138, 0.9801763], train_label: 1
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: THAT S WAS AWESOME BIG COUNTRY roynelsonmma
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.49922133 0.49922133 0.49922133
  0.        ]
 [0.57615236 0.40993715 0.40993715 0.         0.         0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: THAT S WAS AWESOME BIG COUNTRY roynelsonmma
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['that', 's', 'was', 'awesome', 'big', 'country']
cosine_similarity: 0.969735324382782
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Well done Big Country well done
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133]
 [0.70710678 0.70710678 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.50232878]
 [0.50232878 1.        ]]
cosine_similarity: 0.5023287782256717
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: Well done Big Country well done
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['well', 'done', 'big', 'country', 'well', 'done']
cosine_similarity: 0.9198932647705078
train_input: [0.5023287782256717, 0.91989326], train_label: 1
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: anyone with the nickname Big Country I want no parts of
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133 0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: anyone with the nickname Big Country I want no parts of
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['anyone', 'with', 'the', 'nickname', 'big', 'country', 'i', 'want', 'no', 'parts', 'of']
cosine_similarity: 0.9548448920249939
train_input: [0.2523342014336961, 0.9548449], train_label: 0
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: no way gotta love big country man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.         0.37930349 0.53309782
  0.53309782 0.        ]
 [0.33471228 0.33471228 0.47042643 0.47042643 0.33471228 0.
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: no way gotta love big country man
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['no', 'way', 'gotta', 'love', 'big', 'country', 'man']
cosine_similarity: 0.9389796853065491
TF_IDF_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: please tell me you bet Roy big country Nelson for a k
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.4090901  0.57496187 0.4090901  0.4090901
  0.        ]
 [0.49844628 0.35464863 0.35464863 0.         0.35464863 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Roy Big Country Nelson is the MAN, sentence2: please tell me you bet Roy big country Nelson for a k
After tokenization, sentence1: ['roy', 'big', 'country', 'nelson', 'is', 'the', 'man'], sentence2: ['please', 'tell', 'me', 'you', 'bet', 'roy', 'big', 'country', 'nelson', 'for', 'a', 'k']
cosine_similarity: 0.9396692514419556
train_input: [0.5803329846765685, 0.93966925], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: After the Bisping and Belcher fight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133]
 [0.50154891 0.50154891 0.         0.70490949 0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: After the Bisping and Belcher fight
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['after', 'the', 'bisping', 'and', 'belcher', 'fight']
cosine_similarity: 0.9722192883491516
train_input: [0.3563004293331381, 0.9722193], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: But Bisping was beating him up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.33517574 0.47107781 0.47107781 0.47107781]
 [0.81480247 0.         0.57973867 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: But Bisping was beating him up
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['but', 'bisping', 'was', 'beating', 'him', 'up']
cosine_similarity: 0.9572573900222778
train_input: [0.19431434016858146, 0.9572574], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: Excellent performance from bisping with unfortunate ending
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.         0.         0.47107781 0.
  0.47107781 0.         0.47107781]
 [0.         0.33517574 0.47107781 0.47107781 0.         0.47107781
  0.         0.47107781 0.        ]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: Excellent performance from bisping with unfortunate ending
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['excellent', 'performance', 'from', 'bisping', 'with', 'unfortunate', 'ending']
cosine_similarity: 0.9077865481376648
train_input: [0.11234277891542777, 0.90778655], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: I think bisping took that round as well
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.47107781 0.         0.
  0.         0.47107781]
 [0.         0.37997836 0.         0.         0.53404633 0.53404633
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: I think bisping took that round as well
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['i', 'think', 'bisping', 'took', 'that', 'round', 'as', 'well']
cosine_similarity: 0.9374106526374817
train_input: [0.1273595297947935, 0.93741065], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: I thought Bisping looked good tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.         0.47107781
  0.         0.         0.47107781]
 [0.         0.33517574 0.         0.47107781 0.47107781 0.
  0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: I thought Bisping looked good tonight
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['i', 'thought', 'bisping', 'looked', 'good', 'tonight']
cosine_similarity: 0.9094617962837219
train_input: [0.11234277891542777, 0.9094618], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: Michael Bisping is NEVER going to knock any One out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.         0.
  0.47107781 0.47107781]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: Michael Bisping is NEVER going to knock any One out
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['michael', 'bisping', 'is', 'never', 'going', 'to', 'knock', 'any', 'one', 'out']
cosine_similarity: 0.9436284899711609
train_input: [0.1273595297947935, 0.9436285], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: That boy bisping lookin good threw two
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.         0.47107781 0.         0.
  0.47107781 0.         0.47107781]
 [0.         0.33517574 0.47107781 0.         0.47107781 0.47107781
  0.         0.47107781 0.        ]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: That boy bisping lookin good threw two
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['that', 'boy', 'bisping', 'lookin', 'good', 'threw', 'two']
cosine_similarity: 0.925237774848938
train_input: [0.11234277891542777, 0.9252378], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: The way BispingBelcher were talking I was expecting a better fight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.         0.4472136  0.         0.         0.4472136
  0.         0.4472136  0.         0.         0.4472136 ]
 [0.         0.40824829 0.         0.40824829 0.40824829 0.
  0.40824829 0.         0.40824829 0.40824829 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: The way BispingBelcher were talking I was expecting a better fight
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['the', 'way', 'were', 'talking', 'i', 'was', 'expecting', 'a', 'better', 'fight']
cosine_similarity: 0.9010257720947266
train_input: [0.0, 0.9010258], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: This Bisping fight needs to be stopped
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.         0.47107781
  0.         0.47107781]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: This Bisping fight needs to be stopped
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['this', 'bisping', 'fight', 'needs', 'to', 'be', 'stopped']
cosine_similarity: 0.9428759217262268
train_input: [0.1273595297947935, 0.9428759], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: bisping is still my man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.47107781 0.47107781]
 [0.         0.57973867 0.         0.81480247 0.         0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Eye poke win for Bisping over Belcher, sentence2: bisping is still my man
After tokenization, sentence1: ['eye', 'poke', 'win', 'for', 'bisping', 'over', 'belcher'], sentence2: ['bisping', 'is', 'still', 'my', 'man']
cosine_similarity: 0.9421378374099731
train_input: [0.19431434016858146, 0.94213784], train_label: 0
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake Griffin and Chris Paul are a joke
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.        ]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake Griffin and Chris Paul are a joke
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['blake', 'griffin', 'and', 'chris', 'paul', 'are', 'a', 'joke']
cosine_similarity: 0.974137008190155
train_input: [0.3563004293331381, 0.974137], train_label: 0
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake Griffin is long over due for an ass beating
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.70490949 0.50154891 0.50154891 0.        ]
 [0.49922133 0.49922133 0.         0.35520009 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake Griffin is long over due for an ass beating
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['blake', 'griffin', 'is', 'long', 'over', 'due', 'for', 'an', 'ass', 'beating']
cosine_similarity: 0.9465203881263733
train_input: [0.3563004293331381, 0.9465204], train_label: 1
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake Griffin why arent you inside me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.70490949 0.50154891 0.50154891 0.        ]
 [0.57615236 0.         0.40993715 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake Griffin why arent you inside me
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['blake', 'griffin', 'why', 'arent', 'you', 'inside', 'me']
cosine_similarity: 0.9847311973571777
train_input: [0.4112070550676187, 0.9847312], train_label: 0
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake Griffin will always be a circus freak and a little bitch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.57735027 0.         0.         0.57735027 0.        ]
 [0.33471228 0.33471228 0.47042643 0.47042643 0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376658
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake Griffin will always be a circus freak and a little bitch
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['blake', 'griffin', 'will', 'always', 'be', 'a', 'circus', 'freak', 'and', 'a', 'little', 'bitch']
cosine_similarity: 0.9907981157302856
train_input: [0.5797386715376658, 0.9907981], train_label: 1
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake griffin is such a flopping idiot
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.        ]
 [0.         0.40993715 0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Blake griffin is such a flopping idiot
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['blake', 'griffin', 'is', 'such', 'a', 'flopping', 'idiot']
cosine_similarity: 0.9662227034568787
train_input: [0.4112070550676187, 0.9662227], train_label: 1
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Everything is a foul on Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891]
 [0.         0.50154891 0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Everything is a foul on Blake Griffin
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['everything', 'is', 'a', 'foul', 'on', 'blake', 'griffin']
cosine_similarity: 0.9711945056915283
train_input: [0.5031026124151314, 0.9711945], train_label: 0
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Yo yall see Blake Griffin playing down there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.         0.        ]
 [0.         0.35520009 0.35520009 0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Yo yall see Blake Griffin playing down there
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['yo', 'yall', 'see', 'blake', 'griffin', 'playing', 'down', 'there']
cosine_similarity: 0.9681562781333923
train_input: [0.3563004293331381, 0.9681563], train_label: 0
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Zach Randolph makes Blake Griffin look soft frfr
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.         0.         0.        ]
 [0.         0.26868528 0.37762778 0.26868528 0.37762778 0.37762778
  0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.26951761]
 [0.26951761 1.        ]]
cosine_similarity: 0.26951761324603224
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: Zach Randolph makes Blake Griffin look soft frfr
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['zach', 'randolph', 'makes', 'blake', 'griffin', 'look', 'soft', 'frfr']
cosine_similarity: 0.9312891960144043
train_input: [0.26951761324603224, 0.9312892], train_label: 0
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: i have no respect for blake griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.        ]
 [0.         0.50154891 0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: i have no respect for blake griffin
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['i', 'have', 'no', 'respect', 'for', 'blake', 'griffin']
cosine_similarity: 0.9817335605621338
train_input: [0.5031026124151314, 0.98173356], train_label: 1
TF_IDF_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: or about how we cant cheer for a Blake Griffin dunk
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891]
 [0.         0.40993715 0.57615236 0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: You are such a bitch Blake Griffin, sentence2: or about how we cant cheer for a Blake Griffin dunk
After tokenization, sentence1: ['you', 'are', 'such', 'a', 'bitch', 'blake', 'griffin'], sentence2: ['or', 'about', 'how', 'we', 'cant', 'cheer', 'for', 'a', 'blake', 'griffin', 'dunk']
cosine_similarity: 0.9745007753372192
train_input: [0.4112070550676187, 0.9745008], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: At the moment I cant imagine Blake Griffin playing for another team
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.         0.
  0.49922133 0.         0.        ]
 [0.31779954 0.         0.         0.31779954 0.44665616 0.44665616
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: At the moment I cant imagine Blake Griffin playing for another team
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['at', 'the', 'moment', 'i', 'cant', 'imagine', 'blake', 'griffin', 'playing', 'for', 'another', 'team']
cosine_similarity: 0.9510836601257324
train_input: [0.22576484600261604, 0.95108366], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Blake Griffin did knock ZBO headband off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.49922133 0.35520009 0.
  0.         0.49922133 0.        ]
 [0.31779954 0.         0.44665616 0.         0.31779954 0.44665616
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Blake Griffin did knock ZBO headband off
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'did', 'knock', 'zbo', 'headband', 'off']
cosine_similarity: 0.9209548830986023
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Blake Griffin got that Norman Osborne hair
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.35520009 0.
  0.         0.         0.49922133]
 [0.31779954 0.         0.         0.44665616 0.31779954 0.44665616
  0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Blake Griffin got that Norman Osborne hair
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'got', 'that', 'norman', 'osborne', 'hair']
cosine_similarity: 0.9547380805015564
train_input: [0.22576484600261604, 0.9547381], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Blake Griffin shouldve been ejected by now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.35520009 0.49922133
  0.        ]
 [0.40993715 0.         0.         0.57615236 0.40993715 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Blake Griffin shouldve been ejected by now
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'shouldve', 'been', 'ejected', 'by', 'now']
cosine_similarity: 0.9269276261329651
train_input: [0.29121941856368966, 0.9269276], train_label: 1
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Can Game 7 be decided by a Blake GriffinZach Randolph 10 round fight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33517574 0.47107781 0.         0.47107781 0.
  0.         0.47107781 0.         0.47107781 0.         0.        ]
 [0.36499647 0.25969799 0.         0.36499647 0.         0.36499647
  0.36499647 0.         0.36499647 0.         0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.08704447]
 [0.08704447 1.        ]]
cosine_similarity: 0.08704446792504218
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Can Game 7 be decided by a Blake GriffinZach Randolph 10 round fight
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['can', 'game', 'be', 'decided', 'by', 'a', 'blake', 'randolph', 'round', 'fight']
cosine_similarity: 0.9585933089256287
train_input: [0.08704446792504218, 0.9585933], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: I fucking hate this bum son a bitch Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.49922133 0.49922133 0.
  0.35520009 0.         0.49922133 0.        ]
 [0.4078241  0.29017021 0.4078241  0.         0.         0.4078241
  0.29017021 0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: I fucking hate this bum son a bitch Blake Griffin
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['i', 'fucking', 'hate', 'this', 'bum', 'son', 'a', 'bitch', 'blake', 'griffin']
cosine_similarity: 0.9675951600074768
train_input: [0.20613696606828605, 0.96759516], train_label: 1
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: I hate when people say Blake Griffin is raw
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.         0.
  0.49922133 0.         0.        ]
 [0.31779954 0.         0.         0.31779954 0.44665616 0.44665616
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: I hate when people say Blake Griffin is raw
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['i', 'hate', 'when', 'people', 'say', 'blake', 'griffin', 'is', 'raw']
cosine_similarity: 0.952508270740509
train_input: [0.22576484600261604, 0.9525083], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: I wonder what Blake Griffin s game is gonna be like when he gets old lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.         0.
  0.35520009 0.         0.         0.         0.49922133 0.        ]
 [0.25136004 0.         0.         0.35327777 0.35327777 0.35327777
  0.25136004 0.35327777 0.35327777 0.35327777 0.         0.35327777]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: I wonder what Blake Griffin s game is gonna be like when he gets old lol
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['i', 'wonder', 'what', 'blake', 'griffin', 's', 'game', 'is', 'gonna', 'be', 'like', 'when', 'he', 'gets', 'old', 'lol']
cosine_similarity: 0.9620065093040466
train_input: [0.17856621555757476, 0.9620065], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Still a Huge Fan of Blake Griffin Tho Even Before He Was a LAC
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.35520009 0.
  0.         0.49922133 0.        ]
 [0.31779954 0.         0.         0.44665616 0.31779954 0.44665616
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: Still a Huge Fan of Blake Griffin Tho Even Before He Was a LAC
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['still', 'a', 'huge', 'fan', 'of', 'blake', 'griffin', 'tho', 'even', 'before', 'he', 'was', 'a', 'lac']
cosine_similarity: 0.9735617637634277
train_input: [0.22576484600261604, 0.97356176], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: ZBo getting physical with Blake griffin out there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.35520009 0.
  0.49922133 0.        ]
 [0.35520009 0.         0.         0.49922133 0.35520009 0.49922133
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Damn Blake Griffin is a dirty player, sentence2: ZBo getting physical with Blake griffin out there
After tokenization, sentence1: ['damn', 'blake', 'griffin', 'is', 'a', 'dirty', 'player'], sentence2: ['zbo', 'getting', 'physical', 'with', 'blake', 'griffin', 'out', 'there']
cosine_similarity: 0.9561967849731445
train_input: [0.2523342014336961, 0.9561968], train_label: 0
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Big Gingered fucker FUCK YOU Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44665616 0.31779954 0.44665616 0.         0.
  0.         0.31779954 0.44665616 0.44665616]
 [0.44665616 0.         0.31779954 0.         0.44665616 0.44665616
  0.44665616 0.31779954 0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Big Gingered fucker FUCK YOU Blake Griffin
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['big', 'fucker', 'fuck', 'you', 'blake', 'griffin']
cosine_similarity: 0.9786686301231384
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin Aint Going Off Tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44665616 0.31779954 0.44665616 0.         0.31779954
  0.44665616 0.44665616 0.        ]
 [0.49922133 0.         0.35520009 0.         0.49922133 0.35520009
  0.         0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin Aint Going Off Tonight
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'aint', 'going', 'off', 'tonight']
cosine_similarity: 0.9506076574325562
train_input: [0.22576484600261604, 0.95060766], train_label: 0
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin Chris Paul have no class
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.         0.44665616 0.31779954
  0.44665616 0.         0.44665616]
 [0.         0.35520009 0.49922133 0.49922133 0.         0.35520009
  0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin Chris Paul have no class
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'chris', 'paul', 'have', 'no', 'class']
cosine_similarity: 0.9554176330566406
train_input: [0.22576484600261604, 0.95541763], train_label: 1
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin always gets owned by Randolph
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.31779954 0.44665616
  0.         0.44665616 0.        ]
 [0.         0.35520009 0.         0.49922133 0.35520009 0.
  0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin always gets owned by Randolph
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'always', 'gets', 'owned', 'by', 'randolph']
cosine_similarity: 0.9393395781517029
train_input: [0.22576484600261604, 0.9393396], train_label: 0
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin frustrated and just hitting people
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.31779954 0.
  0.         0.44665616 0.         0.44665616]
 [0.         0.31779954 0.         0.44665616 0.31779954 0.44665616
  0.44665616 0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin frustrated and just hitting people
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'frustrated', 'and', 'just', 'hitting', 'people']
cosine_similarity: 0.9651452302932739
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin just picked up his 8th foul
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44665616 0.31779954 0.44665616 0.         0.31779954
  0.         0.44665616 0.         0.44665616]
 [0.44665616 0.         0.31779954 0.         0.44665616 0.31779954
  0.44665616 0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Blake Griffin just picked up his 8th foul
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'just', 'picked', 'up', 'his', 'foul']
cosine_similarity: 0.9556655287742615
train_input: [0.20199309249791833, 0.9556655], train_label: 0
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: I really really cant stand Chris Paul nor Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.44665616 0.31779954 0.44665616
  0.         0.44665616 0.         0.        ]
 [0.         0.25136004 0.35327777 0.         0.25136004 0.
  0.35327777 0.         0.70655553 0.35327777]]
pairwise_similarity: [[1.         0.15976421]
 [0.15976421 1.        ]]
cosine_similarity: 0.1597642092414444
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: I really really cant stand Chris Paul nor Blake Griffin
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['i', 'really', 'really', 'cant', 'stand', 'chris', 'paul', 'nor', 'blake', 'griffin']
cosine_similarity: 0.9771559238433838
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Marc Gasol and Blake Griffin having some words
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.31779954 0.
  0.44665616 0.         0.44665616 0.        ]
 [0.         0.31779954 0.         0.44665616 0.31779954 0.44665616
  0.         0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Marc Gasol and Blake Griffin having some words
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['marc', 'gasol', 'and', 'blake', 'griffin', 'having', 'some', 'words']
cosine_similarity: 0.9633449912071228
train_input: [0.20199309249791833, 0.963345], train_label: 0
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Not Afraid of dogs Not Afraid of Blake Griffin But Afraid of Cats
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44665616 0.31779954 0.         0.44665616 0.
  0.31779954 0.44665616 0.44665616]
 [0.86557514 0.         0.20528795 0.28852505 0.         0.28852505
  0.20528795 0.         0.        ]]
pairwise_similarity: [[1.         0.13048083]
 [0.13048083 1.        ]]
cosine_similarity: 0.13048082869525368
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Not Afraid of dogs Not Afraid of Blake Griffin But Afraid of Cats
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['not', 'afraid', 'of', 'dogs', 'not', 'afraid', 'of', 'blake', 'griffin', 'but', 'afraid', 'of', 'cats']
cosine_similarity: 0.9414948225021362
train_input: [0.13048082869525368, 0.9414948], train_label: 0
TF_IDF_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Yall talk that shit about Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.31779954 0.44665616 0.44665616
  0.         0.         0.        ]
 [0.         0.35520009 0.         0.35520009 0.         0.
  0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: blake griffin is such a bitch and lowkey dirty player, sentence2: Yall talk that shit about Blake Griffin
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'bitch', 'and', 'lowkey', 'dirty', 'player'], sentence2: ['yall', 'talk', 'that', 'shit', 'about', 'blake', 'griffin']
cosine_similarity: 0.9740337133407593
train_input: [0.22576484600261604, 0.9740337], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: At the moment I cant imagine Blake Griffin playing for another team
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.35520009 0.         0.         0.49922133
  0.         0.49922133 0.        ]
 [0.31779954 0.         0.31779954 0.44665616 0.44665616 0.
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: At the moment I cant imagine Blake Griffin playing for another team
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['at', 'the', 'moment', 'i', 'cant', 'imagine', 'blake', 'griffin', 'playing', 'for', 'another', 'team']
cosine_similarity: 0.9377594590187073
train_input: [0.22576484600261604, 0.93775946], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: Blake griffin and Chris Paul can eat a hot bowl of dick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.49922133 0.
  0.35520009 0.         0.         0.49922133 0.49922133]
 [0.26868528 0.37762778 0.37762778 0.37762778 0.         0.37762778
  0.26868528 0.37762778 0.37762778 0.         0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: Blake griffin and Chris Paul can eat a hot bowl of dick
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'and', 'chris', 'paul', 'can', 'eat', 'a', 'hot', 'bowl', 'of', 'dick']
cosine_similarity: 0.9646654725074768
train_input: [0.1908740661302035, 0.9646655], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: Blake griffin dont want those problems Breh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.35520009 0.49922133
  0.         0.49922133 0.        ]
 [0.31779954 0.44665616 0.         0.44665616 0.31779954 0.
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: Blake griffin dont want those problems Breh
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'dont', 'want', 'those', 'problems', 'breh']
cosine_similarity: 0.9359757900238037
train_input: [0.22576484600261604, 0.9359758], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: Blake griffin look like he can still do algebra
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.35520009 0.         0.
  0.49922133 0.49922133]
 [0.49922133 0.35520009 0.         0.35520009 0.49922133 0.49922133
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: Blake griffin look like he can still do algebra
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['blake', 'griffin', 'look', 'like', 'he', 'can', 'still', 'do', 'algebra']
cosine_similarity: 0.9461803436279297
train_input: [0.2523342014336961, 0.94618034], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: Fuck you Blake Griffin fuck the Clippers fuck LA and all of Flop City
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.         0.
  0.35520009 0.         0.49922133 0.49922133]
 [0.19007382 0.26714212 0.26714212 0.         0.26714212 0.80142637
  0.19007382 0.26714212 0.         0.        ]]
pairwise_similarity: [[1.         0.13502848]
 [0.13502848 1.        ]]
cosine_similarity: 0.13502847506727242
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: Fuck you Blake Griffin fuck the Clippers fuck LA and all of Flop City
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['fuck', 'you', 'blake', 'griffin', 'fuck', 'the', 'clippers', 'fuck', 'la', 'and', 'all', 'of', 'flop', 'city']
cosine_similarity: 0.9666362404823303
train_input: [0.13502847506727242, 0.96663624], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: I am tired of the Clippers Blake Griffin had 2 rebounds
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.35520009 0.49922133 0.
  0.49922133 0.        ]
 [0.35520009 0.49922133 0.         0.35520009 0.         0.49922133
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: I am tired of the Clippers Blake Griffin had 2 rebounds
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['i', 'am', 'tired', 'of', 'the', 'clippers', 'blake', 'griffin', 'had', 'rebounds']
cosine_similarity: 0.9509013891220093
train_input: [0.2523342014336961, 0.9509014], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: I have no idea why but I think Blake Griffin is cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.35520009 0.         0.49922133
  0.49922133 0.        ]
 [0.35520009 0.49922133 0.         0.35520009 0.49922133 0.
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: I have no idea why but I think Blake Griffin is cute
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['i', 'have', 'no', 'idea', 'why', 'but', 'i', 'think', 'blake', 'griffin', 'is', 'cute']
cosine_similarity: 0.932064950466156
train_input: [0.2523342014336961, 0.93206495], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: My baby Blake Griffin did good tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.49922133 0.         0.35520009
  0.49922133 0.49922133 0.        ]
 [0.44665616 0.31779954 0.44665616 0.         0.44665616 0.31779954
  0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: My baby Blake Griffin did good tho
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['my', 'baby', 'blake', 'griffin', 'did', 'good', 'tho']
cosine_similarity: 0.9328439831733704
train_input: [0.22576484600261604, 0.932844], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: You couldnt pay Pau to talk shit to Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.35520009 0.         0.         0.49922133
  0.         0.49922133 0.        ]
 [0.31779954 0.         0.31779954 0.44665616 0.44665616 0.
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: You couldnt pay Pau to talk shit to Blake Griffin
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['you', 'couldnt', 'pay', 'pau', 'to', 'talk', 'shit', 'to', 'blake', 'griffin']
cosine_similarity: 0.9295749664306641
train_input: [0.22576484600261604, 0.92957497], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: why would anyone like Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.35520009 0.         0.49922133 0.49922133]
 [0.50154891 0.         0.50154891 0.70490949 0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Blake griffin is a sneaky dirty player, sentence2: why would anyone like Blake Griffin
After tokenization, sentence1: ['blake', 'griffin', 'is', 'a', 'sneaky', 'dirty', 'player'], sentence2: ['why', 'would', 'anyone', 'like', 'blake', 'griffin']
cosine_similarity: 0.9495413303375244
train_input: [0.3563004293331381, 0.94954133], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin gets rattled too easily
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.31779954 0.44665616 0.
  0.44665616 0.44665616 0.44665616]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.         0.49922133
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin gets rattled too easily
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['blake', 'griffin', 'gets', 'rattled', 'too', 'easily']
cosine_similarity: 0.8796949982643127
train_input: [0.22576484600261604, 0.879695], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin had 2 rebounds in the game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.44665616 0.         0.44665616
  0.44665616 0.44665616]
 [0.40993715 0.57615236 0.40993715 0.         0.57615236 0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin had 2 rebounds in the game
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['blake', 'griffin', 'had', 'rebounds', 'in', 'the', 'game']
cosine_similarity: 0.9710438251495361
train_input: [0.2605556710562624, 0.9710438], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin is playing dirty this quarter man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.         0.         0.
  0.44665616 0.44665616 0.44665616 0.44665616]
 [0.31779954 0.44665616 0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin is playing dirty this quarter man
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['blake', 'griffin', 'is', 'playing', 'dirty', 'this', 'quarter', 'man']
cosine_similarity: 0.974189281463623
train_input: [0.20199309249791833, 0.9741893], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin part of another double foul
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.31779954 0.44665616 0.44665616
  0.44665616 0.44665616]
 [0.40993715 0.57615236 0.57615236 0.40993715 0.         0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin part of another double foul
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['blake', 'griffin', 'part', 'of', 'another', 'double', 'foul']
cosine_similarity: 0.9658052921295166
train_input: [0.2605556710562624, 0.9658053], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin would pick on the smallest guy out there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.         0.44665616 0.44665616
  0.         0.44665616 0.44665616]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133 0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake Griffin would pick on the smallest guy out there
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['blake', 'griffin', 'would', 'pick', 'on', 'the', 'smallest', 'guy', 'out', 'there']
cosine_similarity: 0.9591087102890015
train_input: [0.22576484600261604, 0.9591087], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake griffin and gasol was getting into it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.31779954 0.44665616 0.44665616
  0.44665616 0.44665616]
 [0.40993715 0.57615236 0.57615236 0.40993715 0.         0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Blake griffin and gasol was getting into it
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['blake', 'griffin', 'and', 'gasol', 'was', 'getting', 'into', 'it']
cosine_similarity: 0.9709190130233765
train_input: [0.2605556710562624, 0.970919], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Lamar Odom now joining Blake Griffin and Chris Paul in flopping
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.31779954 0.         0.
  0.         0.         0.44665616 0.44665616 0.44665616 0.44665616]
 [0.26868528 0.37762778 0.37762778 0.26868528 0.37762778 0.37762778
  0.37762778 0.37762778 0.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Lamar Odom now joining Blake Griffin and Chris Paul in flopping
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['lamar', 'odom', 'now', 'joining', 'blake', 'griffin', 'and', 'chris', 'paul', 'in', 'flopping']
cosine_similarity: 0.946260392665863
train_input: [0.1707761131901165, 0.9462604], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Man Blake Griffin s dunk got everyone so hype
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.31779954 0.         0.
  0.44665616 0.44665616 0.44665616 0.44665616]
 [0.31779954 0.44665616 0.44665616 0.31779954 0.44665616 0.44665616
  0.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: Man Blake Griffin s dunk got everyone so hype
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['man', 'blake', 'griffin', 's', 'dunk', 'got', 'everyone', 'so', 'hype']
cosine_similarity: 0.9518638849258423
train_input: [0.20199309249791833, 0.9518639], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: My little brother thinks Blake Griffin is white
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.         0.44665616 0.44665616
  0.44665616 0.         0.44665616 0.        ]
 [0.31779954 0.44665616 0.31779954 0.44665616 0.         0.
  0.         0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: My little brother thinks Blake Griffin is white
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['my', 'little', 'brother', 'thinks', 'blake', 'griffin', 'is', 'white']
cosine_similarity: 0.9429501295089722
train_input: [0.20199309249791833, 0.9429501], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: blake griffin the only white person on the clippers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.         0.44665616 0.44665616
  0.44665616 0.44665616 0.        ]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.         0.
  0.         0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Randolph is takin a toll on blake griffin this series, sentence2: blake griffin the only white person on the clippers
After tokenization, sentence1: ['randolph', 'is', 'takin', 'a', 'toll', 'on', 'blake', 'griffin', 'this', 'series'], sentence2: ['blake', 'griffin', 'the', 'only', 'white', 'person', 'on', 'the', 'clippers']
cosine_similarity: 0.9725462794303894
train_input: [0.22576484600261604, 0.9725463], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin and all this flopping
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.57615236 0.40993715]
 [0.50154891 0.         0.70490949 0.         0.50154891]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin and all this flopping
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['blake', 'griffin', 'and', 'all', 'this', 'flopping']
cosine_similarity: 0.9599508047103882
train_input: [0.4112070550676187, 0.9599508], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin dunks are sometimes like it s an afterthought
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.         0.57615236 0.40993715
  0.        ]
 [0.49922133 0.35520009 0.         0.49922133 0.         0.35520009
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin dunks are sometimes like it s an afterthought
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['blake', 'griffin', 'dunks', 'are', 'sometimes', 'like', 'it', 's', 'an', 'afterthought']
cosine_similarity: 0.9576197266578674
train_input: [0.29121941856368966, 0.9576197], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin gets rattled too easily
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.57615236 0.         0.40993715
  0.        ]
 [0.35520009 0.         0.49922133 0.         0.49922133 0.35520009
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin gets rattled too easily
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['blake', 'griffin', 'gets', 'rattled', 'too', 'easily']
cosine_similarity: 0.9167115092277527
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin is hella athletic funny makes great commercials
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.57615236 0.
  0.         0.40993715 0.         0.        ]
 [0.37762778 0.26868528 0.37762778 0.         0.         0.37762778
  0.37762778 0.26868528 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin is hella athletic funny makes great commercials
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['blake', 'griffin', 'is', 'hella', 'athletic', 'funny', 'makes', 'great', 'commercials']
cosine_similarity: 0.9705889821052551
train_input: [0.2202881505618297, 0.970589], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin isnt about that Marc Gasol life
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.57615236 0.         0.40993715 0.
  0.         0.        ]
 [0.31779954 0.         0.         0.44665616 0.31779954 0.44665616
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Blake Griffin isnt about that Marc Gasol life
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['blake', 'griffin', 'isnt', 'about', 'that', 'marc', 'gasol', 'life']
cosine_similarity: 0.9666908979415894
train_input: [0.2605556710562624, 0.9666909], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Can Game 7 be decided by a Blake GriffinZach Randolph 10 round fight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.         0.53404633
  0.         0.53404633 0.         0.         0.        ]
 [0.36499647 0.25969799 0.         0.36499647 0.36499647 0.
  0.36499647 0.         0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.09867962]
 [0.09867962 1.        ]]
cosine_similarity: 0.09867961797986956
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Can Game 7 be decided by a Blake GriffinZach Randolph 10 round fight
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['can', 'game', 'be', 'decided', 'by', 'a', 'blake', 'randolph', 'round', 'fight']
cosine_similarity: 0.9417031407356262
train_input: [0.09867961797986956, 0.94170314], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: I feel like Blake Griffin is tough as long as he knows nothing s gonna happen
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.57615236 0.         0.40993715
  0.         0.         0.         0.         0.        ]
 [0.25136004 0.         0.35327777 0.         0.35327777 0.25136004
  0.35327777 0.35327777 0.35327777 0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: I feel like Blake Griffin is tough as long as he knows nothing s gonna happen
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['i', 'feel', 'like', 'blake', 'griffin', 'is', 'tough', 'as', 'long', 'as', 'he', 'knows', 'nothing', 's', 'gonna', 'happen']
cosine_similarity: 0.951657772064209
train_input: [0.20608363501393823, 0.9516578], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Marc Gasol and Blake Griffin having some words
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.57615236 0.         0.40993715 0.
  0.         0.        ]
 [0.31779954 0.         0.         0.44665616 0.31779954 0.44665616
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Marc Gasol and Blake Griffin having some words
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['marc', 'gasol', 'and', 'blake', 'griffin', 'having', 'some', 'words']
cosine_similarity: 0.9582140445709229
train_input: [0.2605556710562624, 0.95821404], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Ohhhhhhh the flopper chant was towards Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.57615236 0.40993715
  0.        ]
 [0.35520009 0.49922133 0.         0.49922133 0.         0.35520009
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: Ohhhhhhh the flopper chant was towards Blake Griffin
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['the', 'flopper', 'chant', 'was', 'towards', 'blake', 'griffin']
cosine_similarity: 0.9421866536140442
train_input: [0.29121941856368966, 0.94218665], train_label: 0
TF_IDF_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: you talkin to Blake Griffin here
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.57615236 0.40993715 0.        ]
 [0.50154891 0.         0.         0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Blake griffin is such a fuckin cunt, sentence2: you talkin to Blake Griffin here
After tokenization, sentence1: ['blake', 'griffin', 'is', 'such', 'a', 'fuckin', 'cunt'], sentence2: ['you', 'talkin', 'to', 'blake', 'griffin', 'here']
cosine_similarity: 0.9479548931121826
train_input: [0.4112070550676187, 0.9479549], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Blake Griffin Aint Going Off Tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.4078241  0.         0.29017021 0.4078241
  0.4078241  0.4078241  0.4078241  0.        ]
 [0.49922133 0.35520009 0.         0.49922133 0.35520009 0.
  0.         0.         0.         0.49922133]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Blake Griffin Aint Going Off Tonight
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['blake', 'griffin', 'aint', 'going', 'off', 'tonight']
cosine_similarity: 0.960051417350769
train_input: [0.20613696606828605, 0.9600514], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Blake Griffin did knock ZBO headband off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.         0.4078241  0.29017021 0.         0.
  0.4078241  0.4078241  0.4078241  0.4078241  0.        ]
 [0.31779954 0.44665616 0.         0.31779954 0.44665616 0.44665616
  0.         0.         0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Blake Griffin did knock ZBO headband off
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['blake', 'griffin', 'did', 'knock', 'zbo', 'headband', 'off']
cosine_similarity: 0.9287579655647278
train_input: [0.18443191662261305, 0.92875797], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Blake Griffin is a fucking bitch for that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.4078241  0.         0.29017021 0.4078241
  0.4078241  0.4078241  0.4078241 ]
 [0.57615236 0.40993715 0.         0.57615236 0.40993715 0.
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Blake Griffin is a fucking bitch for that
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['blake', 'griffin', 'is', 'a', 'fucking', 'bitch', 'for', 'that']
cosine_similarity: 0.9882577657699585
train_input: [0.23790309463326234, 0.98825777], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: I love how Zbo and Blake griffin just hate each other
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.29017021 0.         0.         0.4078241
  0.4078241  0.         0.4078241  0.4078241  0.        ]
 [0.31779954 0.         0.31779954 0.44665616 0.44665616 0.
  0.         0.44665616 0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: I love how Zbo and Blake griffin just hate each other
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['i', 'love', 'how', 'zbo', 'and', 'blake', 'griffin', 'just', 'hate', 'each', 'other']
cosine_similarity: 0.9688384532928467
train_input: [0.18443191662261305, 0.96883845], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: I really wonder if Blake griffin can do anything on a basketball court other than dunk
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.         0.         0.4078241  0.29017021
  0.4078241  0.4078241  0.4078241  0.4078241  0.         0.        ]
 [0.4078241  0.29017021 0.4078241  0.4078241  0.         0.29017021
  0.         0.         0.         0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.1683975]
 [0.1683975 1.       ]]
cosine_similarity: 0.16839750037215276
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: I really wonder if Blake griffin can do anything on a basketball court other than dunk
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['i', 'really', 'wonder', 'if', 'blake', 'griffin', 'can', 'do', 'anything', 'on', 'a', 'basketball', 'court', 'other', 'than', 'dunk']
cosine_similarity: 0.9691101312637329
train_input: [0.16839750037215276, 0.96911013], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Just looks like Blake Griffin Worn Out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.30287281 0.         0.30287281 0.42567716
  0.         0.42567716 0.42567716 0.        ]
 [0.33471228 0.         0.33471228 0.47042643 0.33471228 0.
  0.47042643 0.         0.         0.47042643]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Just looks like Blake Griffin Worn Out
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['just', 'looks', 'like', 'blake', 'griffin', 'worn', 'out']
cosine_similarity: 0.96954345703125
train_input: [0.3041257418754935, 0.96954346], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Turiaf act like he get a Hard On every time Blake Griffin dunk
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30287281 0.         0.42567716 0.30287281 0.
  0.30287281 0.42567716 0.42567716 0.42567716 0.         0.        ]
 [0.39166832 0.27867523 0.39166832 0.         0.27867523 0.39166832
  0.27867523 0.         0.         0.         0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.25320945]
 [0.25320945 1.        ]]
cosine_similarity: 0.2532094495161745
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Turiaf act like he get a Hard On every time Blake Griffin dunk
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['turiaf', 'act', 'like', 'he', 'get', 'a', 'hard', 'on', 'every', 'time', 'blake', 'griffin', 'dunk']
cosine_similarity: 0.9820713400840759
train_input: [0.2532094495161745, 0.98207134], train_label: 0
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Zach Randolph is making Blake Griffin his little bitch in the post
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.3174044  0.44610081 0.3174044  0.44610081 0.
  0.44610081 0.3174044  0.         0.3174044  0.        ]
 [0.40740124 0.28986934 0.         0.28986934 0.         0.40740124
  0.         0.28986934 0.40740124 0.28986934 0.40740124]]
pairwise_similarity: [[1.         0.36802321]
 [0.36802321 1.        ]]
cosine_similarity: 0.36802320875611494
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: Zach Randolph is making Blake Griffin his little bitch in the post
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['zach', 'randolph', 'is', 'making', 'blake', 'griffin', 'his', 'little', 'bitch', 'in', 'the', 'post']
cosine_similarity: 0.9873661994934082
train_input: [0.36802320875611494, 0.9873662], train_label: 1
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: blake griffin gettin abused out there by zach randolph
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30287281 0.42567716 0.         0.30287281 0.42567716
  0.42567716 0.42567716 0.30287281 0.        ]
 [0.47042643 0.33471228 0.         0.47042643 0.33471228 0.
  0.         0.         0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: blake griffin gettin abused out there by zach randolph
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['blake', 'griffin', 'gettin', 'abused', 'out', 'there', 'by', 'zach', 'randolph']
cosine_similarity: 0.9637641310691833
train_input: [0.3041257418754935, 0.96376413], train_label: 1
TF_IDF_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: i wish blake griffin would keep his hair low
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.29017021 0.         0.4078241  0.4078241
  0.         0.4078241  0.4078241  0.        ]
 [0.35520009 0.         0.35520009 0.49922133 0.         0.
  0.49922133 0.         0.         0.49922133]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Randolph is making Blake Griffin look like a fool, sentence2: i wish blake griffin would keep his hair low
After tokenization, sentence1: ['randolph', 'is', 'making', 'blake', 'griffin', 'look', 'like', 'a', 'fool'], sentence2: ['i', 'wish', 'blake', 'griffin', 'would', 'keep', 'his', 'hair', 'low']
cosine_similarity: 0.9628559350967407
train_input: [0.20613696606828605, 0.96285594], train_label: 0
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: AAAWWW Did someone touch u Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.35520009 0.49922133 0.49922133
  0.         0.49922133]
 [0.49922133 0.35520009 0.49922133 0.35520009 0.         0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: AAAWWW Did someone touch u Blake Griffin
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['did', 'someone', 'touch', 'u', 'blake', 'griffin']
cosine_similarity: 0.9664137363433838
train_input: [0.2523342014336961, 0.96641374], train_label: 0
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Blake Griffin MacBo50 will beat yo ass
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35520009 0.35520009 0.         0.49922133
  0.49922133 0.49922133 0.        ]
 [0.44665616 0.44665616 0.31779954 0.31779954 0.44665616 0.
  0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Blake Griffin MacBo50 will beat yo ass
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['blake', 'griffin', 'will', 'beat', 'yo', 'ass']
cosine_similarity: 0.944571316242218
train_input: [0.22576484600261604, 0.9445713], train_label: 0
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Blake Griffin had 2 rebounds my nigga
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.         0.49922133
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Blake Griffin had 2 rebounds my nigga
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['blake', 'griffin', 'had', 'rebounds', 'my', 'nigga']
cosine_similarity: 0.9110503196716309
train_input: [0.29121941856368966, 0.9110503], train_label: 0
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Blake Griffin is a hoe ass nigga
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.         0.         0.49922133
  0.49922133 0.49922133]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.49922133 0.
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Blake Griffin is a hoe ass nigga
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['blake', 'griffin', 'is', 'a', 'hoe', 'ass', 'nigga']
cosine_similarity: 0.9408073425292969
train_input: [0.2523342014336961, 0.94080734], train_label: 1
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Blake Griffin only got 2 rebounds
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.49922133 0.         0.49922133
  0.49922133]
 [0.40993715 0.57615236 0.40993715 0.         0.57615236 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Blake Griffin only got 2 rebounds
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['blake', 'griffin', 'only', 'got', 'rebounds']
cosine_similarity: 0.8754367828369141
train_input: [0.29121941856368966, 0.8754368], train_label: 0
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: I hate Blake Griffin so much he s a bitch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.         0.49922133 0.49922133
  0.49922133]
 [0.57615236 0.40993715 0.40993715 0.57615236 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: I hate Blake Griffin so much he s a bitch
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['i', 'hate', 'blake', 'griffin', 'so', 'much', 'he', 's', 'a', 'bitch']
cosine_similarity: 0.9856704473495483
train_input: [0.29121941856368966, 0.98567045], train_label: 1
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: I hate basketball but I love Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.         0.         0.49922133
  0.49922133 0.49922133]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.49922133 0.
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: I hate basketball but I love Blake Griffin
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['i', 'hate', 'basketball', 'but', 'i', 'love', 'blake', 'griffin']
cosine_similarity: 0.9767768979072571
train_input: [0.2523342014336961, 0.9767769], train_label: 0
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: I only watch CLIPPERS games just to see Blake Griffin DUNK
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.         0.37930349 0.
  0.53309782 0.53309782 0.37930349]
 [0.30287281 0.42567716 0.42567716 0.42567716 0.30287281 0.42567716
  0.         0.         0.30287281]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: I only watch CLIPPERS games just to see Blake Griffin DUNK
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['i', 'only', 'watch', 'clippers', 'games', 'just', 'to', 'see', 'blake', 'griffin', 'dunk']
cosine_similarity: 0.964781641960144
train_input: [0.34464214103805474, 0.96478164], train_label: 0
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: The hang time that Blake Griffin gets looks fake
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.35520009 0.         0.
  0.49922133 0.49922133 0.         0.49922133]
 [0.29017021 0.4078241  0.4078241  0.29017021 0.4078241  0.4078241
  0.         0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: The hang time that Blake Griffin gets looks fake
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['the', 'hang', 'time', 'that', 'blake', 'griffin', 'gets', 'looks', 'fake']
cosine_similarity: 0.9685391187667847
TF_IDF_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Zach Randolph Blake Griffin going at it hard this series lol I love it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.         0.
  0.49922133 0.         0.         0.49922133 0.49922133 0.        ]
 [0.25136004 0.35327777 0.25136004 0.35327777 0.35327777 0.35327777
  0.         0.35327777 0.35327777 0.         0.         0.35327777]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: More I watch Blake Griffin the more I think he s a pussy, sentence2: Zach Randolph Blake Griffin going at it hard this series lol I love it
After tokenization, sentence1: ['more', 'i', 'watch', 'blake', 'griffin', 'the', 'more', 'i', 'think', 'he', 's', 'a', 'pussy'], sentence2: ['zach', 'randolph', 'blake', 'griffin', 'going', 'at', 'it', 'hard', 'this', 'series', 'lol', 'i', 'love', 'it']
cosine_similarity: 0.9735193252563477
train_input: [0.17856621555757476, 0.9735193], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Blake Griffin is suck a fucking bitch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.40993715 0.57615236 0.
  0.57615236]
 [0.49922133 0.35520009 0.49922133 0.35520009 0.         0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Blake Griffin is suck a fucking bitch
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['blake', 'griffin', 'is', 'suck', 'a', 'fucking', 'bitch']
cosine_similarity: 0.9498631954193115
train_input: [0.29121941856368966, 0.9498632], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Blake Griffin isnt doing to much this game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.         0.57615236
  0.57615236]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Blake Griffin isnt doing to much this game
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['blake', 'griffin', 'isnt', 'doing', 'to', 'much', 'this', 'game']
cosine_similarity: 0.9446089863777161
train_input: [0.29121941856368966, 0.944609], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Blake Griffin look weird as Fukk
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.57615236 0.57615236
  0.        ]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.         0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Blake Griffin look weird as Fukk
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['blake', 'griffin', 'look', 'weird', 'as', 'fukk']
cosine_similarity: 0.9245492219924927
train_input: [0.29121941856368966, 0.9245492], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: I hate Blake Griffin so much he s a bitch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.57615236]
 [0.57615236 0.40993715 0.40993715 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: I hate Blake Griffin so much he s a bitch
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['i', 'hate', 'blake', 'griffin', 'so', 'much', 'he', 's', 'a', 'bitch']
cosine_similarity: 0.934174656867981
train_input: [0.3360969272762575, 0.93417466], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: I used to like Blake Griffin until this game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.57615236 0.57615236
  0.        ]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.         0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: I used to like Blake Griffin until this game
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['i', 'used', 'to', 'like', 'blake', 'griffin', 'until', 'this', 'game']
cosine_similarity: 0.9450462460517883
train_input: [0.29121941856368966, 0.94504625], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: I wonder what Blake Griffin s game is gonna be like when he gets old lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.40993715 0.
  0.         0.         0.57615236 0.57615236 0.        ]
 [0.25136004 0.35327777 0.35327777 0.35327777 0.25136004 0.35327777
  0.35327777 0.35327777 0.         0.         0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: I wonder what Blake Griffin s game is gonna be like when he gets old lol
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['i', 'wonder', 'what', 'blake', 'griffin', 's', 'game', 'is', 'gonna', 'be', 'like', 'when', 'he', 'gets', 'old', 'lol']
cosine_similarity: 0.9429194927215576
train_input: [0.20608363501393823, 0.9429195], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Lmao blake griffin crying for fouls
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.         0.57615236
  0.57615236]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Lmao blake griffin crying for fouls
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['lmao', 'blake', 'griffin', 'crying', 'for', 'fouls']
cosine_similarity: 0.9110738039016724
train_input: [0.29121941856368966, 0.9110738], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Loving the doublefoul calls on ZBo Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.         0.57615236
  0.57615236 0.        ]
 [0.31779954 0.44665616 0.44665616 0.31779954 0.44665616 0.
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Loving the doublefoul calls on ZBo Blake Griffin
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['loving', 'the', 'calls', 'on', 'zbo', 'blake', 'griffin']
cosine_similarity: 0.9563390612602234
train_input: [0.2605556710562624, 0.95633906], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: What little respect for Blake Griffin I had has just been lost
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.
  0.57615236 0.57615236]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.44665616
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: What little respect for Blake Griffin I had has just been lost
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['what', 'little', 'respect', 'for', 'blake', 'griffin', 'i', 'had', 'has', 'just', 'been', 'lost']
cosine_similarity: 0.9511688351631165
train_input: [0.2605556710562624, 0.95116884], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Zach Randolph probably would not have patted Blake Griffin on the head like that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.
  0.         0.57615236 0.57615236 0.        ]
 [0.26868528 0.26868528 0.37762778 0.37762778 0.37762778 0.37762778
  0.37762778 0.         0.         0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Is Blake griffin a superstar or star, sentence2: Zach Randolph probably would not have patted Blake Griffin on the head like that
After tokenization, sentence1: ['is', 'blake', 'griffin', 'a', 'superstar', 'or', 'star'], sentence2: ['zach', 'randolph', 'probably', 'would', 'not', 'have', 'patted', 'blake', 'griffin', 'on', 'the', 'head', 'like', 'that']
cosine_similarity: 0.9415310025215149
train_input: [0.2202881505618297, 0.941531], train_label: 0
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake Griffin a lil slick dirty mf
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.70490949 0.
  0.        ]
 [0.31779954 0.44665616 0.31779954 0.44665616 0.         0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake Griffin a lil slick dirty mf
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['blake', 'griffin', 'a', 'lil', 'slick', 'dirty', 'mf']
cosine_similarity: 0.8998449444770813
train_input: [0.31878402175377923, 0.89984494], train_label: 0
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake Griffin bitches about every single call
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.50154891 0.70490949 0.        ]
 [0.57615236 0.40993715 0.40993715 0.         0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake Griffin bitches about every single call
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['blake', 'griffin', 'bitches', 'about', 'every', 'single', 'call']
cosine_similarity: 0.9690167307853699
train_input: [0.4112070550676187, 0.96901673], train_label: 0
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake Griffin hates ZBo so much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake Griffin hates ZBo so much
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['blake', 'griffin', 'hates', 'zbo', 'so', 'much']
cosine_similarity: 0.923273503780365
train_input: [0.4112070550676187, 0.9232735], train_label: 0
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake Griffin is probably my least favorite player in the NBA
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.70490949 0.         0.
  0.        ]
 [0.31779954 0.44665616 0.31779954 0.         0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake Griffin is probably my least favorite player in the NBA
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['blake', 'griffin', 'is', 'probably', 'my', 'least', 'favorite', 'player', 'in', 'the', 'nba']
cosine_similarity: 0.9862712025642395
train_input: [0.31878402175377923, 0.9862712], train_label: 0
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake griffin picking on the point guard
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Blake griffin picking on the point guard
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['blake', 'griffin', 'picking', 'on', 'the', 'point', 'guard']
cosine_similarity: 0.9798499941825867
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: I wish I could jump like Blake Griffin maybe I should start driving a Kia Optima
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.         0.
  0.70490949 0.         0.         0.         0.        ]
 [0.23700504 0.33310232 0.23700504 0.33310232 0.33310232 0.33310232
  0.         0.33310232 0.33310232 0.33310232 0.33310232]]
pairwise_similarity: [[1.         0.23773924]
 [0.23773924 1.        ]]
cosine_similarity: 0.23773923857522397
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: I wish I could jump like Blake Griffin maybe I should start driving a Kia Optima
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['i', 'wish', 'i', 'could', 'jump', 'like', 'blake', 'griffin', 'maybe', 'i', 'should', 'start', 'driving', 'a', 'kia', 'optima']
cosine_similarity: 0.9686172008514404
train_input: [0.23773923857522397, 0.9686172], train_label: 0
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Okay Blake Griffin that was a cheap shot
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.70490949 0.         0.        ]
 [0.35520009 0.49922133 0.35520009 0.         0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Okay Blake Griffin that was a cheap shot
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['okay', 'blake', 'griffin', 'that', 'was', 'a', 'cheap', 'shot']
cosine_similarity: 0.9819238185882568
train_input: [0.3563004293331381, 0.9819238], train_label: 0
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: The things Margella says about look a like Blake Griffin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.
  0.         0.        ]
 [0.29017021 0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: The things Margella says about look a like Blake Griffin
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['the', 'things', 'says', 'about', 'look', 'a', 'like', 'blake', 'griffin']
cosine_similarity: 0.9770891666412354
train_input: [0.2910691023819054, 0.97708917], train_label: 0
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Were on top by 10 on the clippers Blake griffin isnt doing shit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.         0.         0.50154891 0.
  0.70490949 0.        ]
 [0.4078241  0.29017021 0.4078241  0.4078241  0.29017021 0.4078241
  0.         0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: Were on top by 10 on the clippers Blake griffin isnt doing shit
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['were', 'on', 'top', 'by', 'on', 'the', 'clippers', 'blake', 'griffin', 'isnt', 'doing', 'shit']
cosine_similarity: 0.9919694662094116
train_input: [0.2910691023819054, 0.99196947], train_label: 0
TF_IDF_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: hahaha blake griffin sucks in the post
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: when Blake Griffin was on the line, sentence2: hahaha blake griffin sucks in the post
After tokenization, sentence1: ['when', 'blake', 'griffin', 'was', 'on', 'the', 'line'], sentence2: ['hahaha', 'blake', 'griffin', 'sucks', 'in', 'the', 'post']
cosine_similarity: 0.9828037023544312
train_input: [0.3563004293331381, 0.9828037], train_label: 0
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Blue Ivy I found who unfollowed me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.40993715 0.        ]
 [0.         0.50154891 0.         0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Blue Ivy I found who unfollowed me
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['blue', 'ivy', 'i', 'found', 'who', 'unfollowed', 'me']
cosine_similarity: 0.8831331133842468
train_input: [0.4112070550676187, 0.8831331], train_label: 0
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Blue Ivy looks so mature in the face
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.40993715 0.
  0.        ]
 [0.         0.35520009 0.         0.49922133 0.35520009 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Blue Ivy looks so mature in the face
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['blue', 'ivy', 'looks', 'so', 'mature', 'in', 'the', 'face']
cosine_similarity: 0.9500710964202881
train_input: [0.29121941856368966, 0.9500711], train_label: 0
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Doesnt it seem like Blue Ivy was born like a few months ago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.         0.57615236 0.
  0.40993715 0.         0.        ]
 [0.33310232 0.         0.23700504 0.33310232 0.         0.33310232
  0.23700504 0.66620463 0.33310232]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858148
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Doesnt it seem like Blue Ivy was born like a few months ago
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['doesnt', 'it', 'seem', 'like', 'blue', 'ivy', 'was', 'born', 'like', 'a', 'few', 'months', 'ago']
cosine_similarity: 0.9198642373085022
train_input: [0.19431434016858148, 0.91986424], train_label: 0
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Forgive me but Blue Ivy is ugly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.40993715 0.        ]
 [0.         0.40993715 0.         0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Forgive me but Blue Ivy is ugly
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['forgive', 'me', 'but', 'blue', 'ivy', 'is', 'ugly']
cosine_similarity: 0.9281119704246521
train_input: [0.3360969272762575, 0.928112], train_label: 0
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: OMG blue ivy is too cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.57615236 0.40993715 0.        ]
 [0.         0.40993715 0.57615236 0.         0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: OMG blue ivy is too cute
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['omg', 'blue', 'ivy', 'is', 'too', 'cute']
cosine_similarity: 0.9661539793014526
train_input: [0.3360969272762575, 0.966154], train_label: 1
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Oh my god Blue Ivy is on E
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.40993715 0.        ]
 [0.         0.40993715 0.         0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Oh my god Blue Ivy is on E
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['oh', 'my', 'god', 'blue', 'ivy', 'is', 'on', 'e']
cosine_similarity: 0.9162377119064331
train_input: [0.3360969272762575, 0.9162377], train_label: 0
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Oiii what s the beef with Blue Ivy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.         0.40993715 0.57615236 0.40993715 0.        ]
 [0.         0.57615236 0.40993715 0.         0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Oiii what s the beef with Blue Ivy
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['what', 's', 'the', 'beef', 'with', 'blue', 'ivy']
cosine_similarity: 0.9079753160476685
train_input: [0.3360969272762575, 0.9079753], train_label: 0
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: The way Beyonce holds Blue Ivy worries me sometimes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.         0.40993715 0.57615236 0.         0.40993715
  0.         0.        ]
 [0.         0.44665616 0.31779954 0.         0.44665616 0.31779954
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: The way Beyonce holds Blue Ivy worries me sometimes
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['the', 'way', 'beyonce', 'holds', 'blue', 'ivy', 'worries', 'me', 'sometimes']
cosine_similarity: 0.9131332635879517
train_input: [0.2605556710562624, 0.91313326], train_label: 0
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Those people who say that Blue Ivy is hideous should go to hell
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.         0.40993715
  0.         0.        ]
 [0.         0.31779954 0.         0.44665616 0.44665616 0.31779954
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: Those people who say that Blue Ivy is hideous should go to hell
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['those', 'people', 'who', 'say', 'that', 'blue', 'ivy', 'is', 'hideous', 'should', 'go', 'to', 'hell']
cosine_similarity: 0.9184969663619995
train_input: [0.2605556710562624, 0.91849697], train_label: 0
TF_IDF_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: What is up with all these hate towards Blue Ivy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.40993715]
 [0.         0.50154891 0.         0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Awwwwwwwwwwwww Blue Ivy is so cuuuuuuuuute, sentence2: What is up with all these hate towards Blue Ivy
After tokenization, sentence1: ['blue', 'ivy', 'is', 'so', 'cuuuuuuuuute'], sentence2: ['what', 'is', 'up', 'with', 'all', 'these', 'hate', 'towards', 'blue', 'ivy']
cosine_similarity: 0.9126509428024292
train_input: [0.4112070550676187, 0.91265094], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Body Party video was so cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.47042643 0.33471228 0.33471228
  0.47042643]
 [0.44832087 0.         0.63009934 0.         0.44832087 0.44832087
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Body Party video was so cute
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['body', 'party', 'video', 'was', 'so', 'cute']
cosine_similarity: 0.9691542983055115
train_input: [0.4501755023269897, 0.9691543], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Ci Ci slick showing out in that body party video 2 cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.47042643 0.         0.47042643 0.33471228
  0.         0.         0.33471228 0.47042643]
 [0.24377685 0.68523971 0.         0.34261985 0.         0.24377685
  0.34261985 0.34261985 0.24377685 0.        ]]
pairwise_similarity: [[1.         0.24478531]
 [0.24478531 1.        ]]
cosine_similarity: 0.2447853117345521
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Ci Ci slick showing out in that body party video 2 cute
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['ci', 'ci', 'slick', 'showing', 'out', 'in', 'that', 'body', 'party', 'video', 'cute']
cosine_similarity: 0.9551798105239868
train_input: [0.2447853117345521, 0.9551798], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Future look real good in Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.         0.47042643 0.
  0.33471228 0.         0.33471228 0.47042643]
 [0.30287281 0.         0.42567716 0.42567716 0.         0.42567716
  0.30287281 0.42567716 0.30287281 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Future look real good in Body Party video
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['future', 'look', 'real', 'good', 'in', 'body', 'party', 'video']
cosine_similarity: 0.9625212550163269
train_input: [0.30412574187549346, 0.96252126], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: I feel like Ciara is teasing me every time I see the Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.49844628 0.         0.35464863
  0.         0.         0.35464863 0.49844628]
 [0.28986934 0.28986934 0.40740124 0.         0.40740124 0.28986934
  0.40740124 0.40740124 0.28986934 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: I feel like Ciara is teasing me every time I see the Body Party video
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['i', 'feel', 'like', 'ciara', 'is', 'teasing', 'me', 'every', 'time', 'i', 'see', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9719471335411072
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: I have not even seen that Body Party video yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.47042643 0.33471228 0.         0.33471228
  0.47042643]
 [0.44832087 0.         0.         0.44832087 0.63009934 0.44832087
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: I have not even seen that Body Party video yet
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['i', 'have', 'not', 'even', 'seen', 'that', 'body', 'party', 'video', 'yet']
cosine_similarity: 0.9638225436210632
train_input: [0.4501755023269897, 0.96382254], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: I just keep watching ciara s body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.37863221 0.37863221 0.37863221 0.53215436
  0.        ]
 [0.37863221 0.37863221 0.37863221 0.37863221 0.37863221 0.
  0.53215436]]
pairwise_similarity: [[1.         0.71681174]
 [0.71681174 1.        ]]
cosine_similarity: 0.7168117414430624
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: I just keep watching ciara s body party video
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['i', 'just', 'keep', 'watching', 'ciara', 's', 'body', 'party', 'video']
cosine_similarity: 0.9781422019004822
train_input: [0.7168117414430624, 0.9781422], train_label: 1
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Lmaoo That Video Some Heat Ciara Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.49844628 0.         0.35464863
  0.35464863 0.49844628]
 [0.35464863 0.35464863 0.49844628 0.         0.49844628 0.35464863
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Lmaoo That Video Some Heat Ciara Body Party
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['lmaoo', 'that', 'video', 'some', 'heat', 'ciara', 'body', 'party']
cosine_similarity: 0.9671419858932495
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Why is Ciara popping so much ass in that body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.49844628 0.35464863 0.
  0.35464863 0.49844628]
 [0.49844628 0.35464863 0.35464863 0.         0.35464863 0.49844628
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: Why is Ciara popping so much ass in that body party video
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['why', 'is', 'ciara', 'popping', 'so', 'much', 'ass', 'in', 'that', 'body', 'party', 'video']
cosine_similarity: 0.9710479378700256
train_input: [0.5031026124151313, 0.97104794], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: aint even gon lie Future was sexy af in the Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.33471228 0.47042643 0.         0.
  0.47042643 0.         0.33471228 0.         0.33471228 0.47042643]
 [0.36469323 0.36469323 0.25948224 0.         0.36469323 0.36469323
  0.         0.36469323 0.25948224 0.36469323 0.25948224 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562623
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: aint even gon lie Future was sexy af in the Body Party video
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['aint', 'even', 'gon', 'lie', 'future', 'was', 'sexy', 'af', 'in', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9616232514381409
train_input: [0.2605556710562623, 0.96162325], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: lordd ciara body party video REWTARDID
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.         0.35464863 0.
  0.35464863 0.49844628]
 [0.35464863 0.35464863 0.         0.49844628 0.35464863 0.49844628
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Just watched ciara body party video, sentence2: lordd ciara body party video REWTARDID
After tokenization, sentence1: ['just', 'watched', 'ciara', 'body', 'party', 'video'], sentence2: ['lordd', 'ciara', 'body', 'party', 'video']
cosine_similarity: 0.9596601724624634
train_input: [0.5031026124151313, 0.9596602], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Anybody Notices Trinidad James in the Body Party Video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.47042643 0.         0.47042643 0.
  0.33471228 0.         0.33471228 0.47042643]
 [0.42567716 0.30287281 0.         0.42567716 0.         0.42567716
  0.30287281 0.42567716 0.30287281 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Anybody Notices Trinidad James in the Body Party Video
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['anybody', 'notices', 'trinidad', 'james', 'in', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9697770476341248
train_input: [0.30412574187549346, 0.96977705], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Body Party is amazing ciara is officially back
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.33471228 0.47042643 0.         0.33471228
  0.47042643 0.47042643]
 [0.53309782 0.37930349 0.37930349 0.         0.53309782 0.37930349
  0.         0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Body Party is amazing ciara is officially back
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['body', 'party', 'is', 'amazing', 'ciara', 'is', 'officially', 'back']
cosine_similarity: 0.9725041389465332
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Ciara s hair in her Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.49844628 0.35464863 0.35464863
  0.49844628]
 [0.4090901  0.4090901  0.57496187 0.         0.4090901  0.4090901
  0.        ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Ciara s hair in her Body Party video
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['ciara', 's', 'hair', 'in', 'her', 'body', 'party', 'video']
cosine_similarity: 0.9688742160797119
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: He caught me offfff guarddddd in this Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.47042643 0.         0.47042643 0.
  0.33471228 0.33471228 0.47042643]
 [0.33471228 0.47042643 0.         0.47042643 0.         0.47042643
  0.33471228 0.33471228 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: He caught me offfff guarddddd in this Body Party video
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['he', 'caught', 'me', 'in', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9758708477020264
train_input: [0.3360969272762574, 0.97587085], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: I am really feeling that Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.31779954 0.
  0.44665616 0.44665616]
 [0.40993715 0.         0.57615236 0.         0.40993715 0.57615236
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: I am really feeling that Body Party
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['i', 'am', 'really', 'feeling', 'that', 'body', 'party']
cosine_similarity: 0.9461842775344849
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: I just fell in love with Ciara body party song and video like hands down
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.         0.         0.37863221 0.
  0.         0.37863221 0.         0.37863221 0.53215436]
 [0.25926702 0.25926702 0.36439074 0.36439074 0.25926702 0.36439074
  0.36439074 0.25926702 0.36439074 0.25926702 0.        ]]
pairwise_similarity: [[1.         0.49083421]
 [0.49083421 1.        ]]
cosine_similarity: 0.49083421206610733
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: I just fell in love with Ciara body party song and video like hands down
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['i', 'just', 'fell', 'in', 'love', 'with', 'ciara', 'body', 'party', 'song', 'and', 'video', 'like', 'hands', 'down']
cosine_similarity: 0.9784282445907593
train_input: [0.49083421206610733, 0.97842824], train_label: 1
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Just seen Ciara s video for body party the song is better
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37863221 0.37863221 0.37863221 0.37863221 0.
  0.         0.37863221 0.53215436]
 [0.42519636 0.30253071 0.30253071 0.30253071 0.30253071 0.42519636
  0.42519636 0.30253071 0.        ]]
pairwise_similarity: [[1.         0.57273936]
 [0.57273936 1.        ]]
cosine_similarity: 0.57273935841962
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Just seen Ciara s video for body party the song is better
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['just', 'seen', 'ciara', 's', 'video', 'for', 'body', 'party', 'the', 'song', 'is', 'better']
cosine_similarity: 0.9911708831787109
train_input: [0.57273935841962, 0.9911709], train_label: 1
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: That Body Party video is stupid
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.47042643 0.33471228 0.         0.33471228
  0.47042643]
 [0.44832087 0.         0.         0.44832087 0.63009934 0.44832087
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: That Body Party video is stupid
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['that', 'body', 'party', 'video', 'is', 'stupid']
cosine_similarity: 0.9761626720428467
train_input: [0.4501755023269897, 0.9761627], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Thinking Ciara body party is my song to you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.33471228 0.         0.
  0.47042643 0.47042643]
 [0.37930349 0.37930349 0.         0.37930349 0.53309782 0.53309782
  0.         0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: Thinking Ciara body party is my song to you
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['thinking', 'ciara', 'body', 'party', 'is', 'my', 'song', 'to', 'you']
cosine_similarity: 0.9723250269889832
train_input: [0.3808726084759436, 0.972325], train_label: 0
TF_IDF_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: i swear ciara killed this body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.         0.35464863 0.
  0.35464863 0.49844628]
 [0.35464863 0.35464863 0.         0.49844628 0.35464863 0.49844628
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Just watched ciara s Body Party video, sentence2: i swear ciara killed this body party video
After tokenization, sentence1: ['just', 'watched', 'ciara', 's', 'body', 'party', 'video'], sentence2: ['i', 'swear', 'ciara', 'killed', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9783622026443481
train_input: [0.5031026124151313, 0.9783622], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Ciara Was Finna Suck Future Dick In That Body Party Video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.         0.         0.         0.5
  0.         0.5       ]
 [0.28986934 0.28986934 0.40740124 0.40740124 0.40740124 0.28986934
  0.40740124 0.28986934]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Ciara Was Finna Suck Future Dick In That Body Party Video
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['ciara', 'was', 'finna', 'suck', 'future', 'dick', 'in', 'that', 'body', 'party', 'video']
cosine_similarity: 0.9878181219100952
train_input: [0.5797386715376657, 0.9878181], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Ciara body party is my shit lOl
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.44832087 0.         0.63009934]
 [0.37930349 0.37930349 0.53309782 0.37930349 0.53309782 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Ciara body party is my shit lOl
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['ciara', 'body', 'party', 'is', 'my', 'shit', 'lol']
cosine_similarity: 0.9799149036407471
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: I didnt like Ciara Song Body Party At 1st But Now I Love it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.         0.         0.
  0.44832087 0.         0.63009934]
 [0.39166832 0.27867523 0.27867523 0.39166832 0.39166832 0.39166832
  0.27867523 0.39166832 0.        ]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: I didnt like Ciara Song Body Party At 1st But Now I Love it
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['i', 'didnt', 'like', 'ciara', 'song', 'body', 'party', 'at', 'but', 'now', 'i', 'love', 'it']
cosine_similarity: 0.9799214005470276
train_input: [0.3748077700589726, 0.9799214], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: I wouldnt mind making a video to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.63009934 0.         0.         0.44832087 0.44832087
  0.        ]
 [0.33471228 0.         0.47042643 0.47042643 0.33471228 0.33471228
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: I wouldnt mind making a video to Body Party
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['i', 'wouldnt', 'mind', 'making', 'a', 'video', 'to', 'body', 'party']
cosine_similarity: 0.9710785746574402
train_input: [0.4501755023269898, 0.9710786], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Is that Jazze Pha in the Body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.63009934 0.         0.44832087 0.         0.44832087]
 [0.37930349 0.         0.53309782 0.37930349 0.53309782 0.37930349]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Is that Jazze Pha in the Body party video
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['is', 'that', 'jazze', 'pha', 'in', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9715285301208496
train_input: [0.5101490193104813, 0.97152853], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Let me watch this body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.63009934 0.         0.44832087 0.44832087 0.        ]
 [0.37930349 0.         0.53309782 0.37930349 0.37930349 0.53309782]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Let me watch this body party video
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['let', 'me', 'watch', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9753521680831909
train_input: [0.5101490193104813, 0.97535217], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: That Ciara Body Party Joint Is DOPE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.         0.44832087 0.63009934]
 [0.37930349 0.37930349 0.53309782 0.53309782 0.37930349 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: That Ciara Body Party Joint Is DOPE
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['that', 'ciara', 'body', 'party', 'joint', 'is', 'dope']
cosine_similarity: 0.9793316721916199
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: ciara s Body Party video is on point
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.5        0.         0.5       ]
 [0.4090901  0.4090901  0.4090901  0.57496187 0.4090901 ]]
pairwise_similarity: [[1.         0.81818021]
 [0.81818021 1.        ]]
cosine_similarity: 0.8181802073667197
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: ciara s Body Party video is on point
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['ciara', 's', 'body', 'party', 'video', 'is', 'on', 'point']
cosine_similarity: 0.9781243205070496
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: did u see the body party video yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.63009934 0.         0.44832087 0.44832087]
 [0.44832087 0.         0.63009934 0.44832087 0.44832087]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: did u see the body party video yet
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['did', 'u', 'see', 'the', 'body', 'party', 'video', 'yet']
cosine_similarity: 0.9846331477165222
train_input: [0.6029748160380572, 0.98463315], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: lordd ciara body party video REWTARDID
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.         0.5        0.         0.5       ]
 [0.35464863 0.35464863 0.49844628 0.35464863 0.49844628 0.35464863]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: lordd ciara body party video REWTARDID
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['lordd', 'ciara', 'body', 'party', 'video']
cosine_similarity: 0.9569122195243835
train_input: [0.7092972666062737, 0.9569122], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Ciara Body Party be having you ready to work a nigga
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.30287281 0.         0.42567716 0.
  0.30287281 0.         0.42567716 0.42567716 0.        ]
 [0.30287281 0.         0.30287281 0.42567716 0.         0.42567716
  0.30287281 0.42567716 0.         0.         0.42567716]]
pairwise_similarity: [[1.         0.27519581]
 [0.27519581 1.        ]]
cosine_similarity: 0.275195812175023
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Ciara Body Party be having you ready to work a nigga
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['ciara', 'body', 'party', 'be', 'having', 'you', 'ready', 'to', 'work', 'a', 'nigga']
cosine_similarity: 0.9846151471138
train_input: [0.275195812175023, 0.98461515], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Ciara lookin right in that Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.3174044  0.         0.44610081 0.3174044
  0.         0.44610081 0.3174044 ]
 [0.35464863 0.         0.35464863 0.49844628 0.         0.35464863
  0.49844628 0.         0.35464863]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Ciara lookin right in that Body Party video
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['ciara', 'lookin', 'right', 'in', 'that', 'body', 'party', 'video']
cosine_similarity: 0.9751959443092346
train_input: [0.4502681446556265, 0.97519594], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Ciara took over twitter with this body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.3174044  0.44610081 0.3174044  0.44610081
  0.         0.         0.3174044 ]
 [0.35464863 0.         0.35464863 0.         0.35464863 0.
  0.49844628 0.49844628 0.35464863]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Ciara took over twitter with this body party video
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['ciara', 'took', 'over', 'twitter', 'with', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9769166707992554
train_input: [0.4502681446556265, 0.9769167], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Ciara video to body party tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.3174044  0.44610081 0.3174044  0.44610081
  0.         0.3174044 ]
 [0.4090901  0.         0.4090901  0.         0.4090901  0.
  0.57496187 0.4090901 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Ciara video to body party tho
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['ciara', 'video', 'to', 'body', 'party', 'tho']
cosine_similarity: 0.9819990396499634
train_input: [0.5193879933129156, 0.98199904], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: I slick wanna do something w oomf to ciara x body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.30287281 0.42567716 0.         0.30287281
  0.         0.42567716 0.42567716 0.        ]
 [0.33471228 0.         0.33471228 0.         0.47042643 0.33471228
  0.47042643 0.         0.         0.47042643]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: I slick wanna do something w oomf to ciara x body party
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['i', 'slick', 'wanna', 'do', 'something', 'w', 'oomf', 'to', 'ciara', 'x', 'body', 'party']
cosine_similarity: 0.9695102572441101
train_input: [0.3041257418754935, 0.96951026], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Im FINALLY watching Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.42567716 0.         0.         0.42567716
  0.30287281 0.42567716 0.30287281 0.        ]
 [0.33471228 0.         0.         0.47042643 0.47042643 0.
  0.33471228 0.         0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Im FINALLY watching Body Party video
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['im', 'finally', 'watching', 'body', 'party', 'video']
cosine_similarity: 0.9578272700309753
train_input: [0.3041257418754935, 0.95782727], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Im feeling Body Party s video Ciara is nice
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.3174044  0.         0.         0.44610081
  0.         0.3174044  0.44610081 0.3174044 ]
 [0.3174044  0.         0.3174044  0.44610081 0.44610081 0.
  0.44610081 0.3174044  0.         0.3174044 ]]
pairwise_similarity: [[1.         0.40298221]
 [0.40298221 1.        ]]
cosine_similarity: 0.40298220897396103
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: Im feeling Body Party s video Ciara is nice
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['im', 'feeling', 'body', 'party', 's', 'video', 'ciara', 'is', 'nice']
cosine_similarity: 0.9654950499534607
train_input: [0.40298220897396103, 0.96549505], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: That Body Party video is like that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.42567716 0.         0.42567716 0.30287281
  0.42567716 0.30287281]
 [0.44832087 0.         0.         0.63009934 0.         0.44832087
  0.         0.44832087]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: That Body Party video is like that
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['that', 'body', 'party', 'video', 'is', 'like', 'that']
cosine_similarity: 0.9716235995292664
train_input: [0.4073526042885674, 0.9716236], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: the body party music video is such a good vibe
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.42567716 0.         0.42567716 0.
  0.30287281 0.42567716 0.         0.30287281]
 [0.33471228 0.         0.         0.47042643 0.         0.47042643
  0.33471228 0.         0.47042643 0.33471228]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: the body party music video is such a good vibe
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['the', 'body', 'party', 'music', 'video', 'is', 'such', 'a', 'good', 'vibe']
cosine_similarity: 0.977619469165802
train_input: [0.3041257418754935, 0.97761947], train_label: 0
TF_IDF_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: wait to I learn how to do that body party dance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.4078241  0.         0.         0.4078241
  0.29017021 0.4078241  0.4078241  0.        ]
 [0.35520009 0.         0.         0.49922133 0.49922133 0.
  0.35520009 0.         0.         0.49922133]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Thinking bout making a video to Ciara Body Party, sentence2: wait to I learn how to do that body party dance
After tokenization, sentence1: ['thinking', 'bout', 'making', 'a', 'video', 'to', 'ciara', 'body', 'party'], sentence2: ['wait', 'to', 'i', 'learn', 'how', 'to', 'do', 'that', 'body', 'party', 'dance']
cosine_similarity: 0.9694765210151672
train_input: [0.20613696606828605, 0.9694765], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Ciara Body Party is one of her best videos honestly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.25948224 0.25948224 0.36469323 0.         0.36469323
  0.25948224 0.72938646 0.        ]
 [0.47042643 0.33471228 0.33471228 0.         0.47042643 0.
  0.33471228 0.         0.47042643]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Ciara Body Party is one of her best videos honestly
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['ciara', 'body', 'party', 'is', 'one', 'of', 'her', 'best', 'videos', 'honestly']
cosine_similarity: 0.9766594171524048
train_input: [0.2605556710562624, 0.9766594], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Ciara did her thing In body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30218978 0.30218978 0.42471719 0.         0.42471719 0.30218978
  0.         0.60437955]
 [0.35464863 0.35464863 0.         0.49844628 0.         0.35464863
  0.49844628 0.35464863]]
pairwise_similarity: [[1.         0.53585595]
 [0.53585595 1.        ]]
cosine_similarity: 0.5358559548726151
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Ciara did her thing In body party video
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['ciara', 'did', 'her', 'thing', 'in', 'body', 'party', 'video']
cosine_similarity: 0.9804132580757141
train_input: [0.5358559548726151, 0.98041326], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Ciara in that music video body party making bitches go lesbo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30218978 0.30218978 0.42471719 0.         0.42471719
  0.         0.         0.30218978 0.60437955]
 [0.40740124 0.28986934 0.28986934 0.         0.40740124 0.
  0.40740124 0.40740124 0.28986934 0.28986934]]
pairwise_similarity: [[1.         0.43797775]
 [0.43797775 1.        ]]
cosine_similarity: 0.43797774789091437
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Ciara in that music video body party making bitches go lesbo
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['ciara', 'in', 'that', 'music', 'video', 'body', 'party', 'making', 'bitches', 'go', 'lesbo']
cosine_similarity: 0.981000542640686
train_input: [0.43797774789091437, 0.98100054], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Ciara video for Body Party with her bf Future is actually kinda cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.30218978 0.30218978 0.         0.42471719
  0.         0.         0.42471719 0.30218978 0.60437955]
 [0.37729199 0.37729199 0.26844636 0.26844636 0.37729199 0.
  0.37729199 0.37729199 0.         0.26844636 0.26844636]]
pairwise_similarity: [[1.         0.40560872]
 [0.40560872 1.        ]]
cosine_similarity: 0.40560872051554336
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Ciara video for Body Party with her bf Future is actually kinda cute
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['ciara', 'video', 'for', 'body', 'party', 'with', 'her', 'bf', 'future', 'is', 'actually', 'kinda', 'cute']
cosine_similarity: 0.9793245196342468
train_input: [0.40560872051554336, 0.9793245], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Future look kinda good in the body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.25136004 0.35327777 0.35327777 0.         0.         0.
  0.35327777 0.         0.25136004 0.70655553]
 [0.31779954 0.         0.         0.44665616 0.44665616 0.44665616
  0.         0.44665616 0.31779954 0.        ]]
pairwise_similarity: [[1.         0.15976421]
 [0.15976421 1.        ]]
cosine_similarity: 0.1597642092414444
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Future look kinda good in the body party
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['future', 'look', 'kinda', 'good', 'in', 'the', 'body', 'party']
cosine_similarity: 0.9745948314666748
train_input: [0.1597642092414444, 0.97459483], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: I cant find the video of body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.2895694  0.40697968 0.40697968 0.40697968 0.2895694  0.57913879]
 [0.57735027 0.         0.         0.         0.57735027 0.57735027]]
pairwise_similarity: [[1.         0.66873188]
 [0.66873188 1.        ]]
cosine_similarity: 0.6687318761258383
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: I cant find the video of body party
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['i', 'cant', 'find', 'the', 'video', 'of', 'body', 'party']
cosine_similarity: 0.9844472408294678
train_input: [0.6687318761258383, 0.98444724], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: I dont care for Ciara but I love her song Body Party and the video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30218978 0.         0.30218978 0.42471719 0.         0.42471719
  0.         0.30218978 0.         0.60437955]
 [0.28986934 0.40740124 0.28986934 0.         0.40740124 0.
  0.40740124 0.28986934 0.40740124 0.28986934]]
pairwise_similarity: [[1.         0.43797775]
 [0.43797775 1.        ]]
cosine_similarity: 0.43797774789091437
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: I dont care for Ciara but I love her song Body Party and the video
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['i', 'dont', 'care', 'for', 'ciara', 'but', 'i', 'love', 'her', 'song', 'body', 'party', 'and', 'the', 'video']
cosine_similarity: 0.982606828212738
train_input: [0.43797774789091437, 0.9826068], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: I love Ciara body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30218978 0.30218978 0.42471719 0.42471719 0.         0.30218978
  0.60437955]
 [0.4090901  0.4090901  0.         0.         0.57496187 0.4090901
  0.4090901 ]]
pairwise_similarity: [[1.         0.61811423]
 [0.61811423 1.        ]]
cosine_similarity: 0.6181142335061833
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: I love Ciara body party video
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['i', 'love', 'ciara', 'body', 'party', 'video']
cosine_similarity: 0.9873434901237488
train_input: [0.6181142335061833, 0.9873435], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Seeing Ciara s Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30218978 0.30218978 0.42471719 0.42471719 0.30218978 0.
  0.60437955]
 [0.4090901  0.4090901  0.         0.         0.4090901  0.57496187
  0.4090901 ]]
pairwise_similarity: [[1.         0.61811423]
 [0.61811423 1.        ]]
cosine_similarity: 0.6181142335061833
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Seeing Ciara s Body Party video
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['seeing', 'ciara', 's', 'body', 'party', 'video']
cosine_similarity: 0.9923925995826721
train_input: [0.6181142335061833, 0.9923926], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Swear ciara kilt that body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30218978 0.30218978 0.42471719 0.         0.42471719 0.30218978
  0.         0.60437955]
 [0.35464863 0.35464863 0.         0.49844628 0.         0.35464863
  0.49844628 0.35464863]]
pairwise_similarity: [[1.         0.53585595]
 [0.53585595 1.        ]]
cosine_similarity: 0.5358559548726151
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is like the Dance For You video, sentence2: Swear ciara kilt that body party video
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'like', 'the', 'dance', 'for', 'you', 'video'], sentence2: ['swear', 'ciara', 'kilt', 'that', 'body', 'party', 'video']
cosine_similarity: 0.9632529616355896
train_input: [0.5358559548726151, 0.96325296], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Body Party is amazing ciara is officially back
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.33471228 0.47042643 0.         0.33471228
  0.47042643 0.47042643]
 [0.53309782 0.37930349 0.37930349 0.         0.53309782 0.37930349
  0.         0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Body Party is amazing ciara is officially back
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['body', 'party', 'is', 'amazing', 'ciara', 'is', 'officially', 'back']
cosine_similarity: 0.977594256401062
train_input: [0.3808726084759436, 0.97759426], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Ciara body party is my shit lOl
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.         0.33471228 0.47042643
  0.         0.47042643]
 [0.37930349 0.37930349 0.         0.53309782 0.37930349 0.
  0.53309782 0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Ciara body party is my shit lOl
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['ciara', 'body', 'party', 'is', 'my', 'shit', 'lol']
cosine_similarity: 0.9587639570236206
train_input: [0.3808726084759436, 0.95876396], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Ciara s hair is cute in that body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.49844628 0.         0.35464863
  0.49844628 0.35464863]
 [0.35464863 0.35464863 0.49844628 0.         0.49844628 0.35464863
  0.         0.35464863]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Ciara s hair is cute in that body party video
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['ciara', 's', 'hair', 'is', 'cute', 'in', 'that', 'body', 'party', 'video']
cosine_similarity: 0.9724447131156921
train_input: [0.5031026124151313, 0.9724447], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Future look SO GOOD in Ciara s body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.         0.         0.
  0.35464863 0.49844628 0.35464863]
 [0.3174044  0.3174044  0.         0.44610081 0.44610081 0.44610081
  0.3174044  0.         0.3174044 ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Future look SO GOOD in Ciara s body party video
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['future', 'look', 'so', 'good', 'in', 'ciara', 's', 'body', 'party', 'video']
cosine_similarity: 0.9726170897483826
train_input: [0.4502681446556265, 0.9726171], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Hey girl that body party video is the ish
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.47042643 0.         0.         0.
  0.33471228 0.47042643 0.33471228]
 [0.33471228 0.         0.         0.47042643 0.47042643 0.47042643
  0.33471228 0.         0.33471228]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Hey girl that body party video is the ish
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['hey', 'girl', 'that', 'body', 'party', 'video', 'is', 'the', 'ish']
cosine_similarity: 0.9782469272613525
train_input: [0.3360969272762574, 0.9782469], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: I absolutely love the video to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.47042643 0.47042643 0.         0.33471228
  0.47042643 0.33471228]
 [0.53309782 0.37930349 0.         0.         0.53309782 0.37930349
  0.         0.37930349]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: I absolutely love the video to Body Party
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['i', 'absolutely', 'love', 'the', 'video', 'to', 'body', 'party']
cosine_similarity: 0.9787760972976685
train_input: [0.3808726084759436, 0.9787761], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: I slick wanna do something w oomf to ciara x body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.         0.33471228 0.47042643
  0.         0.47042643 0.        ]
 [0.33471228 0.33471228 0.         0.47042643 0.33471228 0.
  0.47042643 0.         0.47042643]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: I slick wanna do something w oomf to ciara x body party
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['i', 'slick', 'wanna', 'do', 'something', 'w', 'oomf', 'to', 'ciara', 'x', 'body', 'party']
cosine_similarity: 0.9346864223480225
train_input: [0.3360969272762574, 0.9346864], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Swear ciara kilt that body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.         0.35464863 0.49844628
  0.         0.35464863]
 [0.35464863 0.35464863 0.         0.49844628 0.35464863 0.
  0.49844628 0.35464863]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: Swear ciara kilt that body party video
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['swear', 'ciara', 'kilt', 'that', 'body', 'party', 'video']
cosine_similarity: 0.9705259203910828
train_input: [0.5031026124151313, 0.9705259], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: You see her sexy body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.47042643 0.33471228 0.47042643 0.
  0.33471228]
 [0.44832087 0.         0.         0.44832087 0.         0.63009934
  0.44832087]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: You see her sexy body party video
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['you', 'see', 'her', 'sexy', 'body', 'party', 'video']
cosine_similarity: 0.9653212428092957
train_input: [0.4501755023269897, 0.96532124], train_label: 0
TF_IDF_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: ciara s Body Party video is on point
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.         0.49844628
  0.35464863]
 [0.4090901  0.4090901  0.         0.4090901  0.57496187 0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Finally saw the Ciara body party video, sentence2: ciara s Body Party video is on point
After tokenization, sentence1: ['finally', 'saw', 'the', 'ciara', 'body', 'party', 'video'], sentence2: ['ciara', 's', 'body', 'party', 'video', 'is', 'on', 'point']
cosine_similarity: 0.9580436944961548
train_input: [0.5803329846765685, 0.9580437], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Body party video isnt even that serious
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.42567716 0.42567716 0.         0.30287281
  0.42567716 0.30287281]
 [0.44832087 0.         0.         0.         0.63009934 0.44832087
  0.         0.44832087]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Body party video isnt even that serious
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['body', 'party', 'video', 'isnt', 'even', 'that', 'serious']
cosine_similarity: 0.971323549747467
train_input: [0.4073526042885674, 0.97132355], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Ciara in the Body Party video is nearly perfection
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.44610081 0.44610081 0.         0.3174044
  0.         0.44610081 0.3174044 ]
 [0.35464863 0.35464863 0.         0.         0.49844628 0.35464863
  0.49844628 0.         0.35464863]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Ciara in the Body Party video is nearly perfection
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['ciara', 'in', 'the', 'body', 'party', 'video', 'is', 'nearly', 'perfection']
cosine_similarity: 0.9795841574668884
train_input: [0.4502681446556265, 0.97958416], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Ciara on this music video body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.44610081 0.44610081 0.         0.3174044
  0.44610081 0.3174044 ]
 [0.4090901  0.4090901  0.         0.         0.57496187 0.4090901
  0.         0.4090901 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Ciara on this music video body party
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['ciara', 'on', 'this', 'music', 'video', 'body', 'party']
cosine_similarity: 0.9761409759521484
train_input: [0.5193879933129156, 0.976141], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: I Love Watching Ciara Music Video to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.44610081 0.44610081 0.         0.
  0.3174044  0.44610081 0.3174044  0.        ]
 [0.3174044  0.3174044  0.         0.         0.44610081 0.44610081
  0.3174044  0.         0.3174044  0.44610081]]
pairwise_similarity: [[1.         0.40298221]
 [0.40298221 1.        ]]
cosine_similarity: 0.40298220897396103
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: I Love Watching Ciara Music Video to Body Party
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['i', 'love', 'watching', 'ciara', 'music', 'video', 'to', 'body', 'party']
cosine_similarity: 0.9777562618255615
train_input: [0.40298220897396103, 0.97775626], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: I can lightweight vibe to Body party tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.4078241  0.4078241  0.         0.29017021
  0.4078241  0.         0.         0.4078241 ]
 [0.35520009 0.         0.         0.         0.49922133 0.35520009
  0.         0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: I can lightweight vibe to Body party tho
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['i', 'can', 'lightweight', 'vibe', 'to', 'body', 'party', 'tho']
cosine_similarity: 0.9629597067832947
train_input: [0.20613696606828605, 0.9629597], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: I love Ciara body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.44610081 0.44610081 0.         0.3174044
  0.44610081 0.3174044 ]
 [0.4090901  0.4090901  0.         0.         0.57496187 0.4090901
  0.         0.4090901 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: I love Ciara body party video
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['i', 'love', 'ciara', 'body', 'party', 'video']
cosine_similarity: 0.9738295078277588
train_input: [0.5193879933129156, 0.9738295], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Stevie J in the Body Party video lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.42567716 0.42567716 0.         0.30287281
  0.         0.42567716 0.30287281]
 [0.37930349 0.         0.         0.         0.53309782 0.37930349
  0.53309782 0.         0.37930349]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Stevie J in the Body Party video lol
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['stevie', 'j', 'in', 'the', 'body', 'party', 'video', 'lol']
cosine_similarity: 0.9842159748077393
train_input: [0.34464214103805474, 0.984216], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Trinidad james making an appearance in ciara s new video body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.3174044  0.3174044  0.44610081 0.44610081 0.
  0.         0.         0.3174044  0.44610081 0.         0.3174044 ]
 [0.37729199 0.26844636 0.26844636 0.         0.         0.37729199
  0.37729199 0.37729199 0.26844636 0.         0.37729199 0.26844636]]
pairwise_similarity: [[1.         0.34082422]
 [0.34082422 1.        ]]
cosine_similarity: 0.3408242166238352
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: Trinidad james making an appearance in ciara s new video body party
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['trinidad', 'james', 'making', 'an', 'appearance', 'in', 'ciara', 's', 'new', 'video', 'body', 'party']
cosine_similarity: 0.9783090949058533
train_input: [0.3408242166238352, 0.9783091], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: body party by Ciara is my favorite song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.42567716 0.42567716 0.         0.30287281
  0.         0.42567716 0.42567716]
 [0.37930349 0.37930349 0.         0.         0.53309782 0.37930349
  0.53309782 0.         0.        ]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: body party by Ciara is my favorite song
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['body', 'party', 'by', 'ciara', 'is', 'my', 'favorite', 'song']
cosine_similarity: 0.9834098219871521
TF_IDF_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: ciara reallly did this body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.46977774 0.33425073 0.33425073 0.
  0.46977774 0.33425073]
 [0.37863221 0.37863221 0.         0.37863221 0.37863221 0.53215436
  0.         0.37863221]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: Ciara did the damn thang in the body party video, sentence2: ciara reallly did this body party video
After tokenization, sentence1: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video'], sentence2: ['ciara', 'reallly', 'did', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9750231504440308
train_input: [0.6327904583679949, 0.97502315], train_label: 1
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: Ciara did the damn thang in the body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.63009934 0.44832087
  0.         0.44832087]
 [0.30287281 0.42567716 0.42567716 0.42567716 0.         0.30287281
  0.42567716 0.30287281]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: Ciara did the damn thang in the body party video
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['ciara', 'did', 'the', 'damn', 'thang', 'in', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9865216016769409
train_input: [0.4073526042885674, 0.9865216], train_label: 1
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: Ciara x Body Party ohhhhhh ok I see youuuuuu C
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.         0.40993715
  0.57615236 0.        ]
 [0.31779954 0.44665616 0.         0.44665616 0.44665616 0.31779954
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: Ciara x Body Party ohhhhhh ok I see youuuuuu C
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['ciara', 'x', 'body', 'party', 'ok', 'i', 'see', 'c']
cosine_similarity: 0.95201176404953
train_input: [0.2605556710562624, 0.95201176], train_label: 0
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: I feel like Ciara is teasing me every time I see the Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.63009934 0.44832087
  0.         0.         0.44832087]
 [0.27867523 0.39166832 0.39166832 0.39166832 0.         0.27867523
  0.39166832 0.39166832 0.27867523]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: I feel like Ciara is teasing me every time I see the Body Party video
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['i', 'feel', 'like', 'ciara', 'is', 'teasing', 'me', 'every', 'time', 'i', 'see', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9886409044265747
train_input: [0.3748077700589726, 0.9886409], train_label: 0
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: I like that body party joint from Ciara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.57615236 0.40993715
  0.57615236]
 [0.35520009 0.49922133 0.49922133 0.49922133 0.         0.35520009
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: I like that body party joint from Ciara
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['i', 'like', 'that', 'body', 'party', 'joint', 'from', 'ciara']
cosine_similarity: 0.9852097034454346
train_input: [0.29121941856368966, 0.9852097], train_label: 1
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: Im FINALLY watching Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.63009934 0.44832087 0.44832087
  0.        ]
 [0.33471228 0.47042643 0.47042643 0.         0.33471228 0.33471228
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: Im FINALLY watching Body Party video
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['im', 'finally', 'watching', 'body', 'party', 'video']
cosine_similarity: 0.9783607125282288
train_input: [0.4501755023269898, 0.9783607], train_label: 0
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: Just watched ciara body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.63009934 0.44832087 0.44832087
  0.        ]
 [0.33471228 0.47042643 0.47042643 0.         0.33471228 0.33471228
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: Just watched ciara body party video
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['just', 'watched', 'ciara', 'body', 'party', 'video']
cosine_similarity: 0.98468416929245
train_input: [0.4501755023269898, 0.98468417], train_label: 0
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: Just watched the music video to Body Party by Ciara and she killed it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.63009934 0.
  0.44832087 0.44832087 0.        ]
 [0.27867523 0.39166832 0.39166832 0.39166832 0.         0.39166832
  0.27867523 0.27867523 0.39166832]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: Just watched the music video to Body Party by Ciara and she killed it
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['just', 'watched', 'the', 'music', 'video', 'to', 'body', 'party', 'by', 'ciara', 'and', 'she', 'killed', 'it']
cosine_similarity: 0.9902281761169434
train_input: [0.3748077700589726, 0.9902282], train_label: 1
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: OHHHH I LOVE CIARA NEW SONG BODY PARTY THE VIDEO DECENT
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.63009934 0.         0.
  0.         0.44832087 0.         0.44832087]
 [0.25948224 0.36469323 0.36469323 0.         0.36469323 0.36469323
  0.36469323 0.25948224 0.36469323 0.25948224]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: OHHHH I LOVE CIARA NEW SONG BODY PARTY THE VIDEO DECENT
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['i', 'love', 'ciara', 'new', 'song', 'body', 'party', 'the', 'video', 'decent']
cosine_similarity: 0.9880297780036926
train_input: [0.3489939079552687, 0.9880298], train_label: 1
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: Think I will make choreography to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.40993715 0.
  0.57615236]
 [0.35520009 0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: Think I will make choreography to Body Party
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['think', 'i', 'will', 'make', 'choreography', 'to', 'body', 'party']
cosine_similarity: 0.9726769924163818
train_input: [0.29121941856368966, 0.972677], train_label: 0
TF_IDF_cosine_similarity: sentence1: I liked that video body party, sentence2: that damn body party song is stuck in my head
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.40993715 0.
  0.         0.57615236]
 [0.31779954 0.44665616 0.44665616 0.         0.31779954 0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I liked that video body party, sentence2: that damn body party song is stuck in my head
After tokenization, sentence1: ['i', 'liked', 'that', 'video', 'body', 'party'], sentence2: ['that', 'damn', 'body', 'party', 'song', 'is', 'stuck', 'in', 'my', 'head']
cosine_similarity: 0.9786736965179443
train_input: [0.2605556710562624, 0.9786737], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Body Party by Ciara is my new shit I cant stop listening to it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70819948 0.35409974 0.         0.         0.35409974 0.
  0.         0.49767483]
 [0.30287281 0.30287281 0.42567716 0.42567716 0.30287281 0.42567716
  0.42567716 0.        ]]
pairwise_similarity: [[1.         0.42898873]
 [0.42898873 1.        ]]
cosine_similarity: 0.42898873118158365
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Body Party by Ciara is my new shit I cant stop listening to it
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['body', 'party', 'by', 'ciara', 'is', 'my', 'new', 'shit', 'i', 'cant', 'stop', 'listening', 'to', 'it']
cosine_similarity: 0.9696975350379944
train_input: [0.42898873118158365, 0.96969754], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Ciara Body Party video is sooooo cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.75592895 0.37796447 0.         0.37796447 0.         0.37796447]
 [0.35464863 0.35464863 0.49844628 0.35464863 0.49844628 0.35464863]]
pairwise_similarity: [[1.         0.67022292]
 [0.67022292 1.        ]]
cosine_similarity: 0.6702229189493139
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Ciara Body Party video is sooooo cute
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['ciara', 'body', 'party', 'video', 'is', 'sooooo', 'cute']
cosine_similarity: 0.9451638460159302
train_input: [0.6702229189493139, 0.94516385], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Ciara Was Finna Suck Future Dick In That Body Party Video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.75592895 0.37796447 0.         0.         0.         0.37796447
  0.         0.37796447]
 [0.28986934 0.28986934 0.40740124 0.40740124 0.40740124 0.28986934
  0.40740124 0.28986934]]
pairwise_similarity: [[1.         0.54780155]
 [0.54780155 1.        ]]
cosine_similarity: 0.5478015536770082
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Ciara Was Finna Suck Future Dick In That Body Party Video
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['ciara', 'was', 'finna', 'suck', 'future', 'dick', 'in', 'that', 'body', 'party', 'video']
cosine_similarity: 0.969772219657898
train_input: [0.5478015536770082, 0.9697722], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I wana see the Body Party Video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70819948 0.49767483 0.35409974 0.35409974 0.        ]
 [0.44832087 0.         0.44832087 0.44832087 0.63009934]]
pairwise_similarity: [[1.         0.63500122]
 [0.63500122 1.        ]]
cosine_similarity: 0.6350012214071283
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I wana see the Body Party Video
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['i', 'wana', 'see', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9702764749526978
train_input: [0.6350012214071283, 0.9702765], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: So I missed the Body Party video AGAIN
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70819948 0.49767483 0.         0.35409974 0.35409974]
 [0.44832087 0.         0.63009934 0.44832087 0.44832087]]
pairwise_similarity: [[1.         0.63500122]
 [0.63500122 1.        ]]
cosine_similarity: 0.6350012214071283
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: So I missed the Body Party video AGAIN
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['so', 'i', 'missed', 'the', 'body', 'party', 'video', 'again']
cosine_similarity: 0.9693654775619507
train_input: [0.6350012214071283, 0.9693655], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Stevie J Joseline are in Ciara s Body Party video in case yall didnt notice
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.75592895 0.         0.37796447 0.         0.         0.
  0.37796447 0.         0.37796447 0.        ]
 [0.25116439 0.35300279 0.25116439 0.35300279 0.35300279 0.35300279
  0.25116439 0.35300279 0.25116439 0.35300279]]
pairwise_similarity: [[1.         0.47465608]
 [0.47465608 1.        ]]
cosine_similarity: 0.4746560798485875
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Stevie J Joseline are in Ciara s Body Party video in case yall didnt notice
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['stevie', 'j', 'joseline', 'are', 'in', 'ciara', 's', 'body', 'party', 'video', 'in', 'case', 'yall', 'didnt', 'notice']
cosine_similarity: 0.9650693535804749
train_input: [0.4746560798485875, 0.96506935], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: That Body Party is that work
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.66850146 0.46977774 0.33425073 0.46977774 0.        ]
 [0.50154891 0.         0.50154891 0.         0.70490949]]
pairwise_similarity: [[1.         0.50292927]
 [0.50292927 1.        ]]
cosine_similarity: 0.5029292651136352
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: That Body Party is that work
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['that', 'body', 'party', 'is', 'that', 'work']
cosine_similarity: 0.9651589393615723
train_input: [0.5029292651136352, 0.96515894], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Think I will make choreography to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.66850146 0.         0.46977774 0.         0.33425073 0.
  0.46977774]
 [0.35520009 0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.35617766]
 [0.35617766 1.        ]]
cosine_similarity: 0.3561776636856882
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Think I will make choreography to Body Party
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['think', 'i', 'will', 'make', 'choreography', 'to', 'body', 'party']
cosine_similarity: 0.9558566808700562
train_input: [0.3561776636856882, 0.9558567], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: i listen to body party every day thts my shit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.66850146 0.46977774 0.         0.         0.33425073 0.
  0.         0.46977774]
 [0.31779954 0.         0.44665616 0.44665616 0.31779954 0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.31867418]
 [0.31867418 1.        ]]
cosine_similarity: 0.3186741826303729
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: i listen to body party every day thts my shit
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['i', 'listen', 'to', 'body', 'party', 'every', 'day', 'thts', 'my', 'shit']
cosine_similarity: 0.9484670758247375
train_input: [0.3186741826303729, 0.9484671], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: wait to I learn how to do that body party dance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.66850146 0.46977774 0.         0.         0.33425073 0.46977774
  0.        ]
 [0.35520009 0.         0.49922133 0.49922133 0.35520009 0.
  0.49922133]]
pairwise_similarity: [[1.         0.35617766]
 [0.35617766 1.        ]]
cosine_similarity: 0.3561776636856882
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: wait to I learn how to do that body party dance
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['wait', 'to', 'i', 'learn', 'how', 'to', 'do', 'that', 'body', 'party', 'dance']
cosine_similarity: 0.9422169327735901
train_input: [0.3561776636856882, 0.94221693], train_label: 0
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Ci Ci slick showing out in that body party video 2 cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.49922133 0.35520009
  0.         0.         0.49922133 0.        ]
 [0.23700504 0.66620463 0.         0.33310232 0.         0.23700504
  0.33310232 0.33310232 0.         0.33310232]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Ci Ci slick showing out in that body party video 2 cute
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['ci', 'ci', 'slick', 'showing', 'out', 'in', 'that', 'body', 'party', 'video', 'cute']
cosine_similarity: 0.9620929956436157
train_input: [0.16836842163679844, 0.962093], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Ciara Future showed out for this Body Party video lol I love it bro
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.4090901  0.         0.         0.4090901
  0.4090901  0.         0.57496187 0.        ]
 [0.26844636 0.37729199 0.26844636 0.37729199 0.37729199 0.26844636
  0.26844636 0.37729199 0.         0.37729199]]
pairwise_similarity: [[1.         0.43927499]
 [0.43927499 1.        ]]
cosine_similarity: 0.43927499031635536
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Ciara Future showed out for this Body Party video lol I love it bro
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['ciara', 'future', 'showed', 'out', 'for', 'this', 'body', 'party', 'video', 'lol', 'i', 'love', 'it', 'bro']
cosine_similarity: 0.980700671672821
train_input: [0.43927499031635536, 0.9807007], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Ciara x Body Party video Her Future is sooo cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.         0.53309782 0.37930349
  0.53309782 0.         0.        ]
 [0.30287281 0.30287281 0.42567716 0.42567716 0.         0.30287281
  0.         0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Ciara x Body Party video Her Future is sooo cute
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['ciara', 'x', 'body', 'party', 'video', 'her', 'future', 'is', 'sooo', 'cute']
cosine_similarity: 0.9809433221817017
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Is this Body Party video pooty
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.         0.49922133
  0.        ]
 [0.40993715 0.         0.         0.40993715 0.57615236 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Is this Body Party video pooty
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['is', 'this', 'body', 'party', 'video', 'pooty']
cosine_similarity: 0.9625104665756226
train_input: [0.29121941856368966, 0.96251047], train_label: 0
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Just seen Ciara s video for body party the song is better
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.4090901  0.         0.57496187 0.4090901
  0.         0.4090901  0.        ]
 [0.40740124 0.28986934 0.28986934 0.40740124 0.         0.28986934
  0.40740124 0.28986934 0.40740124]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Just seen Ciara s video for body party the song is better
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['just', 'seen', 'ciara', 's', 'video', 'for', 'body', 'party', 'the', 'song', 'is', 'better']
cosine_similarity: 0.9763368368148804
train_input: [0.4743307064971939, 0.97633684], train_label: 0
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Just watched the video for Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.         0.        ]
 [0.35520009 0.         0.49922133 0.         0.35520009 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Just watched the video for Body Party
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['just', 'watched', 'the', 'video', 'for', 'body', 'party']
cosine_similarity: 0.9602056741714478
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Say yes to the body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.         0.49922133
  0.         0.        ]
 [0.35520009 0.         0.         0.35520009 0.49922133 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: Say yes to the body party video
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['say', 'yes', 'to', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9631877541542053
train_input: [0.2523342014336961, 0.96318775], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: So I know Im late but Im just seeing the body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.         0.
  0.49922133 0.35520009 0.         0.49922133 0.        ]
 [0.2248583  0.         0.632061   0.3160305  0.3160305  0.3160305
  0.         0.2248583  0.3160305  0.         0.3160305 ]]
pairwise_similarity: [[1.         0.15973938]
 [0.15973938 1.        ]]
cosine_similarity: 0.15973937686327605
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: So I know Im late but Im just seeing the body party video
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['so', 'i', 'know', 'im', 'late', 'but', 'im', 'just', 'seeing', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9598631858825684
train_input: [0.15973937686327605, 0.9598632], train_label: 0
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: The dances that Ciara were doing in Body Party though
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.         0.53309782 0.37930349
  0.53309782]
 [0.37930349 0.37930349 0.53309782 0.53309782 0.         0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: The dances that Ciara were doing in Body Party though
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['the', 'dances', 'that', 'ciara', 'were', 'doing', 'in', 'body', 'party', 'though']
cosine_similarity: 0.9626138806343079
train_input: [0.43161341897075145, 0.9626139], train_label: 0
TF_IDF_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: that damn body party song is stuck in my head
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.         0.         0.53309782 0.37930349
  0.37930349 0.        ]
 [0.33471228 0.         0.47042643 0.47042643 0.         0.33471228
  0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: I love Ciara song Body Party, sentence2: that damn body party song is stuck in my head
After tokenization, sentence1: ['i', 'love', 'ciara', 'song', 'body', 'party'], sentence2: ['that', 'damn', 'body', 'party', 'song', 'is', 'stuck', 'in', 'my', 'head']
cosine_similarity: 0.9636819958686829
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: And that Body Party video just further confirmed that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.37930349 0.53309782 0.         0.         0.37930349
  0.37930349]
 [0.         0.37930349 0.         0.53309782 0.53309782 0.37930349
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: And that Body Party video just further confirmed that
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['and', 'that', 'body', 'party', 'video', 'just', 'further', 'confirmed', 'that']
cosine_similarity: 0.9517449140548706
train_input: [0.43161341897075145, 0.9517449], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: Body Party by Ciara is my new favorite song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.37930349 0.37930349 0.         0.         0.37930349
  0.         0.53309782]
 [0.         0.33471228 0.33471228 0.47042643 0.47042643 0.33471228
  0.47042643 0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: Body Party by Ciara is my new favorite song
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['body', 'party', 'by', 'ciara', 'is', 'my', 'new', 'favorite', 'song']
cosine_similarity: 0.9746823906898499
train_input: [0.38087260847594373, 0.9746824], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: Ciara in body party is too damn sexy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.37930349 0.37930349 0.         0.37930349 0.
  0.53309782]
 [0.         0.37930349 0.37930349 0.53309782 0.37930349 0.53309782
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: Ciara in body party is too damn sexy
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['ciara', 'in', 'body', 'party', 'is', 'too', 'damn', 'sexy']
cosine_similarity: 0.9747645258903503
train_input: [0.43161341897075145, 0.9747645], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I get hypnotized every time I see the Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.37930349 0.53309782 0.         0.37930349 0.
  0.37930349]
 [0.         0.37930349 0.         0.53309782 0.37930349 0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I get hypnotized every time I see the Body Party video
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['i', 'get', 'hypnotized', 'every', 'time', 'i', 'see', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9586011171340942
train_input: [0.43161341897075145, 0.9586011], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I keep thinking of a whole routine for Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.35520009 0.         0.
  0.49922133]
 [0.         0.40993715 0.         0.40993715 0.57615236 0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I keep thinking of a whole routine for Body Party
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['i', 'keep', 'thinking', 'of', 'a', 'whole', 'routine', 'for', 'body', 'party']
cosine_similarity: 0.9380054473876953
train_input: [0.29121941856368966, 0.93800545], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I knew the body party song after all
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.         0.35520009 0.
  0.49922133]
 [0.         0.40993715 0.         0.57615236 0.40993715 0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I knew the body party song after all
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['i', 'knew', 'the', 'body', 'party', 'song', 'after', 'all']
cosine_similarity: 0.9620076417922974
train_input: [0.29121941856368966, 0.96200764], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I put ria on that body party tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.35520009 0.         0.
  0.49922133]
 [0.         0.40993715 0.         0.40993715 0.57615236 0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I put ria on that body party tho
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['i', 'put', 'ria', 'on', 'that', 'body', 'party', 'tho']
cosine_similarity: 0.9425992965698242
train_input: [0.29121941856368966, 0.9425993], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I would give a legit lap dance to body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.         0.         0.
  0.35520009 0.49922133]
 [0.         0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.35520009 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: I would give a legit lap dance to body party
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['i', 'would', 'give', 'a', 'legit', 'lap', 'dance', 'to', 'body', 'party']
cosine_similarity: 0.9460970163345337
train_input: [0.2523342014336961, 0.946097], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: It s safe to say body party the new dance for you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.         0.         0.35520009
  0.         0.         0.49922133]
 [0.         0.31779954 0.         0.44665616 0.44665616 0.31779954
  0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: It s safe to say body party the new dance for you
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['it', 's', 'safe', 'to', 'say', 'body', 'party', 'the', 'new', 'dance', 'for', 'you']
cosine_similarity: 0.9373103380203247
TF_IDF_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: These girls went from Bad by Wale to Body Party by Ciara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.         0.37930349 0.37930349 0.         0.37930349
  0.53309782 0.         0.        ]
 [0.         0.42567716 0.30287281 0.30287281 0.42567716 0.30287281
  0.         0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Ciara body party video was A1, sentence2: These girls went from Bad by Wale to Body Party by Ciara
After tokenization, sentence1: ['ciara', 'body', 'party', 'video', 'was'], sentence2: ['these', 'girls', 'went', 'from', 'bad', 'by', 'wale', 'to', 'body', 'party', 'by', 'ciara']
cosine_similarity: 0.9578852653503418
train_input: [0.34464214103805474, 0.95788527], train_label: 0
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Ciara goes hard in her Body Party music video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.         0.         0.53309782
  0.37930349 0.53309782 0.37930349]
 [0.30287281 0.42567716 0.42567716 0.42567716 0.42567716 0.
  0.30287281 0.         0.30287281]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Ciara goes hard in her Body Party music video
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['ciara', 'goes', 'hard', 'in', 'her', 'body', 'party', 'music', 'video']
cosine_similarity: 0.9708415269851685
train_input: [0.34464214103805474, 0.9708415], train_label: 0
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Grindin in My Bed Cause Ciara x Body Party Started Playin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.         0.         0.49922133
  0.35520009 0.49922133 0.         0.         0.49922133]
 [0.37762778 0.26868528 0.37762778 0.37762778 0.37762778 0.
  0.26868528 0.         0.37762778 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Grindin in My Bed Cause Ciara x Body Party Started Playin
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['grindin', 'in', 'my', 'bed', 'cause', 'ciara', 'x', 'body', 'party', 'started', 'playin']
cosine_similarity: 0.9532577395439148
train_input: [0.1908740661302035, 0.95325774], train_label: 0
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: I cant get enough of Ciara new body party song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.35520009 0.49922133
  0.         0.49922133]
 [0.35520009 0.49922133 0.         0.49922133 0.35520009 0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: I cant get enough of Ciara new body party song
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['i', 'cant', 'get', 'enough', 'of', 'ciara', 'new', 'body', 'party', 'song']
cosine_similarity: 0.9825212955474854
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Imma learn the dance to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.49922133 0.35520009
  0.49922133 0.49922133]
 [0.35520009 0.49922133 0.49922133 0.49922133 0.         0.35520009
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Imma learn the dance to Body Party
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['imma', 'learn', 'the', 'dance', 'to', 'body', 'party']
cosine_similarity: 0.9714727997779846
train_input: [0.2523342014336961, 0.9714728], train_label: 0
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Listening to Body Party thinking I can sing nothing new lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.         0.35520009
  0.49922133 0.         0.         0.49922133]
 [0.29017021 0.4078241  0.4078241  0.         0.4078241  0.29017021
  0.         0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Listening to Body Party thinking I can sing nothing new lol
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['listening', 'to', 'body', 'party', 'thinking', 'i', 'can', 'sing', 'nothing', 'new', 'lol']
cosine_similarity: 0.9769216775894165
train_input: [0.20613696606828605, 0.9769217], train_label: 0
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Looking at body party video for the first time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.37930349 0.53309782 0.
  0.37930349]
 [0.37930349 0.53309782 0.         0.37930349 0.         0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Looking at body party video for the first time
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['looking', 'at', 'body', 'party', 'video', 'for', 'the', 'first', 'time']
cosine_similarity: 0.9714632630348206
train_input: [0.43161341897075145, 0.97146326], train_label: 0
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Oomgggg body party is my favorite song right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.35520009 0.49922133
  0.         0.         0.49922133]
 [0.31779954 0.44665616 0.         0.44665616 0.31779954 0.
  0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Oomgggg body party is my favorite song right now
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['body', 'party', 'is', 'my', 'favorite', 'song', 'right', 'now']
cosine_similarity: 0.9819125533103943
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Someone get me that shirt ciara has on in her body party video please
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.37930349 0.53309782 0.
  0.37930349]
 [0.37930349 0.53309782 0.         0.37930349 0.         0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Someone get me that shirt ciara has on in her body party video please
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['someone', 'get', 'me', 'that', 'shirt', 'ciara', 'has', 'on', 'in', 'her', 'body', 'party', 'video', 'please']
cosine_similarity: 0.9838739037513733
train_input: [0.43161341897075145, 0.9838739], train_label: 0
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Why is ciara so sexy in this body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.37930349 0.53309782 0.
  0.37930349]
 [0.37930349 0.53309782 0.         0.37930349 0.         0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: Why is ciara so sexy in this body party video
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['why', 'is', 'ciara', 'so', 'sexy', 'in', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9775615334510803
train_input: [0.43161341897075145, 0.97756153], train_label: 0
TF_IDF_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: dancing in her Ride and Body Party video is just amazing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.         0.         0.53309782 0.37930349
  0.53309782 0.         0.37930349]
 [0.42567716 0.30287281 0.42567716 0.42567716 0.         0.30287281
  0.         0.42567716 0.30287281]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: I need this body party video on my phone, sentence2: dancing in her Ride and Body Party video is just amazing
After tokenization, sentence1: ['i', 'need', 'this', 'body', 'party', 'video', 'on', 'my', 'phone'], sentence2: ['dancing', 'in', 'her', 'ride', 'and', 'body', 'party', 'video', 'is', 'just', 'amazing']
cosine_similarity: 0.9763931632041931
train_input: [0.34464214103805474, 0.97639316], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: Body party is explicit Lol go head ciara tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.         0.         0.44832087
  0.         0.63009934]
 [0.30287281 0.30287281 0.42567716 0.42567716 0.42567716 0.30287281
  0.42567716 0.        ]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: Body party is explicit Lol go head ciara tho
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['body', 'party', 'is', 'explicit', 'lol', 'go', 'head', 'ciara', 'tho']
cosine_similarity: 0.9646356105804443
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: Ciara Performs Body Party Acapella Song To Premiere Today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.44832087 0.         0.
  0.         0.         0.63009934]
 [0.39166832 0.27867523 0.27867523 0.27867523 0.39166832 0.39166832
  0.39166832 0.39166832 0.        ]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: Ciara Performs Body Party Acapella Song To Premiere Today
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['ciara', 'performs', 'body', 'party', 'acapella', 'song', 'to', 'premiere', 'today']
cosine_similarity: 0.9437130093574524
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: Ciara deserve all the praise he is getting for that Body Party song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.         0.44832087 0.
  0.         0.63009934]
 [0.30287281 0.30287281 0.42567716 0.42567716 0.30287281 0.42567716
  0.42567716 0.        ]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: Ciara deserve all the praise he is getting for that Body Party song
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['ciara', 'deserve', 'all', 'the', 'praise', 'he', 'is', 'getting', 'for', 'that', 'body', 'party', 'song']
cosine_similarity: 0.966078519821167
train_input: [0.4073526042885674, 0.9660785], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: Droppin that Body Party for the ladies tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.40993715 0.
  0.57615236]
 [0.35520009 0.         0.49922133 0.49922133 0.35520009 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: Droppin that Body Party for the ladies tonight
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['droppin', 'that', 'body', 'party', 'for', 'the', 'ladies', 'tonight']
cosine_similarity: 0.952732503414154
train_input: [0.29121941856368966, 0.9527325], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I Got Body Party Stuck In My Head
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.40993715 0.
  0.57615236]
 [0.35520009 0.         0.49922133 0.49922133 0.35520009 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I Got Body Party Stuck In My Head
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['i', 'got', 'body', 'party', 'stuck', 'in', 'my', 'head']
cosine_similarity: 0.9491647481918335
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I already had a thing for Future but him in the Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.63009934 0.         0.44832087 0.         0.44832087]
 [0.37930349 0.         0.53309782 0.37930349 0.53309782 0.37930349]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I already had a thing for Future but him in the Body Party video
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['i', 'already', 'had', 'a', 'thing', 'for', 'future', 'but', 'him', 'in', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9751785397529602
train_input: [0.5101490193104813, 0.97517854], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I finally seen the Body Party video it was tooo poppppin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.63009934 0.         0.44832087 0.         0.
  0.         0.44832087]
 [0.30287281 0.         0.42567716 0.30287281 0.42567716 0.42567716
  0.42567716 0.30287281]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I finally seen the Body Party video it was tooo poppppin
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['i', 'finally', 'seen', 'the', 'body', 'party', 'video', 'it', 'was', 'tooo', 'poppppin']
cosine_similarity: 0.9542796015739441
train_input: [0.4073526042885674, 0.9542796], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I get hypnotized every time I see the Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.63009934 0.         0.44832087 0.         0.44832087]
 [0.37930349 0.         0.53309782 0.37930349 0.53309782 0.37930349]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I get hypnotized every time I see the Body Party video
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['i', 'get', 'hypnotized', 'every', 'time', 'i', 'see', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9642343521118164
train_input: [0.5101490193104813, 0.96423435], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I need a dance to that Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.40993715 0.57615236]
 [0.40993715 0.         0.57615236 0.57615236 0.40993715 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I need a dance to that Body Party
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['i', 'need', 'a', 'dance', 'to', 'that', 'body', 'party']
cosine_similarity: 0.9683713912963867
TF_IDF_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I really enjoyed watching body party video Im bout to watch it again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.63009934 0.         0.         0.44832087
  0.         0.44832087 0.         0.        ]
 [0.25948224 0.36469323 0.         0.36469323 0.36469323 0.25948224
  0.36469323 0.25948224 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: Ciara s Body Party video is everything, sentence2: I really enjoyed watching body party video Im bout to watch it again
After tokenization, sentence1: ['ciara', 's', 'body', 'party', 'video', 'is', 'everything'], sentence2: ['i', 'really', 'enjoyed', 'watching', 'body', 'party', 'video', 'im', 'bout', 'to', 'watch', 'it', 'again']
cosine_similarity: 0.958499014377594
train_input: [0.3489939079552687, 0.958499], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Body party by Ciara is my shit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.37930349 0.         0.53309782]
 [0.44832087 0.44832087 0.         0.44832087 0.63009934 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Body party by Ciara is my shit
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['body', 'party', 'by', 'ciara', 'is', 'my', 'shit']
cosine_similarity: 0.9816560745239258
train_input: [0.5101490193104813, 0.9816561], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Body party is my new favorite song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.49922133 0.         0.35520009
  0.         0.49922133]
 [0.35520009 0.         0.49922133 0.         0.49922133 0.35520009
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Body party is my new favorite song
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['body', 'party', 'is', 'my', 'new', 'favorite', 'song']
cosine_similarity: 0.9858542680740356
train_input: [0.2523342014336961, 0.98585427], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Ciara Performs Body Party Acapella Song To Premiere Today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.37930349 0.53309782 0.37930349 0.
  0.         0.         0.         0.53309782]
 [0.39166832 0.27867523 0.27867523 0.         0.27867523 0.39166832
  0.39166832 0.39166832 0.39166832 0.        ]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Ciara Performs Body Party Acapella Song To Premiere Today
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['ciara', 'performs', 'body', 'party', 'acapella', 'song', 'to', 'premiere', 'today']
cosine_similarity: 0.9592987895011902
train_input: [0.31710746658027095, 0.9592988], train_label: 0
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Ciara body party brings my alterego out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.         0.37930349 0.53309782 0.37930349
  0.53309782]
 [0.53309782 0.37930349 0.53309782 0.37930349 0.         0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Ciara body party brings my alterego out
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['ciara', 'body', 'party', 'brings', 'my', 'alterego', 'out']
cosine_similarity: 0.9647303223609924
train_input: [0.43161341897075145, 0.9647303], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Ciara body party video number 5 I love it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.4472136  0.4472136  0.         0.4472136  0.4472136 ]
 [0.37863221 0.37863221 0.37863221 0.53215436 0.37863221 0.37863221]]
pairwise_similarity: [[1.         0.84664735]
 [0.84664735 1.        ]]
cosine_similarity: 0.8466473536503036
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Ciara body party video number 5 I love it
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['ciara', 'body', 'party', 'video', 'number', 'i', 'love', 'it']
cosine_similarity: 0.990475058555603
train_input: [0.8466473536503036, 0.99047506], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Ciara did dat in body party video oms
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.         0.         0.57496187 0.
  0.4090901  0.4090901 ]
 [0.3174044  0.3174044  0.44610081 0.44610081 0.         0.44610081
  0.3174044  0.3174044 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Ciara did dat in body party video oms
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['ciara', 'did', 'dat', 'in', 'body', 'party', 'video', 'oms']
cosine_similarity: 0.9509231448173523
train_input: [0.5193879933129156, 0.95092314], train_label: 0
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: I Wanna Watch Body Party Video So 106 Park Better Show It
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37930349 0.53309782 0.53309782 0.
  0.37930349 0.37930349 0.         0.        ]
 [0.39166832 0.39166832 0.27867523 0.         0.         0.39166832
  0.27867523 0.27867523 0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: I Wanna Watch Body Party Video So 106 Park Better Show It
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['i', 'wanna', 'watch', 'body', 'party', 'video', 'so', 'park', 'better', 'show', 'it']
cosine_similarity: 0.9775577187538147
train_input: [0.31710746658027095, 0.9775577], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Listening to Body Party I love this song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.         0.37930349 0.37930349 0.
  0.53309782]
 [0.37930349 0.         0.53309782 0.37930349 0.37930349 0.53309782
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Listening to Body Party I love this song
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['listening', 'to', 'body', 'party', 'i', 'love', 'this', 'song']
cosine_similarity: 0.9869986176490784
train_input: [0.43161341897075145, 0.9869986], train_label: 1
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Lol the song is called body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.49922133 0.35520009
  0.         0.49922133]
 [0.35520009 0.49922133 0.         0.49922133 0.         0.35520009
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Lol the song is called body party
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['lol', 'the', 'song', 'is', 'called', 'body', 'party']
cosine_similarity: 0.981444239616394
train_input: [0.2523342014336961, 0.98144424], train_label: 0
TF_IDF_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Oomgggg body party is my favorite song right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.49922133 0.         0.35520009
  0.         0.         0.49922133]
 [0.31779954 0.         0.44665616 0.         0.44665616 0.31779954
  0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: I love the Body Party video by ciara, sentence2: Oomgggg body party is my favorite song right now
After tokenization, sentence1: ['i', 'love', 'the', 'body', 'party', 'video', 'by', 'ciara'], sentence2: ['body', 'party', 'is', 'my', 'favorite', 'song', 'right', 'now']
cosine_similarity: 0.9823269248008728
train_input: [0.22576484600261604, 0.9823269], train_label: 1
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Body Party x Ciara makes me want to give somebody a strip tease
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.35520009 0.49922133 0.49922133
  0.         0.         0.         0.49922133 0.        ]
 [0.26868528 0.37762778 0.37762778 0.26868528 0.         0.
  0.37762778 0.37762778 0.37762778 0.         0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Body Party x Ciara makes me want to give somebody a strip tease
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['body', 'party', 'x', 'ciara', 'makes', 'me', 'want', 'to', 'give', 'somebody', 'a', 'strip', 'tease']
cosine_similarity: 0.9655619263648987
train_input: [0.1908740661302035, 0.9655619], train_label: 0
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Body party video make you question ya sexiness
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.37930349 0.53309782 0.         0.
  0.53309782 0.37930349 0.        ]
 [0.30287281 0.42567716 0.30287281 0.         0.42567716 0.42567716
  0.         0.30287281 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Body party video make you question ya sexiness
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['body', 'party', 'video', 'make', 'you', 'question', 'ya', 'sexiness']
cosine_similarity: 0.9692708849906921
train_input: [0.34464214103805474, 0.9692709], train_label: 0
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Ciara did her damn thing in body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.         0.37930349 0.53309782
  0.53309782 0.         0.37930349]
 [0.30287281 0.42567716 0.42567716 0.42567716 0.30287281 0.
  0.         0.42567716 0.30287281]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Ciara did her damn thing in body party video
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['ciara', 'did', 'her', 'damn', 'thing', 'in', 'body', 'party', 'video']
cosine_similarity: 0.9901254773139954
train_input: [0.34464214103805474, 0.9901255], train_label: 0
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Ciara s Body Party is my shittttt
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.49922133 0.         0.49922133
  0.49922133]
 [0.40993715 0.57615236 0.40993715 0.         0.57615236 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Ciara s Body Party is my shittttt
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['ciara', 's', 'body', 'party', 'is', 'my']
cosine_similarity: 0.9798202514648438
train_input: [0.29121941856368966, 0.97982025], train_label: 0
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Everybody finally catching on to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.35520009 0.49922133
  0.49922133 0.49922133]
 [0.35520009 0.49922133 0.49922133 0.49922133 0.35520009 0.
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Everybody finally catching on to Body Party
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['everybody', 'finally', 'catching', 'on', 'to', 'body', 'party']
cosine_similarity: 0.9820941090583801
train_input: [0.2523342014336961, 0.9820941], train_label: 0
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: I always been in love with ciara but this body party video is insane
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.         0.37930349 0.53309782
  0.53309782 0.37930349]
 [0.33471228 0.47042643 0.47042643 0.47042643 0.33471228 0.
  0.         0.33471228]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: I always been in love with ciara but this body party video is insane
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['i', 'always', 'been', 'in', 'love', 'with', 'ciara', 'but', 'this', 'body', 'party', 'video', 'is', 'insane']
cosine_similarity: 0.9854961037635803
train_input: [0.38087260847594373, 0.9854961], train_label: 0
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: I wanted to see more dancing from Ciara in Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.35520009 0.49922133 0.49922133
  0.49922133 0.        ]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.         0.
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: I wanted to see more dancing from Ciara in Body Party
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['i', 'wanted', 'to', 'see', 'more', 'dancing', 'from', 'ciara', 'in', 'body', 'party']
cosine_similarity: 0.9866399765014648
train_input: [0.2523342014336961, 0.98664], train_label: 0
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Just showed nelle my body party dance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.         0.37930349 0.53309782
  0.37930349 0.53309782]
 [0.33471228 0.47042643 0.47042643 0.47042643 0.33471228 0.
  0.33471228 0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Just showed nelle my body party dance
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['just', 'showed', 'nelle', 'my', 'body', 'party', 'dance']
cosine_similarity: 0.9879214763641357
train_input: [0.38087260847594373, 0.9879215], train_label: 1
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Why is Future cute in this Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.37930349 0.53309782 0.53309782
  0.37930349]
 [0.37930349 0.53309782 0.53309782 0.37930349 0.         0.
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Why is Future cute in this Body Party video
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['why', 'is', 'future', 'cute', 'in', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9863758087158203
train_input: [0.43161341897075145, 0.9863758], train_label: 0
TF_IDF_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Why was everybody piping Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.37930349 0.         0.53309782 0.53309782
  0.37930349]
 [0.37930349 0.53309782 0.37930349 0.53309782 0.         0.
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: I showed my poo the body party video, sentence2: Why was everybody piping Body Party video
After tokenization, sentence1: ['i', 'showed', 'my', 'poo', 'the', 'body', 'party', 'video'], sentence2: ['why', 'was', 'everybody', 'piping', 'body', 'party', 'video']
cosine_similarity: 0.9822707772254944
train_input: [0.43161341897075145, 0.9822708], train_label: 0
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: Body Party by Ciara is my new favorite song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.25116439 0.25116439 0.         0.35300279 0.70600557 0.
  0.25116439 0.25116439 0.35300279]
 [0.35464863 0.35464863 0.49844628 0.         0.         0.49844628
  0.35464863 0.35464863 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.35630042933313805
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: Body Party by Ciara is my new favorite song
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['body', 'party', 'by', 'ciara', 'is', 'my', 'new', 'favorite', 'song']
cosine_similarity: 0.9739720225334167
train_input: [0.35630042933313805, 0.973972], train_label: 1
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: Body party is a newer version of dance for you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23700504 0.33310232 0.         0.33310232 0.66620463 0.
  0.23700504 0.33310232 0.         0.33310232]
 [0.35520009 0.         0.49922133 0.         0.         0.49922133
  0.35520009 0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: Body party is a newer version of dance for you
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['body', 'party', 'is', 'a', 'newer', 'version', 'of', 'dance', 'for', 'you']
cosine_similarity: 0.971400260925293
train_input: [0.16836842163679844, 0.97140026], train_label: 0
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: Can I hear body party now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23700504 0.33310232 0.         0.33310232 0.66620463 0.23700504
  0.33310232 0.33310232]
 [0.50154891 0.         0.70490949 0.         0.         0.50154891
  0.         0.        ]]
pairwise_similarity: [[1.         0.23773924]
 [0.23773924 1.        ]]
cosine_similarity: 0.23773923857522397
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: Can I hear body party now
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['can', 'i', 'hear', 'body', 'party', 'now']
cosine_similarity: 0.9898513555526733
train_input: [0.23773923857522397, 0.98985136], train_label: 0
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: I get hypnotized every time I see the Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.24377685 0.34261985 0.         0.34261985 0.68523971 0.24377685
  0.34261985 0.         0.24377685]
 [0.37930349 0.         0.53309782 0.         0.         0.37930349
  0.         0.53309782 0.37930349]]
pairwise_similarity: [[1.         0.27739623]
 [0.27739623 1.        ]]
cosine_similarity: 0.27739622897624144
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: I get hypnotized every time I see the Body Party video
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['i', 'get', 'hypnotized', 'every', 'time', 'i', 'see', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9917446970939636
train_input: [0.27739622897624144, 0.9917447], train_label: 1
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: I heard body party looong time ago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.23700504 0.33310232 0.         0.33310232 0.66620463
  0.         0.23700504 0.33310232 0.         0.33310232]
 [0.44665616 0.31779954 0.         0.44665616 0.         0.
  0.44665616 0.31779954 0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: I heard body party looong time ago
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['i', 'heard', 'body', 'party', 'looong', 'time', 'ago']
cosine_similarity: 0.9638805389404297
train_input: [0.15064018498706508, 0.96388054], train_label: 0
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: Imma learn the dance to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23700504 0.33310232 0.         0.         0.         0.33310232
  0.66620463 0.23700504 0.33310232 0.33310232]
 [0.35520009 0.         0.49922133 0.49922133 0.49922133 0.
  0.         0.35520009 0.         0.        ]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: Imma learn the dance to Body Party
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['imma', 'learn', 'the', 'dance', 'to', 'body', 'party']
cosine_similarity: 0.975745439529419
train_input: [0.16836842163679844, 0.97574544], train_label: 0
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: That Body Party video really aint all that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.24377685 0.34261985 0.34261985 0.68523971 0.24377685
  0.         0.34261985 0.24377685]
 [0.53309782 0.37930349 0.         0.         0.         0.37930349
  0.53309782 0.         0.37930349]]
pairwise_similarity: [[1.         0.27739623]
 [0.27739623 1.        ]]
cosine_similarity: 0.27739622897624144
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: That Body Party video really aint all that
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['that', 'body', 'party', 'video', 'really', 'aint', 'all', 'that']
cosine_similarity: 0.9902511239051819
train_input: [0.27739622897624144, 0.9902511], train_label: 0
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: That lil bounce that Ciara do in Body Party i love that party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.24377685 0.         0.24377685 0.34261985 0.68523971 0.
  0.         0.24377685 0.34261985 0.34261985]
 [0.2895694  0.40697968 0.2895694  0.         0.         0.40697968
  0.40697968 0.57913879 0.         0.        ]]
pairwise_similarity: [[1.         0.28236126]
 [0.28236126 1.        ]]
cosine_similarity: 0.2823612582719693
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: That lil bounce that Ciara do in Body Party i love that party
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['that', 'lil', 'bounce', 'that', 'ciara', 'do', 'in', 'body', 'party', 'i', 'love', 'that', 'party']
cosine_similarity: 0.9940188527107239
train_input: [0.2823612582719693, 0.99401885], train_label: 1
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: did er thang in dat Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.24377685 0.34261985 0.         0.         0.         0.34261985
  0.68523971 0.24377685 0.34261985 0.         0.24377685]
 [0.30287281 0.         0.42567716 0.42567716 0.42567716 0.
  0.         0.30287281 0.         0.42567716 0.30287281]]
pairwise_similarity: [[1.         0.22150013]
 [0.22150013 1.        ]]
cosine_similarity: 0.22150013430590973
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: did er thang in dat Body Party video
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['did', 'er', 'thang', 'in', 'dat', 'body', 'party', 'video']
cosine_similarity: 0.9409642815589905
train_input: [0.22150013430590973, 0.9409643], train_label: 0
TF_IDF_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: the way she was dancing for future in the body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.24377685 0.34261985 0.         0.         0.34261985 0.68523971
  0.24377685 0.34261985 0.24377685 0.        ]
 [0.33471228 0.         0.47042643 0.47042643 0.         0.
  0.33471228 0.         0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.24478531]
 [0.24478531 1.        ]]
cosine_similarity: 0.24478531173455215
word_to_vector_cosine_similarity: sentence1: I cant lie I like this ciara song Body Party and I like the video, sentence2: the way she was dancing for future in the body party video
After tokenization, sentence1: ['i', 'cant', 'lie', 'i', 'like', 'this', 'ciara', 'song', 'body', 'party', 'and', 'i', 'like', 'the', 'video'], sentence2: ['the', 'way', 'she', 'was', 'dancing', 'for', 'future', 'in', 'the', 'body', 'party', 'video']
cosine_similarity: 0.981904923915863
train_input: [0.24478531173455215, 0.9819049], train_label: 0
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: Body party goes in but I wanted more raunchiness from Ciara in the video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.         0.4090901  0.
  0.4090901  0.        ]
 [0.3174044  0.3174044  0.         0.44610081 0.3174044  0.44610081
  0.3174044  0.44610081]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: Body party goes in but I wanted more raunchiness from Ciara in the video
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['body', 'party', 'goes', 'in', 'but', 'i', 'wanted', 'more', 'from', 'ciara', 'in', 'the', 'video']
cosine_similarity: 0.9774599671363831
train_input: [0.5193879933129156, 0.97745997], train_label: 1
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: But for Ciara to go so hard on Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.         0.37930349 0.53309782]
 [0.44832087 0.44832087 0.         0.63009934 0.44832087 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: But for Ciara to go so hard on Body Party
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['but', 'for', 'ciara', 'to', 'go', 'so', 'hard', 'on', 'body', 'party']
cosine_similarity: 0.972256600856781
train_input: [0.5101490193104813, 0.9722566], train_label: 1
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: I did not know I had body party downloaded on my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.49922133 0.
  0.35520009 0.         0.49922133]
 [0.31779954 0.         0.44665616 0.44665616 0.         0.44665616
  0.31779954 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: I did not know I had body party downloaded on my phone
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['i', 'did', 'not', 'know', 'i', 'had', 'body', 'party', 'downloaded', 'on', 'my', 'phone']
cosine_similarity: 0.9783351421356201
train_input: [0.22576484600261604, 0.97833514], train_label: 0
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: I really enjoyed watching body party video Im bout to watch it again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.         0.53309782 0.
  0.37930349 0.         0.37930349 0.         0.        ]
 [0.25948224 0.36469323 0.         0.36469323 0.         0.36469323
  0.25948224 0.36469323 0.25948224 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.29526756]
 [0.29526756 1.        ]]
cosine_similarity: 0.2952675553824053
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: I really enjoyed watching body party video Im bout to watch it again
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['i', 'really', 'enjoyed', 'watching', 'body', 'party', 'video', 'im', 'bout', 'to', 'watch', 'it', 'again']
cosine_similarity: 0.9806048274040222
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: Im in Love w the body party video Ciara body is poppin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.         0.         0.4090901
  0.         0.4090901 ]
 [0.55628581 0.2781429  0.         0.39092014 0.39092014 0.2781429
  0.39092014 0.2781429 ]]
pairwise_similarity: [[1.         0.56892754]
 [0.56892754 1.        ]]
cosine_similarity: 0.5689275447725676
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: Im in Love w the body party video Ciara body is poppin
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['im', 'in', 'love', 'w', 'the', 'body', 'party', 'video', 'ciara', 'body', 'is', 'poppin']
cosine_similarity: 0.9801327586174011
train_input: [0.5689275447725676, 0.98013276], train_label: 1
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: Ive heard Body Party by Ciara before
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.         0.         0.37930349
  0.53309782]
 [0.37930349 0.37930349 0.         0.53309782 0.53309782 0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: Ive heard Body Party by Ciara before
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['ive', 'heard', 'body', 'party', 'by', 'ciara', 'before']
cosine_similarity: 0.9858205318450928
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: Lawdddd Ciara Future give me life in Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.         0.         0.
  0.4090901  0.4090901 ]
 [0.3174044  0.3174044  0.         0.44610081 0.44610081 0.44610081
  0.3174044  0.3174044 ]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: Lawdddd Ciara Future give me life in Body Party video
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['ciara', 'future', 'give', 'me', 'life', 'in', 'body', 'party', 'video']
cosine_similarity: 0.9754937291145325
train_input: [0.5193879933129156, 0.9754937], train_label: 1
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: That FabCB Ready Ciara Body Party is the hottest songs out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.         0.37930349
  0.         0.         0.53309782]
 [0.30287281 0.30287281 0.42567716 0.         0.42567716 0.30287281
  0.42567716 0.42567716 0.        ]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: That FabCB Ready Ciara Body Party is the hottest songs out
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['that', 'ready', 'ciara', 'body', 'party', 'is', 'the', 'hottest', 'songs', 'out']
cosine_similarity: 0.984683096408844
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: i bet yall aint peep stevie j and joseline n ciara body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.4090901  0.4090901  0.57496187 0.
  0.4090901  0.         0.         0.4090901  0.        ]
 [0.35300279 0.35300279 0.25116439 0.25116439 0.         0.35300279
  0.25116439 0.35300279 0.35300279 0.25116439 0.35300279]]
pairwise_similarity: [[1.         0.41099546]
 [0.41099546 1.        ]]
cosine_similarity: 0.4109954639349511
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: i bet yall aint peep stevie j and joseline n ciara body party video
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['i', 'bet', 'yall', 'aint', 'peep', 'stevie', 'j', 'and', 'joseline', 'n', 'ciara', 'body', 'party', 'video']
cosine_similarity: 0.9721906781196594
TF_IDF_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: this nigga Future gamed Ciara right up in the Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.         0.         0.
  0.4090901  0.         0.4090901 ]
 [0.28986934 0.28986934 0.         0.40740124 0.40740124 0.40740124
  0.28986934 0.40740124 0.28986934]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: But Ciara fucked that Body Party video up, sentence2: this nigga Future gamed Ciara right up in the Body Party video
After tokenization, sentence1: ['but', 'ciara', 'fucked', 'that', 'body', 'party', 'video', 'up'], sentence2: ['this', 'nigga', 'future', 'gamed', 'ciara', 'right', 'up', 'in', 'the', 'body', 'party', 'video']
cosine_similarity: 0.9883641004562378
train_input: [0.4743307064971939, 0.9883641], train_label: 1
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Body Party x Ciara go hard
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.40993715 0.57615236]
 [0.40993715 0.57615236 0.         0.57615236 0.40993715 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Body Party x Ciara go hard
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['body', 'party', 'x', 'ciara', 'go', 'hard']
cosine_similarity: 0.9622434973716736
train_input: [0.3360969272762575, 0.9622435], train_label: 1
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Bruh Ciara in this body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.63009934 0.44832087 0.44832087]
 [0.37930349 0.53309782 0.53309782 0.         0.37930349 0.37930349]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Bruh Ciara in this body party video
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['bruh', 'ciara', 'in', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9696754813194275
train_input: [0.5101490193104813, 0.9696755], train_label: 0
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: CIARA BODY PARTY CHOREOGRAPHY BY ENYCE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.57615236 0.40993715
  0.57615236]
 [0.35520009 0.49922133 0.49922133 0.49922133 0.         0.35520009
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: CIARA BODY PARTY CHOREOGRAPHY BY ENYCE
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['ciara', 'body', 'party', 'choreography', 'by', 'enyce']
cosine_similarity: 0.875272274017334
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Come to Ciara s Body Party on iTunes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.         0.40993715
  0.57615236]
 [0.35520009 0.49922133 0.49922133 0.         0.49922133 0.35520009
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Come to Ciara s Body Party on iTunes
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['come', 'to', 'ciara', 's', 'body', 'party', 'on', 'itunes']
cosine_similarity: 0.9519432783126831
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Every time I hear body party I start dancing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.40993715 0.
  0.         0.57615236]
 [0.31779954 0.44665616 0.         0.44665616 0.31779954 0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Every time I hear body party I start dancing
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['every', 'time', 'i', 'hear', 'body', 'party', 'i', 'start', 'dancing']
cosine_similarity: 0.9534902572631836
train_input: [0.2605556710562624, 0.95349026], train_label: 1
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: I knew the body party song after all
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.40993715 0.         0.57615236]
 [0.40993715 0.         0.57615236 0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: I knew the body party song after all
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['i', 'knew', 'the', 'body', 'party', 'song', 'after', 'all']
cosine_similarity: 0.9684590101242065
train_input: [0.3360969272762575, 0.968459], train_label: 0
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Mannnn Body Party really goes in
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.40993715 0.
  0.57615236]
 [0.35520009 0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Mannnn Body Party really goes in
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['body', 'party', 'really', 'goes', 'in']
cosine_similarity: 0.9661969542503357
train_input: [0.29121941856368966, 0.96619695], train_label: 1
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: So Body Party Everybody Song Now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.40993715 0.         0.57615236]
 [0.40993715 0.57615236 0.         0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: So Body Party Everybody Song Now
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['so', 'body', 'party', 'everybody', 'song', 'now']
cosine_similarity: 0.9665262699127197
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Why was everybody piping Body Party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.63009934 0.44832087 0.         0.44832087]
 [0.37930349 0.53309782 0.         0.37930349 0.53309782 0.37930349]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: Why was everybody piping Body Party video
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['why', 'was', 'everybody', 'piping', 'body', 'party', 'video']
cosine_similarity: 0.964401125907898
train_input: [0.5101490193104813, 0.9644011], train_label: 0
TF_IDF_cosine_similarity: sentence1: Body Party is such a good video, sentence2: body party just makes me wanna
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.40993715 0.57615236
  0.        ]
 [0.35520009 0.         0.49922133 0.49922133 0.35520009 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Body Party is such a good video, sentence2: body party just makes me wanna
After tokenization, sentence1: ['body', 'party', 'is', 'such', 'a', 'good', 'video'], sentence2: ['body', 'party', 'just', 'makes', 'me', 'wanna']
cosine_similarity: 0.9575757384300232
train_input: [0.29121941856368966, 0.95757574], train_label: 1
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: All I see is Body Party in my TL
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.57615236 0.40993715 0.        ]
 [0.50154891 0.         0.         0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: All I see is Body Party in my TL
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['all', 'i', 'see', 'is', 'body', 'party', 'in', 'my', 'tl']
cosine_similarity: 0.9581220149993896
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: Body Party is my shit on so many levels
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.57615236 0.40993715 0.        ]
 [0.40993715 0.         0.57615236 0.         0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: Body Party is my shit on so many levels
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['body', 'party', 'is', 'my', 'shit', 'on', 'so', 'many', 'levels']
cosine_similarity: 0.9501048922538757
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: Body party goes in but I wanted more raunchiness from Ciara in the video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.63009934 0.44832087 0.
  0.         0.        ]
 [0.30287281 0.30287281 0.42567716 0.         0.30287281 0.42567716
  0.42567716 0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: Body party goes in but I wanted more raunchiness from Ciara in the video
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['body', 'party', 'goes', 'in', 'but', 'i', 'wanted', 'more', 'from', 'ciara', 'in', 'the', 'video']
cosine_similarity: 0.9532493948936462
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: But for Ciara to go so hard on Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.63009934 0.44832087]
 [0.44832087 0.44832087 0.63009934 0.         0.44832087]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: But for Ciara to go so hard on Body Party
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['but', 'for', 'ciara', 'to', 'go', 'so', 'hard', 'on', 'body', 'party']
cosine_similarity: 0.9713961482048035
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: Ciara s mad sexy in her Body Party vid
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.44832087 0.
  0.        ]
 [0.33471228 0.33471228 0.         0.47042643 0.33471228 0.47042643
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: Ciara s mad sexy in her Body Party vid
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['ciara', 's', 'mad', 'sexy', 'in', 'her', 'body', 'party', 'vid']
cosine_similarity: 0.951239287853241
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: I always been in love with ciara but this body party video is insane
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.         0.5        0.5        0.        ]
 [0.35464863 0.35464863 0.49844628 0.35464863 0.35464863 0.49844628]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: I always been in love with ciara but this body party video is insane
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['i', 'always', 'been', 'in', 'love', 'with', 'ciara', 'but', 'this', 'body', 'party', 'video', 'is', 'insane']
cosine_similarity: 0.9625713229179382
train_input: [0.7092972666062737, 0.9625713], train_label: 1
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: I think body party might actually debut on the bb100
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.57615236 0.         0.57615236
  0.40993715 0.        ]
 [0.44665616 0.44665616 0.31779954 0.         0.44665616 0.
  0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: I think body party might actually debut on the bb100
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['i', 'think', 'body', 'party', 'might', 'actually', 'debut', 'on', 'the']
cosine_similarity: 0.9491478204727173
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: Id definitely fuck somebody to Body Party by Ciara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.         0.         0.63009934
  0.44832087 0.        ]
 [0.30287281 0.30287281 0.42567716 0.42567716 0.42567716 0.
  0.30287281 0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: Id definitely fuck somebody to Body Party by Ciara
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['id', 'definitely', 'fuck', 'somebody', 'to', 'body', 'party', 'by', 'ciara']
cosine_similarity: 0.974894106388092
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: We did abs to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.         0.57615236 0.40993715]
 [0.57615236 0.40993715 0.         0.57615236 0.         0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: We did abs to Body Party
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['we', 'did', 'abs', 'to', 'body', 'party']
cosine_similarity: 0.961860716342926
train_input: [0.3360969272762575, 0.9618607], train_label: 0
TF_IDF_cosine_similarity: sentence1: do you love Ciara body party, sentence2: You sleeppppp body party is that shii
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.57615236 0.40993715 0.         0.        ]
 [0.40993715 0.         0.         0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: do you love Ciara body party, sentence2: You sleeppppp body party is that shii
After tokenization, sentence1: ['do', 'you', 'love', 'ciara', 'body', 'party'], sentence2: ['you', 'body', 'party', 'is', 'that', 'shii']
cosine_similarity: 0.958838164806366
train_input: [0.3360969272762575, 0.95883816], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Body Party is such a slow jam
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.40993715 0.         0.57615236]
 [0.40993715 0.         0.57615236 0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Body Party is such a slow jam
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['body', 'party', 'is', 'such', 'a', 'slow', 'jam']
cosine_similarity: 0.9553433060646057
train_input: [0.3360969272762575, 0.9553433], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Body Party is the new planking
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.40993715 0.         0.57615236]
 [0.40993715 0.         0.57615236 0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Body Party is the new planking
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['body', 'party', 'is', 'the', 'new', 'planking']
cosine_similarity: 0.9525332450866699
train_input: [0.3360969272762575, 0.95253325], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Body party has to be my favorite song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.40993715 0.         0.57615236]
 [0.40993715 0.         0.57615236 0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Body party has to be my favorite song
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['body', 'party', 'has', 'to', 'be', 'my', 'favorite', 'song']
cosine_similarity: 0.9794572591781616
train_input: [0.3360969272762575, 0.97945726], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Body party video make you question ya sexiness
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.40993715 0.         0.
  0.         0.57615236 0.        ]
 [0.29017021 0.         0.4078241  0.29017021 0.4078241  0.4078241
  0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Body party video make you question ya sexiness
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['body', 'party', 'video', 'make', 'you', 'question', 'ya', 'sexiness']
cosine_similarity: 0.9552680850028992
train_input: [0.23790309463326234, 0.9552681], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Ciara in body party is too damn sexy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.40993715 0.
  0.57615236]
 [0.35520009 0.49922133 0.49922133 0.         0.35520009 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Ciara in body party is too damn sexy
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['ciara', 'in', 'body', 'party', 'is', 'too', 'damn', 'sexy']
cosine_similarity: 0.9575891494750977
train_input: [0.29121941856368966, 0.95758915], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Every since Body Party video came out Okayla been thinking she Ciara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.         0.40993715
  0.         0.         0.57615236]
 [0.29017021 0.4078241  0.4078241  0.         0.4078241  0.29017021
  0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Every since Body Party video came out Okayla been thinking she Ciara
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['every', 'since', 'body', 'party', 'video', 'came', 'out', 'okayla', 'been', 'thinking', 'she', 'ciara']
cosine_similarity: 0.9490113854408264
train_input: [0.23790309463326234, 0.9490114], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Grindin in My Bed Cause Ciara x Body Party Started Playin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.57615236 0.
  0.40993715 0.         0.         0.57615236]
 [0.37762778 0.26868528 0.37762778 0.37762778 0.         0.37762778
  0.26868528 0.37762778 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Grindin in My Bed Cause Ciara x Body Party Started Playin
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['grindin', 'in', 'my', 'bed', 'cause', 'ciara', 'x', 'body', 'party', 'started', 'playin']
cosine_similarity: 0.9444636702537537
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: I keep thinking of a whole routine for Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.40993715 0.         0.         0.57615236]
 [0.40993715 0.         0.40993715 0.57615236 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: I keep thinking of a whole routine for Body Party
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['i', 'keep', 'thinking', 'of', 'a', 'whole', 'routine', 'for', 'body', 'party']
cosine_similarity: 0.9757199883460999
train_input: [0.3360969272762575, 0.97572], train_label: 1
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Mannnn Body Party really goes in
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.40993715 0.
  0.57615236]
 [0.35520009 0.         0.49922133 0.49922133 0.35520009 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: Mannnn Body Party really goes in
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['body', 'party', 'really', 'goes', 'in']
cosine_similarity: 0.9705456495285034
train_input: [0.29121941856368966, 0.97054565], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: body party by ciara is on repeat 20 times a day I swear
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.         0.40993715
  0.         0.         0.         0.57615236]
 [0.37762778 0.26868528 0.37762778 0.         0.37762778 0.26868528
  0.37762778 0.37762778 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: I want to dance to Body Party, sentence2: body party by ciara is on repeat 20 times a day I swear
After tokenization, sentence1: ['i', 'want', 'to', 'dance', 'to', 'body', 'party'], sentence2: ['body', 'party', 'by', 'ciara', 'is', 'on', 'repeat', 'times', 'a', 'day', 'i', 'swear']
cosine_similarity: 0.9785460233688354
train_input: [0.2202881505618297, 0.978546], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Andrew Bogut s a bitch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44943642 0.6316672  0.6316672 ]
 [0.6316672  0.6316672  0.44943642 0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Andrew Bogut s a bitch
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['andrew', 'bogut', 's', 'a', 'bitch']
cosine_similarity: 0.9088984727859497
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Bogut is a huge value for the Warriors
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Bogut is a huge value for the Warriors
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['bogut', 'is', 'a', 'huge', 'value', 'for', 'the', 'warriors']
cosine_similarity: 0.9420086145401001
train_input: [0.17077611319011649, 0.9420086], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Bogut is doing the little things right now and I love it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.
  0.         0.6316672 ]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Bogut is doing the little things right now and I love it
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['bogut', 'is', 'doing', 'the', 'little', 'things', 'right', 'now', 'and', 'i', 'love', 'it']
cosine_similarity: 0.9294139742851257
train_input: [0.1362763414390864, 0.929414], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Bogut s growing on me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.6316672 ]
 [0.57973867 0.81480247 0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Bogut s growing on me
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['bogut', 's', 'growing', 'on', 'me']
cosine_similarity: 0.9004444479942322
train_input: [0.2605556710562624, 0.90044445], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Bullshit technical foul on Andrew Bogut
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.
  0.6316672 ]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.         0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Bullshit technical foul on Andrew Bogut
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['bullshit', 'technical', 'foul', 'on', 'andrew', 'bogut']
cosine_similarity: 0.8938785195350647
train_input: [0.15064018498706508, 0.8938785], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: But Bogut would probably tear apart McGee
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.
  0.6316672 ]
 [0.47107781 0.33517574 0.         0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: But Bogut would probably tear apart McGee
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['but', 'bogut', 'would', 'probably', 'tear', 'apart', 'mcgee']
cosine_similarity: 0.9241663813591003
train_input: [0.15064018498706508, 0.9241664], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: In yo neckbeard face Bogut
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: In yo neckbeard face Bogut
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['in', 'yo', 'neckbeard', 'face', 'bogut']
cosine_similarity: 0.9312744736671448
train_input: [0.17077611319011649, 0.9312745], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Is Bogut playing enforcer a new thing this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.
  0.6316672  0.        ]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Is Bogut playing enforcer a new thing this year
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['is', 'bogut', 'playing', 'enforcer', 'a', 'new', 'thing', 'this', 'year']
cosine_similarity: 0.9631485939025879
train_input: [0.1362763414390864, 0.9631486], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Man Bogut isnt putting up with anything tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.
  0.6316672 ]
 [0.33517574 0.47107781 0.         0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: Man Bogut isnt putting up with anything tonight
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['man', 'bogut', 'isnt', 'putting', 'up', 'with', 'anything', 'tonight']
cosine_similarity: 0.9412490725517273
train_input: [0.15064018498706508, 0.9412491], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: They have yet to call one moving screen on Bogut
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Bogut is a legit villain, sentence2: They have yet to call one moving screen on Bogut
After tokenization, sentence1: ['bogut', 'is', 'a', 'legit', 'villain'], sentence2: ['they', 'have', 'yet', 'to', 'call', 'one', 'moving', 'screen', 'on', 'bogut']
cosine_similarity: 0.9133599400520325
train_input: [0.20199309249791833, 0.91335994], train_label: 0
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: And that s another W for Jonny Bones Jones
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.         0.49922133]
 [0.50154891 0.         0.         0.50154891 0.70490949 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: And that s another W for Jonny Bones Jones
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['and', 'that', 's', 'another', 'w', 'for', 'jonny', 'bones', 'jones']
cosine_similarity: 0.9266147017478943
train_input: [0.3563004293331381, 0.9266147], train_label: 0
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Bones Jones Bones Jones in this revolutionary erotic film Bones Jones Bones Jones
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.49922133 0.35520009
  0.49922133 0.        ]
 [0.64951845 0.22821888 0.22821888 0.         0.         0.64951845
  0.         0.22821888]]
pairwise_similarity: [[1.         0.46141802]
 [0.46141802 1.        ]]
cosine_similarity: 0.46141801678807576
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Bones Jones Bones Jones in this revolutionary erotic film Bones Jones Bones Jones
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['bones', 'jones', 'bones', 'jones', 'in', 'this', 'revolutionary', 'erotic', 'film', 'bones', 'jones', 'bones', 'jones']
cosine_similarity: 0.9619611501693726
train_input: [0.46141801678807576, 0.96196115], train_label: 0
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Bones Jones is just too nice
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.         0.49922133
  0.        ]
 [0.40993715 0.         0.         0.40993715 0.57615236 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Bones Jones is just too nice
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['bones', 'jones', 'is', 'just', 'too', 'nice']
cosine_similarity: 0.9513465762138367
train_input: [0.29121941856368966, 0.9513466], train_label: 0
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Bones Jones please fuck this douchebag up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.49922133 0.35520009
  0.49922133]
 [0.40993715 0.57615236 0.         0.57615236 0.         0.40993715
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Bones Jones please fuck this douchebag up
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['bones', 'jones', 'please', 'fuck', 'this', 'douchebag', 'up']
cosine_similarity: 0.9282270073890686
train_input: [0.29121941856368966, 0.928227], train_label: 0
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Got butterflies now for Jonny Bones Jones
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.49922133 0.35520009
  0.         0.49922133]
 [0.35520009 0.49922133 0.         0.49922133 0.         0.35520009
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Got butterflies now for Jonny Bones Jones
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['got', 'butterflies', 'now', 'for', 'jonny', 'bones', 'jones']
cosine_similarity: 0.9541422128677368
train_input: [0.2523342014336961, 0.9541422], train_label: 0
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: JONNY BONES JONES IS THE TRUTH
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.         0.49922133
  0.        ]
 [0.40993715 0.         0.         0.40993715 0.57615236 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: JONNY BONES JONES IS THE TRUTH
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['jonny', 'bones', 'jones', 'is', 'the', 'truth']
cosine_similarity: 0.9750847220420837
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Johnny Bones Jones is the toughest man alive
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.         0.49922133 0.35520009
  0.49922133 0.         0.        ]
 [0.44665616 0.31779954 0.         0.44665616 0.         0.31779954
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Johnny Bones Jones is the toughest man alive
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['johnny', 'bones', 'jones', 'is', 'the', 'toughest', 'man', 'alive']
cosine_similarity: 0.9681977033615112
train_input: [0.22576484600261604, 0.9681977], train_label: 1
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Jon Bones Jones is just too good for his weight division
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.         0.37930349 0.37930349
  0.         0.53309782 0.        ]
 [0.30287281 0.42567716 0.         0.42567716 0.30287281 0.30287281
  0.42567716 0.         0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Jon Bones Jones is just too good for his weight division
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['jon', 'bones', 'jones', 'is', 'just', 'too', 'good', 'for', 'his', 'weight', 'division']
cosine_similarity: 0.955280601978302
train_input: [0.34464214103805474, 0.9552806], train_label: 1
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Jon bones jones is the definition of a badass
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.         0.53309782 0.37930349 0.37930349
  0.53309782]
 [0.53309782 0.37930349 0.53309782 0.         0.37930349 0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Jon bones jones is the definition of a badass
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['jon', 'bones', 'jones', 'is', 'the', 'definition', 'of', 'a', 'badass']
cosine_similarity: 0.9645029902458191
train_input: [0.43161341897075145, 0.964503], train_label: 1
TF_IDF_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Just heard Bones Jones won in the first round with a broken toe
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.49922133 0.35520009
  0.         0.49922133 0.         0.         0.        ]
 [0.26868528 0.37762778 0.         0.37762778 0.         0.26868528
  0.37762778 0.         0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Jon bones jones is freaking lethal, sentence2: Just heard Bones Jones won in the first round with a broken toe
After tokenization, sentence1: ['jon', 'bones', 'jones', 'is', 'freaking', 'lethal'], sentence2: ['just', 'heard', 'bones', 'jones', 'won', 'in', 'the', 'first', 'round', 'with', 'a', 'broken', 'toe']
cosine_similarity: 0.9376651048660278
train_input: [0.1908740661302035, 0.9376651], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Borussia Dortmund come to Wembley who next
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.31779954 0.44665616
  0.44665616 0.        ]
 [0.40993715 0.         0.57615236 0.         0.40993715 0.
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Borussia Dortmund come to Wembley who next
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['borussia', 'dortmund', 'come', 'to', 'wembley', 'who', 'next']
cosine_similarity: 0.9768151640892029
train_input: [0.2605556710562624, 0.97681516], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Borussia Dortmund won a game of football
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.31779954 0.44665616 0.
  0.         0.44665616 0.        ]
 [0.35520009 0.         0.         0.35520009 0.         0.49922133
  0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Borussia Dortmund won a game of football
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['borussia', 'dortmund', 'won', 'a', 'game', 'of', 'football']
cosine_similarity: 0.963997483253479
train_input: [0.22576484600261604, 0.9639975], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Congratulations to Borussia Dortmund for reaching UEFA Champions League Final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.49844628 0.35464863 0.35464863 0.35464863
  0.         0.49844628 0.         0.        ]
 [0.28986934 0.40740124 0.         0.28986934 0.28986934 0.28986934
  0.40740124 0.         0.40740124 0.40740124]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Congratulations to Borussia Dortmund for reaching UEFA Champions League Final
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'uefa', 'champions', 'league', 'final']
cosine_similarity: 0.9616427421569824
train_input: [0.41120705506761857, 0.96164274], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Real Madrid 20 Borussia Dortmund Real Out From Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.44665616 0.44665616 0.31779954
  0.44665616 0.         0.         0.44665616 0.        ]
 [0.33310232 0.23700504 0.33310232 0.         0.         0.23700504
  0.         0.33310232 0.33310232 0.         0.66620463]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Real Madrid 20 Borussia Dortmund Real Out From Champions League
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['real', 'madrid', 'borussia', 'dortmund', 'real', 'out', 'from', 'champions', 'league']
cosine_similarity: 0.9329017400741577
train_input: [0.15064018498706508, 0.93290174], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Real Madrid 20 Borussia Dortmund SSFootball
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.44665616 0.31779954 0.44665616
  0.         0.44665616 0.         0.        ]
 [0.44665616 0.31779954 0.         0.         0.31779954 0.
  0.44665616 0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Real Madrid 20 Borussia Dortmund SSFootball
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['real', 'madrid', 'borussia', 'dortmund', 'ssfootball']
cosine_similarity: 0.7741808295249939
train_input: [0.20199309249791833, 0.7741808], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Real Madrid 20 Borussia Dortmund agg 34
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.31779954 0.44665616 0.44665616
  0.31779954 0.44665616 0.         0.44665616 0.        ]
 [0.4078241  0.4078241  0.4078241  0.29017021 0.         0.
  0.29017021 0.         0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Real Madrid 20 Borussia Dortmund agg 34
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['real', 'madrid', 'borussia', 'dortmund', 'agg']
cosine_similarity: 0.7771593928337097
train_input: [0.18443191662261305, 0.7771594], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Real Madrid become the only team to beat Borussia Dortmund in ucl this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.44665616 0.31779954 0.44665616
  0.         0.44665616 0.         0.         0.         0.        ]
 [0.37762778 0.26868528 0.         0.         0.26868528 0.
  0.37762778 0.         0.37762778 0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: Real Madrid become the only team to beat Borussia Dortmund in ucl this season
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['real', 'madrid', 'become', 'the', 'only', 'team', 'to', 'beat', 'borussia', 'dortmund', 'in', 'ucl', 'this', 'season']
cosine_similarity: 0.9747750759124756
train_input: [0.1707761131901165, 0.9747751], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: UCL final goes to Bayern Munich vs Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.47042643 0.47042643 0.33471228 0.33471228
  0.         0.         0.47042643 0.         0.        ]
 [0.39166832 0.27867523 0.         0.         0.27867523 0.27867523
  0.39166832 0.39166832 0.         0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: UCL final goes to Bayern Munich vs Borussia Dortmund
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['ucl', 'final', 'goes', 'to', 'bayern', 'munich', 'vs', 'borussia', 'dortmund']
cosine_similarity: 0.8989203572273254
train_input: [0.27982806524328774, 0.89892036], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: What a game the Real Madrid vs borussia Dortmund was
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.31779954 0.44665616 0.
  0.         0.44665616 0.         0.        ]
 [0.31779954 0.         0.         0.31779954 0.         0.44665616
  0.44665616 0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: What a game the Real Madrid vs borussia Dortmund was
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['what', 'a', 'game', 'the', 'real', 'madrid', 'vs', 'borussia', 'dortmund', 'was']
cosine_similarity: 0.95228511095047
train_input: [0.20199309249791833, 0.9522851], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: were the famous borussia dortmund and were off to wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.31779954 0.         0.44665616
  0.44665616 0.        ]
 [0.40993715 0.         0.         0.40993715 0.57615236 0.
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund to qualify for the CL final, sentence2: were the famous borussia dortmund and were off to wembley
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'to', 'qualify', 'for', 'the', 'cl', 'final'], sentence2: ['were', 'the', 'famous', 'borussia', 'dortmund', 'and', 'were', 'off', 'to', 'wembley']
cosine_similarity: 0.9588389992713928
train_input: [0.2605556710562624, 0.958839], train_label: 0
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Borussia Dortmund are 138 to win the Champions League after they progressed to the final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.         0.37930349 0.37930349 0.53309782
  0.         0.53309782 0.         0.        ]
 [0.39166832 0.27867523 0.39166832 0.27867523 0.27867523 0.
  0.39166832 0.         0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Borussia Dortmund are 138 to win the Champions League after they progressed to the final
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'are', 'to', 'win', 'the', 'champions', 'league', 'after', 'they', 'progressed', 'to', 'the', 'final']
cosine_similarity: 0.9859631657600403
train_input: [0.31710746658027095, 0.98596317], train_label: 1
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Borussia Dortmund hasnt been to the final since 97
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.37930349 0.37930349 0.53309782 0.53309782]
 [0.63009934 0.44832087 0.44832087 0.44832087 0.         0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Borussia Dortmund hasnt been to the final since 97
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'hasnt', 'been', 'to', 'the', 'final', 'since']
cosine_similarity: 0.9656311869621277
train_input: [0.5101490193104813, 0.9656312], train_label: 0
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Commiserations to Real Madrid but congratulations to our friends at Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.35520009 0.49922133 0.
  0.49922133 0.         0.49922133 0.        ]
 [0.29017021 0.4078241  0.4078241  0.29017021 0.         0.4078241
  0.         0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Commiserations to Real Madrid but congratulations to our friends at Borussia Dortmund
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['commiserations', 'to', 'real', 'madrid', 'but', 'congratulations', 'to', 'our', 'friends', 'at', 'borussia', 'dortmund']
cosine_similarity: 0.9698562026023865
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Congrats to Borussia Dortmund for making it into UCL finals
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.49922133 0.         0.49922133
  0.         0.49922133 0.        ]
 [0.31779954 0.44665616 0.31779954 0.         0.44665616 0.
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Congrats to Borussia Dortmund for making it into UCL finals
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['congrats', 'to', 'borussia', 'dortmund', 'for', 'making', 'it', 'into', 'ucl', 'finals']
cosine_similarity: 0.9854220151901245
train_input: [0.22576484600261604, 0.985422], train_label: 1
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Congratulations to Borussia Dortmund it was well deserved
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.35520009 0.49922133 0.49922133
  0.49922133]
 [0.40993715 0.57615236 0.57615236 0.40993715 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: Congratulations to Borussia Dortmund it was well deserved
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['congratulations', 'to', 'borussia', 'dortmund', 'it', 'was', 'well', 'deserved']
cosine_similarity: 0.9498224258422852
train_input: [0.29121941856368966, 0.9498224], train_label: 1
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: First lose in a Champions League for Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.49922133 0.49922133 0.
  0.         0.49922133]
 [0.35520009 0.49922133 0.35520009 0.         0.         0.49922133
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: First lose in a Champions League for Borussia Dortmund
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['first', 'lose', 'in', 'a', 'champions', 'league', 'for', 'borussia', 'dortmund']
cosine_similarity: 0.9842337965965271
train_input: [0.2523342014336961, 0.9842338], train_label: 0
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: How on earth are Borussia Dortmund unbeaten in The Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.49922133 0.49922133
  0.         0.49922133 0.        ]
 [0.31779954 0.44665616 0.31779954 0.44665616 0.         0.
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: How on earth are Borussia Dortmund unbeaten in The Champions League
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['how', 'on', 'earth', 'are', 'borussia', 'dortmund', 'unbeaten', 'in', 'the', 'champions', 'league']
cosine_similarity: 0.9810735583305359
train_input: [0.22576484600261604, 0.98107356], train_label: 0
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: On the brighter side were the first team to defeat Borussia Dortmund in the Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.35520009 0.49922133
  0.49922133 0.         0.49922133 0.        ]
 [0.29017021 0.4078241  0.4078241  0.4078241  0.29017021 0.
  0.         0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: On the brighter side were the first team to defeat Borussia Dortmund in the Champions League
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['on', 'the', 'brighter', 'side', 'were', 'the', 'first', 'team', 'to', 'defeat', 'borussia', 'dortmund', 'in', 'the', 'champions', 'league']
cosine_similarity: 0.983484148979187
train_input: [0.20613696606828605, 0.98348415], train_label: 0
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: borussia Dortmund is going to win it all
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.        ]
 [0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: borussia Dortmund is going to win it all
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'is', 'going', 'to', 'win', 'it', 'all']
cosine_similarity: 0.9725326895713806
train_input: [0.29121941856368966, 0.9725327], train_label: 0
TF_IDF_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: has delighted the board of Borussia Dortmund by winning the Euro
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.35520009 0.         0.49922133
  0.49922133 0.49922133 0.        ]
 [0.44665616 0.31779954 0.44665616 0.31779954 0.44665616 0.
  0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Good match Borussia Dortmund through to the Final, sentence2: has delighted the board of Borussia Dortmund by winning the Euro
After tokenization, sentence1: ['good', 'match', 'borussia', 'dortmund', 'through', 'to', 'the', 'final'], sentence2: ['has', 'delighted', 'the', 'board', 'of', 'borussia', 'dortmund', 'by', 'winning', 'the', 'euro']
cosine_similarity: 0.9663043022155762
train_input: [0.22576484600261604, 0.9663043], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: 75 minutes Real Madrid 00 Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57615236 0.40993715 0.40993715 0.57615236
  0.         0.         0.        ]
 [0.4078241  0.4078241  0.         0.29017021 0.29017021 0.
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: 75 minutes Real Madrid 00 Borussia Dortmund
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['minutes', 'real', 'madrid', 'borussia', 'dortmund']
cosine_similarity: 0.9042272567749023
train_input: [0.23790309463326234, 0.90422726], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund has clinched their Champions League final spot
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.44832087 0.         0.         0.44832087 0.44832087
  0.         0.        ]
 [0.         0.30287281 0.42567716 0.42567716 0.30287281 0.30287281
  0.42567716 0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund has clinched their Champions League final spot
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'has', 'clinched', 'their', 'champions', 'league', 'final', 'spot']
cosine_similarity: 0.9420747756958008
train_input: [0.4073526042885674, 0.9420748], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund won a game of football
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.57615236 0.         0.
  0.        ]
 [0.         0.35520009 0.35520009 0.         0.49922133 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund won a game of football
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'won', 'a', 'game', 'of', 'football']
cosine_similarity: 0.9715255498886108
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: But it s Borussia Dortmund whose heading to Wembley Park
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.57615236 0.         0.
  0.        ]
 [0.         0.35520009 0.35520009 0.         0.49922133 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: But it s Borussia Dortmund whose heading to Wembley Park
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['but', 'it', 's', 'borussia', 'dortmund', 'whose', 'heading', 'to', 'wembley', 'park']
cosine_similarity: 0.950097918510437
train_input: [0.29121941856368966, 0.9500979], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Congrats to Borussia Dortmund for reaching this year s Champions League final at Wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.44832087 0.         0.         0.44832087 0.44832087
  0.         0.         0.         0.        ]
 [0.         0.25948224 0.36469323 0.36469323 0.25948224 0.25948224
  0.36469323 0.36469323 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Congrats to Borussia Dortmund for reaching this year s Champions League final at Wembley
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['congrats', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'this', 'year', 's', 'champions', 'league', 'final', 'at', 'wembley']
cosine_similarity: 0.9673977494239807
train_input: [0.3489939079552687, 0.96739775], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Congratulation Borussia Dortmund and the BVB fans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.         0.40993715 0.
  0.57615236]
 [0.         0.35520009 0.49922133 0.49922133 0.35520009 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Congratulation Borussia Dortmund and the BVB fans
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['congratulation', 'borussia', 'dortmund', 'and', 'the', 'bvb', 'fans']
cosine_similarity: 0.9333312511444092
train_input: [0.29121941856368966, 0.93333125], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Congratulations Borussia Dortmund s going to Wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.40993715 0.57615236 0.
  0.        ]
 [0.         0.35520009 0.49922133 0.35520009 0.         0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Congratulations Borussia Dortmund s going to Wembley
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['congratulations', 'borussia', 'dortmund', 's', 'going', 'to', 'wembley']
cosine_similarity: 0.9623847603797913
train_input: [0.29121941856368966, 0.96238476], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Real Madrid efforts are not enough as Cinderella Borussia Dortmund advances to the Champions League Final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.         0.44832087 0.         0.         0.44832087
  0.         0.44832087 0.         0.         0.        ]
 [0.         0.34261985 0.24377685 0.34261985 0.34261985 0.24377685
  0.34261985 0.24377685 0.34261985 0.34261985 0.34261985]]
pairwise_similarity: [[1.         0.32787075]
 [0.32787075 1.        ]]
cosine_similarity: 0.3278707471841718
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Real Madrid efforts are not enough as Cinderella Borussia Dortmund advances to the Champions League Final
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['real', 'madrid', 'efforts', 'are', 'not', 'enough', 'as', 'cinderella', 'borussia', 'dortmund', 'advances', 'to', 'the', 'champions', 'league', 'final']
cosine_similarity: 0.9725430011749268
train_input: [0.3278707471841718, 0.972543], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: This is the first defeat for Borussia Dortmund in the Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.         0.40993715 0.57615236
  0.        ]
 [0.         0.35520009 0.49922133 0.49922133 0.35520009 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: This is the first defeat for Borussia Dortmund in the Champions League
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['this', 'is', 'the', 'first', 'defeat', 'for', 'borussia', 'dortmund', 'in', 'the', 'champions', 'league']
cosine_similarity: 0.9616261124610901
train_input: [0.29121941856368966, 0.9616261], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: What a good team Borussia Dortmund is
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.57615236 0.         0.        ]
 [0.         0.40993715 0.40993715 0.         0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: What a good team Borussia Dortmund is
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['what', 'a', 'good', 'team', 'borussia', 'dortmund', 'is']
cosine_similarity: 0.937815248966217
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: 40000 Borussia Dortmund fans have travelled to Madrid only 10000 have tickets for the match
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57615236 0.40993715 0.40993715 0.
  0.57615236 0.         0.         0.         0.        ]
 [0.35327777 0.35327777 0.         0.25136004 0.25136004 0.35327777
  0.         0.35327777 0.35327777 0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: 40000 Borussia Dortmund fans have travelled to Madrid only 10000 have tickets for the match
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'fans', 'have', 'travelled', 'to', 'madrid', 'only', 'have', 'tickets', 'for', 'the', 'match']
cosine_similarity: 0.947475790977478
train_input: [0.20608363501393823, 0.9474758], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund had the winning strategy played brilliant football
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.40993715 0.57615236 0.
  0.         0.         0.        ]
 [0.         0.29017021 0.4078241  0.29017021 0.         0.4078241
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund had the winning strategy played brilliant football
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'had', 'the', 'winning', 'strategy', 'played', 'brilliant', 'football']
cosine_similarity: 0.9434770345687866
train_input: [0.23790309463326234, 0.94347703], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund hold off Real Madrid to reach final at Wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.44832087 0.44832087 0.44832087 0.         0.
  0.         0.         0.        ]
 [0.         0.27867523 0.27867523 0.27867523 0.39166832 0.39166832
  0.39166832 0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund hold off Real Madrid to reach final at Wembley
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'hold', 'off', 'real', 'madrid', 'to', 'reach', 'final', 'at', 'wembley']
cosine_similarity: 0.964009165763855
train_input: [0.3748077700589726, 0.96400917], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund ousts Real Madrid reaches Champions League final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.44832087 0.         0.44832087 0.44832087 0.
  0.         0.         0.         0.        ]
 [0.         0.25948224 0.36469323 0.25948224 0.25948224 0.36469323
  0.36469323 0.36469323 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund ousts Real Madrid reaches Champions League final
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'ousts', 'real', 'madrid', 'reaches', 'champions', 'league', 'final']
cosine_similarity: 0.9015570878982544
train_input: [0.3489939079552687, 0.9015571], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund secured their place in the Champions League final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.44832087 0.         0.44832087 0.44832087 0.
  0.         0.        ]
 [0.         0.30287281 0.42567716 0.30287281 0.30287281 0.42567716
  0.42567716 0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund secured their place in the Champions League final
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'secured', 'their', 'place', 'in', 'the', 'champions', 'league', 'final']
cosine_similarity: 0.9668667316436768
train_input: [0.4073526042885674, 0.96686673], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund through to Champions League final after a 4
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.44832087 0.         0.44832087 0.44832087 0.        ]
 [0.         0.37930349 0.53309782 0.37930349 0.37930349 0.53309782]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Borussia Dortmund through to Champions League final after a 4
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'through', 'to', 'champions', 'league', 'final', 'after', 'a']
cosine_similarity: 0.9752163290977478
train_input: [0.5101490193104813, 0.9752163], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Im watching the Real Madrid vs Borussia Dortmund match on ITV1
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.57615236 0.         0.
  0.         0.         0.         0.         0.        ]
 [0.         0.25136004 0.25136004 0.         0.35327777 0.35327777
  0.35327777 0.35327777 0.35327777 0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Im watching the Real Madrid vs Borussia Dortmund match on ITV1
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['im', 'watching', 'the', 'real', 'madrid', 'vs', 'borussia', 'dortmund', 'match', 'on']
cosine_similarity: 0.9476281404495239
train_input: [0.20608363501393823, 0.94762814], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Neven Subotic and Borussia Dortmund are off to the Champions League final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.44832087 0.         0.44832087 0.44832087 0.
  0.         0.        ]
 [0.         0.30287281 0.42567716 0.30287281 0.30287281 0.42567716
  0.42567716 0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Neven Subotic and Borussia Dortmund are off to the Champions League final
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['neven', 'subotic', 'and', 'borussia', 'dortmund', 'are', 'off', 'to', 'the', 'champions', 'league', 'final']
cosine_similarity: 0.9729804992675781
train_input: [0.4073526042885674, 0.9729805], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Real Madrid s efforts are not enough as Cinderella Borussia Dortmund advances to the Champions League Final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.         0.44832087 0.         0.         0.44832087
  0.         0.44832087 0.         0.         0.        ]
 [0.         0.34261985 0.24377685 0.34261985 0.34261985 0.24377685
  0.34261985 0.24377685 0.34261985 0.34261985 0.34261985]]
pairwise_similarity: [[1.         0.32787075]
 [0.32787075 1.        ]]
cosine_similarity: 0.3278707471841718
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: Real Madrid s efforts are not enough as Cinderella Borussia Dortmund advances to the Champions League Final
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['real', 'madrid', 's', 'efforts', 'are', 'not', 'enough', 'as', 'cinderella', 'borussia', 'dortmund', 'advances', 'to', 'the', 'champions', 'league', 'final']
cosine_similarity: 0.9727205038070679
train_input: [0.3278707471841718, 0.9727205], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: What a lucky team Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.57615236 0.         0.        ]
 [0.         0.40993715 0.40993715 0.         0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund advanced to the final, sentence2: What a lucky team Borussia Dortmund
After tokenization, sentence1: ['borussia', 'dortmund', 'advanced', 'to', 'the', 'final'], sentence2: ['what', 'a', 'lucky', 'team', 'borussia', 'dortmund']
cosine_similarity: 0.9381346106529236
train_input: [0.3360969272762575, 0.9381346], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Borussia Dortmund VS Bayern Munchen in the Champions League final at Wembley England
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.         0.37930349 0.         0.53309782 0.37930349
  0.         0.37930349 0.         0.         0.         0.        ]
 [0.         0.34261985 0.24377685 0.34261985 0.         0.24377685
  0.34261985 0.24377685 0.34261985 0.34261985 0.34261985 0.34261985]]
pairwise_similarity: [[1.         0.27739623]
 [0.27739623 1.        ]]
cosine_similarity: 0.27739622897624144
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Borussia Dortmund VS Bayern Munchen in the Champions League final at Wembley England
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'vs', 'bayern', 'munchen', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'england']
cosine_similarity: 0.9214572906494141
train_input: [0.27739622897624144, 0.9214573], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Borussia Dortmund had the winning strategy played brilliant football
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.49922133 0.35520009 0.49922133
  0.         0.         0.         0.        ]
 [0.         0.29017021 0.4078241  0.         0.29017021 0.
  0.4078241  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Borussia Dortmund had the winning strategy played brilliant football
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'had', 'the', 'winning', 'strategy', 'played', 'brilliant', 'football']
cosine_similarity: 0.9418119192123413
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Borussia Dortmund wait for me at wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.35520009 0.49922133 0.
  0.        ]
 [0.         0.40993715 0.         0.40993715 0.         0.57615236
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Borussia Dortmund wait for me at wembley
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'wait', 'for', 'me', 'at', 'wembley']
cosine_similarity: 0.9673497676849365
train_input: [0.29121941856368966, 0.96734977], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Congratulations borussia dortmund BVB you goes to final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57496187 0.4090901  0.         0.4090901  0.4090901  0.4090901
  0.        ]
 [0.         0.35464863 0.49844628 0.35464863 0.35464863 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Congratulations borussia dortmund BVB you goes to final
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['congratulations', 'borussia', 'dortmund', 'bvb', 'you', 'goes', 'to', 'final']
cosine_similarity: 0.9797795414924622
train_input: [0.5803329846765685, 0.97977954], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: FINAL Real Madrid 2 Borussia Dortmund 0
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.37930349 0.53309782 0.37930349 0.37930349 0.
  0.        ]
 [0.         0.37930349 0.         0.37930349 0.37930349 0.53309782
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: FINAL Real Madrid 2 Borussia Dortmund 0
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['final', 'real', 'madrid', 'borussia', 'dortmund']
cosine_similarity: 0.8095017671585083
train_input: [0.43161341897075145, 0.80950177], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: For the first time Borussia Dortmund lose at champions league
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.49922133 0.35520009 0.49922133
  0.         0.         0.        ]
 [0.         0.31779954 0.44665616 0.         0.31779954 0.
  0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: For the first time Borussia Dortmund lose at champions league
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['for', 'the', 'first', 'time', 'borussia', 'dortmund', 'lose', 'at', 'champions', 'league']
cosine_similarity: 0.9825491309165955
train_input: [0.22576484600261604, 0.98254913], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: On the brighter side were the first team to defeat Borussia Dortmund in the Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.         0.49922133 0.
  0.35520009 0.49922133 0.         0.        ]
 [0.         0.29017021 0.4078241  0.4078241  0.         0.4078241
  0.29017021 0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: On the brighter side were the first team to defeat Borussia Dortmund in the Champions League
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['on', 'the', 'brighter', 'side', 'were', 'the', 'first', 'team', 'to', 'defeat', 'borussia', 'dortmund', 'in', 'the', 'champions', 'league']
cosine_similarity: 0.9683457016944885
train_input: [0.20613696606828605, 0.9683457], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Real Madrid first team to beat Borussia Dortmund in the Champions League this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.35520009 0.         0.49922133 0.35520009
  0.49922133 0.         0.         0.         0.         0.        ]
 [0.         0.35327777 0.25136004 0.35327777 0.         0.25136004
  0.         0.35327777 0.35327777 0.35327777 0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Real Madrid first team to beat Borussia Dortmund in the Champions League this season
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['real', 'madrid', 'first', 'team', 'to', 'beat', 'borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'this', 'season']
cosine_similarity: 0.9690977334976196
train_input: [0.17856621555757476, 0.96909773], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Real Madrid knocked out by Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.35520009 0.49922133 0.
  0.         0.        ]
 [0.         0.35520009 0.         0.35520009 0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: Real Madrid knocked out by Borussia Dortmund
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['real', 'madrid', 'knocked', 'out', 'by', 'borussia', 'dortmund']
cosine_similarity: 0.9259200692176819
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: well done to borussia dortmund well deserved
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.         0.35520009 0.49922133]
 [0.         0.50154891 0.         0.70490949 0.50154891 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for advancing to the final, sentence2: well done to borussia dortmund well deserved
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'advancing', 'to', 'the', 'final'], sentence2: ['well', 'done', 'to', 'borussia', 'dortmund', 'well', 'deserved']
cosine_similarity: 0.9438656568527222
train_input: [0.3563004293331381, 0.94386566], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Borussia Dortmund fought for the right to compete with the big boys again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.         0.4078241  0.         0.29017021
  0.4078241  0.         0.4078241  0.         0.4078241  0.4078241 ]
 [0.4078241  0.29017021 0.4078241  0.         0.4078241  0.29017021
  0.         0.4078241  0.         0.4078241  0.         0.        ]]
pairwise_similarity: [[1.        0.1683975]
 [0.1683975 1.       ]]
cosine_similarity: 0.16839750037215276
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Borussia Dortmund fought for the right to compete with the big boys again
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['borussia', 'dortmund', 'fought', 'for', 'the', 'right', 'to', 'compete', 'with', 'the', 'big', 'boys', 'again']
cosine_similarity: 0.8818984031677246
train_input: [0.16839750037215276, 0.8818984], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Borussia Dortmund wait for me at wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.30287281 0.42567716 0.42567716 0.42567716
  0.         0.30287281]
 [0.44832087 0.         0.44832087 0.         0.         0.
  0.63009934 0.44832087]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Borussia Dortmund wait for me at wembley
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['borussia', 'dortmund', 'wait', 'for', 'me', 'at', 'wembley']
cosine_similarity: 0.9421269297599792
train_input: [0.4073526042885674, 0.9421269], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: But Borussia Dortmund deserves the win
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.         0.29017021 0.4078241  0.4078241
  0.4078241  0.4078241  0.        ]
 [0.40993715 0.         0.57615236 0.40993715 0.         0.
  0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: But Borussia Dortmund deserves the win
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['but', 'borussia', 'dortmund', 'deserves', 'the', 'win']
cosine_similarity: 0.9214532375335693
train_input: [0.23790309463326234, 0.92145324], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: But it s Borussia Dortmund whose heading to Wembley Park
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.30287281 0.42567716 0.         0.42567716
  0.         0.42567716 0.30287281]
 [0.37930349 0.         0.37930349 0.         0.53309782 0.
  0.53309782 0.         0.37930349]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: But it s Borussia Dortmund whose heading to Wembley Park
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['but', 'it', 's', 'borussia', 'dortmund', 'whose', 'heading', 'to', 'wembley', 'park']
cosine_similarity: 0.9338523149490356
train_input: [0.34464214103805474, 0.9338523], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Congratulations to Borussia Dortmund for making it to final of this season s Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.         0.33425073 0.33425073 0.33425073
  0.         0.         0.46977774 0.46977774]
 [0.30253071 0.30253071 0.42519636 0.30253071 0.30253071 0.30253071
  0.42519636 0.42519636 0.         0.        ]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739692
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Congratulations to Borussia Dortmund for making it to final of this season s Champions League
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'making', 'it', 'to', 'final', 'of', 'this', 'season', 's', 'champions', 'league']
cosine_similarity: 0.9342791438102722
train_input: [0.5056055588739692, 0.93427914], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Gooooooooood Bye Real Madrid Good job Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.         0.4078241  0.29017021 0.4078241  0.
  0.         0.         0.4078241  0.         0.         0.4078241
  0.4078241 ]
 [0.26868528 0.37762778 0.         0.26868528 0.         0.37762778
  0.37762778 0.37762778 0.         0.37762778 0.37762778 0.
  0.        ]]
pairwise_similarity: [[1.         0.15592893]
 [0.15592893 1.        ]]
cosine_similarity: 0.15592892548708362
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Gooooooooood Bye Real Madrid Good job Borussia Dortmund
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['gooooooooood', 'bye', 'real', 'madrid', 'good', 'job', 'borussia', 'dortmund']
cosine_similarity: 0.8971080183982849
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Im watching the Real Madrid vs Borussia Dortmund match on ITV1
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.29017021 0.4078241  0.         0.
  0.4078241  0.         0.         0.         0.4078241  0.
  0.         0.4078241 ]
 [0.25136004 0.         0.25136004 0.         0.35327777 0.35327777
  0.         0.35327777 0.35327777 0.35327777 0.         0.35327777
  0.35327777 0.        ]]
pairwise_similarity: [[1.         0.14587439]
 [0.14587439 1.        ]]
cosine_similarity: 0.14587439082056453
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Im watching the Real Madrid vs Borussia Dortmund match on ITV1
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['im', 'watching', 'the', 'real', 'madrid', 'vs', 'borussia', 'dortmund', 'match', 'on']
cosine_similarity: 0.956745982170105
train_input: [0.14587439082056453, 0.956746], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Real Madrid 20 Borussia Dortmund Real Out From Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.3174044  0.3174044  0.3174044  0.44610081 0.3174044
  0.         0.         0.44610081 0.44610081]
 [0.35300279 0.25116439 0.25116439 0.25116439 0.         0.25116439
  0.35300279 0.70600557 0.         0.        ]]
pairwise_similarity: [[1.         0.31888273]
 [0.31888273 1.        ]]
cosine_similarity: 0.3188827274930885
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Real Madrid 20 Borussia Dortmund Real Out From Champions League
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['real', 'madrid', 'borussia', 'dortmund', 'real', 'out', 'from', 'champions', 'league']
cosine_similarity: 0.9642704725265503
train_input: [0.3188827274930885, 0.9642705], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Wow I missed the Borussia Dortmund vs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.29017021 0.4078241  0.4078241  0.
  0.4078241  0.         0.4078241  0.        ]
 [0.35520009 0.         0.35520009 0.         0.         0.49922133
  0.         0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: Wow I missed the Borussia Dortmund vs
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['wow', 'i', 'missed', 'the', 'borussia', 'dortmund', 'vs']
cosine_similarity: 0.9388042092323303
train_input: [0.20613696606828605, 0.9388042], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: rooting for borussia dortmund to win the champions league this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.3174044  0.44610081 0.3174044  0.
  0.         0.44610081 0.44610081 0.        ]
 [0.3174044  0.3174044  0.3174044  0.         0.3174044  0.44610081
  0.44610081 0.         0.         0.44610081]]
pairwise_similarity: [[1.         0.40298221]
 [0.40298221 1.        ]]
cosine_similarity: 0.40298220897396103
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund in the Champions League final at Wembley Stadium, sentence2: rooting for borussia dortmund to win the champions league this season
After tokenization, sentence1: ['borussia', 'dortmund', 'in', 'the', 'champions', 'league', 'final', 'at', 'wembley', 'stadium'], sentence2: ['rooting', 'for', 'borussia', 'dortmund', 'to', 'win', 'the', 'champions', 'league', 'this', 'season']
cosine_similarity: 0.9567018151283264
train_input: [0.40298220897396103, 0.9567018], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: 40000 Borussia Dortmund fans travelled to Madrid only 10000 of those had tickets for the match
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.26868528 0.37762778 0.37762778 0.26868528
  0.         0.37762778 0.37762778 0.         0.         0.37762778
  0.         0.         0.37762778]
 [0.35327777 0.35327777 0.25136004 0.         0.         0.25136004
  0.35327777 0.         0.         0.35327777 0.35327777 0.
  0.35327777 0.35327777 0.        ]]
pairwise_similarity: [[1.         0.13507348]
 [0.13507348 1.        ]]
cosine_similarity: 0.1350734836708713
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: 40000 Borussia Dortmund fans travelled to Madrid only 10000 of those had tickets for the match
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['borussia', 'dortmund', 'fans', 'travelled', 'to', 'madrid', 'only', 'of', 'those', 'had', 'tickets', 'for', 'the', 'match']
cosine_similarity: 0.9562873244285583
train_input: [0.1350734836708713, 0.9562873], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Borussia Dortmund in the final they never said
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27867523 0.39166832 0.39166832 0.27867523 0.27867523 0.39166832
  0.39166832 0.         0.39166832]
 [0.44832087 0.         0.         0.44832087 0.44832087 0.
  0.         0.63009934 0.        ]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Borussia Dortmund in the final they never said
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['borussia', 'dortmund', 'in', 'the', 'final', 'they', 'never', 'said']
cosine_similarity: 0.9377576112747192
train_input: [0.3748077700589726, 0.9377576], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Borussia Dortmund ousts Real Madrid reaches Champions League final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30253071 0.30253071 0.42519636 0.30253071 0.30253071 0.30253071
  0.         0.         0.         0.42519636 0.         0.42519636]
 [0.27840869 0.27840869 0.         0.27840869 0.27840869 0.27840869
  0.39129369 0.39129369 0.39129369 0.         0.39129369 0.        ]]
pairwise_similarity: [[1.         0.42113589]
 [0.42113589 1.        ]]
cosine_similarity: 0.42113589132819573
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Borussia Dortmund ousts Real Madrid reaches Champions League final
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['borussia', 'dortmund', 'ousts', 'real', 'madrid', 'reaches', 'champions', 'league', 'final']
cosine_similarity: 0.9353778958320618
train_input: [0.42113589132819573, 0.9353779], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Borussia Dortmund wait for me at wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.26868528 0.37762778 0.37762778 0.26868528 0.37762778 0.37762778
  0.37762778 0.37762778 0.         0.        ]
 [0.40993715 0.         0.         0.40993715 0.         0.
  0.         0.         0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Borussia Dortmund wait for me at wembley
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['borussia', 'dortmund', 'wait', 'for', 'me', 'at', 'wembley']
cosine_similarity: 0.949643075466156
train_input: [0.2202881505618297, 0.9496431], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Congratulations Borussia Dortmund for the 2nd CHL Final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.28986934 0.40740124 0.         0.28986934 0.28986934
  0.28986934 0.40740124 0.40740124 0.40740124]
 [0.49844628 0.35464863 0.         0.49844628 0.35464863 0.35464863
  0.35464863 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Congratulations Borussia Dortmund for the 2nd CHL Final
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['congratulations', 'borussia', 'dortmund', 'for', 'the', 'chl', 'final']
cosine_similarity: 0.9841272234916687
train_input: [0.41120705506761857, 0.9841272], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Cue the Borussia Dortmund glory hunters
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.26868528 0.37762778 0.37762778 0.         0.26868528 0.37762778
  0.         0.         0.37762778 0.37762778 0.37762778]
 [0.35520009 0.         0.         0.49922133 0.35520009 0.
  0.49922133 0.49922133 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Cue the Borussia Dortmund glory hunters
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['cue', 'the', 'borussia', 'dortmund', 'glory', 'hunters']
cosine_similarity: 0.9279981851577759
train_input: [0.1908740661302035, 0.9279982], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: I am watching Borussia Dortmund vs Real Madrid
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.26868528 0.37762778 0.37762778 0.26868528 0.37762778 0.37762778
  0.         0.37762778 0.         0.37762778 0.         0.        ]
 [0.31779954 0.         0.         0.31779954 0.         0.
  0.44665616 0.         0.44665616 0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: I am watching Borussia Dortmund vs Real Madrid
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['i', 'am', 'watching', 'borussia', 'dortmund', 'vs', 'real', 'madrid']
cosine_similarity: 0.9428084492683411
train_input: [0.1707761131901165, 0.94280845], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Real Madrid over Borussia Dortmund 20
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.26868528 0.37762778 0.37762778 0.26868528 0.37762778
  0.37762778 0.         0.37762778 0.         0.37762778]
 [0.49922133 0.35520009 0.         0.         0.35520009 0.
  0.         0.49922133 0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: Real Madrid over Borussia Dortmund 20
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['real', 'madrid', 'over', 'borussia', 'dortmund']
cosine_similarity: 0.921847403049469
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: This is the first defeat for Borussia Dortmund in the Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.28986934 0.28986934 0.40740124 0.         0.28986934 0.40740124
  0.28986934 0.40740124 0.40740124]
 [0.4090901  0.4090901  0.         0.57496187 0.4090901  0.
  0.4090901  0.         0.        ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: This is the first defeat for Borussia Dortmund in the Champions League
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['this', 'is', 'the', 'first', 'defeat', 'for', 'borussia', 'dortmund', 'in', 'the', 'champions', 'league']
cosine_similarity: 0.9658352136611938
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: borussia dortmund deserved to win after the way they played in the first leg
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.26868528 0.37762778 0.37762778 0.         0.26868528 0.37762778
  0.37762778 0.         0.         0.37762778 0.37762778 0.
  0.        ]
 [0.29017021 0.         0.         0.4078241  0.29017021 0.
  0.         0.4078241  0.4078241  0.         0.         0.4078241
  0.4078241 ]]
pairwise_similarity: [[1.         0.15592893]
 [0.15592893 1.        ]]
cosine_similarity: 0.15592892548708362
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for reaching the UEFA Champions League final, sentence2: borussia dortmund deserved to win after the way they played in the first leg
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'reaching', 'the', 'uefa', 'champions', 'league', 'final'], sentence2: ['borussia', 'dortmund', 'deserved', 'to', 'win', 'after', 'the', 'way', 'they', 'played', 'in', 'the', 'first', 'leg']
cosine_similarity: 0.9324710369110107
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: All in all Borussia Dortmund deserved it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.40993715 0.57615236]
 [0.50154891 0.70490949 0.         0.50154891 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: All in all Borussia Dortmund deserved it
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['all', 'in', 'all', 'borussia', 'dortmund', 'deserved', 'it']
cosine_similarity: 0.924966037273407
train_input: [0.4112070550676187, 0.92496604], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Big congrats to Borussia Dortmund on progressing to UCL final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.         0.63009934 0.44832087 0.44832087
  0.         0.        ]
 [0.42567716 0.30287281 0.42567716 0.         0.30287281 0.30287281
  0.42567716 0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Big congrats to Borussia Dortmund on progressing to UCL final
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['big', 'congrats', 'to', 'borussia', 'dortmund', 'on', 'progressing', 'to', 'ucl', 'final']
cosine_similarity: 0.9593790769577026
train_input: [0.4073526042885674, 0.9593791], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Borussia Dortmund fans celebrate reaching the Champions League final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.63009934 0.44832087 0.
  0.44832087 0.         0.        ]
 [0.27867523 0.39166832 0.39166832 0.         0.27867523 0.39166832
  0.27867523 0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Borussia Dortmund fans celebrate reaching the Champions League final
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'fans', 'celebrate', 'reaching', 'the', 'champions', 'league', 'final']
cosine_similarity: 0.970853865146637
train_input: [0.3748077700589726, 0.97085387], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Borussia Dortmund will play the ChampionsLeague Final in Wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.63009934 0.44832087 0.44832087 0.
  0.        ]
 [0.33471228 0.47042643 0.         0.33471228 0.33471228 0.47042643
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Borussia Dortmund will play the ChampionsLeague Final in Wembley
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'will', 'play', 'the', 'championsleague', 'final', 'in', 'wembley']
cosine_similarity: 0.9848218560218811
train_input: [0.4501755023269898, 0.98482186], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Congrats for Borussia Dortmund respect and salute for El Real
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.40993715 0.         0.57615236
  0.         0.         0.        ]
 [0.29017021 0.4078241  0.         0.29017021 0.4078241  0.
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Congrats for Borussia Dortmund respect and salute for El Real
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['congrats', 'for', 'borussia', 'dortmund', 'respect', 'and', 'salute', 'for', 'el', 'real']
cosine_similarity: 0.9126846194267273
train_input: [0.23790309463326234, 0.9126846], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Congratulations Borussia Dortmund for the 2nd CHL Final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.         0.         0.63009934 0.44832087
  0.44832087]
 [0.47042643 0.33471228 0.47042643 0.47042643 0.         0.33471228
  0.33471228]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Congratulations Borussia Dortmund for the 2nd CHL Final
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['congratulations', 'borussia', 'dortmund', 'for', 'the', 'chl', 'final']
cosine_similarity: 0.9716259241104126
train_input: [0.4501755023269898, 0.9716259], train_label: 1
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: First lose in a Champions League for Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.40993715 0.57615236 0.
  0.        ]
 [0.35520009 0.49922133 0.         0.35520009 0.         0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: First lose in a Champions League for Borussia Dortmund
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['first', 'lose', 'in', 'a', 'champions', 'league', 'for', 'borussia', 'dortmund']
cosine_similarity: 0.9723429679870605
train_input: [0.29121941856368966, 0.97234297], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: If Barcelona doesnt qualify for the finals I want Borussia Dortmund to win the champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.         0.40993715
  0.57615236 0.         0.         0.         0.         0.        ]
 [0.33310232 0.23700504 0.33310232 0.         0.33310232 0.23700504
  0.         0.33310232 0.33310232 0.33310232 0.33310232 0.33310232]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858148
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: If Barcelona doesnt qualify for the finals I want Borussia Dortmund to win the champions League
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['if', 'barcelona', 'doesnt', 'qualify', 'for', 'the', 'finals', 'i', 'want', 'borussia', 'dortmund', 'to', 'win', 'the', 'champions', 'league']
cosine_similarity: 0.9321361184120178
train_input: [0.19431434016858148, 0.9321361], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Real Madrid are the first team to defeat Borussia Dortmund this season in the Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.40993715 0.57615236
  0.         0.         0.         0.         0.        ]
 [0.25136004 0.35327777 0.35327777 0.         0.25136004 0.
  0.35327777 0.35327777 0.35327777 0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: Real Madrid are the first team to defeat Borussia Dortmund this season in the Champions League
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['real', 'madrid', 'are', 'the', 'first', 'team', 'to', 'defeat', 'borussia', 'dortmund', 'this', 'season', 'in', 'the', 'champions', 'league']
cosine_similarity: 0.9534202218055725
train_input: [0.20608363501393823, 0.9534202], train_label: 0
TF_IDF_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: rooting for borussia dortmund to win the champions league this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.40993715 0.57615236 0.
  0.         0.         0.        ]
 [0.29017021 0.4078241  0.         0.29017021 0.         0.4078241
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Borussia Dortmund deservedly in the final, sentence2: rooting for borussia dortmund to win the champions league this season
After tokenization, sentence1: ['borussia', 'dortmund', 'deservedly', 'in', 'the', 'final'], sentence2: ['rooting', 'for', 'borussia', 'dortmund', 'to', 'win', 'the', 'champions', 'league', 'this', 'season']
cosine_similarity: 0.9561189413070679
train_input: [0.23790309463326234, 0.95611894], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Borussia Dortmund had 40000 fans in Madrid tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.35520009 0.         0.49922133
  0.49922133 0.         0.        ]
 [0.44665616 0.31779954 0.         0.31779954 0.44665616 0.
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Borussia Dortmund had 40000 fans in Madrid tonight
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'had', 'fans', 'in', 'madrid', 'tonight']
cosine_similarity: 0.928524374961853
train_input: [0.22576484600261604, 0.9285244], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Borussia Dortmund survived a late Real Madrid fightback to book their place in the Champi
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.49922133 0.35520009 0.
  0.49922133 0.49922133 0.         0.         0.         0.
  0.        ]
 [0.33310232 0.23700504 0.33310232 0.         0.23700504 0.33310232
  0.         0.         0.33310232 0.33310232 0.33310232 0.33310232
  0.33310232]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Borussia Dortmund survived a late Real Madrid fightback to book their place in the Champi
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'survived', 'a', 'late', 'real', 'madrid', 'fightback', 'to', 'book', 'their', 'place', 'in', 'the', 'champi']
cosine_similarity: 0.9772699475288391
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Borussia Dortmund through to the final after 16 years BVB
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.         0.53309782 0.37930349 0.37930349
  0.53309782 0.        ]
 [0.47042643 0.33471228 0.47042643 0.         0.33471228 0.33471228
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Borussia Dortmund through to the final after 16 years BVB
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['borussia', 'dortmund', 'through', 'to', 'the', 'final', 'after', 'years', 'bvb']
cosine_similarity: 0.9877042174339294
train_input: [0.38087260847594373, 0.9877042], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Congrats to Borussia Dortmund Players Coach Klopp and the fans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.35520009 0.
  0.49922133 0.49922133 0.         0.        ]
 [0.29017021 0.4078241  0.4078241  0.         0.29017021 0.4078241
  0.         0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Congrats to Borussia Dortmund Players Coach Klopp and the fans
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['congrats', 'to', 'borussia', 'dortmund', 'players', 'coach', 'klopp', 'and', 'the', 'fans']
cosine_similarity: 0.9627355337142944
train_input: [0.20613696606828605, 0.96273553], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Congrats to Borussia Dortmund club and there fans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.35520009 0.
  0.49922133 0.49922133]
 [0.35520009 0.49922133 0.49922133 0.         0.35520009 0.49922133
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Congrats to Borussia Dortmund club and there fans
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['congrats', 'to', 'borussia', 'dortmund', 'club', 'and', 'there', 'fans']
cosine_similarity: 0.9791777729988098
train_input: [0.2523342014336961, 0.9791778], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: First defeat of this champions league season for Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.35520009 0.49922133
  0.49922133 0.         0.        ]
 [0.31779954 0.44665616 0.         0.44665616 0.31779954 0.
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: First defeat of this champions league season for Borussia Dortmund
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['first', 'defeat', 'of', 'this', 'champions', 'league', 'season', 'for', 'borussia', 'dortmund']
cosine_similarity: 0.9645798802375793
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Real Madrid Manchester City Borussia Dortmund y Ajax
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.49922133 0.35520009 0.49922133
  0.49922133 0.         0.         0.        ]
 [0.4078241  0.29017021 0.4078241  0.         0.29017021 0.
  0.         0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: Real Madrid Manchester City Borussia Dortmund y Ajax
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['real', 'madrid', 'manchester', 'city', 'borussia', 'dortmund', 'y', 'ajax']
cosine_similarity: 0.7831704020500183
train_input: [0.20613696606828605, 0.7831704], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: The difference between Real Madrid Borussia Dortmund over two legs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.35520009 0.49922133 0.49922133
  0.         0.         0.        ]
 [0.31779954 0.         0.44665616 0.31779954 0.         0.
  0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: The difference between Real Madrid Borussia Dortmund over two legs
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['the', 'difference', 'between', 'real', 'madrid', 'borussia', 'dortmund', 'over', 'two', 'legs']
cosine_similarity: 0.9551735520362854
train_input: [0.22576484600261604, 0.95517355], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: The yellow wall and Borussia Dortmund are in the final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.37930349 0.37930349 0.53309782 0.
  0.        ]
 [0.37930349 0.         0.37930349 0.37930349 0.         0.53309782
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: The yellow wall and Borussia Dortmund are in the final
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['the', 'yellow', 'wall', 'and', 'borussia', 'dortmund', 'are', 'in', 'the', 'final']
cosine_similarity: 0.966568112373352
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: well done to borussia dortmund well deserved
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.35520009 0.49922133 0.49922133]
 [0.50154891 0.         0.70490949 0.50154891 0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund for getting to the final, sentence2: well done to borussia dortmund well deserved
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'for', 'getting', 'to', 'the', 'final'], sentence2: ['well', 'done', 'to', 'borussia', 'dortmund', 'well', 'deserved']
cosine_similarity: 0.9595705270767212
train_input: [0.3563004293331381, 0.9595705], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: 40000 Borussia Dortmund fans traveled to Madrid and only 8000 had tickets
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.         0.         0.26868528 0.37762778 0.37762778
  0.26868528 0.         0.37762778 0.37762778 0.         0.37762778
  0.         0.        ]
 [0.         0.37762778 0.37762778 0.26868528 0.         0.
  0.26868528 0.37762778 0.         0.         0.37762778 0.
  0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.14438355527738672
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: 40000 Borussia Dortmund fans traveled to Madrid and only 8000 had tickets
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['borussia', 'dortmund', 'fans', 'traveled', 'to', 'madrid', 'and', 'only', 'had', 'tickets']
cosine_similarity: 0.9659036993980408
train_input: [0.14438355527738672, 0.9659037], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Borussia Dortmund Are Into The Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.40740124 0.28986934 0.40740124
  0.28986934 0.40740124]
 [0.         0.5        0.5        0.         0.5        0.
  0.5        0.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Borussia Dortmund Are Into The Champions League
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['borussia', 'dortmund', 'are', 'into', 'the', 'champions', 'league']
cosine_similarity: 0.9843512773513794
train_input: [0.5797386715376657, 0.9843513], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Borussia Dortmund hold off Real Madrid to reach final at Wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.39166832 0.39166832 0.27867523 0.27867523
  0.         0.39166832 0.         0.39166832 0.         0.
  0.        ]
 [0.         0.27867523 0.         0.         0.27867523 0.27867523
  0.39166832 0.         0.39166832 0.         0.39166832 0.39166832
  0.39166832]]
pairwise_similarity: [[1.         0.23297965]
 [0.23297965 1.        ]]
cosine_similarity: 0.2329796548048752
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Borussia Dortmund hold off Real Madrid to reach final at Wembley
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['borussia', 'dortmund', 'hold', 'off', 'real', 'madrid', 'to', 'reach', 'final', 'at', 'wembley']
cosine_similarity: 0.9757610559463501
train_input: [0.2329796548048752, 0.97576106], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Borussia Dortmund road to Wembley Stadium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.37762778 0.37762778 0.26868528 0.37762778
  0.37762778 0.37762778 0.         0.         0.        ]
 [0.         0.35520009 0.         0.         0.35520009 0.
  0.         0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Borussia Dortmund road to Wembley Stadium
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['borussia', 'dortmund', 'road', 'to', 'wembley', 'stadium']
cosine_similarity: 0.9327157139778137
train_input: [0.1908740661302035, 0.9327157], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Congrats to Borussia Dortmund and hade to Real madrid
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.37762778 0.         0.37762778 0.26868528
  0.37762778 0.         0.37762778 0.         0.37762778 0.        ]
 [0.         0.31779954 0.         0.44665616 0.         0.31779954
  0.         0.44665616 0.         0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Congrats to Borussia Dortmund and hade to Real madrid
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['congrats', 'to', 'borussia', 'dortmund', 'and', 'hade', 'to', 'real', 'madrid']
cosine_similarity: 0.9634498953819275
train_input: [0.1707761131901165, 0.9634499], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Congratulation Borussia Dortmund to reach the final UCL this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.39166832 0.         0.39166832 0.27867523
  0.27867523 0.39166832 0.39166832 0.         0.         0.        ]
 [0.         0.30287281 0.         0.42567716 0.         0.30287281
  0.30287281 0.         0.         0.42567716 0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.25320945]
 [0.25320945 1.        ]]
cosine_similarity: 0.2532094495161745
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Congratulation Borussia Dortmund to reach the final UCL this season
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['congratulation', 'borussia', 'dortmund', 'to', 'reach', 'the', 'final', 'ucl', 'this', 'season']
cosine_similarity: 0.9892072081565857
train_input: [0.2532094495161745, 0.9892072], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Did you hear that Borussia Dortmund is in final of Champions League
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.30253071 0.30253071 0.42519636 0.         0.30253071
  0.30253071 0.         0.30253071 0.42519636]
 [0.         0.33425073 0.33425073 0.         0.46977774 0.33425073
  0.33425073 0.46977774 0.33425073 0.        ]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739691
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Did you hear that Borussia Dortmund is in final of Champions League
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['did', 'you', 'hear', 'that', 'borussia', 'dortmund', 'is', 'in', 'final', 'of', 'champions', 'league']
cosine_similarity: 0.9708523750305176
train_input: [0.5056055588739691, 0.9708524], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Hats off to Borussia Dortmund getting to a Champions League Final is no mean feat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.30253071 0.30253071 0.42519636 0.30253071 0.
  0.30253071 0.         0.         0.30253071 0.42519636 0.        ]
 [0.         0.27840869 0.27840869 0.         0.27840869 0.39129369
  0.27840869 0.39129369 0.39129369 0.27840869 0.         0.39129369]]
pairwise_similarity: [[1.         0.42113589]
 [0.42113589 1.        ]]
cosine_similarity: 0.42113589132819573
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: Hats off to Borussia Dortmund getting to a Champions League Final is no mean feat
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['hats', 'off', 'to', 'borussia', 'dortmund', 'getting', 'to', 'a', 'champions', 'league', 'final', 'is', 'no', 'mean', 'feat']
cosine_similarity: 0.9670768976211548
train_input: [0.42113589132819573, 0.9670769], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: However Borussia Dortmund is in FINAL
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.39166832 0.39166832 0.27867523 0.27867523
  0.39166832 0.39166832]
 [0.         0.57735027 0.         0.         0.57735027 0.57735027
  0.         0.        ]]
pairwise_similarity: [[1.         0.48267966]
 [0.48267966 1.        ]]
cosine_similarity: 0.4826796606496645
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: However Borussia Dortmund is in FINAL
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['however', 'borussia', 'dortmund', 'is', 'in', 'final']
cosine_similarity: 0.9679412245750427
train_input: [0.4826796606496645, 0.9679412], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: UCL final goes to Bayern Munich vs Borussia Dortmund
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.         0.27867523 0.39166832 0.39166832 0.27867523
  0.27867523 0.         0.39166832 0.39166832 0.         0.
  0.        ]
 [0.         0.39166832 0.27867523 0.         0.         0.27867523
  0.27867523 0.39166832 0.         0.         0.39166832 0.39166832
  0.39166832]]
pairwise_similarity: [[1.         0.23297965]
 [0.23297965 1.        ]]
cosine_similarity: 0.2329796548048752
word_to_vector_cosine_similarity: sentence1: Congratulations to Borussia Dortmund on making the 2013 Champions League Final, sentence2: UCL final goes to Bayern Munich vs Borussia Dortmund
After tokenization, sentence1: ['congratulations', 'to', 'borussia', 'dortmund', 'on', 'making', 'the', 'champions', 'league', 'final'], sentence2: ['ucl', 'final', 'goes', 'to', 'bayern', 'munich', 'vs', 'borussia', 'dortmund']
cosine_similarity: 0.93046635389328
train_input: [0.2329796548048752, 0.93046635], train_label: 0
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass Has 5 Personal Fouls
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass Has 5 Personal Fouls
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['brandon', 'bass', 'has', 'personal', 'fouls']
cosine_similarity: 0.8871728181838989
train_input: [0.3360969272762575, 0.8871728], train_label: 0
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass aint about to let the Celtics get swept
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.
  0.         0.57615236]
 [0.44665616 0.31779954 0.31779954 0.44665616 0.         0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass aint about to let the Celtics get swept
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['brandon', 'bass', 'aint', 'about', 'to', 'let', 'the', 'celtics', 'get', 'swept']
cosine_similarity: 0.9638733267784119
train_input: [0.2605556710562624, 0.9638733], train_label: 0
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass doing a good job on Melo defensively this game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.
  0.57615236 0.         0.         0.57615236]
 [0.26868528 0.26868528 0.37762778 0.37762778 0.37762778 0.37762778
  0.         0.37762778 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass doing a good job on Melo defensively this game
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['brandon', 'bass', 'doing', 'a', 'good', 'job', 'on', 'melo', 'defensively', 'this', 'game']
cosine_similarity: 0.9653501510620117
train_input: [0.2202881505618297, 0.96535015], train_label: 0
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass has too much D
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236]
 [0.70710678 0.70710678 0.         0.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass has too much D
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['brandon', 'bass', 'has', 'too', 'much', 'd']
cosine_similarity: 0.9568460583686829
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass is shutting Carmelo down
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass is shutting Carmelo down
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['brandon', 'bass', 'is', 'shutting', 'carmelo', 'down']
cosine_similarity: 0.9493454694747925
train_input: [0.3360969272762575, 0.94934547], train_label: 0
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass might as well be a tattoo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236]
 [0.50154891 0.50154891 0.         0.70490949 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass might as well be a tattoo
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['brandon', 'bass', 'might', 'as', 'well', 'be', 'a', 'tattoo']
cosine_similarity: 0.9621513485908508
train_input: [0.4112070550676187, 0.96215135], train_label: 0
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass playin great D on melo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass playin great D on melo
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['brandon', 'bass', 'playin', 'great', 'd', 'on', 'melo']
cosine_similarity: 0.9354851841926575
train_input: [0.29121941856368966, 0.9354852], train_label: 0
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass playing some good ass defense on Melo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.57615236
  0.         0.         0.57615236]
 [0.4078241  0.29017021 0.29017021 0.4078241  0.4078241  0.
  0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon Bass playing some good ass defense on Melo
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['brandon', 'bass', 'playing', 'some', 'good', 'ass', 'defense', 'on', 'melo']
cosine_similarity: 0.9705233573913574
train_input: [0.23790309463326234, 0.97052336], train_label: 0
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon bass put the clamps on melo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Brandon bass put the clamps on melo
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['brandon', 'bass', 'put', 'the', 'clamps', 'on', 'melo']
cosine_similarity: 0.9374222755432129
train_input: [0.3360969272762575, 0.9374223], train_label: 0
TF_IDF_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Why is Brandon bass the one dribbling the ball up court
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.57615236
  0.57615236]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: what the hell is Brandon bass thinking, sentence2: Why is Brandon bass the one dribbling the ball up court
After tokenization, sentence1: ['what', 'the', 'hell', 'is', 'brandon', 'bass', 'thinking'], sentence2: ['why', 'is', 'brandon', 'bass', 'the', 'one', 'dribbling', 'the', 'ball', 'up', 'court']
cosine_similarity: 0.9812569618225098
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Aye doe was Brandon Jennings serious when he said theyd win in 6
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.         0.49844628 0.35464863 0.49844628
  0.35464863 0.         0.35464863]
 [0.44610081 0.3174044  0.44610081 0.         0.3174044  0.
  0.3174044  0.44610081 0.3174044 ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Aye doe was Brandon Jennings serious when he said theyd win in 6
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['aye', 'doe', 'was', 'brandon', 'jennings', 'serious', 'when', 'he', 'said', 'theyd', 'win', 'in']
cosine_similarity: 0.9836933612823486
train_input: [0.4502681446556265, 0.98369336], train_label: 1
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Brandon Jennings going to the Knicks at the end of the year watch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.44665616 0.31779954 0.
  0.44665616 0.44665616 0.         0.44665616 0.        ]
 [0.29017021 0.4078241  0.4078241  0.         0.29017021 0.4078241
  0.         0.         0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Brandon Jennings going to the Knicks at the end of the year watch
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['brandon', 'jennings', 'going', 'to', 'the', 'knicks', 'at', 'the', 'end', 'of', 'the', 'year', 'watch']
cosine_similarity: 0.9627019762992859
train_input: [0.18443191662261305, 0.962702], train_label: 0
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Brandon Jennings look stupid AF right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.31779954 0.         0.44665616
  0.         0.44665616 0.         0.44665616]
 [0.44665616 0.31779954 0.         0.31779954 0.44665616 0.
  0.44665616 0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Brandon Jennings look stupid AF right now
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['brandon', 'jennings', 'look', 'stupid', 'af', 'right', 'now']
cosine_similarity: 0.9504554867744446
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Brandon Jennings meant to say Bucks in 7
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.31779954 0.         0.44665616
  0.44665616 0.         0.44665616]
 [0.35520009 0.49922133 0.         0.35520009 0.49922133 0.
  0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Brandon Jennings meant to say Bucks in 7
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['brandon', 'jennings', 'meant', 'to', 'say', 'bucks', 'in']
cosine_similarity: 0.990505576133728
train_input: [0.22576484600261604, 0.9905056], train_label: 0
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Brandon Jennings to the heat 2014
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.44665616 0.31779954 0.44665616
  0.44665616 0.44665616]
 [0.57615236 0.40993715 0.57615236 0.         0.40993715 0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Brandon Jennings to the heat 2014
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['brandon', 'jennings', 'to', 'the', 'heat']
cosine_similarity: 0.9617412090301514
train_input: [0.2605556710562624, 0.9617412], train_label: 0
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: I bet Brandon Jennings is feeling like a dumbass we got it the Heat in 6 games
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.         0.         0.
  0.         0.44665616 0.31779954 0.         0.44665616 0.44665616
  0.44665616]
 [0.35327777 0.25136004 0.35327777 0.35327777 0.35327777 0.35327777
  0.35327777 0.         0.25136004 0.35327777 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.15976421]
 [0.15976421 1.        ]]
cosine_similarity: 0.1597642092414444
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: I bet Brandon Jennings is feeling like a dumbass we got it the Heat in 6 games
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['i', 'bet', 'brandon', 'jennings', 'is', 'feeling', 'like', 'a', 'dumbass', 'we', 'got', 'it', 'the', 'heat', 'in', 'games']
cosine_similarity: 0.9725010395050049
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: I hope the Milwaukee Bucks and their fans enjoyed Brandon Jennings services
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.         0.44665616 0.
  0.31779954 0.         0.44665616 0.44665616 0.         0.44665616]
 [0.26868528 0.37762778 0.37762778 0.37762778 0.         0.37762778
  0.26868528 0.37762778 0.         0.         0.37762778 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: I hope the Milwaukee Bucks and their fans enjoyed Brandon Jennings services
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['i', 'hope', 'the', 'milwaukee', 'bucks', 'and', 'their', 'fans', 'enjoyed', 'brandon', 'jennings', 'services']
cosine_similarity: 0.9647749066352844
train_input: [0.1707761131901165, 0.9647749], train_label: 0
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: If you ask me Brandon Jennings quit on the Bucks in Game 4
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.         0.44665616 0.31779954
  0.         0.44665616 0.44665616 0.44665616]
 [0.44665616 0.31779954 0.44665616 0.44665616 0.         0.31779954
  0.44665616 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: If you ask me Brandon Jennings quit on the Bucks in Game 4
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['if', 'you', 'ask', 'me', 'brandon', 'jennings', 'quit', 'on', 'the', 'bucks', 'in', 'game']
cosine_similarity: 0.9760976433753967
train_input: [0.20199309249791833, 0.97609764], train_label: 0
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Lol brandon jennings what happened to win in 6
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.47042643 0.33471228 0.         0.47042643
  0.47042643 0.33471228]
 [0.37930349 0.53309782 0.         0.37930349 0.53309782 0.
  0.         0.37930349]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: Lol brandon jennings what happened to win in 6
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['lol', 'brandon', 'jennings', 'what', 'happened', 'to', 'win', 'in']
cosine_similarity: 0.9870335459709167
train_input: [0.3808726084759436, 0.98703355], train_label: 1
TF_IDF_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: On a related note did you see how the Bucks froze out Brandon Jennings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.         0.44665616 0.31779954
  0.         0.         0.44665616 0.44665616 0.44665616]
 [0.29017021 0.4078241  0.4078241  0.4078241  0.         0.29017021
  0.4078241  0.4078241  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: remember when Brandon Jennings said hed win in 6, sentence2: On a related note did you see how the Bucks froze out Brandon Jennings
After tokenization, sentence1: ['remember', 'when', 'brandon', 'jennings', 'said', 'hed', 'win', 'in'], sentence2: ['on', 'a', 'related', 'note', 'did', 'you', 'see', 'how', 'the', 'bucks', 'froze', 'out', 'brandon', 'jennings']
cosine_similarity: 0.9781050086021423
train_input: [0.18443191662261305, 0.978105], train_label: 0
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez aint never just hit that 3
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.57615236 0.         0.
  0.40993715]
 [0.49922133 0.         0.35520009 0.         0.49922133 0.49922133
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez aint never just hit that 3
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['brook', 'lopez', 'aint', 'never', 'just', 'hit', 'that']
cosine_similarity: 0.9548650979995728
train_input: [0.29121941856368966, 0.9548651], train_label: 0
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez chose a good time to make the first 3pointer of his career
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.         0.         0.57615236
  0.         0.40993715 0.         0.        ]
 [0.37762778 0.         0.26868528 0.37762778 0.37762778 0.
  0.37762778 0.26868528 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez chose a good time to make the first 3pointer of his career
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['brook', 'lopez', 'chose', 'a', 'good', 'time', 'to', 'make', 'the', 'first', 'of', 'his', 'career']
cosine_similarity: 0.9670061469078064
train_input: [0.2202881505618297, 0.96700615], train_label: 0
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez did not just make that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.         0.40993715
  0.        ]
 [0.         0.35520009 0.         0.49922133 0.49922133 0.35520009
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez did not just make that
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['brook', 'lopez', 'did', 'not', 'just', 'make', 'that']
cosine_similarity: 0.9574843645095825
train_input: [0.29121941856368966, 0.95748436], train_label: 0
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez for 3 point contest
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.57615236 0.40993715 0.        ]
 [0.         0.40993715 0.57615236 0.         0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez for 3 point contest
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['brook', 'lopez', 'for', 'point', 'contest']
cosine_similarity: 0.9394790530204773
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez is really good on offense
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.40993715 0.
  0.        ]
 [0.         0.35520009 0.         0.49922133 0.35520009 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez is really good on offense
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['brook', 'lopez', 'is', 'really', 'good', 'on', 'offense']
cosine_similarity: 0.9625518918037415
train_input: [0.29121941856368966, 0.9625519], train_label: 0
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez is straight out ballin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.57615236 0.40993715 0.        ]
 [0.57615236 0.         0.40993715 0.         0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez is straight out ballin
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['brook', 'lopez', 'is', 'straight', 'out', 'ballin']
cosine_similarity: 0.9451209902763367
train_input: [0.3360969272762575, 0.945121], train_label: 0
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez really just hit a fadeaway 3
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.         0.
  0.40993715 0.        ]
 [0.         0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez really just hit a fadeaway 3
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['brook', 'lopez', 'really', 'just', 'hit', 'a', 'fadeaway']
cosine_similarity: 0.969481885433197
train_input: [0.2605556710562624, 0.9694819], train_label: 1
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez with the step back three
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.40993715 0.        ]
 [0.         0.50154891 0.         0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: Brook Lopez with the step back three
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['brook', 'lopez', 'with', 'the', 'step', 'back', 'three']
cosine_similarity: 0.9698231816291809
train_input: [0.4112070550676187, 0.9698232], train_label: 1
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: By love child I mean Brook Lopez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.57615236 0.40993715 0.
  0.        ]
 [0.         0.35520009 0.49922133 0.         0.35520009 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: By love child I mean Brook Lopez
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['by', 'love', 'child', 'i', 'mean', 'brook', 'lopez']
cosine_similarity: 0.9721248149871826
train_input: [0.29121941856368966, 0.9721248], train_label: 0
TF_IDF_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: brook lopez hit a 3 and i missed it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.40993715 0.        ]
 [0.         0.40993715 0.         0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: That boy Brook Lopez with a deep 3, sentence2: brook lopez hit a 3 and i missed it
After tokenization, sentence1: ['that', 'boy', 'brook', 'lopez', 'with', 'a', 'deep'], sentence2: ['brook', 'lopez', 'hit', 'a', 'and', 'i', 'missed', 'it']
cosine_similarity: 0.9816663861274719
train_input: [0.3360969272762575, 0.9816664], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Brooklyn needs this game tonight LETS GO
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.         0.         0.
  0.4261596  0.         0.4261596  0.4261596 ]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.47107781
  0.         0.47107781 0.         0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Brooklyn needs this game tonight LETS GO
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['brooklyn', 'needs', 'this', 'game', 'tonight', 'lets', 'go']
cosine_similarity: 0.9218446016311646
train_input: [0.10163066979112656, 0.9218446], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Brooklyn nets some how get good in a year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.         0.31779954 0.44665616
  0.44665616 0.        ]
 [0.40993715 0.         0.         0.57615236 0.40993715 0.
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Brooklyn nets some how get good in a year
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['brooklyn', 'nets', 'some', 'how', 'get', 'good', 'in', 'a', 'year']
cosine_similarity: 0.9027548432350159
train_input: [0.2605556710562624, 0.90275484], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: ESPECIAL MEANS SPECIAL team Brooklyn Bowl
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30321606 0.4261596  0.4261596  0.         0.
  0.4261596  0.         0.         0.4261596  0.4261596 ]
 [0.4261596  0.30321606 0.         0.         0.4261596  0.4261596
  0.         0.4261596  0.4261596  0.         0.        ]]
pairwise_similarity: [[1.         0.09193998]
 [0.09193998 1.        ]]
cosine_similarity: 0.09193998174078082
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: ESPECIAL MEANS SPECIAL team Brooklyn Bowl
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['especial', 'means', 'special', 'team', 'brooklyn', 'bowl']
cosine_similarity: 0.8958156108856201
train_input: [0.09193998174078082, 0.8958156], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: I need Brooklyn to get that W tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.         0.4261596  0.
  0.4261596  0.4261596 ]
 [0.44943642 0.         0.         0.6316672  0.         0.6316672
  0.         0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: I need Brooklyn to get that W tonight
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['i', 'need', 'brooklyn', 'to', 'get', 'that', 'w', 'tonight']
cosine_similarity: 0.8604101538658142
train_input: [0.1362763414390864, 0.86041015], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: It s a must win game Monday night for the Brooklyn Nets a
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.         0.         0.31779954
  0.         0.44665616 0.44665616 0.        ]
 [0.31779954 0.         0.         0.44665616 0.44665616 0.31779954
  0.44665616 0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: It s a must win game Monday night for the Brooklyn Nets a
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['it', 's', 'a', 'must', 'win', 'game', 'monday', 'night', 'for', 'the', 'brooklyn', 'nets', 'a']
cosine_similarity: 0.9166831970214844
train_input: [0.20199309249791833, 0.9166832], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Let s go Brooklyn let s go
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.         0.4261596  0.4261596
  0.4261596 ]
 [0.33517574 0.         0.         0.94215562 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Let s go Brooklyn let s go
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['let', 's', 'go', 'brooklyn', 'let', 's', 'go']
cosine_similarity: 0.8636294007301331
train_input: [0.10163066979112656, 0.8636294], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Nets Vs Bulls Game5 GoBrooklyn NetsLife NoFloorSeats We OutChea
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.33471228 0.47042643 0.         0.         0.33471228
  0.         0.         0.         0.33471228 0.47042643]
 [0.         0.27867523 0.         0.39166832 0.39166832 0.27867523
  0.39166832 0.39166832 0.39166832 0.27867523 0.        ]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Nets Vs Bulls Game5 GoBrooklyn NetsLife NoFloorSeats We OutChea
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['nets', 'vs', 'bulls', 'we', 'outchea']
cosine_similarity: 0.9455140829086304
train_input: [0.27982806524328774, 0.9455141], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: That Brooklyn chant is starting to get annoying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30321606 0.4261596  0.         0.4261596  0.4261596
  0.         0.4261596  0.4261596 ]
 [0.53404633 0.37997836 0.         0.53404633 0.         0.
  0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: That Brooklyn chant is starting to get annoying
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['that', 'brooklyn', 'chant', 'is', 'starting', 'to', 'get', 'annoying']
cosine_similarity: 0.8724040985107422
train_input: [0.11521554337793122, 0.8724041], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Them Brooklyn niggas is cold
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.         0.4261596  0.
  0.4261596  0.4261596 ]
 [0.44943642 0.         0.         0.6316672  0.         0.6316672
  0.         0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: Them Brooklyn niggas is cold
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['them', 'brooklyn', 'niggas', 'is', 'cold']
cosine_similarity: 0.8585880994796753
train_input: [0.1362763414390864, 0.8585881], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: no you must be a Brooklyn fan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.         0.4261596  0.4261596
  0.4261596 ]
 [0.57973867 0.         0.         0.81480247 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: Watch the Brooklyn Nets Vs Chicago Bulls, sentence2: no you must be a Brooklyn fan
After tokenization, sentence1: ['watch', 'the', 'brooklyn', 'nets', 'vs', 'chicago', 'bulls'], sentence2: ['no', 'you', 'must', 'be', 'a', 'brooklyn', 'fan']
cosine_similarity: 0.8360302448272705
train_input: [0.17578607839334617, 0.83603024], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: Browns need to get Geno Smith
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: Browns need to get Geno Smith
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['browns', 'need', 'to', 'get', 'geno', 'smith']
cosine_similarity: 0.9456331729888916
train_input: [0.17077611319011649, 0.9456332], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: I dont get why browns fans he hyped up for the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.
  0.6316672 ]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.         0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: I dont get why browns fans he hyped up for the draft
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['i', 'dont', 'get', 'why', 'browns', 'fans', 'he', 'hyped', 'up', 'for', 'the', 'draft']
cosine_similarity: 0.9769769906997681
train_input: [0.15064018498706508, 0.976977], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: I swear to god if the browns take geno smith
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672  0.
  0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.         0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: I swear to god if the browns take geno smith
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['i', 'swear', 'to', 'god', 'if', 'the', 'browns', 'take', 'geno', 'smith']
cosine_similarity: 0.9827015995979309
train_input: [0.15064018498706508, 0.9827016], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: Im going to the nfl draft party at browns stadium haha
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.
  0.         0.         0.6316672  0.        ]
 [0.25969799 0.36499647 0.36499647 0.36499647 0.         0.36499647
  0.36499647 0.36499647 0.         0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: Im going to the nfl draft party at browns stadium haha
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['im', 'going', 'to', 'the', 'nfl', 'draft', 'party', 'at', 'browns', 'stadium', 'haha']
cosine_similarity: 0.9662275910377502
train_input: [0.11671773546032795, 0.9662276], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: So To the Cleveland Browns goooo Brownies
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: So To the Cleveland Browns goooo Brownies
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['so', 'to', 'the', 'cleveland', 'browns', 'brownies']
cosine_similarity: 0.9667983055114746
train_input: [0.17077611319011649, 0.9667983], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: The Browns have fallen on hard times
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672  0.        ]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: The Browns have fallen on hard times
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['the', 'browns', 'have', 'fallen', 'on', 'hard', 'times']
cosine_similarity: 0.9746821522712708
train_input: [0.17077611319011649, 0.97468215], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: Todd McShay really thinks the Browns are gonna take Geno Smith
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.
  0.6316672  0.         0.         0.        ]
 [0.25969799 0.36499647 0.36499647 0.         0.36499647 0.36499647
  0.         0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: Todd McShay really thinks the Browns are gonna take Geno Smith
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['todd', 'mcshay', 'really', 'thinks', 'the', 'browns', 'are', 'gonna', 'take', 'geno', 'smith']
cosine_similarity: 0.9346540570259094
train_input: [0.11671773546032795, 0.93465406], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: and with the 6th pick the browns select
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.53404633 0.37997836 0.         0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: and with the 6th pick the browns select
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['and', 'with', 'the', 'pick', 'the', 'browns', 'select']
cosine_similarity: 0.9675357937812805
train_input: [0.17077611319011649, 0.9675358], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: i keep hearing the browns raiders are doing a trade
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672
  0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: i keep hearing the browns raiders are doing a trade
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['i', 'keep', 'hearing', 'the', 'browns', 'raiders', 'are', 'doing', 'a', 'trade']
cosine_similarity: 0.9779877066612244
train_input: [0.15064018498706508, 0.9779877], train_label: 0
TF_IDF_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: maddym17 are you going to the browns stadium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Hope the Browns take a shot at him, sentence2: maddym17 are you going to the browns stadium
After tokenization, sentence1: ['hope', 'the', 'browns', 'take', 'a', 'shot', 'at', 'him'], sentence2: ['are', 'you', 'going', 'to', 'the', 'browns', 'stadium']
cosine_similarity: 0.9770410656929016
train_input: [0.17077611319011649, 0.97704107], train_label: 0
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: Bucks is really down 20
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]
 [0.6316672  0.44943642 0.         0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: Bucks is really down 20
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['bucks', 'is', 'really', 'down']
cosine_similarity: 0.963034451007843
train_input: [0.17077611319011649, 0.96303445], train_label: 0
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: I jus want the bucks to win one game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.57615236
  0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: I jus want the bucks to win one game
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['i', 'jus', 'want', 'the', 'bucks', 'to', 'win', 'one', 'game']
cosine_similarity: 0.9624817967414856
train_input: [0.29121941856368966, 0.9624818], train_label: 0
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: If anybody can beat the heat its the bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.57615236 0.40993715 0.57615236]
 [0.57615236 0.57615236 0.40993715 0.         0.40993715 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: If anybody can beat the heat its the bucks
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['if', 'anybody', 'can', 'beat', 'the', 'heat', 'its', 'the', 'bucks']
cosine_similarity: 0.9752588868141174
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: Ill give it to the Bucks they are really trying to stop us
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.         0.53404633]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: Ill give it to the Bucks they are really trying to stop us
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['ill', 'give', 'it', 'to', 'the', 'bucks', 'they', 'are', 'really', 'trying', 'to', 'stop', 'us']
cosine_similarity: 0.9346053600311279
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: Only time the bucks will be on ABC
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]
 [0.6316672  0.44943642 0.         0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: Only time the bucks will be on ABC
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['only', 'time', 'the', 'bucks', 'will', 'be', 'on', 'abc']
cosine_similarity: 0.963228702545166
train_input: [0.17077611319011649, 0.9632287], train_label: 0
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: The Heat dont giv a fuk the Bucks jus suck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.         0.40993715
  0.         0.         0.57615236]
 [0.29017021 0.4078241  0.4078241  0.         0.4078241  0.29017021
  0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: The Heat dont giv a fuk the Bucks jus suck
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['the', 'heat', 'dont', 'giv', 'a', 'fuk', 'the', 'bucks', 'jus', 'suck']
cosine_similarity: 0.940865159034729
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: There s a reason the bucks are the eighth seed
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.         0.
  0.53404633]
 [0.37997836 0.53404633 0.         0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: There s a reason the bucks are the eighth seed
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['there', 's', 'a', 'reason', 'the', 'bucks', 'are', 'the', 'eighth', 'seed']
cosine_similarity: 0.9617286324501038
train_input: [0.1443835552773867, 0.96172863], train_label: 0
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: This HeatBucks game is starting to look like a high school game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.
  0.         0.         0.         0.53404633]
 [0.         0.50232878 0.         0.35300279 0.35300279 0.35300279
  0.35300279 0.35300279 0.35300279 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: This HeatBucks game is starting to look like a high school game
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['this', 'game', 'is', 'starting', 'to', 'look', 'like', 'a', 'high', 'school', 'game']
cosine_similarity: 0.9640780687332153
train_input: [0.1908740661302035, 0.96407807], train_label: 1
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: they playin the bucks lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.53404633]
 [0.44943642 0.         0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: they playin the bucks lol
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['they', 'playin', 'the', 'bucks', 'lol']
cosine_similarity: 0.9712282419204712
train_input: [0.17077611319011649, 0.97122824], train_label: 0
TF_IDF_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: watchn heat whoop the bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.40993715 0.57615236 0.         0.        ]
 [0.40993715 0.         0.40993715 0.         0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: watchin the Bucks and Heat game, sentence2: watchn heat whoop the bucks
After tokenization, sentence1: ['watchin', 'the', 'bucks', 'and', 'heat', 'game'], sentence2: ['watchn', 'heat', 'whoop', 'the', 'bucks']
cosine_similarity: 0.961270809173584
train_input: [0.3360969272762575, 0.9612708], train_label: 1
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: At this point KingJames is just toying with the bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633 0.        ]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.47107781
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: At this point KingJames is just toying with the bucks
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['at', 'this', 'point', 'kingjames', 'is', 'just', 'toying', 'with', 'the', 'bucks']
cosine_similarity: 0.9672199487686157
train_input: [0.1273595297947935, 0.96721995], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: Heat vs bucks is pretty interesting lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.         0.53404633 0.        ]
 [0.30321606 0.         0.         0.4261596  0.4261596  0.4261596
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: Heat vs bucks is pretty interesting lol
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['heat', 'vs', 'bucks', 'is', 'pretty', 'interesting', 'lol']
cosine_similarity: 0.973507821559906
train_input: [0.11521554337793122, 0.9735078], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: I aint no Heat fan but they need to close out the Bucks today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.         0.         0.53404633 0.        ]
 [0.39204401 0.27894255 0.39204401 0.         0.39204401 0.
  0.39204401 0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: I aint no Heat fan but they need to close out the Bucks today
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['i', 'aint', 'no', 'heat', 'fan', 'but', 'they', 'need', 'to', 'close', 'out', 'the', 'bucks', 'today']
cosine_similarity: 0.9759552478790283
train_input: [0.1059921313509325, 0.97595525], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: Is this the heat and bucks or drunk people
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.53404633]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: Is this the heat and bucks or drunk people
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people']
cosine_similarity: 0.9847334027290344
train_input: [0.1443835552773867, 0.9847334], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: Swear the way the Heat are playing they want the Bucks to win
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633 0.         0.         0.        ]
 [0.27894255 0.         0.         0.39204401 0.39204401 0.39204401
  0.         0.39204401 0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: Swear the way the Heat are playing they want the Bucks to win
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['swear', 'the', 'way', 'the', 'heat', 'are', 'playing', 'they', 'want', 'the', 'bucks', 'to', 'win']
cosine_similarity: 0.9762920141220093
train_input: [0.1059921313509325, 0.976292], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: This Heat And Bucks Game Is Boring
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.53404633 0.
  0.53404633]
 [0.53404633 0.37997836 0.         0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: This Heat And Bucks Game Is Boring
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['this', 'heat', 'and', 'bucks', 'game', 'is', 'boring']
cosine_similarity: 0.9771464467048645
train_input: [0.1443835552773867, 0.97714645], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: This Heat Bucks Game Weak Ass Fuck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.         0.53404633
  0.         0.53404633 0.        ]
 [0.4261596  0.30321606 0.         0.4261596  0.4261596  0.
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: This Heat Bucks Game Weak Ass Fuck
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['this', 'heat', 'bucks', 'game', 'weak', 'ass', 'fuck']
cosine_similarity: 0.9701382517814636
train_input: [0.11521554337793122, 0.97013825], train_label: 1
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: This Heat Bucks Game Whack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.53404633
  0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: This Heat Bucks Game Whack
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['this', 'heat', 'bucks', 'game', 'whack']
cosine_similarity: 0.945666491985321
train_input: [0.1443835552773867, 0.9456665], train_label: 1
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: This heat vs bucks game is beyond boring
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.53404633 0.
  0.53404633 0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.         0.47107781
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: This heat vs bucks game is beyond boring
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['this', 'heat', 'vs', 'bucks', 'game', 'is', 'beyond', 'boring']
cosine_similarity: 0.9607505798339844
train_input: [0.1273595297947935, 0.9607506], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: lol i mean it is the bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.53404633]
 [0.44943642 0.         0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: The Bucks are got damn TERRIBLE, sentence2: lol i mean it is the bucks
After tokenization, sentence1: ['the', 'bucks', 'are', 'got', 'damn', 'terrible'], sentence2: ['lol', 'i', 'mean', 'it', 'is', 'the', 'bucks']
cosine_similarity: 0.9715600609779358
train_input: [0.17077611319011649, 0.97156006], train_label: 0
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: And this is why Im not a fan of the nba the bucks freakin suck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.81480247 0.         0.
  0.        ]
 [0.30321606 0.4261596  0.4261596  0.         0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: And this is why Im not a fan of the nba the bucks freakin suck
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['and', 'this', 'is', 'why', 'im', 'not', 'a', 'fan', 'of', 'the', 'nba', 'the', 'bucks', 'freakin', 'suck']
cosine_similarity: 0.9821202754974365
train_input: [0.17578607839334617, 0.9821203], train_label: 1
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: Going to stay consistent and say the Bucks lose by 15
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.         0.81480247 0.
  0.         0.        ]
 [0.39204401 0.27894255 0.39204401 0.39204401 0.         0.39204401
  0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.16171378]
 [0.16171378 1.        ]]
cosine_similarity: 0.16171378066252898
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: Going to stay consistent and say the Bucks lose by 15
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['going', 'to', 'stay', 'consistent', 'and', 'say', 'the', 'bucks', 'lose', 'by']
cosine_similarity: 0.9742294549942017
train_input: [0.16171378066252898, 0.97422945], train_label: 0
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: Idk why the Bucks even bothered to make the playoffs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.81480247 0.         0.         0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: Idk why the Bucks even bothered to make the playoffs
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['idk', 'why', 'the', 'bucks', 'even', 'bothered', 'to', 'make', 'the', 'playoffs']
cosine_similarity: 0.9766992330551147
train_input: [0.19431434016858146, 0.97669923], train_label: 0
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: If the Bucks would step up they could actually have a chance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.81480247 0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: If the Bucks would step up they could actually have a chance
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['if', 'the', 'bucks', 'would', 'step', 'up', 'they', 'could', 'actually', 'have', 'a', 'chance']
cosine_similarity: 0.9831611514091492
train_input: [0.22028815056182965, 0.98316115], train_label: 0
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: It amazes me how the Bucks can stay in the game with the Heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.81480247 0.         0.        ]
 [0.47107781 0.33517574 0.47107781 0.         0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: It amazes me how the Bucks can stay in the game with the Heat
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['it', 'amazes', 'me', 'how', 'the', 'bucks', 'can', 'stay', 'in', 'the', 'game', 'with', 'the', 'heat']
cosine_similarity: 0.9801446199417114
train_input: [0.19431434016858146, 0.9801446], train_label: 0
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: Need to worry about the Bucks tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.        ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: Need to worry about the Bucks tho
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['need', 'to', 'worry', 'about', 'the', 'bucks', 'tho']
cosine_similarity: 0.9736012816429138
train_input: [0.22028815056182965, 0.9736013], train_label: 0
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: The Bucks are horrible at the game of basketball
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.81480247 0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: The Bucks are horrible at the game of basketball
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['the', 'bucks', 'are', 'horrible', 'at', 'the', 'game', 'of', 'basketball']
cosine_similarity: 0.9698734879493713
train_input: [0.22028815056182965, 0.9698735], train_label: 1
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: The Bucks are not winning
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.        ]
 [0.57973867 0.         0.81480247]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: The Bucks are not winning
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['the', 'bucks', 'are', 'not', 'winning']
cosine_similarity: 0.9813154339790344
train_input: [0.3360969272762574, 0.98131543], train_label: 0
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: The Bucks have no shot just give up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.        ]
 [0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: The Bucks have no shot just give up
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['the', 'bucks', 'have', 'no', 'shot', 'just', 'give', 'up']
cosine_similarity: 0.979865550994873
TF_IDF_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: The heat and bucks series is painful to watch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.         0.        ]
 [0.33517574 0.         0.47107781 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Have the bucks ever been good, sentence2: The heat and bucks series is painful to watch
After tokenization, sentence1: ['have', 'the', 'bucks', 'ever', 'been', 'good'], sentence2: ['the', 'heat', 'and', 'bucks', 'series', 'is', 'painful', 'to', 'watch']
cosine_similarity: 0.9739121198654175
train_input: [0.19431434016858146, 0.9739121], train_label: 0
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: Mad me and kaykay032 couldnt go to the bucks game today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.53404633 0.
  0.         0.        ]
 [0.         0.33517574 0.47107781 0.         0.         0.47107781
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: Mad me and kaykay032 couldnt go to the bucks game today
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['mad', 'me', 'and', 'couldnt', 'go', 'to', 'the', 'bucks', 'game', 'today']
cosine_similarity: 0.9818661212921143
train_input: [0.1273595297947935, 0.9818661], train_label: 0
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: Not even sure why I watch the Bucks play they suck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.
  0.         0.        ]
 [0.         0.33517574 0.         0.         0.47107781 0.47107781
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: Not even sure why I watch the Bucks play they suck
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['not', 'even', 'sure', 'why', 'i', 'watch', 'the', 'bucks', 'play', 'they', 'suck']
cosine_similarity: 0.9732810258865356
train_input: [0.1273595297947935, 0.973281], train_label: 0
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: The Bucks are keeping it close
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.53404633 0.        ]
 [0.         0.44943642 0.6316672  0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: The Bucks are keeping it close
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['the', 'bucks', 'are', 'keeping', 'it', 'close']
cosine_similarity: 0.9756004214286804
train_input: [0.17077611319011649, 0.9756004], train_label: 0
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: The Bucks just cant take the lead
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]
 [0.         0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: The Bucks just cant take the lead
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['the', 'bucks', 'just', 'cant', 'take', 'the', 'lead']
cosine_similarity: 0.9912112951278687
train_input: [0.17077611319011649, 0.9912113], train_label: 0
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: The bucks are fucking stupid
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.53404633 0.        ]
 [0.         0.44943642 0.6316672  0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: The bucks are fucking stupid
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['the', 'bucks', 'are', 'fucking', 'stupid']
cosine_similarity: 0.966623067855835
train_input: [0.17077611319011649, 0.96662307], train_label: 0
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: bucks gettin at the heat tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.40993715 0.57615236 0.        ]
 [0.         0.40993715 0.57615236 0.40993715 0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: bucks gettin at the heat tho
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['bucks', 'gettin', 'at', 'the', 'heat', 'tho']
cosine_similarity: 0.9772909283638
train_input: [0.3360969272762575, 0.9772909], train_label: 1
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: does anyone really feel like the bucks are threatening right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.         0.53404633 0.53404633
  0.         0.         0.         0.        ]
 [0.         0.27894255 0.39204401 0.39204401 0.         0.
  0.39204401 0.39204401 0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: does anyone really feel like the bucks are threatening right now
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['does', 'anyone', 'really', 'feel', 'like', 'the', 'bucks', 'are', 'threatening', 'right', 'now']
cosine_similarity: 0.9614235162734985
train_input: [0.1059921313509325, 0.9614235], train_label: 0
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: haha playing the bucks is a bye haha
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.         0.53404633 0.53404633
  0.        ]
 [0.         0.27894255 0.39204401 0.78408803 0.         0.
  0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: haha playing the bucks is a bye haha
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['haha', 'playing', 'the', 'bucks', 'is', 'a', 'bye', 'haha']
cosine_similarity: 0.9404005408287048
train_input: [0.1059921313509325, 0.94040054], train_label: 0
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: they said it during half time of the heat and bucks game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.         0.40993715 0.57615236
  0.         0.        ]
 [0.         0.31779954 0.44665616 0.44665616 0.31779954 0.
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: they said it during half time of the heat and bucks game
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['they', 'said', 'it', 'during', 'half', 'time', 'of', 'the', 'heat', 'and', 'bucks', 'game']
cosine_similarity: 0.988427460193634
train_input: [0.2605556710562624, 0.98842746], train_label: 0
TF_IDF_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: this HeatBucks game is sloppy as all get out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.         0.5        0.         0.5
  0.        ]
 [0.         0.         0.57735027 0.         0.57735027 0.
  0.57735027]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: I hope the bucks beat the heat, sentence2: this HeatBucks game is sloppy as all get out
After tokenization, sentence1: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat'], sentence2: ['this', 'game', 'is', 'sloppy', 'as', 'all', 'get', 'out']
cosine_similarity: 0.9769909381866455
train_input: [0.0, 0.97699094], train_label: 0
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Brandon Jennings said the Bucks were gona win in 6 games
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.         0.53404633 0.53404633
  0.         0.53404633 0.         0.        ]
 [0.39204401 0.27894255 0.39204401 0.39204401 0.         0.
  0.39204401 0.         0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Brandon Jennings said the Bucks were gona win in 6 games
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['brandon', 'jennings', 'said', 'the', 'bucks', 'were', 'gona', 'win', 'in', 'games']
cosine_similarity: 0.9677279591560364
train_input: [0.1059921313509325, 0.96772796], train_label: 0
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Heat cruising and the bucks just all out wack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.40993715 0.         0.57615236
  0.        ]
 [0.35520009 0.49922133 0.         0.35520009 0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Heat cruising and the bucks just all out wack
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['heat', 'cruising', 'and', 'the', 'bucks', 'just', 'all', 'out', 'wack']
cosine_similarity: 0.9847217202186584
train_input: [0.29121941856368966, 0.9847217], train_label: 1
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: I hope the bucks beat the heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.40993715 0.         0.57615236]
 [0.57615236 0.40993715 0.         0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: I hope the bucks beat the heat
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['i', 'hope', 'the', 'bucks', 'beat', 'the', 'heat']
cosine_similarity: 0.9890283346176147
train_input: [0.3360969272762575, 0.98902833], train_label: 0
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Ill give it to the Bucks they are really trying to stop us
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.53404633 0.
  0.         0.        ]
 [0.33517574 0.         0.         0.47107781 0.         0.47107781
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Ill give it to the Bucks they are really trying to stop us
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['ill', 'give', 'it', 'to', 'the', 'bucks', 'they', 'are', 'really', 'trying', 'to', 'stop', 'us']
cosine_similarity: 0.9534263014793396
train_input: [0.1273595297947935, 0.9534263], train_label: 0
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Midway through the 3rd the Bucks trail the Heat by 4
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.40993715 0.         0.57615236
  0.        ]
 [0.49922133 0.35520009 0.         0.35520009 0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Midway through the 3rd the Bucks trail the Heat by 4
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['midway', 'through', 'the', 'the', 'bucks', 'trail', 'the', 'heat', 'by']
cosine_similarity: 0.9757946729660034
train_input: [0.29121941856368966, 0.9757947], train_label: 1
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: RealSkipBayless is just me or the Bucks Heat game is boring as hell
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.40993715 0.
  0.         0.57615236 0.        ]
 [0.4078241  0.29017021 0.4078241  0.         0.29017021 0.4078241
  0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: RealSkipBayless is just me or the Bucks Heat game is boring as hell
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['is', 'just', 'me', 'or', 'the', 'bucks', 'heat', 'game', 'is', 'boring', 'as', 'hell']
cosine_similarity: 0.9646832942962646
train_input: [0.23790309463326234, 0.9646833], train_label: 0
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: This bucks and heat game is awful
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.40993715 0.57615236]
 [0.57615236 0.40993715 0.57615236 0.         0.40993715 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: This bucks and heat game is awful
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['this', 'bucks', 'and', 'heat', 'game', 'is', 'awful']
cosine_similarity: 0.9774804711341858
train_input: [0.3360969272762575, 0.9774805], train_label: 0
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: When is game 4 of bucks heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.40993715 0.57615236]
 [0.50154891 0.70490949 0.         0.50154891 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: When is game 4 of bucks heat
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['when', 'is', 'game', 'of', 'bucks', 'heat']
cosine_similarity: 0.9831328392028809
train_input: [0.4112070550676187, 0.98313284], train_label: 0
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Why the fuck am I watching the HeatBucks game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5 0.  0.  0.5 0.5 0.  0.5 0. ]
 [0.  0.5 0.5 0.  0.  0.5 0.  0.5]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: Why the fuck am I watching the HeatBucks game
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['why', 'the', 'fuck', 'am', 'i', 'watching', 'the', 'game']
cosine_similarity: 0.9527660012245178
train_input: [0.0, 0.952766], train_label: 0
TF_IDF_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: they about to sweep the fuck out the bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]
 [0.44943642 0.6316672  0.         0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: The milwaukee Bucks cant handle the Heat, sentence2: they about to sweep the fuck out the bucks
After tokenization, sentence1: ['the', 'milwaukee', 'bucks', 'cant', 'handle', 'the', 'heat'], sentence2: ['they', 'about', 'to', 'sweep', 'the', 'fuck', 'out', 'the', 'bucks']
cosine_similarity: 0.9867275357246399
train_input: [0.17077611319011649, 0.98672754], train_label: 1
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: Heat v Bucks then MOTD2
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.40993715 0.         0.57615236]
 [0.50154891 0.         0.50154891 0.70490949 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: Heat v Bucks then MOTD2
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['heat', 'v', 'bucks', 'then']
cosine_similarity: 0.9271658062934875
train_input: [0.4112070550676187, 0.9271658], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: I had ticket to go the heat n bucks game but I threw them away
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.         0.40993715 0.57615236
  0.         0.        ]
 [0.44665616 0.31779954 0.         0.44665616 0.31779954 0.
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: I had ticket to go the heat n bucks game but I threw them away
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['i', 'had', 'ticket', 'to', 'go', 'the', 'heat', 'n', 'bucks', 'game', 'but', 'i', 'threw', 'them', 'away']
cosine_similarity: 0.9802596569061279
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: If the Bucks win this game do they slander Lebron or nah
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.53404633 0.         0.        ]
 [0.30321606 0.         0.4261596  0.         0.4261596  0.4261596
  0.         0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: If the Bucks win this game do they slander Lebron or nah
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['if', 'the', 'bucks', 'win', 'this', 'game', 'do', 'they', 'slander', 'lebron', 'or', 'nah']
cosine_similarity: 0.9788519144058228
train_input: [0.11521554337793122, 0.9788519], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: One more quarter till the BUCKS season is over
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633 0.         0.
  0.        ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: One more quarter till the BUCKS season is over
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['one', 'more', 'quarter', 'till', 'the', 'bucks', 'season', 'is', 'over']
cosine_similarity: 0.9712900519371033
train_input: [0.1443835552773867, 0.97129005], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: Sitting by the the Claymaker Bucks game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.53404633
  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: Sitting by the the Claymaker Bucks game
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['sitting', 'by', 'the', 'the', 'bucks', 'game']
cosine_similarity: 0.9806810021400452
train_input: [0.1443835552773867, 0.980681], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: The Bucks are only down by 6
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633]
 [1.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: The Bucks are only down by 6
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['the', 'bucks', 'are', 'only', 'down', 'by']
cosine_similarity: 0.9894300103187561
train_input: [0.37997836159100784, 0.98943], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: The bucks str8 keeping up with the heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.40993715 0.         0.57615236 0.        ]
 [0.40993715 0.         0.40993715 0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: The bucks str8 keeping up with the heat
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['the', 'bucks', 'keeping', 'up', 'with', 'the', 'heat']
cosine_similarity: 0.9850527048110962
train_input: [0.3360969272762575, 0.9850527], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: Watching my Heat about to sweep the Bucks lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.40993715 0.         0.57615236 0.
  0.        ]
 [0.35520009 0.         0.35520009 0.49922133 0.         0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: Watching my Heat about to sweep the Bucks lol
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['watching', 'my', 'heat', 'about', 'to', 'sweep', 'the', 'bucks', 'lol']
cosine_similarity: 0.9880216717720032
train_input: [0.29121941856368966, 0.9880217], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: that just showed you the Bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]
 [0.44943642 0.         0.         0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: that just showed you the Bucks
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['that', 'just', 'showed', 'you', 'the', 'bucks']
cosine_similarity: 0.9895304441452026
train_input: [0.17077611319011649, 0.98953044], train_label: 0
TF_IDF_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: the bucks got alot of long niggas
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.53404633 0.
  0.         0.53404633]
 [0.47107781 0.33517574 0.         0.47107781 0.         0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Is this the heat and bucks or drunk people, sentence2: the bucks got alot of long niggas
After tokenization, sentence1: ['is', 'this', 'the', 'heat', 'and', 'bucks', 'or', 'drunk', 'people'], sentence2: ['the', 'bucks', 'got', 'alot', 'of', 'long', 'niggas']
cosine_similarity: 0.9830833077430725
train_input: [0.1273595297947935, 0.9830833], train_label: 0
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: All of Rockford at the BucksHeat game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.         0.57735027 0.         0.         0.57735027]
 [0.         0.57735027 0.         0.57735027 0.57735027 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: All of Rockford at the BucksHeat game
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['all', 'of', 'rockford', 'at', 'the', 'game']
cosine_similarity: 0.9475709199905396
train_input: [0.0, 0.9475709], train_label: 0
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: Bucks Game Is So Slow Boring
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.53404633 0.37997836 0.         0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: Bucks Game Is So Slow Boring
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['bucks', 'game', 'is', 'so', 'slow', 'boring']
cosine_similarity: 0.9594601988792419
train_input: [0.17077611319011649, 0.9594602], train_label: 0
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: Everybody knew the Bucks had no chance against the Heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.
  0.6316672 ]
 [0.33517574 0.47107781 0.47107781 0.         0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: Everybody knew the Bucks had no chance against the Heat
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['everybody', 'knew', 'the', 'bucks', 'had', 'no', 'chance', 'against', 'the', 'heat']
cosine_similarity: 0.9687042236328125
train_input: [0.15064018498706508, 0.9687042], train_label: 1
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: I really want the Bucks to win the series
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672  0.
  0.        ]
 [0.33517574 0.         0.47107781 0.47107781 0.         0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: I really want the Bucks to win the series
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['i', 'really', 'want', 'the', 'bucks', 'to', 'win', 'the', 'series']
cosine_similarity: 0.9807527661323547
train_input: [0.15064018498706508, 0.98075277], train_label: 0
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: That the Bucks are shooting 5
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672 ]
 [0.57973867 0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: That the Bucks are shooting 5
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['that', 'the', 'bucks', 'are', 'shooting']
cosine_similarity: 0.9651530385017395
train_input: [0.2605556710562624, 0.96515304], train_label: 0
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: The Bucks are boring to watch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.        ]
 [0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: The Bucks are boring to watch
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['the', 'bucks', 'are', 'boring', 'to', 'watch']
cosine_similarity: 0.9811093211174011
train_input: [0.20199309249791833, 0.9811093], train_label: 0
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: The bucks are too close for our crowd not to be turnt
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672  0.        ]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: The bucks are too close for our crowd not to be turnt
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['the', 'bucks', 'are', 'too', 'close', 'for', 'our', 'crowd', 'not', 'to', 'be', 'turnt']
cosine_similarity: 0.9864233732223511
train_input: [0.17077611319011649, 0.9864234], train_label: 0
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: The bucks arent going to win against the heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672
  0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: The bucks arent going to win against the heat
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['the', 'bucks', 'arent', 'going', 'to', 'win', 'against', 'the', 'heat']
cosine_similarity: 0.9638664126396179
train_input: [0.15064018498706508, 0.9638664], train_label: 1
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: The good news is that I managed to spent 10 bucks there in the end
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.         0.
  0.         0.6316672  0.        ]
 [0.39204401 0.27894255 0.39204401 0.         0.39204401 0.39204401
  0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: The good news is that I managed to spent 10 bucks there in the end
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['the', 'good', 'news', 'is', 'that', 'i', 'managed', 'to', 'spent', 'bucks', 'there', 'in', 'the', 'end']
cosine_similarity: 0.9849485754966736
train_input: [0.12536693798731732, 0.9849486], train_label: 0
TF_IDF_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: Time to say good bye to the bucks tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.6316672
  0.         0.        ]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.
  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: I feel sorry for the bucks, sentence2: Time to say good bye to the bucks tonight
After tokenization, sentence1: ['i', 'feel', 'sorry', 'for', 'the', 'bucks'], sentence2: ['time', 'to', 'say', 'good', 'bye', 'to', 'the', 'bucks', 'tonight']
cosine_similarity: 0.9842104315757751
train_input: [0.1362763414390864, 0.98421043], train_label: 1
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Damn the Bucks are horrible
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57735027 0.57735027 0.         0.57735027]
 [0.57735027 0.57735027 0.         0.         0.57735027 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Damn the Bucks are horrible
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['damn', 'the', 'bucks', 'are', 'horrible']
cosine_similarity: 0.9725614786148071
train_input: [0.0, 0.9725615], train_label: 1
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Fail of a possession for the bucks smh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57735027 0.57735027 0.         0.57735027
  0.        ]
 [0.5        0.5        0.         0.         0.5        0.
  0.5       ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Fail of a possession for the bucks smh
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['fail', 'of', 'a', 'possession', 'for', 'the', 'bucks', 'smh']
cosine_similarity: 0.9672816395759583
train_input: [0.0, 0.96728164], train_label: 0
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Heat not even trying vs the Bucks weak ass
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57735027 0.         0.57735027 0.57735027
  0.         0.         0.        ]
 [0.40824829 0.40824829 0.         0.40824829 0.         0.
  0.40824829 0.40824829 0.40824829]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Heat not even trying vs the Bucks weak ass
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['heat', 'not', 'even', 'trying', 'vs', 'the', 'bucks', 'weak', 'ass']
cosine_similarity: 0.9727314114570618
train_input: [0.0, 0.9727314], train_label: 1
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: If u at the bucks vs heat game but u sittin upper level
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.         0.6316672
  0.         0.         0.        ]
 [0.39204401 0.27894255 0.39204401 0.         0.39204401 0.
  0.39204401 0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: If u at the bucks vs heat game but u sittin upper level
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['if', 'u', 'at', 'the', 'bucks', 'vs', 'heat', 'game', 'but', 'u', 'sittin', 'upper', 'level']
cosine_similarity: 0.9681981801986694
train_input: [0.12536693798731732, 0.9681982], train_label: 0
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Lebron is just playing with the Bucks right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.57735027 0.         0.         0.
  0.         0.57735027]
 [0.4472136  0.         0.         0.4472136  0.4472136  0.4472136
  0.4472136  0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Lebron is just playing with the Bucks right now
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['lebron', 'is', 'just', 'playing', 'with', 'the', 'bucks', 'right', 'now']
cosine_similarity: 0.9830990433692932
train_input: [0.0, 0.98309904], train_label: 0
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Man I thought the bucks could a beat Miami
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57735027 0.57735027 0.         0.
  0.57735027 0.        ]
 [0.4472136  0.4472136  0.         0.         0.4472136  0.4472136
  0.         0.4472136 ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: Man I thought the bucks could a beat Miami
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['man', 'i', 'thought', 'the', 'bucks', 'could', 'a', 'beat', 'miami']
cosine_similarity: 0.9656070470809937
train_input: [0.0, 0.96560705], train_label: 1
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: My Bucks Are Playing Great Against The Heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.         0.57735027 0.
  0.57735027]
 [0.5        0.         0.5        0.5        0.         0.5
  0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: My Bucks Are Playing Great Against The Heat
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['my', 'bucks', 'are', 'playing', 'great', 'against', 'the', 'heat']
cosine_similarity: 0.9691388607025146
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: The Bucks say their not going down easy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57735027 0.         0.57735027 0.
  0.57735027]
 [0.5        0.5        0.         0.5        0.         0.5
  0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: The Bucks say their not going down easy
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['the', 'bucks', 'say', 'their', 'not', 'going', 'down', 'easy']
cosine_similarity: 0.9602208137512207
train_input: [0.0, 0.9602208], train_label: 0
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: The bucks are too close for our crowd not to be turnt
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.57735027 0.57735027 0.57735027
  0.        ]
 [0.5        0.5        0.5        0.         0.         0.
  0.5       ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: The bucks are too close for our crowd not to be turnt
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['the', 'bucks', 'are', 'too', 'close', 'for', 'our', 'crowd', 'not', 'to', 'be', 'turnt']
cosine_similarity: 0.9535675644874573
TF_IDF_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: This HeatBucks game is some seriously ugly basketball
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.50154891 0.         0.70490949 0.        ]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: The HeatBucks game is shit, sentence2: This HeatBucks game is some seriously ugly basketball
After tokenization, sentence1: ['the', 'game', 'is', 'shit'], sentence2: ['this', 'game', 'is', 'some', 'seriously', 'ugly', 'basketball']
cosine_similarity: 0.9843891859054565
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: As Soon As They Said The Bucks Had To Play The Heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.35520009 0.         0.49922133 0.
  0.         0.49922133]
 [0.35520009 0.         0.35520009 0.49922133 0.         0.49922133
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: As Soon As They Said The Bucks Had To Play The Heat
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['as', 'soon', 'as', 'they', 'said', 'the', 'bucks', 'had', 'to', 'play', 'the', 'heat']
cosine_similarity: 0.9409890174865723
train_input: [0.2523342014336961, 0.940989], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: I Hope Tha Bucks Win
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.47107781 0.        ]
 [0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: I Hope Tha Bucks Win
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['i', 'hope', 'tha', 'bucks', 'win']
cosine_similarity: 0.9718906283378601
train_input: [0.1273595297947935, 0.9718906], train_label: 1
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: It must suck to be the Bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.47107781 0.         0.47107781]
 [0.57973867 0.         0.         0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: It must suck to be the Bucks
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['it', 'must', 'suck', 'to', 'be', 'the', 'bucks']
cosine_similarity: 0.9559190273284912
train_input: [0.19431434016858146, 0.955919], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: This bucks and heat game is weak nigga
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.35520009 0.         0.49922133
  0.         0.49922133]
 [0.35520009 0.49922133 0.         0.35520009 0.49922133 0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: This bucks and heat game is weak nigga
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['this', 'bucks', 'and', 'heat', 'game', 'is', 'weak', 'nigga']
cosine_similarity: 0.9700063467025757
train_input: [0.2523342014336961, 0.97000635], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: This bucks heat game is zzzzzzzzzzzzzzzz
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.35520009 0.49922133 0.49922133
  0.        ]
 [0.40993715 0.57615236 0.         0.40993715 0.         0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: This bucks heat game is zzzzzzzzzzzzzzzz
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['this', 'bucks', 'heat', 'game', 'is', 'zzzzzzzzzzzzzzzz']
cosine_similarity: 0.9659374356269836
train_input: [0.29121941856368966, 0.96593744], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: This heat vs bucks game is beyond boring
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.49922133 0.35520009 0.49922133
  0.         0.49922133]
 [0.49922133 0.35520009 0.49922133 0.         0.35520009 0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: This heat vs bucks game is beyond boring
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['this', 'heat', 'vs', 'bucks', 'game', 'is', 'beyond', 'boring']
cosine_similarity: 0.9375532269477844
train_input: [0.2523342014336961, 0.9375532], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: Watching the Heat hunt the Bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.35520009 0.         0.49922133 0.
  0.49922133]
 [0.40993715 0.         0.40993715 0.57615236 0.         0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: Watching the Heat hunt the Bucks
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['watching', 'the', 'heat', 'hunt', 'the', 'bucks']
cosine_similarity: 0.9470821619033813
train_input: [0.29121941856368966, 0.94708216], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: actin a foo at the MiamiHEAT vs Bucks game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33517574 0.         0.         0.47107781 0.47107781
  0.         0.47107781 0.         0.47107781]
 [0.4261596  0.30321606 0.4261596  0.4261596  0.         0.
  0.4261596  0.         0.4261596  0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: actin a foo at the MiamiHEAT vs Bucks game
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['actin', 'a', 'foo', 'at', 'the', 'miamiheat', 'vs', 'bucks', 'game']
cosine_similarity: 0.9432097673416138
train_input: [0.10163066979112656, 0.94320977], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: fantards are REALLY hoping THEY can be the Bucks next year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.47107781 0.         0.47107781
  0.         0.47107781 0.        ]
 [0.33517574 0.47107781 0.         0.         0.47107781 0.
  0.47107781 0.         0.47107781]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: fantards are REALLY hoping THEY can be the Bucks next year
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['fantards', 'are', 'really', 'hoping', 'they', 'can', 'be', 'the', 'bucks', 'next', 'year']
cosine_similarity: 0.9406232237815857
train_input: [0.11234277891542777, 0.9406232], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: wants to play for the Bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.47107781]
 [0.44943642 0.         0.         0.6316672  0.         0.6316672
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Bucks pls gone whoop the heat, sentence2: wants to play for the Bucks
After tokenization, sentence1: ['bucks', 'pls', 'gone', 'whoop', 'the', 'heat'], sentence2: ['wants', 'to', 'play', 'for', 'the', 'bucks']
cosine_similarity: 0.9445589184761047
train_input: [0.15064018498706508, 0.9445589], train_label: 0
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: Ellis bout to win one for the bucks thou
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30321606 0.8523192  0.         0.4261596  0.
  0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.         0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: Ellis bout to win one for the bucks thou
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['ellis', 'bout', 'to', 'win', 'one', 'for', 'the', 'bucks', 'thou']
cosine_similarity: 0.9677248001098633
train_input: [0.10163066979112656, 0.9677248], train_label: 0
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: If the bucks win imma be surprised
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.8523192  0.         0.4261596  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: If the bucks win imma be surprised
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['if', 'the', 'bucks', 'win', 'imma', 'be', 'surprised']
cosine_similarity: 0.9563242793083191
train_input: [0.11521554337793122, 0.9563243], train_label: 0
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: Let s finish the Bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.8523192  0.         0.         0.4261596 ]
 [0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: Let s finish the Bucks
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['let', 's', 'finish', 'the', 'bucks']
cosine_similarity: 0.9804596304893494
train_input: [0.1362763414390864, 0.98045963], train_label: 1
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: Miami Heat 67 62 Milwaukee Bucks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.30321606 0.8523192  0.         0.
  0.         0.4261596 ]
 [0.4261596  0.4261596  0.30321606 0.         0.4261596  0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.09193998]
 [0.09193998 1.        ]]
cosine_similarity: 0.09193998174078082
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: Miami Heat 67 62 Milwaukee Bucks
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['miami', 'heat', 'milwaukee', 'bucks']
cosine_similarity: 0.8146381378173828
train_input: [0.09193998174078082, 0.81463814], train_label: 0
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: The Bucks areana aint even loud tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.30321606 0.8523192  0.         0.4261596
  0.        ]
 [0.47107781 0.47107781 0.33517574 0.         0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: The Bucks areana aint even loud tho
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['the', 'bucks', 'areana', 'aint', 'even', 'loud', 'tho']
cosine_similarity: 0.9156804084777832
train_input: [0.10163066979112656, 0.9156804], train_label: 0
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: The Milwaukee Bucks are terrible
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.8523192  0.         0.4261596  0.        ]
 [0.44943642 0.         0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: The Milwaukee Bucks are terrible
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['the', 'milwaukee', 'bucks', 'are', 'terrible']
cosine_similarity: 0.9305490851402283
train_input: [0.1362763414390864, 0.9305491], train_label: 1
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: This MiamiBucks Game MADD WHACK
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40824829 0.81649658 0.         0.         0.         0.40824829
  0.        ]
 [0.         0.         0.5        0.5        0.5        0.
  0.5       ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: This MiamiBucks Game MADD WHACK
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['this', 'game', 'madd', 'whack']
cosine_similarity: 0.8721957206726074
train_input: [0.0, 0.8721957], train_label: 0
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: This a good game bucks staying right there with the heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.8523192  0.         0.         0.         0.
  0.4261596  0.        ]
 [0.30321606 0.         0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.09193998]
 [0.09193998 1.        ]]
cosine_similarity: 0.09193998174078082
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: This a good game bucks staying right there with the heat
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['this', 'a', 'good', 'game', 'bucks', 'staying', 'right', 'there', 'with', 'the', 'heat']
cosine_similarity: 0.9813494682312012
train_input: [0.09193998174078082, 0.98134947], train_label: 0
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: Watching heat bucks no wade for heat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.8523192  0.         0.4261596  0.         0.        ]
 [0.27894255 0.         0.78408803 0.         0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.08457986]
 [0.08457986 1.        ]]
cosine_similarity: 0.0845798608014702
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: Watching heat bucks no wade for heat
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['watching', 'heat', 'bucks', 'no', 'wade', 'for', 'heat']
cosine_similarity: 0.9354339241981506
train_input: [0.0845798608014702, 0.9354339], train_label: 0
TF_IDF_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: What channel is the bucks game on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.8523192  0.         0.         0.4261596 ]
 [0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: it s bye bye to the bucks season, sentence2: What channel is the bucks game on
After tokenization, sentence1: ['it', 's', 'bye', 'bye', 'to', 'the', 'bucks', 'season'], sentence2: ['what', 'channel', 'is', 'the', 'bucks', 'game', 'on']
cosine_similarity: 0.9625760316848755
train_input: [0.1362763414390864, 0.96257603], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Atleast the 4 goals we received were from the BundesLiga CHAMP
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.         0.53404633 0.53404633
  0.         0.        ]
 [0.47107781 0.         0.33517574 0.47107781 0.         0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Atleast the 4 goals we received were from the BundesLiga CHAMP
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['atleast', 'the', 'goals', 'we', 'received', 'were', 'from', 'the', 'bundesliga', 'champ']
cosine_similarity: 0.9552116394042969
train_input: [0.1273595297947935, 0.95521164], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Bundesliga s prob going to Wembley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.
  0.        ]
 [0.         0.37997836 0.         0.         0.53404633 0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Bundesliga s prob going to Wembley
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['bundesliga', 's', 'prob', 'going', 'to', 'wembley']
cosine_similarity: 0.8869448304176331
train_input: [0.1443835552773867, 0.88694483], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Do the bundesliga and la liga teams display a higher quality of football
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.53404633 0.
  0.         0.         0.         0.         0.        ]
 [0.         0.25969799 0.         0.36499647 0.         0.36499647
  0.36499647 0.36499647 0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.09867962]
 [0.09867962 1.        ]]
cosine_similarity: 0.09867961797986956
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Do the bundesliga and la liga teams display a higher quality of football
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['do', 'the', 'bundesliga', 'and', 'la', 'liga', 'teams', 'display', 'a', 'higher', 'quality', 'of', 'football']
cosine_similarity: 0.9468936324119568
train_input: [0.09867961797986956, 0.94689363], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Does the B in BMW stand for Bundesliga
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633
  0.        ]
 [0.         0.53404633 0.37997836 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Does the B in BMW stand for Bundesliga
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['does', 'the', 'b', 'in', 'bmw', 'stand', 'for', 'bundesliga']
cosine_similarity: 0.9557989835739136
train_input: [0.1443835552773867, 0.955799], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Interesting to see the changing in power between EPL La Liga Serie A and Bundesliga
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.         0.53404633
  0.         0.         0.         0.         0.        ]
 [0.         0.25969799 0.36499647 0.         0.36499647 0.
  0.36499647 0.36499647 0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.09867962]
 [0.09867962 1.        ]]
cosine_similarity: 0.09867961797986956
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Interesting to see the changing in power between EPL La Liga Serie A and Bundesliga
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['interesting', 'to', 'see', 'the', 'changing', 'in', 'power', 'between', 'epl', 'la', 'liga', 'serie', 'a', 'and', 'bundesliga']
cosine_similarity: 0.9433746933937073
train_input: [0.09867961797986956, 0.9433747], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Starting to think Dortmund are just the Bundesligas Arsenal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.5        0.5        0.         0.5        0.
  0.5        0.         0.         0.        ]
 [0.40824829 0.         0.         0.40824829 0.         0.40824829
  0.         0.40824829 0.40824829 0.40824829]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: Starting to think Dortmund are just the Bundesligas Arsenal
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['starting', 'to', 'think', 'dortmund', 'are', 'just', 'the', 'arsenal']
cosine_similarity: 0.9273906946182251
train_input: [0.0, 0.9273907], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: The Bundesliga doesnt get much coverage here in the US
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.         0.53404633]
 [0.         0.44943642 0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: The Bundesliga doesnt get much coverage here in the US
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['the', 'bundesliga', 'doesnt', 'get', 'much', 'coverage', 'here', 'in', 'the', 'us']
cosine_similarity: 0.9566981196403503
train_input: [0.17077611319011649, 0.9566981], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: The Bundesliga will be the best league soon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.        ]
 [0.         0.53404633 0.37997836 0.         0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: The Bundesliga will be the best league soon
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['the', 'bundesliga', 'will', 'be', 'the', 'best', 'league', 'soon']
cosine_similarity: 0.9544503688812256
train_input: [0.1443835552773867, 0.95445037], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: bundesliga has the goals and Bundesbank has the gold
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.        ]
 [0.         0.53404633 0.37997836 0.         0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: bundesliga has the goals and Bundesbank has the gold
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['bundesliga', 'has', 'the', 'goals', 'and', 'bundesbank', 'has', 'the', 'gold']
cosine_similarity: 0.9469754099845886
train_input: [0.1443835552773867, 0.9469754], train_label: 0
TF_IDF_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: this Bundesliga final is going to be mesmerising
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.53404633 0.
  0.        ]
 [0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: avid follower and critique of the Bundesliga, sentence2: this Bundesliga final is going to be mesmerising
After tokenization, sentence1: ['avid', 'follower', 'and', 'critique', 'of', 'the', 'bundesliga'], sentence2: ['this', 'bundesliga', 'final', 'is', 'going', 'to', 'be', 'mesmerising']
cosine_similarity: 0.9392605423927307
train_input: [0.1443835552773867, 0.93926054], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: As a librarian the words Bush and library together always make me giggle
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.         0.50154891 0.
  0.        ]
 [0.31779954 0.         0.44665616 0.44665616 0.31779954 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: As a librarian the words Bush and library together always make me giggle
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['as', 'a', 'librarian', 'the', 'words', 'bush', 'and', 'library', 'together', 'always', 'make', 'me', 'giggle']
cosine_similarity: 0.9587357640266418
train_input: [0.31878402175377923, 0.95873576], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Awesome celebration at the Georges W Bush Presidential Library Museum
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.         0.70490949 0.         0.50154891
  0.         0.        ]
 [0.4078241  0.29017021 0.4078241  0.         0.4078241  0.29017021
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Awesome celebration at the Georges W Bush Presidential Library Museum
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['awesome', 'celebration', 'at', 'the', 'georges', 'w', 'bush', 'presidential', 'library', 'museum']
cosine_similarity: 0.9436061978340149
train_input: [0.2910691023819054, 0.9436062], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Bush Memorial Library is torture
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.50154891 0.         0.        ]
 [0.40993715 0.         0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Bush Memorial Library is torture
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['bush', 'memorial', 'library', 'is', 'torture']
cosine_similarity: 0.9483044147491455
train_input: [0.4112070550676187, 0.9483044], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Bush Presidential Center library at SMU in Dallas
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.         0.70490949 0.50154891 0.
  0.        ]
 [0.31779954 0.44665616 0.44665616 0.         0.31779954 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Bush Presidential Center library at SMU in Dallas
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['bush', 'presidential', 'center', 'library', 'at', 'smu', 'in', 'dallas']
cosine_similarity: 0.9071800112724304
train_input: [0.31878402175377923, 0.90718], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Bush Presidential Library Museum I hope everyone had the chance to see
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.70490949 0.         0.50154891 0.
  0.        ]
 [0.31779954 0.44665616 0.         0.44665616 0.31779954 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Bush Presidential Library Museum I hope everyone had the chance to see
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['bush', 'presidential', 'library', 'museum', 'i', 'hope', 'everyone', 'had', 'the', 'chance', 'to', 'see']
cosine_similarity: 0.9838243722915649
train_input: [0.31878402175377923, 0.9838244], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Bush says he wants his b
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.         0.        ]
 [0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Bush says he wants his b
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['bush', 'says', 'he', 'wants', 'his', 'b']
cosine_similarity: 0.9262299537658691
train_input: [0.20199309249791833, 0.92622995], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: By the way Bush is a drunk TwitchyTeam
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: By the way Bush is a drunk TwitchyTeam
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['by', 'the', 'way', 'bush', 'is', 'a', 'drunk']
cosine_similarity: 0.9647940993309021
train_input: [0.17077611319011649, 0.9647941], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: George Bush is less confused than you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672 ]
 [0.44943642 0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: George Bush is less confused than you
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['george', 'bush', 'is', 'less', 'confused', 'than', 'you']
cosine_similarity: 0.9611907005310059
train_input: [0.20199309249791833, 0.9611907], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: I dont care what W Bush is like personally
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.
  0.        ]
 [0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: I dont care what W Bush is like personally
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['i', 'dont', 'care', 'what', 'w', 'bush', 'is', 'like', 'personally']
cosine_similarity: 0.9418325424194336
train_input: [0.15064018498706508, 0.94183254], train_label: 0
TF_IDF_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Texas has Johnson and Bush presidential libraries
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672  0.
  0.        ]
 [0.33517574 0.         0.47107781 0.47107781 0.         0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Bush dedicating the library that will, sentence2: Texas has Johnson and Bush presidential libraries
After tokenization, sentence1: ['bush', 'dedicating', 'the', 'library', 'that', 'will'], sentence2: ['texas', 'has', 'johnson', 'and', 'bush', 'presidential', 'libraries']
cosine_similarity: 0.938554048538208
train_input: [0.15064018498706508, 0.93855405], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: CJ Watson missed dunk in the 4th quarter
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.         0.44665616 0.
  0.44665616 0.         0.44665616 0.31779954]
 [0.44665616 0.31779954 0.         0.44665616 0.         0.44665616
  0.         0.44665616 0.         0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: CJ Watson missed dunk in the 4th quarter
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['cj', 'watson', 'missed', 'dunk', 'in', 'the', 'quarter']
cosine_similarity: 0.9510514736175537
train_input: [0.20199309249791833, 0.9510515], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: CJ Watson s life was in Nate Robinson s hands for at least 30 seconds
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.49844628 0.49844628 0.         0.
  0.35464863 0.35464863 0.         0.35464863]
 [0.40740124 0.28986934 0.         0.         0.40740124 0.40740124
  0.28986934 0.28986934 0.40740124 0.28986934]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: CJ Watson s life was in Nate Robinson s hands for at least 30 seconds
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['cj', 'watson', 's', 'life', 'was', 'in', 'nate', 'robinson', 's', 'hands', 'for', 'at', 'least', 'seconds']
cosine_similarity: 0.9543930292129517
train_input: [0.41120705506761857, 0.954393], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Momentum changed when Nate slammed CJ Watson on the scorer s table
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.47042643 0.47042643 0.         0.33471228
  0.47042643 0.         0.         0.         0.33471228]
 [0.39166832 0.27867523 0.         0.         0.39166832 0.27867523
  0.         0.39166832 0.39166832 0.39166832 0.27867523]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Momentum changed when Nate slammed CJ Watson on the scorer s table
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['momentum', 'changed', 'when', 'nate', 'slammed', 'cj', 'watson', 'on', 'the', 'scorer', 's', 'table']
cosine_similarity: 0.9570614099502563
train_input: [0.27982806524328774, 0.9570614], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Nate Robinson made Cj Watson his bitch lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.49844628 0.49844628 0.         0.35464863
  0.35464863 0.35464863]
 [0.49844628 0.35464863 0.         0.         0.49844628 0.35464863
  0.35464863 0.35464863]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Nate Robinson made Cj Watson his bitch lol
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['nate', 'robinson', 'made', 'cj', 'watson', 'his', 'bitch', 'lol']
cosine_similarity: 0.9783199429512024
train_input: [0.5031026124151313, 0.97831994], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Nate Robinson was bout to flex CJ Watson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.49844628 0.49844628 0.         0.35464863
  0.35464863 0.35464863]
 [0.49844628 0.35464863 0.         0.         0.49844628 0.35464863
  0.35464863 0.35464863]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Nate Robinson was bout to flex CJ Watson
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['nate', 'robinson', 'was', 'bout', 'to', 'flex', 'cj', 'watson']
cosine_similarity: 0.9728081226348877
train_input: [0.5031026124151313, 0.9728081], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Nate would mess CJ Watson up in a fight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.49844628 0.35464863 0.         0.35464863 0.49844628
  0.35464863]
 [0.4090901  0.         0.4090901  0.57496187 0.4090901  0.
  0.4090901 ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Nate would mess CJ Watson up in a fight
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['nate', 'would', 'mess', 'cj', 'watson', 'up', 'in', 'a', 'fight']
cosine_similarity: 0.9730129837989807
train_input: [0.5803329846765685, 0.973013], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: That Cj Watson blown dunk might cost the Nets this game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.44665616 0.         0.44665616
  0.         0.44665616 0.         0.44665616 0.31779954]
 [0.4078241  0.29017021 0.4078241  0.         0.4078241  0.
  0.4078241  0.         0.4078241  0.         0.29017021]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: That Cj Watson blown dunk might cost the Nets this game
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['that', 'cj', 'watson', 'blown', 'dunk', 'might', 'cost', 'the', 'nets', 'this', 'game']
cosine_similarity: 0.9560356736183167
train_input: [0.18443191662261305, 0.9560357], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: This CJ Watson and Nate Robinson match is fun as hell
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.49844628 0.49844628 0.         0.         0.
  0.35464863 0.35464863 0.35464863]
 [0.3174044  0.         0.         0.44610081 0.44610081 0.44610081
  0.3174044  0.3174044  0.3174044 ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: This CJ Watson and Nate Robinson match is fun as hell
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['this', 'cj', 'watson', 'and', 'nate', 'robinson', 'match', 'is', 'fun', 'as', 'hell']
cosine_similarity: 0.9769139885902405
train_input: [0.4502681446556265, 0.976914], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Why naterobinson tie CJ Watson up like this
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.         0.44665616 0.
  0.44665616 0.         0.31779954]
 [0.35520009 0.         0.         0.49922133 0.         0.49922133
  0.         0.49922133 0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: Why naterobinson tie CJ Watson up like this
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['why', 'tie', 'cj', 'watson', 'up', 'like', 'this']
cosine_similarity: 0.9669581651687622
train_input: [0.22576484600261604, 0.96695817], train_label: 0
TF_IDF_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: that CJ Watson missed dunk may have cost us the series
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.44665616 0.
  0.44665616 0.44665616 0.         0.31779954]
 [0.31779954 0.44665616 0.         0.44665616 0.         0.44665616
  0.         0.         0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Nate Robinson and CJ Watson did NOT fight, sentence2: that CJ Watson missed dunk may have cost us the series
After tokenization, sentence1: ['nate', 'robinson', 'and', 'cj', 'watson', 'did', 'not', 'fight'], sentence2: ['that', 'cj', 'watson', 'missed', 'dunk', 'may', 'have', 'cost', 'us', 'the', 'series']
cosine_similarity: 0.950610876083374
train_input: [0.20199309249791833, 0.9506109], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: CALI Im too ready for you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.8523192  0.         0.4261596  0.        ]
 [0.44943642 0.         0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: CALI Im too ready for you
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['cali', 'im', 'too', 'ready', 'for', 'you']
cosine_similarity: 0.9714170694351196
train_input: [0.1362763414390864, 0.97141707], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: Come with me to cali
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.8523192  0.4261596 ]
 [0.57973867 0.81480247 0.         0.        ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: Come with me to cali
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['come', 'with', 'me', 'to', 'cali']
cosine_similarity: 0.9485208988189697
train_input: [0.17578607839334617, 0.9485209], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: Get me the fuck to Cali
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.8523192  0.4261596 ]
 [0.57973867 0.81480247 0.         0.        ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: Get me the fuck to Cali
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['get', 'me', 'the', 'fuck', 'to', 'cali']
cosine_similarity: 0.9702858328819275
train_input: [0.17578607839334617, 0.97028583], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: I need a Cali trip soon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.89331232 0.31779954 0.         0.        ]
 [0.40993715 0.         0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: I need a Cali trip soon
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['i', 'need', 'a', 'cali', 'trip', 'soon']
cosine_similarity: 0.9625294208526611
train_input: [0.2605556710562624, 0.9625294], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: I seriously need to pay a visit to Cali one day
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.89331232 0.31779954 0.         0.
  0.        ]
 [0.31779954 0.44665616 0.         0.31779954 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: I seriously need to pay a visit to Cali one day
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['i', 'seriously', 'need', 'to', 'pay', 'a', 'visit', 'to', 'cali', 'one', 'day']
cosine_similarity: 0.9808590412139893
train_input: [0.20199309249791833, 0.98085904], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: I would say Cali is one of the best
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30321606 0.8523192  0.4261596  0.        ]
 [0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: I would say Cali is one of the best
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['i', 'would', 'say', 'cali', 'is', 'one', 'of', 'the', 'best']
cosine_similarity: 0.9814339280128479
train_input: [0.1362763414390864, 0.9814339], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: If I eve hit lottery Id have a house in NY Cali and Florida
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.         0.8523192  0.         0.
  0.         0.         0.4261596  0.        ]
 [0.25969799 0.36499647 0.36499647 0.         0.36499647 0.36499647
  0.36499647 0.36499647 0.         0.36499647]]
pairwise_similarity: [[1.        0.0787446]
 [0.0787446 1.       ]]
cosine_similarity: 0.07874460345594508
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: If I eve hit lottery Id have a house in NY Cali and Florida
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['if', 'i', 'eve', 'hit', 'lottery', 'id', 'have', 'a', 'house', 'in', 'ny', 'cali', 'and', 'florida']
cosine_similarity: 0.9765287041664124
train_input: [0.07874460345594508, 0.9765287], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: Soon as I get back to Cali Gym a Tattoo is a must
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.8523192  0.4261596  0.         0.        ]
 [0.37997836 0.53404633 0.         0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: Soon as I get back to Cali Gym a Tattoo is a must
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['soon', 'as', 'i', 'get', 'back', 'to', 'cali', 'gym', 'a', 'tattoo', 'is', 'a', 'must']
cosine_similarity: 0.9747570753097534
train_input: [0.11521554337793122, 0.9747571], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: You got fucked over by someone in Cali too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.         0.8523192  0.4261596 ]
 [0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: You got fucked over by someone in Cali too
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['you', 'got', 'fucked', 'over', 'by', 'someone', 'in', 'cali', 'too']
cosine_similarity: 0.9834920763969421
train_input: [0.1362763414390864, 0.9834921], train_label: 0
TF_IDF_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: to the person moving from Cali
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.8523192  0.         0.4261596  0.        ]
 [0.44943642 0.         0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Once I handle what I need to handle in Cali, sentence2: to the person moving from Cali
After tokenization, sentence1: ['once', 'i', 'handle', 'what', 'i', 'need', 'to', 'handle', 'in', 'cali'], sentence2: ['to', 'the', 'person', 'moving', 'from', 'cali']
cosine_similarity: 0.9740806221961975
train_input: [0.1362763414390864, 0.9740806], train_label: 0
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: CALUM PLEASE FOLLOW ME ITD MEAN THE WORLD TO ME PLEASE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.40993715 0.         0.
  0.        ]
 [0.         0.35520009 0.         0.35520009 0.49922133 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: CALUM PLEASE FOLLOW ME ITD MEAN THE WORLD TO ME PLEASE
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['calum', 'please', 'follow', 'me', 'itd', 'mean', 'the', 'world', 'to', 'me', 'please']
cosine_similarity: 0.9804362058639526
train_input: [0.29121941856368966, 0.9804362], train_label: 1
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: CALUM PLS FOLLOW ME PLS I LOVE YOU
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.40993715 0.         0.        ]
 [0.         0.29017021 0.         0.29017021 0.4078241  0.81564821]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: CALUM PLS FOLLOW ME PLS I LOVE YOU
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['calum', 'pls', 'follow', 'me', 'pls', 'i', 'love', 'you']
cosine_similarity: 0.9864558577537537
train_input: [0.23790309463326234, 0.98645586], train_label: 1
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: CALUM WHY WONT YOU FOLLOW ME AND ashtonirwow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.57615236 0.40993715 0.        ]
 [0.57615236 0.         0.40993715 0.         0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: CALUM WHY WONT YOU FOLLOW ME AND ashtonirwow
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['calum', 'why', 'wont', 'you', 'follow', 'me', 'and']
cosine_similarity: 0.9745445847511292
train_input: [0.3360969272762575, 0.9745446], train_label: 1
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: Calum please tell me youre still online
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.
  0.        ]
 [0.         0.37997836 0.         0.         0.53404633 0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: Calum please tell me youre still online
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['calum', 'please', 'tell', 'me', 'youre', 'still', 'online']
cosine_similarity: 0.9744188189506531
train_input: [0.1443835552773867, 0.9744188], train_label: 0
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: UGH I JUST WANT A FOLLOW FROM CALUM
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.40993715 0.         0.
  0.        ]
 [0.         0.35520009 0.         0.35520009 0.49922133 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: UGH I JUST WANT A FOLLOW FROM CALUM
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['ugh', 'i', 'just', 'want', 'a', 'follow', 'from', 'calum']
cosine_similarity: 0.9722621440887451
train_input: [0.29121941856368966, 0.97226214], train_label: 1
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: WHY WONT CALUM FOLLOW ME IM GOING TO STRANGLE HIM IN HIS FUCKING SLEEP
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.40993715 0.         0.
  0.         0.         0.         0.        ]
 [0.         0.26868528 0.         0.26868528 0.37762778 0.37762778
  0.37762778 0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: WHY WONT CALUM FOLLOW ME IM GOING TO STRANGLE HIM IN HIS FUCKING SLEEP
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['why', 'wont', 'calum', 'follow', 'me', 'im', 'going', 'to', 'strangle', 'him', 'in', 'his', 'fucking', 'sleep']
cosine_similarity: 0.9359199404716492
train_input: [0.2202881505618297, 0.93591994], train_label: 1
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: calum my child come to me embrace me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.         0.53404633 0.
  0.53404633]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: calum my child come to me embrace me
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['calum', 'my', 'child', 'come', 'to', 'me', 'embrace', 'me']
cosine_similarity: 0.9620603322982788
train_input: [0.1443835552773867, 0.96206033], train_label: 0
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: calum please follow babe itd mean so so much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.57615236 0.40993715 0.
  0.        ]
 [0.49922133 0.         0.35520009 0.         0.35520009 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: calum please follow babe itd mean so so much
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['calum', 'please', 'follow', 'babe', 'itd', 'mean', 'so', 'so', 'much']
cosine_similarity: 0.9501392841339111
train_input: [0.29121941856368966, 0.9501393], train_label: 1
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: hiiiii Calum I love you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]
 [0.         0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: hiiiii Calum I love you
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['calum', 'i', 'love', 'you']
cosine_similarity: 0.9603021740913391
train_input: [0.17077611319011649, 0.9603022], train_label: 0
TF_IDF_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: why wont calum notice me am i not cool enough
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.53404633 0.
  0.        ]
 [0.         0.37997836 0.53404633 0.         0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: you da bomb please follow me calum, sentence2: why wont calum notice me am i not cool enough
After tokenization, sentence1: ['you', 'da', 'bomb', 'please', 'follow', 'me', 'calum'], sentence2: ['why', 'wont', 'calum', 'notice', 'me', 'am', 'i', 'not', 'cool', 'enough']
cosine_similarity: 0.9460024833679199
train_input: [0.1443835552773867, 0.9460025], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: C A L U M please follow me calum follow me please calum follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678]
 [0.5547002  0.83205029]]
pairwise_similarity: [[1.         0.98058068]
 [0.98058068 1.        ]]
cosine_similarity: 0.9805806756909201
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: C A L U M please follow me calum follow me please calum follow me
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['c', 'a', 'l', 'u', 'm', 'please', 'follow', 'me', 'calum', 'follow', 'me', 'please', 'calum', 'follow', 'me']
cosine_similarity: 0.9487375617027283
train_input: [0.9805806756909201, 0.94873756], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: Calum freaking hell please notice me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.        ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: Calum freaking hell please notice me
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['calum', 'freaking', 'hell', 'please', 'notice', 'me']
cosine_similarity: 0.9594520926475525
train_input: [0.22028815056182965, 0.9594521], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: Calum if you follow me Ill make you toast for breakfast every morning
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.70710678 0.70710678 0.         0.         0.
  0.        ]
 [0.4078241  0.29017021 0.29017021 0.4078241  0.4078241  0.4078241
  0.4078241 ]]
pairwise_similarity: [[1.         0.41036264]
 [0.41036264 1.        ]]
cosine_similarity: 0.4103626449521846
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: Calum if you follow me Ill make you toast for breakfast every morning
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['calum', 'if', 'you', 'follow', 'me', 'ill', 'make', 'you', 'toast', 'for', 'breakfast', 'every', 'morning']
cosine_similarity: 0.9509784579277039
train_input: [0.4103626449521846, 0.95097846], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: Calum why cant you follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678]
 [0.70710678 0.70710678]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 0.9999999999999998
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: Calum why cant you follow me
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['calum', 'why', 'cant', 'you', 'follow', 'me']
cosine_similarity: 0.9828527569770813
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: Calum why cant you notice me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.        ]
 [0.57973867 0.         0.81480247]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: Calum why cant you notice me
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['calum', 'why', 'cant', 'you', 'notice', 'me']
cosine_similarity: 0.9630630612373352
train_input: [0.3360969272762574, 0.96306306], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678]
 [0.70710678 0.70710678]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 0.9999999999999998
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me']
cosine_similarity: 0.9895114302635193
train_input: [0.9999999999999998, 0.98951143], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: calum do you know how much i love you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.        ]
 [0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: calum do you know how much i love you
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['calum', 'do', 'you', 'know', 'how', 'much', 'i', 'love', 'you']
cosine_similarity: 0.9438050985336304
train_input: [0.2605556710562624, 0.9438051], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: can i just get a calum follow please ugh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.        ]
 [0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: can i just get a calum follow please ugh
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['can', 'i', 'just', 'get', 'a', 'calum', 'follow', 'please', 'ugh']
cosine_similarity: 0.9704440236091614
train_input: [0.5797386715376657, 0.970444], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: i would go streaking if calum followed me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.        ]
 [0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: i would go streaking if calum followed me
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['i', 'would', 'go', 'streaking', 'if', 'calum', 'followed', 'me']
cosine_similarity: 0.9549723863601685
TF_IDF_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: ilysm calum Im crying u will never notice me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.81480247 0.         0.         0.        ]
 [0.33517574 0.47107781 0.         0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: CALUM CAN YOU PLEASE PLEASE PLEASE FOLLOW ME, sentence2: ilysm calum Im crying u will never notice me
After tokenization, sentence1: ['calum', 'can', 'you', 'please', 'please', 'please', 'follow', 'me'], sentence2: ['ilysm', 'calum', 'im', 'crying', 'u', 'will', 'never', 'notice', 'me']
cosine_similarity: 0.9560682773590088
train_input: [0.19431434016858146, 0.9560683], train_label: 0
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: CALUM FOLLOW ME OR THE SPIDER WILL CRAWL UP YOUR BUM HOLE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.40993715 0.57615236 0.
  0.57615236 0.        ]
 [0.44665616 0.31779954 0.44665616 0.31779954 0.         0.44665616
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: CALUM FOLLOW ME OR THE SPIDER WILL CRAWL UP YOUR BUM HOLE
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['calum', 'follow', 'me', 'or', 'the', 'spider', 'will', 'crawl', 'up', 'your', 'bum', 'hole']
cosine_similarity: 0.828529953956604
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: CONGRATS TO EVERYONE WHO GOT A CALUM FOLLOW
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.57615236 0.57615236]
 [0.40993715 0.57615236 0.40993715 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: CONGRATS TO EVERYONE WHO GOT A CALUM FOLLOW
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['congrats', 'to', 'everyone', 'who', 'got', 'a', 'calum', 'follow']
cosine_similarity: 0.9158268570899963
train_input: [0.3360969272762575, 0.91582686], train_label: 0
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: Im so fkn hungry and sad bc calum wont follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.40993715 0.57615236 0.
  0.57615236 0.         0.         0.        ]
 [0.37762778 0.26868528 0.37762778 0.26868528 0.         0.37762778
  0.         0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: Im so fkn hungry and sad bc calum wont follow me
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['im', 'so', 'fkn', 'hungry', 'and', 'sad', 'bc', 'calum', 'wont', 'follow', 'me']
cosine_similarity: 0.8744964003562927
train_input: [0.2202881505618297, 0.8744964], train_label: 0
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: OH PLEASE FOLLOW ME CALUM
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.        ]
 [0.50154891 0.50154891 0.         0.         0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: OH PLEASE FOLLOW ME CALUM
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['oh', 'please', 'follow', 'me', 'calum']
cosine_similarity: 0.9892185926437378
train_input: [0.4112070550676187, 0.9892186], train_label: 1
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: OMFG CALUM FOLLOWED SOMEBODY THAT I FOLLOW
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.57615236 0.
  0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.         0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: OMFG CALUM FOLLOWED SOMEBODY THAT I FOLLOW
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['omfg', 'calum', 'followed', 'somebody', 'that', 'i', 'follow']
cosine_similarity: 0.9382773041725159
train_input: [0.29121941856368966, 0.9382773], train_label: 0
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: PLEASE FOLLOW ME CALUM IT WOULD MEAN A LOT x48
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: PLEASE FOLLOW ME CALUM IT WOULD MEAN A LOT x48
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['please', 'follow', 'me', 'calum', 'it', 'would', 'mean', 'a', 'lot']
cosine_similarity: 0.9374991655349731
train_input: [0.29121941856368966, 0.93749917], train_label: 1
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: WILL CALUM EVER NOTICE ME I THINK NO
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633 0.         0.        ]
 [0.44943642 0.         0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: WILL CALUM EVER NOTICE ME I THINK NO
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['will', 'calum', 'ever', 'notice', 'me', 'i', 'think', 'no']
cosine_similarity: 0.8915862441062927
train_input: [0.17077611319011649, 0.89158624], train_label: 0
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: calum i love you i love you sososo much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633 0.         0.        ]
 [0.30321606 0.         0.         0.         0.8523192  0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: calum i love you i love you sososo much
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['calum', 'i', 'love', 'you', 'i', 'love', 'you', 'sososo', 'much']
cosine_similarity: 0.9088903665542603
train_input: [0.11521554337793122, 0.90889037], train_label: 0
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: please follow me I nerly started crying because Calum didnt follow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.57615236 0.57615236
  0.         0.        ]
 [0.27840869 0.39129369 0.39129369 0.55681737 0.         0.
  0.39129369 0.39129369]]
pairwise_similarity: [[1.         0.34239019]
 [0.34239019 1.        ]]
cosine_similarity: 0.34239018611349886
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: please follow me I nerly started crying because Calum didnt follow
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['please', 'follow', 'me', 'i', 'started', 'crying', 'because', 'calum', 'didnt', 'follow']
cosine_similarity: 0.9378708004951477
train_input: [0.34239018611349886, 0.9378708], train_label: 0
TF_IDF_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: please follow me calum please please please please
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236]
 [0.70710678 0.70710678 0.         0.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: hi calum please follow me ily, sentence2: please follow me calum please please please please
After tokenization, sentence1: ['hi', 'calum', 'please', 'follow', 'me', 'ily'], sentence2: ['please', 'follow', 'me', 'calum', 'please', 'please', 'please', 'please']
cosine_similarity: 0.9808316230773926
train_input: [0.5797386715376657, 0.9808316], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: CALUM IF YOUR READING THIS PLZ FOLLLOW ME
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.53404633 0.
  0.        ]
 [0.37997836 0.53404633 0.         0.         0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: CALUM IF YOUR READING THIS PLZ FOLLLOW ME
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'if', 'your', 'reading', 'this', 'plz', 'folllow', 'me']
cosine_similarity: 0.9770314693450928
train_input: [0.1443835552773867, 0.97703147], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: CALUM ITS RUDE NOT TO FOLLOW SOMEONE BACK FOLLOW ME PLS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]
 [0.33425073 0.66850146 0.         0.         0.46977774 0.46977774]]
pairwise_similarity: [[1.         0.41106537]
 [0.41106537 1.        ]]
cosine_similarity: 0.4110653709825173
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: CALUM ITS RUDE NOT TO FOLLOW SOMEONE BACK FOLLOW ME PLS
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'its', 'rude', 'not', 'to', 'follow', 'someone', 'back', 'follow', 'me', 'pls']
cosine_similarity: 0.987847626209259
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Calum babycake would you please follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.57615236]
 [0.70490949 0.50154891 0.50154891 0.         0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Calum babycake would you please follow me
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me']
cosine_similarity: 0.9744604229927063
train_input: [0.4112070550676187, 0.9744604], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Calum remember when you followed me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.53404633 0.        ]
 [0.44943642 0.         0.6316672  0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Calum remember when you followed me
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'remember', 'when', 'you', 'followed', 'me']
cosine_similarity: 0.9828115701675415
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Forever a 04 loser because Calum will never see me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.53404633 0.
  0.53404633]
 [0.53404633 0.37997836 0.         0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Forever a 04 loser because Calum will never see me
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['forever', 'a', 'loser', 'because', 'calum', 'will', 'never', 'see', 'me']
cosine_similarity: 0.9718549847602844
train_input: [0.1443835552773867, 0.971855], train_label: 0
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Someone tell Calum that he needs to follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Someone tell Calum that he needs to follow me
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['someone', 'tell', 'calum', 'that', 'he', 'needs', 'to', 'follow', 'me']
cosine_similarity: 0.9870937466621399
train_input: [0.3360969272762575, 0.98709375], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum I would love to wake up with a follow from you because I love youuuu
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.44832087 0.         0.        ]
 [0.31701073 0.31701073 0.         0.63402146 0.44554752 0.44554752]]
pairwise_similarity: [[1.         0.56849011]
 [0.56849011 1.        ]]
cosine_similarity: 0.5684901084069616
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum I would love to wake up with a follow from you because I love youuuu
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'i', 'would', 'love', 'to', 'wake', 'up', 'with', 'a', 'follow', 'from', 'you', 'because', 'i', 'love']
cosine_similarity: 0.988946259021759
train_input: [0.5684901084069616, 0.98894626], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: can i just get a calum follow please ugh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.44832087 0.63009934 0.        ]
 [0.44832087 0.44832087 0.44832087 0.         0.63009934]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: can i just get a calum follow please ugh
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['can', 'i', 'just', 'get', 'a', 'calum', 'follow', 'please', 'ugh']
cosine_similarity: 0.9931598901748657
train_input: [0.6029748160380572, 0.9931599], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: could you dm me to calum please and ask if he will follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.40993715 0.57615236 0.57615236]
 [0.57615236 0.40993715 0.57615236 0.40993715 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: could you dm me to calum please and ask if he will follow me
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['could', 'you', 'dm', 'me', 'to', 'calum', 'please', 'and', 'ask', 'if', 'he', 'will', 'follow', 'me']
cosine_similarity: 0.9867317080497742
train_input: [0.3360969272762575, 0.9867317], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: would you like to follow me Calum
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236]
 [0.50154891 0.50154891 0.         0.70490949 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: would you like to follow me Calum
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['would', 'you', 'like', 'to', 'follow', 'me', 'calum']
cosine_similarity: 0.9941017627716064
train_input: [0.4112070550676187, 0.99410176], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: CALUM ILYSM ITD MEAN THE WORLD TO ME IF I COULD GET A FOLLOW
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.         0.
  0.        ]
 [0.         0.31779954 0.31779954 0.44665616 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: CALUM ILYSM ITD MEAN THE WORLD TO ME IF I COULD GET A FOLLOW
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['calum', 'ilysm', 'itd', 'mean', 'the', 'world', 'to', 'me', 'if', 'i', 'could', 'get', 'a', 'follow']
cosine_similarity: 0.965811550617218
train_input: [0.31878402175377923, 0.96581155], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: Calum babycake would you please follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.57735027 0.57735027]
 [0.57735027 0.57735027 0.57735027]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 1.0000000000000002
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: Calum babycake would you please follow me
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me']
cosine_similarity: 1.0
train_input: [1.0000000000000002, 1.0], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: Calum isnt here anymore and I didnt get the follow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.70490949 0.50154891 0.         0.50154891 0.        ]
 [0.49922133 0.         0.35520009 0.49922133 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: Calum isnt here anymore and I didnt get the follow
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['calum', 'isnt', 'here', 'anymore', 'and', 'i', 'didnt', 'get', 'the', 'follow']
cosine_similarity: 0.9437333941459656
train_input: [0.3563004293331381, 0.9437334], train_label: 0
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: Calum wont follow me so i am leaving
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.        ]
 [0.         0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: Calum wont follow me so i am leaving
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['calum', 'wont', 'follow', 'me', 'so', 'i', 'am', 'leaving']
cosine_similarity: 0.9513499736785889
train_input: [0.4112070550676187, 0.95135], train_label: 0
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: YOULL MAKE M SO HAPPY CALUM DONT YOU WANT TO MAKE ME HAPPY
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.6316672  0.         0.
  0.         0.        ]
 [0.         0.2097554  0.29480389 0.         0.58960778 0.58960778
  0.29480389 0.29480389]]
pairwise_similarity: [[1.         0.09427171]
 [0.09427171 1.        ]]
cosine_similarity: 0.09427171364984961
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: YOULL MAKE M SO HAPPY CALUM DONT YOU WANT TO MAKE ME HAPPY
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['youll', 'make', 'm', 'so', 'happy', 'calum', 'dont', 'you', 'want', 'to', 'make', 'me', 'happy']
cosine_similarity: 0.9521050453186035
train_input: [0.09427171364984961, 0.95210505], train_label: 0
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: calum follow me god dammit i have homework to do Calum5SOS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891 0.
  0.        ]
 [0.         0.31779954 0.44665616 0.44665616 0.31779954 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: calum follow me god dammit i have homework to do Calum5SOS
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['calum', 'follow', 'me', 'god', 'dammit', 'i', 'have', 'homework', 'to', 'do']
cosine_similarity: 0.938927412033081
train_input: [0.31878402175377923, 0.9389274], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: calum hi please follow HeyyyItsLizz and cutiecalum
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.        ]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: calum hi please follow HeyyyItsLizz and cutiecalum
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['calum', 'hi', 'please', 'follow', 'and']
cosine_similarity: 0.9796684980392456
train_input: [0.3563004293331381, 0.9796685], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: calum please follow me thats all i want pleaseeeee
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.         0.        ]
 [0.         0.35520009 0.35520009 0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: calum please follow me thats all i want pleaseeeee
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['calum', 'please', 'follow', 'me', 'thats', 'all', 'i', 'want']
cosine_similarity: 0.976262092590332
train_input: [0.3563004293331381, 0.9762621], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: how did you get Calum to follow you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891]
 [0.         0.50154891 0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: how did you get Calum to follow you
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['how', 'did', 'you', 'get', 'calum', 'to', 'follow', 'you']
cosine_similarity: 0.963350772857666
train_input: [0.5031026124151314, 0.9633508], train_label: 0
TF_IDF_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: im sad sick and without a calum follow do you feel my pain
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.         0.        ]
 [0.         0.29017021 0.4078241  0.29017021 0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Calum babycake would you please follow me, sentence2: im sad sick and without a calum follow do you feel my pain
After tokenization, sentence1: ['calum', 'babycake', 'would', 'you', 'please', 'follow', 'me'], sentence2: ['im', 'sad', 'sick', 'and', 'without', 'a', 'calum', 'follow', 'do', 'you', 'feel', 'my', 'pain']
cosine_similarity: 0.9148362278938293
train_input: [0.2910691023819054, 0.9148362], train_label: 0
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: CALUM FOLLLOW ME YOU BEAUTIFUL CREATURE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.         0.81480247]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: CALUM FOLLLOW ME YOU BEAUTIFUL CREATURE
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['calum', 'folllow', 'me', 'you', 'beautiful', 'creature']
cosine_similarity: 0.9106354713439941
train_input: [0.22028815056182965, 0.9106355], train_label: 1
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: CALUM FOLLOW ME PLEASE I LOVE YOU SO FUCKING MUCH
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.        ]
 [0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: CALUM FOLLOW ME PLEASE I LOVE YOU SO FUCKING MUCH
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['calum', 'follow', 'me', 'please', 'i', 'love', 'you', 'so', 'fucking', 'much']
cosine_similarity: 0.940875232219696
train_input: [0.5797386715376657, 0.94087523], train_label: 1
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: HEY CALUM PLEASE NOTICE ME OR FOLLOW ME
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.        ]
 [0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: HEY CALUM PLEASE NOTICE ME OR FOLLOW ME
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['hey', 'calum', 'please', 'notice', 'me', 'or', 'follow', 'me']
cosine_similarity: 0.972591757774353
train_input: [0.5797386715376657, 0.97259176], train_label: 1
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: I know youre on Calum
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.        ]
 [0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: I know youre on Calum
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['i', 'know', 'youre', 'on', 'calum']
cosine_similarity: 0.8945457935333252
train_input: [0.2605556710562624, 0.8945458], train_label: 0
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: I really want a Calum follow though
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.        ]
 [0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: I really want a Calum follow though
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['i', 'really', 'want', 'a', 'calum', 'follow', 'though']
cosine_similarity: 0.9258632063865662
train_input: [0.5797386715376657, 0.9258632], train_label: 1
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: IM LAUGHING SO MUCH CALUM PLEASE FOLLOW ME PLEASE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.        ]
 [0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: IM LAUGHING SO MUCH CALUM PLEASE FOLLOW ME PLEASE
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['im', 'laughing', 'so', 'much', 'calum', 'please', 'follow', 'me', 'please']
cosine_similarity: 0.9520117044448853
train_input: [0.5797386715376657, 0.9520117], train_label: 1
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: Seriously 5SOSUpdate are nice people they actually respond to me and they got a Calum follow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.70710678 0.70710678 0.         0.
  0.         0.         0.        ]
 [0.35327777 0.35327777 0.25136004 0.25136004 0.35327777 0.35327777
  0.35327777 0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.35547678]
 [0.35547678 1.        ]]
cosine_similarity: 0.35547677795468385
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: Seriously 5SOSUpdate are nice people they actually respond to me and they got a Calum follow
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['seriously', 'are', 'nice', 'people', 'they', 'actually', 'respond', 'to', 'me', 'and', 'they', 'got', 'a', 'calum', 'follow']
cosine_similarity: 0.8769150972366333
train_input: [0.35547677795468385, 0.8769151], train_label: 0
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: Sorry about the Calum spam I just really want my follow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.         0.         0.
  0.        ]
 [0.29017021 0.29017021 0.4078241  0.4078241  0.4078241  0.4078241
  0.4078241 ]]
pairwise_similarity: [[1.         0.41036264]
 [0.41036264 1.        ]]
cosine_similarity: 0.4103626449521846
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: Sorry about the Calum spam I just really want my follow
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['sorry', 'about', 'the', 'calum', 'spam', 'i', 'just', 'really', 'want', 'my', 'follow']
cosine_similarity: 0.9027186632156372
train_input: [0.4103626449521846, 0.90271866], train_label: 1
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: calum will never notice me okay
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.        ]
 [0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: calum will never notice me okay
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['calum', 'will', 'never', 'notice', 'me', 'okay']
cosine_similarity: 0.9226551055908203
train_input: [0.2605556710562624, 0.9226551], train_label: 0
TF_IDF_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: follow me or Ill eat my sock calum follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.         0.70710678 0.         0.        ]
 [0.30253071 0.42519636 0.60506143 0.42519636 0.42519636]]
pairwise_similarity: [[1.         0.64176456]
 [0.64176456 1.        ]]
cosine_similarity: 0.6417645565491049
word_to_vector_cosine_similarity: sentence1: PLEASE PLEASE PLEASE PLEASE PLEASE CALUM PLEASE FOLLOW ME, sentence2: follow me or Ill eat my sock calum follow me
After tokenization, sentence1: ['please', 'please', 'please', 'please', 'please', 'calum', 'please', 'follow', 'me'], sentence2: ['follow', 'me', 'or', 'ill', 'eat', 'my', 'sock', 'calum', 'follow', 'me']
cosine_similarity: 0.9370460510253906
train_input: [0.6417645565491049, 0.93704605], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: CALUM PLEASE FOLLOW ME BABE xx
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.57615236 0.        ]
 [0.57615236 0.40993715 0.40993715 0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: CALUM PLEASE FOLLOW ME BABE xx
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['calum', 'please', 'follow', 'me', 'babe', 'xx']
cosine_similarity: 0.994635820388794
train_input: [0.3360969272762575, 0.9946358], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Calum please follow me so I can get some sleep
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236]
 [0.50154891 0.50154891 0.         0.70490949 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Calum please follow me so I can get some sleep
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['calum', 'please', 'follow', 'me', 'so', 'i', 'can', 'get', 'some', 'sleep']
cosine_similarity: 0.906858503818512
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Calum s not going to notice me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.53404633]
 [0.44943642 0.         0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Calum s not going to notice me
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['calum', 's', 'not', 'going', 'to', 'notice', 'me']
cosine_similarity: 0.8948992490768433
train_input: [0.17077611319011649, 0.89489925], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Calum s not gonna follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.57615236]
 [0.50154891 0.50154891 0.70490949 0.         0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Calum s not gonna follow me
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['calum', 's', 'not', 'gonna', 'follow', 'me']
cosine_similarity: 0.9434608817100525
train_input: [0.4112070550676187, 0.9434609], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Calum5SOS will you help me reach my goal of
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5 0.  0.5 0.  0.  0.5 0.  0.5]
 [0.  0.5 0.  0.5 0.5 0.  0.5 0. ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Calum5SOS will you help me reach my goal of
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['will', 'you', 'help', 'me', 'reach', 'my', 'goal', 'of']
cosine_similarity: 0.8249801397323608
train_input: [0.0, 0.82498014], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Maybe I should stop trying getting a follow of Calum
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.         0.57615236]
 [0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: Maybe I should stop trying getting a follow of Calum
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['maybe', 'i', 'should', 'stop', 'trying', 'getting', 'a', 'follow', 'of', 'calum']
cosine_similarity: 0.8510990142822266
train_input: [0.2605556710562624, 0.851099], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: YO CALUM PLEASE FOLLOW ME IT WOULD MAKE ME SO SO SO HAPPY
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.57615236
  0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: YO CALUM PLEASE FOLLOW ME IT WOULD MAKE ME SO SO SO HAPPY
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['yo', 'calum', 'please', 'follow', 'me', 'it', 'would', 'make', 'me', 'so', 'so', 'so', 'happy']
cosine_similarity: 0.9234897494316101
train_input: [0.29121941856368966, 0.92348975], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: hey Calum do you know what
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.53404633]
 [0.44943642 0.         0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: hey Calum do you know what
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['hey', 'calum', 'do', 'you', 'know', 'what']
cosine_similarity: 0.8849013447761536
train_input: [0.17077611319011649, 0.88490134], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: if you have a calum follow im jealous
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.57615236]
 [0.40993715 0.40993715 0.         0.57615236 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: if you have a calum follow im jealous
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['if', 'you', 'have', 'a', 'calum', 'follow', 'im', 'jealous']
cosine_similarity: 0.9048159122467041
TF_IDF_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: youre a cock Only you Calum
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]
 [0.44943642 0.6316672  0.         0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: CALUM FOLLOW ME PLEASE ILY x39, sentence2: youre a cock Only you Calum
After tokenization, sentence1: ['calum', 'follow', 'me', 'please', 'ily'], sentence2: ['youre', 'a', 'cock', 'only', 'you', 'calum']
cosine_similarity: 0.8812836408615112
train_input: [0.17077611319011649, 0.88128364], train_label: 0
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: CALUM CAN YOU PLEASE FOLLOW ME OR almondnarry WE REALLY LOVE YOU 5
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.70490949 0.50154891 0.         0.        ]
 [0.49922133 0.35520009 0.         0.35520009 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: CALUM CAN YOU PLEASE FOLLOW ME OR almondnarry WE REALLY LOVE YOU 5
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['calum', 'can', 'you', 'please', 'follow', 'me', 'or', 'we', 'really', 'love', 'you']
cosine_similarity: 0.9293294548988342
train_input: [0.3563004293331381, 0.92932945], train_label: 1
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: CALUM FOLLOW ME I KNOW YOURE HERE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.50154891 0.         0.        ]
 [0.40993715 0.         0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: CALUM FOLLOW ME I KNOW YOURE HERE
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['calum', 'follow', 'me', 'i', 'know', 'youre', 'here']
cosine_similarity: 0.9431833028793335
train_input: [0.4112070550676187, 0.9431833], train_label: 1
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: CALUM HOOD WHY WONT YOU FOLLOW ME
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.50154891 0.         0.        ]
 [0.40993715 0.         0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: CALUM HOOD WHY WONT YOU FOLLOW ME
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['calum', 'hood', 'why', 'wont', 'you', 'follow', 'me']
cosine_similarity: 0.9411662220954895
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: CALUM PLEASE IM BEGGING YOU
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.        ]
 [0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: CALUM PLEASE IM BEGGING YOU
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['calum', 'please', 'im', 'begging', 'you']
cosine_similarity: 0.9097728729248047
train_input: [0.20199309249791833, 0.9097729], train_label: 1
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: Calum Hood follow me too please you sexy mofo Calum5SOS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.70490949 0.50154891 0.         0.
  0.        ]
 [0.31779954 0.44665616 0.         0.31779954 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: Calum Hood follow me too please you sexy mofo Calum5SOS
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['calum', 'hood', 'follow', 'me', 'too', 'please', 'you', 'sexy', 'mofo']
cosine_similarity: 0.9512016177177429
train_input: [0.31878402175377923, 0.9512016], train_label: 1
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: Calum isnt here anymore and I didnt get the follow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.70490949 0.         0.50154891 0.        ]
 [0.49922133 0.35520009 0.         0.49922133 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: Calum isnt here anymore and I didnt get the follow
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['calum', 'isnt', 'here', 'anymore', 'and', 'i', 'didnt', 'get', 'the', 'follow']
cosine_similarity: 0.8665626049041748
train_input: [0.3563004293331381, 0.8665626], train_label: 0
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: I spam calum and michael so much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.         0.        ]
 [0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: I spam calum and michael so much
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['i', 'spam', 'calum', 'and', 'michael', 'so', 'much']
cosine_similarity: 0.8906006217002869
train_input: [0.20199309249791833, 0.8906006], train_label: 0
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: Sigh Calum hasnt followed me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.         0.        ]
 [0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: Sigh Calum hasnt followed me
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['sigh', 'calum', 'hasnt', 'followed', 'me']
cosine_similarity: 0.9114397168159485
train_input: [0.20199309249791833, 0.9114397], train_label: 0
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: calum come back online and make my day my following me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672  0.
  0.         0.        ]
 [0.30321606 0.         0.4261596  0.4261596  0.         0.4261596
  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: calum come back online and make my day my following me
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['calum', 'come', 'back', 'online', 'and', 'make', 'my', 'day', 'my', 'following', 'me']
cosine_similarity: 0.9009494185447693
train_input: [0.1362763414390864, 0.9009494], train_label: 1
TF_IDF_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: think Calum s away and he didnt even follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.70490949 0.         0.50154891 0.        ]
 [0.49922133 0.35520009 0.         0.49922133 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: calum calummmm please follow me, sentence2: think Calum s away and he didnt even follow me
After tokenization, sentence1: ['calum', 'please', 'follow', 'me'], sentence2: ['think', 'calum', 's', 'away', 'and', 'he', 'didnt', 'even', 'follow', 'me']
cosine_similarity: 0.9067707657814026
train_input: [0.3563004293331381, 0.90677077], train_label: 0
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: CALUM CAN YOU FOLLOW ME OR RETWEET ME OR FAVORITE ME OR
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.57615236 0.57615236 0.        ]
 [0.40993715 0.57615236 0.40993715 0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: CALUM CAN YOU FOLLOW ME OR RETWEET ME OR FAVORITE ME OR
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'can', 'you', 'follow', 'me', 'or', 'retweet', 'me', 'or', 'favorite', 'me', 'or']
cosine_similarity: 0.9861210584640503
train_input: [0.3360969272762575, 0.98612106], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: CALUM ID DO ANYTHING TO GET YOU TO FOLLOW ME
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.57615236]
 [0.50154891 0.50154891 0.70490949 0.         0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: CALUM ID DO ANYTHING TO GET YOU TO FOLLOW ME
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'id', 'do', 'anything', 'to', 'get', 'you', 'to', 'follow', 'me']
cosine_similarity: 0.9777913689613342
train_input: [0.4112070550676187, 0.97779137], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Calum I know you see me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.53404633]
 [0.57973867 0.         0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Calum I know you see me
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'i', 'know', 'you', 'see', 'me']
cosine_similarity: 0.9880643486976624
train_input: [0.22028815056182965, 0.98806435], train_label: 0
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Calum please follow me and theirgroupie we love you so much babe please x70
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.63009934 0.44832087 0.
  0.        ]
 [0.47042643 0.33471228 0.33471228 0.         0.33471228 0.47042643
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: Calum please follow me and theirgroupie we love you so much babe please x70
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'please', 'follow', 'me', 'and', 'we', 'love', 'you', 'so', 'much', 'babe', 'please']
cosine_similarity: 0.9944026470184326
train_input: [0.4501755023269898, 0.99440265], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: PLEASE calum have a look at this and maybe follow me It s your choice
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.57615236 0.         0.57615236
  0.        ]
 [0.35520009 0.49922133 0.35520009 0.         0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: PLEASE calum have a look at this and maybe follow me It s your choice
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['please', 'calum', 'have', 'a', 'look', 'at', 'this', 'and', 'maybe', 'follow', 'me', 'it', 's', 'your', 'choice']
cosine_similarity: 0.984738826751709
train_input: [0.29121941856368966, 0.9847388], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum are you stick there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633 0.        ]
 [0.57973867 0.         0.         0.         0.81480247]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum are you stick there
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'are', 'you', 'stick', 'there']
cosine_similarity: 0.9772921800613403
train_input: [0.22028815056182965, 0.9772922], train_label: 0
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum hi please follow HeyyyItsLizz and cutiecalum
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.57615236
  0.57615236]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum hi please follow HeyyyItsLizz and cutiecalum
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'hi', 'please', 'follow', 'and']
cosine_similarity: 0.959805428981781
train_input: [0.29121941856368966, 0.9598054], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum if you see this please please follow me it would mean the world xx
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum if you see this please please follow me it would mean the world xx
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'if', 'you', 'see', 'this', 'please', 'please', 'follow', 'me', 'it', 'would', 'mean', 'the', 'world', 'xx']
cosine_similarity: 0.9932233095169067
train_input: [0.29121941856368966, 0.9932233], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum please follow babe itd mean so so much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.57615236
  0.        ]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.         0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: calum please follow babe itd mean so so much
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['calum', 'please', 'follow', 'babe', 'itd', 'mean', 'so', 'so', 'much']
cosine_similarity: 0.9753466248512268
train_input: [0.29121941856368966, 0.9753466], train_label: 1
TF_IDF_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: please follow me i love you itd mean a lot thank u calum
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.63009934 0.         0.44832087
  0.         0.        ]
 [0.30287281 0.30287281 0.42567716 0.         0.42567716 0.30287281
  0.42567716 0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Calum I love you and can you just please follow me, sentence2: please follow me i love you itd mean a lot thank u calum
After tokenization, sentence1: ['calum', 'i', 'love', 'you', 'and', 'can', 'you', 'just', 'please', 'follow', 'me'], sentence2: ['please', 'follow', 'me', 'i', 'love', 'you', 'itd', 'mean', 'a', 'lot', 'thank', 'u', 'calum']
cosine_similarity: 0.9901196956634521
train_input: [0.4073526042885674, 0.9901197], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: CALUM FOLLOW ME OR I WILL CRY
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.57615236]
 [0.         0.70710678 0.70710678 0.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: CALUM FOLLOW ME OR I WILL CRY
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['calum', 'follow', 'me', 'or', 'i', 'will', 'cry']
cosine_similarity: 0.9350231885910034
train_input: [0.5797386715376657, 0.9350232], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: Calum freaking hell please notice me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.
  0.53404633]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: Calum freaking hell please notice me
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['calum', 'freaking', 'hell', 'please', 'notice', 'me']
cosine_similarity: 0.9413672685623169
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: Calum hood is never going to notice me brb crying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.         0.         0.53404633]
 [0.         0.4261596  0.30321606 0.4261596  0.         0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: Calum hood is never going to notice me brb crying
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['calum', 'hood', 'is', 'never', 'going', 'to', 'notice', 'me', 'brb', 'crying']
cosine_similarity: 0.8783473372459412
train_input: [0.11521554337793122, 0.87834734], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: I just wanted calum to follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.         0.         0.57615236]
 [0.         0.40993715 0.40993715 0.57615236 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: I just wanted calum to follow me
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['i', 'just', 'wanted', 'calum', 'to', 'follow', 'me']
cosine_similarity: 0.9334802031517029
train_input: [0.3360969272762575, 0.9334802], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: SOMEONE HELP ME GET CALUM TO FOLLOW ME BEFORE I CRY
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.         0.57615236]
 [0.         0.50154891 0.50154891 0.70490949 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: SOMEONE HELP ME GET CALUM TO FOLLOW ME BEFORE I CRY
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['someone', 'help', 'me', 'get', 'calum', 'to', 'follow', 'me', 'before', 'i', 'cry']
cosine_similarity: 0.9194368720054626
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: calum y wont u follow me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.         0.57615236]
 [0.         0.50154891 0.50154891 0.70490949 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: calum y wont u follow me
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['calum', 'y', 'wont', 'u', 'follow', 'me']
cosine_similarity: 0.9428187012672424
train_input: [0.4112070550676187, 0.9428187], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: i really really want a follow from calum im sad really sad
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.         0.         0.
  0.         0.57615236]
 [0.         0.17780768 0.17780768 0.2499025  0.74970749 0.49980499
  0.2499025  0.        ]]
pairwise_similarity: [[1.         0.14577995]
 [0.14577995 1.        ]]
cosine_similarity: 0.14577994856555232
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: i really really want a follow from calum im sad really sad
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['i', 'really', 'really', 'want', 'a', 'follow', 'from', 'calum', 'im', 'sad', 'really', 'sad']
cosine_similarity: 0.8806716799736023
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: love this picture of you calum
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]
 [0.         0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: love this picture of you calum
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['love', 'this', 'picture', 'of', 'you', 'calum']
cosine_similarity: 0.8864317536354065
train_input: [0.17077611319011649, 0.88643175], train_label: 0
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: please calum can you be my 14 ily I just would love a follow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.40993715 0.         0.
  0.         0.57615236]
 [0.44665616 0.         0.31779954 0.31779954 0.44665616 0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: please calum can you be my 14 ily I just would love a follow
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['please', 'calum', 'can', 'you', 'be', 'my', 'ily', 'i', 'just', 'would', 'love', 'a', 'follow']
cosine_similarity: 0.9303902983665466
train_input: [0.2605556710562624, 0.9303903], train_label: 1
TF_IDF_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: you re so lucky to have a michael and calum follow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.40993715 0.         0.         0.57615236]
 [0.         0.40993715 0.40993715 0.57615236 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: CALUM PLEASE FOLLOW ME BABE xx, sentence2: you re so lucky to have a michael and calum follow
After tokenization, sentence1: ['calum', 'please', 'follow', 'me', 'babe', 'xx'], sentence2: ['you', 're', 'so', 'lucky', 'to', 'have', 'a', 'michael', 'and', 'calum', 'follow']
cosine_similarity: 0.9083077311515808
train_input: [0.3360969272762575, 0.90830773], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Canada is no longer respected internationally
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Canada is no longer respected internationally
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['canada', 'is', 'no', 'longer', 'respected', 'internationally']
cosine_similarity: 0.9097424149513245
train_input: [0.22028815056182965, 0.9097424], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Canada should be the one leading its revamping
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.        ]
 [0.         0.44943642 0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Canada should be the one leading its revamping
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['canada', 'should', 'be', 'the', 'one', 'leading', 'its', 'revamping']
cosine_similarity: 0.9771512150764465
train_input: [0.2605556710562624, 0.9771512], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Comparison of food costs between UK Canada
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.         0.        ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Comparison of food costs between UK Canada
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['comparison', 'of', 'food', 'costs', 'between', 'uk', 'canada']
cosine_similarity: 0.9534136056900024
train_input: [0.19431434016858146, 0.9534136], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Hey TogetherMoms leahmclaren am I the only mom in Canada who doesnt know your story
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.         0.
  0.         0.         0.        ]
 [0.         0.25969799 0.36499647 0.36499647 0.36499647 0.36499647
  0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.15055697]
 [0.15055697 1.        ]]
cosine_similarity: 0.15055696960204948
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Hey TogetherMoms leahmclaren am I the only mom in Canada who doesnt know your story
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['hey', 'am', 'i', 'the', 'only', 'mom', 'in', 'canada', 'who', 'doesnt', 'know', 'your', 'story']
cosine_similarity: 0.952716588973999
train_input: [0.15055696960204948, 0.9527166], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Hope you like Canada Tebow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Hope you like Canada Tebow
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['hope', 'you', 'like', 'canada', 'tebow']
cosine_similarity: 0.9345658421516418
train_input: [0.22028815056182965, 0.93456584], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: NEW BOARDS OF CANADA ALBUM ON JUNE 10th
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.81480247 0.         0.57973867 0.
  0.        ]
 [0.4261596  0.4261596  0.         0.4261596  0.30321606 0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: NEW BOARDS OF CANADA ALBUM ON JUNE 10th
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['new', 'boards', 'of', 'canada', 'album', 'on', 'june']
cosine_similarity: 0.9601126909255981
train_input: [0.17578607839334617, 0.9601127], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Today s hit YouTube Video in Canada
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.         0.        ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: Today s hit YouTube Video in Canada
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['today', 's', 'hit', 'youtube', 'video', 'in', 'canada']
cosine_similarity: 0.9305239915847778
train_input: [0.19431434016858146, 0.930524], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: but im in canada SoTheresThat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.        ]
 [0.         0.44943642 0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: but im in canada SoTheresThat
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['but', 'im', 'in', 'canada']
cosine_similarity: 0.9515795707702637
train_input: [0.2605556710562624, 0.9515796], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: if you moved to canada itd be harder for them to harass you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.         0.        ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: if you moved to canada itd be harder for them to harass you
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['if', 'you', 'moved', 'to', 'canada', 'itd', 'be', 'harder', 'for', 'them', 'to', 'harass', 'you']
cosine_similarity: 0.9271456599235535
train_input: [0.19431434016858146, 0.92714566], train_label: 0
TF_IDF_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: what has that guy ever done for Canada
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.        ]
 [0.         0.57973867 0.81480247]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: The Bank of Canada had some, sentence2: what has that guy ever done for Canada
After tokenization, sentence1: ['the', 'bank', 'of', 'canada', 'had', 'some'], sentence2: ['what', 'has', 'that', 'guy', 'ever', 'done', 'for', 'canada']
cosine_similarity: 0.9605377912521362
train_input: [0.3360969272762574, 0.9605378], train_label: 0
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: CANDICE JUST DID HER THING
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: CANDICE JUST DID HER THING
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['candice', 'just', 'did', 'her', 'thing']
cosine_similarity: 0.9476177096366882
train_input: [0.22028815056182965, 0.9476177], train_label: 1
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Candice is much better singer than Amber
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57973867 0.         0.81480247]
 [0.53404633 0.53404633 0.37997836 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Candice is much better singer than Amber
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['candice', 'is', 'much', 'better', 'singer', 'than', 'amber']
cosine_similarity: 0.9609113335609436
train_input: [0.22028815056182965, 0.96091133], train_label: 1
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Candice picked a great song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Candice picked a great song
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['candice', 'picked', 'a', 'great', 'song']
cosine_similarity: 0.9423070549964905
train_input: [0.22028815056182965, 0.94230705], train_label: 1
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Candice slayed our lives with a cold
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Candice slayed our lives with a cold
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['candice', 'slayed', 'our', 'lives', 'with', 'a', 'cold']
cosine_similarity: 0.9438946843147278
train_input: [0.22028815056182965, 0.9438947], train_label: 0
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: I hate Candice but she s so damn great
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: I hate Candice but she s so damn great
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['i', 'hate', 'candice', 'but', 'she', 's', 'so', 'damn', 'great']
cosine_similarity: 0.9603058099746704
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Man Candice you cannot be stopped
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.81480247]
 [0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Man Candice you cannot be stopped
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['man', 'candice', 'you', 'cannot', 'be', 'stopped']
cosine_similarity: 0.9538208246231079
train_input: [0.2605556710562624, 0.9538208], train_label: 1
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Now they are praising Amber during Candice s critiques too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.         0.81480247]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Now they are praising Amber during Candice s critiques too
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['now', 'they', 'are', 'praising', 'amber', 'during', 'candice', 's', 'critiques', 'too']
cosine_similarity: 0.9668477177619934
train_input: [0.22028815056182965, 0.9668477], train_label: 0
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: So were talking about Amber during Candice s time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.         0.81480247]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: So were talking about Amber during Candice s time
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['so', 'were', 'talking', 'about', 'amber', 'during', 'candice', 's', 'time']
cosine_similarity: 0.9641227126121521
train_input: [0.22028815056182965, 0.9641227], train_label: 0
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Tonight is not Candice s night
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.81480247]
 [0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: Tonight is not Candice s night
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['tonight', 'is', 'not', 'candice', 's', 'night']
cosine_similarity: 0.977719247341156
train_input: [0.2605556710562624, 0.97771925], train_label: 0
TF_IDF_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: but i also really like candice and kree
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Candice is in it 2 win it, sentence2: but i also really like candice and kree
After tokenization, sentence1: ['candice', 'is', 'in', 'it', 'win', 'it'], sentence2: ['but', 'i', 'also', 'really', 'like', 'candice', 'and', 'kree']
cosine_similarity: 0.9631597399711609
train_input: [0.22028815056182965, 0.96315974], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Bryan Caraway derrota a Johnny Bedford via sumisin a los 444 del 3er round
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.         0.         0.         0.         0.50154891
  0.         0.         0.         0.         0.50154891 0.        ]
 [0.         0.3160305  0.3160305  0.3160305  0.3160305  0.2248583
  0.3160305  0.3160305  0.3160305  0.3160305  0.2248583  0.3160305 ]]
pairwise_similarity: [[1.         0.22555487]
 [0.22555487 1.        ]]
cosine_similarity: 0.22555487220684337
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Bryan Caraway derrota a Johnny Bedford via sumisin a los 444 del 3er round
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['bryan', 'caraway', 'derrota', 'a', 'johnny', 'bedford', 'via', 'a', 'los', 'del', 'round']
cosine_similarity: 0.7557724714279175
train_input: [0.22555487220684337, 0.7557725], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Caraway with the guilitine late in the 3rd
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.6316672 ]
 [0.         0.53404633 0.37997836 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Caraway with the guilitine late in the 3rd
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['caraway', 'with', 'the', 'late', 'in', 'the']
cosine_similarity: 0.9792855978012085
train_input: [0.17077611319011649, 0.9792856], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Cool pic of Caraway between rounds
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672  0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Cool pic of Caraway between rounds
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['cool', 'pic', 'of', 'caraway', 'between', 'rounds']
cosine_similarity: 0.967755913734436
train_input: [0.17077611319011649, 0.9677559], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: First fight on ESPNUK tonight is Johnny Bedford v Bryan Caraway
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.         0.44943642 0.         0.
  0.         0.6316672  0.        ]
 [0.         0.39204401 0.39204401 0.27894255 0.39204401 0.39204401
  0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: First fight on ESPNUK tonight is Johnny Bedford v Bryan Caraway
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['first', 'fight', 'on', 'tonight', 'is', 'johnny', 'bedford', 'v', 'bryan', 'caraway']
cosine_similarity: 0.9561710357666016
train_input: [0.12536693798731732, 0.95617104], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Johnny Bedford is about to fucking RUIN Bryan Caraway
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.         0.44943642 0.         0.
  0.6316672  0.        ]
 [0.         0.4261596  0.4261596  0.30321606 0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Johnny Bedford is about to fucking RUIN Bryan Caraway
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['johnny', 'bedford', 'is', 'about', 'to', 'fucking', 'ruin', 'bryan', 'caraway']
cosine_similarity: 0.9422062039375305
train_input: [0.1362763414390864, 0.9422062], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Johnny Bedford only 120 vs Bryan Caraway
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.         0.         0.44943642 0.
  0.6316672  0.        ]
 [0.         0.4261596  0.4261596  0.4261596  0.30321606 0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Johnny Bedford only 120 vs Bryan Caraway
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['johnny', 'bedford', 'only', 'vs', 'bryan', 'caraway']
cosine_similarity: 0.8770009875297546
train_input: [0.1362763414390864, 0.877001], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Kick some Bedford ass Caraway
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.         0.44943642 0.         0.6316672 ]
 [0.         0.53404633 0.53404633 0.37997836 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Kick some Bedford ass Caraway
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['kick', 'some', 'bedford', 'ass', 'caraway']
cosine_similarity: 0.9092548489570618
train_input: [0.17077611319011649, 0.90925485], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Poor choice of music both Caraway and Bedford
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.
  0.6316672 ]
 [0.         0.47107781 0.33517574 0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Poor choice of music both Caraway and Bedford
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['poor', 'choice', 'of', 'music', 'both', 'caraway', 'and', 'bedford']
cosine_similarity: 0.9436479806900024
train_input: [0.15064018498706508, 0.943648], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Surprised Bedford got Caraway off his back
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.6316672  0.        ]
 [0.         0.53404633 0.37997836 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: Surprised Bedford got Caraway off his back
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['surprised', 'bedford', 'got', 'caraway', 'off', 'his', 'back']
cosine_similarity: 0.9422575235366821
train_input: [0.17077611319011649, 0.9422575], train_label: 0
TF_IDF_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: We have Caraway up 20 on Johnny Bedford
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.         0.44943642 0.         0.6316672 ]
 [0.         0.53404633 0.53404633 0.37997836 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Caraway 109 in round one, sentence2: We have Caraway up 20 on Johnny Bedford
After tokenization, sentence1: ['caraway', 'in', 'round', 'one'], sentence2: ['we', 'have', 'caraway', 'up', 'on', 'johnny', 'bedford']
cosine_similarity: 0.9691891074180603
train_input: [0.17077611319011649, 0.9691891], train_label: 0
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: But In Real Life He Got Banged On By Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.35520009 0.49922133 0.49922133
  0.         0.         0.        ]
 [0.44665616 0.31779954 0.         0.31779954 0.         0.
  0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: But In Real Life He Got Banged On By Carlos Delfino
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['but', 'in', 'real', 'life', 'he', 'got', 'banged', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9349092245101929
train_input: [0.22576484600261604, 0.9349092], train_label: 0
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Carlos Delfino did not just do that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.35520009 0.         0.49922133 0.49922133
  0.        ]
 [0.40993715 0.         0.40993715 0.57615236 0.         0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Carlos Delfino did not just do that
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'did', 'not', 'just', 'do', 'that']
cosine_similarity: 0.9071309566497803
train_input: [0.29121941856368966, 0.90713096], train_label: 0
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Carlos Delfino just shitted on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.37930349 0.37930349 0.53309782 0.
  0.         0.        ]
 [0.33471228 0.         0.33471228 0.33471228 0.         0.47042643
  0.47042643 0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Carlos Delfino just shitted on Kevin Durant
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'just', 'shitted', 'on', 'kevin', 'durant']
cosine_similarity: 0.970162034034729
train_input: [0.38087260847594373, 0.97016203], train_label: 1
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Carlos delfino dunks all over durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.37930349 0.         0.37930349 0.53309782]
 [0.44832087 0.         0.44832087 0.63009934 0.44832087 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Carlos delfino dunks all over durant
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'dunks', 'all', 'over', 'durant']
cosine_similarity: 0.9668394327163696
train_input: [0.5101490193104813, 0.96683943], train_label: 1
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Damn Kevin Durant got dunked on by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.         0.37930349 0.         0.37930349
  0.53309782 0.         0.        ]
 [0.30287281 0.         0.42567716 0.30287281 0.42567716 0.30287281
  0.         0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Damn Kevin Durant got dunked on by Carlos Delfino
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['damn', 'kevin', 'durant', 'got', 'dunked', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.969514012336731
train_input: [0.34464214103805474, 0.969514], train_label: 1
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Durant just got yammed on by Carlos Delfino OO
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.37930349 0.37930349 0.53309782 0.
  0.         0.         0.        ]
 [0.30287281 0.         0.30287281 0.30287281 0.         0.42567716
  0.42567716 0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Durant just got yammed on by Carlos Delfino OO
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino', 'oo']
cosine_similarity: 0.9599008560180664
train_input: [0.34464214103805474, 0.95990086], train_label: 1
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: I Cant Believe KD Just Got Dunked On By Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.35520009 0.         0.49922133
  0.49922133 0.         0.         0.        ]
 [0.4078241  0.29017021 0.         0.29017021 0.4078241  0.
  0.         0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: I Cant Believe KD Just Got Dunked On By Carlos Delfino
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['i', 'cant', 'believe', 'kd', 'just', 'got', 'dunked', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9304837584495544
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: I refuse to believe what Carlos Delfino just did
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.35520009 0.         0.49922133
  0.49922133 0.         0.        ]
 [0.44665616 0.31779954 0.         0.31779954 0.44665616 0.
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: I refuse to believe what Carlos Delfino just did
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['i', 'refuse', 'to', 'believe', 'what', 'carlos', 'delfino', 'just', 'did']
cosine_similarity: 0.9024606347084045
train_input: [0.22576484600261604, 0.90246063], train_label: 0
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: OMG Kevin Durant just got dunked on by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.37930349 0.         0.37930349 0.53309782
  0.         0.         0.         0.        ]
 [0.27867523 0.         0.27867523 0.39166832 0.27867523 0.
  0.39166832 0.39166832 0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: OMG Kevin Durant just got dunked on by Carlos Delfino
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['omg', 'kevin', 'durant', 'just', 'got', 'dunked', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.965194046497345
train_input: [0.31710746658027095, 0.96519405], train_label: 1
TF_IDF_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Okay Carlos Delfino just lowkey put Kevin Durant on a poster
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.37930349 0.37930349 0.53309782 0.
  0.         0.         0.         0.        ]
 [0.27867523 0.         0.27867523 0.27867523 0.         0.39166832
  0.39166832 0.39166832 0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: Durant gets crammed on by Carlos Delfino, sentence2: Okay Carlos Delfino just lowkey put Kevin Durant on a poster
After tokenization, sentence1: ['durant', 'gets', 'crammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['okay', 'carlos', 'delfino', 'just', 'lowkey', 'put', 'kevin', 'durant', 'on', 'a', 'poster']
cosine_similarity: 0.9344620704650879
train_input: [0.31710746658027095, 0.9344621], train_label: 0
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Carlos Delfino just sh on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30253071 0.30253071 0.42519636 0.42519636 0.30253071 0.30253071
  0.30253071 0.42519636 0.        ]
 [0.37863221 0.37863221 0.         0.         0.37863221 0.37863221
  0.37863221 0.         0.53215436]]
pairwise_similarity: [[1.         0.57273936]
 [0.57273936 1.        ]]
cosine_similarity: 0.5727393584196199
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Carlos Delfino just sh on Kevin Durant
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'sh', 'on', 'kevin', 'durant']
cosine_similarity: 0.982379674911499
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Carlos Delfino should just retire now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27867523 0.27867523 0.39166832 0.39166832 0.39166832 0.27867523
  0.39166832 0.39166832 0.        ]
 [0.44832087 0.44832087 0.         0.         0.         0.44832087
  0.         0.         0.63009934]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Carlos Delfino should just retire now
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'should', 'just', 'retire', 'now']
cosine_similarity: 0.9728301167488098
train_input: [0.3748077700589726, 0.9728301], train_label: 0
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Carlos Delfino yamming on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.28986934 0.28986934 0.40740124 0.40740124 0.28986934 0.40740124
  0.28986934 0.40740124 0.        ]
 [0.4090901  0.4090901  0.         0.         0.4090901  0.
  0.4090901  0.         0.57496187]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Carlos Delfino yamming on Kevin Durant
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'on', 'kevin', 'durant']
cosine_similarity: 0.9253329634666443
train_input: [0.4743307064971939, 0.92533296], train_label: 1
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: DID CARLOS DELFINO JUST DUNK ON KD WOAAAAAH stephenasmith
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30253071 0.30253071 0.30253071 0.30253071 0.42519636 0.30253071
  0.         0.42519636 0.42519636 0.         0.        ]
 [0.30253071 0.30253071 0.30253071 0.30253071 0.         0.30253071
  0.42519636 0.         0.         0.42519636 0.42519636]]
pairwise_similarity: [[1.         0.45762416]
 [0.45762416 1.        ]]
cosine_similarity: 0.4576241622696326
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: DID CARLOS DELFINO JUST DUNK ON KD WOAAAAAH stephenasmith
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['did', 'carlos', 'delfino', 'just', 'dunk', 'on', 'kd', 'woaaaaah']
cosine_similarity: 0.9806197285652161
train_input: [0.4576241622696326, 0.9806197], train_label: 1
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: He got dunked on by Carlos Delfino lmfaooo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.26868528 0.26868528 0.37762778 0.37762778 0.         0.37762778
  0.         0.37762778 0.37762778 0.         0.37762778]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.         0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: He got dunked on by Carlos Delfino lmfaooo
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['he', 'got', 'dunked', 'on', 'by', 'carlos', 'delfino', 'lmfaooo']
cosine_similarity: 0.9689693450927734
train_input: [0.1908740661302035, 0.96896935], train_label: 1
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: I cant believe Carlos delfino just dunked on Kevin durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30253071 0.30253071 0.42519636 0.42519636 0.
  0.30253071 0.30253071 0.30253071 0.42519636]
 [0.46977774 0.33425073 0.33425073 0.         0.         0.46977774
  0.33425073 0.33425073 0.33425073 0.        ]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739691
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: I cant believe Carlos delfino just dunked on Kevin durant
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['i', 'cant', 'believe', 'carlos', 'delfino', 'just', 'dunked', 'on', 'kevin', 'durant']
cosine_similarity: 0.9864315986633301
train_input: [0.5056055588739691, 0.9864316], train_label: 1
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Kevin Durant just got welcomed to the playoffs by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30253071 0.30253071 0.42519636 0.42519636 0.30253071 0.
  0.30253071 0.30253071 0.         0.42519636 0.        ]
 [0.30253071 0.30253071 0.         0.         0.30253071 0.42519636
  0.30253071 0.30253071 0.42519636 0.         0.42519636]]
pairwise_similarity: [[1.         0.45762416]
 [0.45762416 1.        ]]
cosine_similarity: 0.4576241622696326
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Kevin Durant just got welcomed to the playoffs by Carlos Delfino
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['kevin', 'durant', 'just', 'got', 'welcomed', 'to', 'the', 'playoffs', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9780662655830383
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Oh shit did Carlos Delfino really just dunk on KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31701073 0.31701073 0.31701073 0.31701073 0.44554752 0.31701073
  0.         0.44554752 0.         0.31701073 0.        ]
 [0.2895694  0.2895694  0.2895694  0.2895694  0.         0.2895694
  0.40697968 0.         0.40697968 0.2895694  0.40697968]]
pairwise_similarity: [[1.         0.55077963]
 [0.55077963 1.        ]]
cosine_similarity: 0.5507796339456278
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Oh shit did Carlos Delfino really just dunk on KD
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['oh', 'shit', 'did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kd']
cosine_similarity: 0.9748913049697876
train_input: [0.5507796339456278, 0.9748913], train_label: 1
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Refs should give Durant a tech for getting dunked on by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27867523 0.27867523 0.39166832 0.39166832 0.         0.27867523
  0.         0.39166832 0.39166832 0.39166832 0.         0.        ]
 [0.30287281 0.30287281 0.         0.         0.42567716 0.30287281
  0.42567716 0.         0.         0.         0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.25320945]
 [0.25320945 1.        ]]
cosine_similarity: 0.2532094495161745
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Refs should give Durant a tech for getting dunked on by Carlos Delfino
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['refs', 'should', 'give', 'durant', 'a', 'tech', 'for', 'getting', 'dunked', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9693087339401245
train_input: [0.2532094495161745, 0.96930873], train_label: 1
TF_IDF_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Your boy Carlos Delfino just dunked on Kevin durant connorclark15
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30253071 0.         0.30253071 0.42519636 0.42519636
  0.         0.30253071 0.30253071 0.30253071 0.42519636]
 [0.42519636 0.30253071 0.42519636 0.30253071 0.         0.
  0.42519636 0.30253071 0.30253071 0.30253071 0.        ]]
pairwise_similarity: [[1.         0.45762416]
 [0.45762416 1.        ]]
cosine_similarity: 0.4576241622696326
word_to_vector_cosine_similarity: sentence1: Did Carlos Delfino really just dunk on Kevin Durant, sentence2: Your boy Carlos Delfino just dunked on Kevin durant connorclark15
After tokenization, sentence1: ['did', 'carlos', 'delfino', 'really', 'just', 'dunk', 'on', 'kevin', 'durant'], sentence2: ['your', 'boy', 'carlos', 'delfino', 'just', 'dunked', 'on', 'kevin', 'durant']
cosine_similarity: 0.9919286966323853
train_input: [0.4576241622696326, 0.9919287], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Carlos Delfino just dunked on KD Darrenevanss lmao
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.35464863 0.49844628 0.35464863
  0.         0.49844628 0.        ]
 [0.3174044  0.44610081 0.3174044  0.3174044  0.         0.3174044
  0.44610081 0.         0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Carlos Delfino just dunked on KD Darrenevanss lmao
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'dunked', 'on', 'kd', 'lmao']
cosine_similarity: 0.957440972328186
train_input: [0.4502681446556265, 0.957441], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Carlos Delfino just dunked on Kevin Durant and 1
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]
 [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 1.0000000000000002
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Carlos Delfino just dunked on Kevin Durant and 1
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'dunked', 'on', 'kevin', 'durant', 'and']
cosine_similarity: 0.9905635118484497
train_input: [1.0000000000000002, 0.9905635], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Carlos Delfino just slammed on Kevin Durant lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.53215436 0.37863221 0.37863221 0.37863221
  0.         0.        ]
 [0.33425073 0.33425073 0.         0.33425073 0.33425073 0.33425073
  0.46977774 0.46977774]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Carlos Delfino just slammed on Kevin Durant lol
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'slammed', 'on', 'kevin', 'durant', 'lol']
cosine_similarity: 0.9740785956382751
train_input: [0.6327904583679949, 0.9740786], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Carlos Delfino puttin Kevin Durant on a poster
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.49844628 0.35464863
  0.         0.        ]
 [0.35464863 0.35464863 0.         0.35464863 0.         0.35464863
  0.49844628 0.49844628]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Carlos Delfino puttin Kevin Durant on a poster
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'puttin', 'kevin', 'durant', 'on', 'a', 'poster']
cosine_similarity: 0.9469153881072998
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Durant you just got dunked on by Carlos delfino my man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.37863221 0.37863221 0.         0.37863221
  0.53215436 0.        ]
 [0.33425073 0.33425073 0.33425073 0.33425073 0.46977774 0.33425073
  0.         0.46977774]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Durant you just got dunked on by Carlos delfino my man
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['durant', 'you', 'just', 'got', 'dunked', 'on', 'by', 'carlos', 'delfino', 'my', 'man']
cosine_similarity: 0.9466135501861572
train_input: [0.6327904583679949, 0.94661355], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: I know Kevin Durant didnt just get banged on by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37863221 0.37863221 0.         0.53215436 0.37863221
  0.37863221 0.37863221 0.        ]
 [0.42519636 0.30253071 0.30253071 0.42519636 0.         0.30253071
  0.30253071 0.30253071 0.42519636]]
pairwise_similarity: [[1.         0.57273936]
 [0.57273936 1.        ]]
cosine_similarity: 0.57273935841962
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: I know Kevin Durant didnt just get banged on by Carlos Delfino
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['i', 'know', 'kevin', 'durant', 'didnt', 'just', 'get', 'banged', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9350593090057373
train_input: [0.57273935841962, 0.9350593], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: The next Gatorade commercial will feature Kevin Durant and Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.35464863 0.49844628 0.35464863 0.
  0.         0.49844628 0.35464863]
 [0.3174044  0.44610081 0.3174044  0.         0.3174044  0.44610081
  0.44610081 0.         0.3174044 ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: The next Gatorade commercial will feature Kevin Durant and Carlos Delfino
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['the', 'next', 'gatorade', 'commercial', 'will', 'feature', 'kevin', 'durant', 'and', 'carlos', 'delfino']
cosine_similarity: 0.9472132921218872
train_input: [0.4502681446556265, 0.9472133], train_label: 0
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: What a dunk from Carlos delfino over Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.47042643 0.33471228 0.47042643
  0.47042643]
 [0.44832087 0.44832087 0.63009934 0.         0.44832087 0.
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: What a dunk from Carlos delfino over Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['what', 'a', 'dunk', 'from', 'carlos', 'delfino', 'over', 'durant']
cosine_similarity: 0.9645622968673706
train_input: [0.4501755023269897, 0.9645623], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Who just saw Carlos Delfino dunk on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.         0.53215436 0.37863221 0.37863221
  0.37863221 0.        ]
 [0.33425073 0.33425073 0.46977774 0.         0.33425073 0.33425073
  0.33425073 0.46977774]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: Who just saw Carlos Delfino dunk on Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['who', 'just', 'saw', 'carlos', 'delfino', 'dunk', 'on', 'kevin', 'durant']
cosine_similarity: 0.9732243418693542
train_input: [0.6327904583679949, 0.97322434], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: was that really CARLOS DELFINO YAMMIN ON DURANT
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.33471228 0.47042643 0.47042643
  0.         0.        ]
 [0.37930349 0.37930349 0.         0.37930349 0.         0.
  0.53309782 0.53309782]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked of Kevin Durant, sentence2: was that really CARLOS DELFINO YAMMIN ON DURANT
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'of', 'kevin', 'durant'], sentence2: ['was', 'that', 'really', 'carlos', 'delfino', 'yammin', 'on', 'durant']
cosine_similarity: 0.9621654748916626
train_input: [0.3808726084759436, 0.9621655], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino did not just do that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.         0.42567716 0.42567716 0.30287281
  0.42567716 0.42567716]
 [0.44832087 0.44832087 0.63009934 0.         0.         0.44832087
  0.         0.        ]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino did not just do that
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'did', 'not', 'just', 'do', 'that']
cosine_similarity: 0.9380831718444824
train_input: [0.4073526042885674, 0.9380832], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino just dunked on Durants head
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.         0.42567716 0.         0.42567716
  0.         0.30287281 0.42567716 0.42567716]
 [0.33471228 0.33471228 0.47042643 0.         0.47042643 0.
  0.47042643 0.33471228 0.         0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino just dunked on Durants head
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durants', 'head']
cosine_similarity: 0.9622372388839722
train_input: [0.3041257418754935, 0.96223724], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino just got a lot of new fans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.44610081 0.         0.3174044  0.3174044
  0.44610081 0.         0.         0.44610081]
 [0.3174044  0.3174044  0.         0.44610081 0.3174044  0.3174044
  0.         0.44610081 0.44610081 0.        ]]
pairwise_similarity: [[1.         0.40298221]
 [0.40298221 1.        ]]
cosine_similarity: 0.40298220897396103
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino just got a lot of new fans
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'just', 'got', 'a', 'lot', 'of', 'new', 'fans']
cosine_similarity: 0.9539268612861633
train_input: [0.40298220897396103, 0.95392686], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino just shamed Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.33425073 0.46977774 0.33425073 0.33425073
  0.46977774 0.        ]
 [0.37863221 0.37863221 0.37863221 0.         0.37863221 0.37863221
  0.         0.53215436]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino just shamed Kevin Durant
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'just', 'shamed', 'kevin', 'durant']
cosine_similarity: 0.9612663984298706
train_input: [0.6327904583679949, 0.9612664], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino with a nasty dunk on Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.         0.30287281 0.42567716 0.42567716
  0.42567716 0.         0.42567716]
 [0.37930349 0.37930349 0.53309782 0.37930349 0.         0.
  0.         0.53309782 0.        ]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Carlos Delfino with a nasty dunk on Durant
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'with', 'a', 'nasty', 'dunk', 'on', 'durant']
cosine_similarity: 0.9690497517585754
train_input: [0.34464214103805474, 0.96904975], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Did Kevin Durant really just get dunked on by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.         0.         0.33425073 0.46977774
  0.33425073 0.33425073 0.46977774 0.        ]
 [0.30253071 0.30253071 0.42519636 0.42519636 0.30253071 0.
  0.30253071 0.30253071 0.         0.42519636]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739692
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Did Kevin Durant really just get dunked on by Carlos Delfino
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['did', 'kevin', 'durant', 'really', 'just', 'get', 'dunked', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9827418327331543
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: It just tried to tell me Carlos Delfino dunked on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.         0.33425073 0.46977774 0.33425073
  0.33425073 0.46977774 0.         0.        ]
 [0.30253071 0.30253071 0.42519636 0.30253071 0.         0.30253071
  0.30253071 0.         0.42519636 0.42519636]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739692
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: It just tried to tell me Carlos Delfino dunked on Kevin Durant
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['it', 'just', 'tried', 'to', 'tell', 'me', 'carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant']
cosine_similarity: 0.9705639481544495
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: OMG DURANT JUST GOT BOOFED ON BY CARLOS DELFINO LMFAOOO
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33425073 0.33425073 0.33425073 0.33425073 0.33425073
  0.46977774 0.         0.         0.46977774]
 [0.42519636 0.30253071 0.30253071 0.30253071 0.30253071 0.30253071
  0.         0.42519636 0.42519636 0.        ]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739692
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: OMG DURANT JUST GOT BOOFED ON BY CARLOS DELFINO LMFAOOO
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['omg', 'durant', 'just', 'got', 'on', 'by', 'carlos', 'delfino', 'lmfaooo']
cosine_similarity: 0.9786890149116516
train_input: [0.5056055588739692, 0.978689], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Turned on the game just in time to watch Carlos Delfino dunk on Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.         0.3174044  0.         0.44610081
  0.3174044  0.44610081 0.44610081 0.         0.         0.        ]
 [0.26844636 0.26844636 0.37729199 0.26844636 0.37729199 0.
  0.26844636 0.         0.         0.37729199 0.37729199 0.37729199]]
pairwise_similarity: [[1.         0.34082422]
 [0.34082422 1.        ]]
cosine_similarity: 0.3408242166238352
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: Turned on the game just in time to watch Carlos Delfino dunk on Durant
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['turned', 'on', 'the', 'game', 'just', 'in', 'time', 'to', 'watch', 'carlos', 'delfino', 'dunk', 'on', 'durant']
cosine_similarity: 0.9393654465675354
train_input: [0.3408242166238352, 0.93936545], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: kd is gonna have nightmares about carlos delfino dunking on him
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.29017021 0.         0.4078241  0.         0.4078241
  0.4078241  0.         0.4078241  0.         0.4078241 ]
 [0.31779954 0.31779954 0.44665616 0.         0.44665616 0.
  0.         0.44665616 0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got postered by Carlos Delfino, sentence2: kd is gonna have nightmares about carlos delfino dunking on him
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'by', 'carlos', 'delfino'], sentence2: ['kd', 'is', 'gonna', 'have', 'nightmares', 'about', 'carlos', 'delfino', 'dunking', 'on', 'him']
cosine_similarity: 0.952176570892334
train_input: [0.18443191662261305, 0.9521766], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Carlos Delfino did not just do that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.47042643 0.47042643 0.33471228
  0.47042643]
 [0.44832087 0.44832087 0.63009934 0.         0.         0.44832087
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Carlos Delfino did not just do that
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'did', 'not', 'just', 'do', 'that']
cosine_similarity: 0.8967050313949585
train_input: [0.4501755023269898, 0.89670503], train_label: 0
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Carlos Delfino just stared down KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.47042643 0.33471228 0.
  0.47042643 0.        ]
 [0.37930349 0.37930349 0.         0.         0.37930349 0.53309782
  0.         0.53309782]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Carlos Delfino just stared down KD
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'stared', 'down', 'kd']
cosine_similarity: 0.9388939142227173
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Carlos Delfino plucked dude then dunked on KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.         0.44665616 0.44665616
  0.44665616 0.         0.44665616 0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.         0.
  0.         0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Carlos Delfino plucked dude then dunked on KD
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'plucked', 'dude', 'then', 'dunked', 'on', 'kd']
cosine_similarity: 0.9565012454986572
train_input: [0.20199309249791833, 0.95650125], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: DURANT just got doo dooed on by Carlos Delfino lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.         0.35464863 0.
  0.49844628 0.35464863 0.49844628 0.        ]
 [0.28986934 0.28986934 0.40740124 0.40740124 0.28986934 0.40740124
  0.         0.28986934 0.         0.40740124]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: DURANT just got doo dooed on by Carlos Delfino lol
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['durant', 'just', 'got', 'doo', 'on', 'by', 'carlos', 'delfino', 'lol']
cosine_similarity: 0.960675060749054
train_input: [0.41120705506761857, 0.96067506], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Lol Durant let Carlos Delfino bag on him
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.33471228 0.33471228 0.47042643 0.47042643
  0.47042643 0.         0.        ]
 [0.47042643 0.33471228 0.33471228 0.33471228 0.         0.
  0.         0.47042643 0.47042643]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Lol Durant let Carlos Delfino bag on him
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['lol', 'durant', 'let', 'carlos', 'delfino', 'bag', 'on', 'him']
cosine_similarity: 0.9632272124290466
train_input: [0.3360969272762574, 0.9632272], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Ohhhh Carlos Delfino just duuuunked on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.37863221 0.         0.53215436 0.37863221
  0.37863221 0.        ]
 [0.33425073 0.33425073 0.33425073 0.46977774 0.         0.33425073
  0.33425073 0.46977774]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Ohhhh Carlos Delfino just duuuunked on Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'on', 'kevin', 'durant']
cosine_similarity: 0.9893650412559509
train_input: [0.6327904583679949, 0.98936504], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Still cant process that Carlos Delfino dunked on Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.         0.33471228 0.47042643 0.47042643
  0.47042643 0.        ]
 [0.37930349 0.37930349 0.53309782 0.37930349 0.         0.
  0.         0.53309782]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: Still cant process that Carlos Delfino dunked on Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['still', 'cant', 'process', 'that', 'carlos', 'delfino', 'dunked', 'on', 'durant']
cosine_similarity: 0.9603816270828247
train_input: [0.3808726084759436, 0.9603816], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: by Carlos Delfino of all the players
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.44665616
  0.        ]
 [0.50154891 0.50154891 0.         0.         0.         0.
  0.70490949]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: by Carlos Delfino of all the players
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['by', 'carlos', 'delfino', 'of', 'all', 'the', 'players']
cosine_similarity: 0.901718020439148
train_input: [0.31878402175377923, 0.901718], train_label: 0
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: carlos delfino jus dunked on kevin durant heavy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.         0.35464863 0.         0.49844628
  0.         0.49844628 0.35464863]
 [0.3174044  0.3174044  0.44610081 0.3174044  0.44610081 0.
  0.44610081 0.         0.3174044 ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: carlos delfino jus dunked on kevin durant heavy
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['carlos', 'delfino', 'jus', 'dunked', 'on', 'kevin', 'durant', 'heavy']
cosine_similarity: 0.9846859574317932
train_input: [0.4502681446556265, 0.98468596], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: thank you for the poster Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.44665616
  0.         0.        ]
 [0.40993715 0.40993715 0.         0.         0.         0.
  0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just JAMMED on Kevin Durant, sentence2: thank you for the poster Carlos Delfino
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'jammed', 'on', 'kevin', 'durant'], sentence2: ['thank', 'you', 'for', 'the', 'poster', 'carlos', 'delfino']
cosine_similarity: 0.8884206414222717
train_input: [0.2605556710562624, 0.88842064], train_label: 0
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino actually dunked on Kevin Duran
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.37930349 0.37930349 0.         0.53309782
  0.53309782 0.        ]
 [0.47042643 0.33471228 0.33471228 0.33471228 0.47042643 0.
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino actually dunked on Kevin Duran
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['carlos', 'delfino', 'actually', 'dunked', 'on', 'kevin', 'duran']
cosine_similarity: 0.9656919240951538
train_input: [0.38087260847594373, 0.9656919], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino is having a podiumgame
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.        ]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino is having a podiumgame
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['carlos', 'delfino', 'is', 'having', 'a']
cosine_similarity: 0.8831179141998291
train_input: [0.29121941856368966, 0.8831179], train_label: 0
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino is still a faggot
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133]
 [0.50154891 0.50154891 0.         0.         0.70490949 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino is still a faggot
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['carlos', 'delfino', 'is', 'still', 'a', 'faggot']
cosine_similarity: 0.9050541520118713
train_input: [0.3563004293331381, 0.90505415], train_label: 0
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino s Dunk on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.         0.49922133
  0.49922133 0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino s Dunk on Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['carlos', 'delfino', 's', 'dunk', 'on', 'kevin', 'durant']
cosine_similarity: 0.9102823734283447
train_input: [0.2523342014336961, 0.9102824], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino s steal and dunk on KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.         0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Carlos Delfino s steal and dunk on KD
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['carlos', 'delfino', 's', 'steal', 'and', 'dunk', 'on', 'kd']
cosine_similarity: 0.8968468308448792
train_input: [0.2523342014336961, 0.89684683], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Did I just see Carlos delfino dunk on KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.         0.53309782 0.53309782
  0.37930349 0.        ]
 [0.33471228 0.33471228 0.47042643 0.47042643 0.         0.
  0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Did I just see Carlos delfino dunk on KD
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['did', 'i', 'just', 'see', 'carlos', 'delfino', 'dunk', 'on', 'kd']
cosine_similarity: 0.9013659954071045
train_input: [0.38087260847594373, 0.901366], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Did I just watch Carlos Delfino dunk on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.         0.53309782 0.
  0.53309782 0.37930349 0.         0.        ]
 [0.27867523 0.27867523 0.39166832 0.39166832 0.         0.39166832
  0.         0.27867523 0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Did I just watch Carlos Delfino dunk on Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['did', 'i', 'just', 'watch', 'carlos', 'delfino', 'dunk', 'on', 'kevin', 'durant']
cosine_similarity: 0.9375040531158447
train_input: [0.31710746658027095, 0.93750405], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: I cant believe of all people Carlos Delfino dunked on boney Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37930349 0.37930349 0.37930349 0.
  0.53309782 0.53309782 0.         0.        ]
 [0.39166832 0.39166832 0.27867523 0.27867523 0.27867523 0.39166832
  0.         0.         0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: I cant believe of all people Carlos Delfino dunked on boney Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['i', 'cant', 'believe', 'of', 'all', 'people', 'carlos', 'delfino', 'dunked', 'on', 'boney', 'kevin', 'durant']
cosine_similarity: 0.9076254963874817
train_input: [0.31710746658027095, 0.9076255], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Kevin Durant Just got punched on by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.         0.53309782 0.
  0.37930349 0.         0.        ]
 [0.30287281 0.30287281 0.         0.42567716 0.         0.42567716
  0.30287281 0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: Kevin Durant Just got punched on by Carlos Delfino
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['kevin', 'durant', 'just', 'got', 'punched', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9575085639953613
train_input: [0.34464214103805474, 0.95750856], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: You cant let Carlos Delfino put you on a poster KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked on Durrant, sentence2: You cant let Carlos Delfino put you on a poster KD
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durrant'], sentence2: ['you', 'cant', 'let', 'carlos', 'delfino', 'put', 'you', 'on', 'a', 'poster', 'kd']
cosine_similarity: 0.8478623032569885
train_input: [0.2523342014336961, 0.8478623], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: A man just asked for the Dunk intensity for Carlos Delfino dunk on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.         0.49844628 0.35464863
  0.         0.         0.35464863 0.         0.49844628]
 [0.3158336  0.22471821 0.22471821 0.6316672  0.         0.22471821
  0.3158336  0.3158336  0.22471821 0.3158336  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.3187840217537792
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: A man just asked for the Dunk intensity for Carlos Delfino dunk on Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['a', 'man', 'just', 'asked', 'for', 'the', 'dunk', 'intensity', 'for', 'carlos', 'delfino', 'dunk', 'on', 'kevin', 'durant']
cosine_similarity: 0.9215038418769836
train_input: [0.3187840217537792, 0.92150384], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Carlos Delfino just dunked the shit outta KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.33471228 0.47042643 0.         0.
  0.47042643 0.         0.         0.47042643]
 [0.30287281 0.30287281 0.30287281 0.         0.42567716 0.42567716
  0.         0.42567716 0.42567716 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Carlos Delfino just dunked the shit outta KD
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['carlos', 'delfino', 'just', 'dunked', 'the', 'shit', 'outta', 'kd']
cosine_similarity: 0.9180141091346741
train_input: [0.3041257418754935, 0.9180141], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Carlos Delfino just yammed it on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.         0.35464863
  0.49844628 0.        ]
 [0.35464863 0.35464863 0.         0.35464863 0.49844628 0.35464863
  0.         0.49844628]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Carlos Delfino just yammed it on Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['carlos', 'delfino', 'just', 'yammed', 'it', 'on', 'kevin', 'durant']
cosine_similarity: 0.9757718443870544
train_input: [0.5031026124151313, 0.97577184], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Carlos Delfino steal returned for an and1 dunk ON Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.35464863 0.         0.49844628 0.35464863
  0.35464863 0.         0.         0.49844628]
 [0.40740124 0.28986934 0.28986934 0.40740124 0.         0.28986934
  0.28986934 0.40740124 0.40740124 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Carlos Delfino steal returned for an and1 dunk ON Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['carlos', 'delfino', 'steal', 'returned', 'for', 'an', 'dunk', 'on', 'kevin', 'durant']
cosine_similarity: 0.9464686512947083
train_input: [0.41120705506761857, 0.94646865], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Did I just witness Kevin Durant get dunked on by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37863221 0.37863221 0.         0.37863221 0.37863221 0.
  0.37863221 0.53215436 0.        ]
 [0.30253071 0.30253071 0.42519636 0.30253071 0.30253071 0.42519636
  0.30253071 0.         0.42519636]]
pairwise_similarity: [[1.         0.57273936]
 [0.57273936 1.        ]]
cosine_similarity: 0.57273935841962
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Did I just witness Kevin Durant get dunked on by Carlos Delfino
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['did', 'i', 'just', 'witness', 'kevin', 'durant', 'get', 'dunked', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9326534867286682
train_input: [0.57273935841962, 0.9326535], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Durant just got dunked on my Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.35464863 0.35464863 0.         0.
  0.49844628 0.49844628]
 [0.35464863 0.35464863 0.35464863 0.35464863 0.49844628 0.49844628
  0.         0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Durant just got dunked on my Carlos Delfino
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['durant', 'just', 'got', 'dunked', 'on', 'my', 'carlos', 'delfino']
cosine_similarity: 0.9562965035438538
train_input: [0.5031026124151313, 0.9562965], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Durant s Gateraide Commerical Just Came True Thanks Carlos Delfino LetsGo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.         0.33471228 0.47042643 0.33471228
  0.         0.         0.47042643 0.         0.         0.47042643
  0.        ]
 [0.34261985 0.24377685 0.34261985 0.24377685 0.         0.24377685
  0.34261985 0.34261985 0.         0.34261985 0.34261985 0.
  0.34261985]]
pairwise_similarity: [[1.         0.24478531]
 [0.24478531 1.        ]]
cosine_similarity: 0.2447853117345521
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Durant s Gateraide Commerical Just Came True Thanks Carlos Delfino LetsGo
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['durant', 's', 'commerical', 'just', 'came', 'true', 'thanks', 'carlos', 'delfino', 'letsgo']
cosine_similarity: 0.9361743927001953
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Hahaha Goddamn Durant Got Postered By Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.33471228 0.         0.
  0.         0.47042643 0.         0.47042643]
 [0.30287281 0.30287281 0.         0.30287281 0.42567716 0.42567716
  0.42567716 0.         0.42567716 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Hahaha Goddamn Durant Got Postered By Carlos Delfino
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['hahaha', 'goddamn', 'durant', 'got', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9415555000305176
train_input: [0.30412574187549346, 0.9415555], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Kevin Durant monster slam on Carlos delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.35464863 0.
  0.         0.49844628]
 [0.35464863 0.35464863 0.         0.35464863 0.35464863 0.49844628
  0.49844628 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: Kevin Durant monster slam on Carlos delfino
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['kevin', 'durant', 'monster', 'slam', 'on', 'carlos', 'delfino']
cosine_similarity: 0.9762924313545227
train_input: [0.5031026124151313, 0.97629243], train_label: 0
TF_IDF_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: OMG Carlos Delfino just dunk on KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.44665616 0.44665616 0.
  0.         0.44665616 0.         0.44665616]
 [0.31779954 0.31779954 0.44665616 0.         0.         0.44665616
  0.44665616 0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Carlos Delfino dunked on Kevin Durant tonight, sentence2: OMG Carlos Delfino just dunk on KD
After tokenization, sentence1: ['carlos', 'delfino', 'dunked', 'on', 'kevin', 'durant', 'tonight'], sentence2: ['omg', 'carlos', 'delfino', 'just', 'dunk', 'on', 'kd']
cosine_similarity: 0.9282897114753723
train_input: [0.20199309249791833, 0.9282897], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino is such a badass
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.57615236]
 [0.70490949 0.50154891 0.50154891 0.         0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino is such a badass
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['carlos', 'delfino', 'is', 'such', 'a', 'badass']
cosine_similarity: 0.9261361360549927
train_input: [0.4112070550676187, 0.92613614], train_label: 0
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino just CROWNED Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.44832087 0.44832087 0.         0.
  0.63009934]
 [0.33471228 0.47042643 0.33471228 0.33471228 0.47042643 0.47042643
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino just CROWNED Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'crowned', 'kevin', 'durant']
cosine_similarity: 0.9383776783943176
train_input: [0.4501755023269898, 0.9383777], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino just dunked all over KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino just dunked all over KD
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'dunked', 'all', 'over', 'kd']
cosine_similarity: 0.9350302815437317
train_input: [0.29121941856368966, 0.9350303], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino just gave Kevin Durant a facial
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.44832087 0.         0.         0.
  0.         0.63009934]
 [0.30287281 0.30287281 0.30287281 0.42567716 0.42567716 0.42567716
  0.42567716 0.        ]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino just gave Kevin Durant a facial
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'gave', 'kevin', 'durant', 'a', 'facial']
cosine_similarity: 0.9696918725967407
train_input: [0.4073526042885674, 0.9696919], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino just jammed it on KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Carlos Delfino just jammed it on KD
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'jammed', 'it', 'on', 'kd']
cosine_similarity: 0.9255695343017578
train_input: [0.29121941856368966, 0.92556953], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Get dunked on KD haha by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Get dunked on KD haha by Carlos Delfino
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['get', 'dunked', 'on', 'kd', 'haha', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9219216704368591
train_input: [0.29121941856368966, 0.9219217], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Hold on did Carlos Delfino just dunk on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.         0.44832087 0.
  0.         0.         0.63009934]
 [0.27867523 0.27867523 0.39166832 0.39166832 0.27867523 0.39166832
  0.39166832 0.39166832 0.        ]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Hold on did Carlos Delfino just dunk on Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['hold', 'on', 'did', 'carlos', 'delfino', 'just', 'dunk', 'on', 'kevin', 'durant']
cosine_similarity: 0.9658976197242737
train_input: [0.3748077700589726, 0.9658976], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Kd got dunk on like in that commercial but by Carlos delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.57615236 0.
  0.         0.         0.57615236]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.         0.4078241
  0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Kd got dunk on like in that commercial but by Carlos delfino
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['kd', 'got', 'dunk', 'on', 'like', 'in', 'that', 'commercial', 'but', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9209365248680115
train_input: [0.23790309463326234, 0.9209365], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Or did Carlos Delfino just dunk on KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.         0.57615236]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: Or did Carlos Delfino just dunk on KD
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['or', 'did', 'carlos', 'delfino', 'just', 'dunk', 'on', 'kd']
cosine_similarity: 0.928350031375885
train_input: [0.2605556710562624, 0.92835003], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: THAT NIGGA CARLOS DELFINO PUT THAT BITCH ON DURANT HEAD MAN
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.44832087 0.         0.
  0.         0.63009934]
 [0.42567716 0.30287281 0.30287281 0.30287281 0.42567716 0.42567716
  0.42567716 0.        ]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Carlos Delfino pokin one on Durant, sentence2: THAT NIGGA CARLOS DELFINO PUT THAT BITCH ON DURANT HEAD MAN
After tokenization, sentence1: ['carlos', 'delfino', 'pokin', 'one', 'on', 'durant'], sentence2: ['that', 'nigga', 'carlos', 'delfino', 'put', 'that', 'bitch', 'on', 'durant', 'head', 'man']
cosine_similarity: 0.9264184236526489
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Carlos Delfino ALL OVER Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.3174044  0.44610081 0.44610081 0.3174044
  0.44610081]
 [0.5        0.5        0.5        0.         0.         0.5
  0.        ]]
pairwise_similarity: [[1.        0.6348088]
 [0.6348088 1.       ]]
cosine_similarity: 0.6348087971775132
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Carlos Delfino ALL OVER Kevin Durant
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'all', 'over', 'kevin', 'durant']
cosine_similarity: 0.9691578149795532
train_input: [0.6348087971775132, 0.9691578], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Carlos Delfino got cookies and dipped em Durant s head
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.         0.3174044  0.         0.3174044  0.
  0.3174044  0.         0.44610081 0.44610081 0.44610081]
 [0.28986934 0.40740124 0.28986934 0.40740124 0.28986934 0.40740124
  0.28986934 0.40740124 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.36802321]
 [0.36802321 1.        ]]
cosine_similarity: 0.36802320875611494
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Carlos Delfino got cookies and dipped em Durant s head
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'got', 'cookies', 'and', 'dipped', 'em', 'durant', 's', 'head']
cosine_similarity: 0.9420264959335327
train_input: [0.36802320875611494, 0.9420265], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Carlos Delfino just dunked on Durant lmao
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.         0.3174044  0.44610081 0.3174044
  0.44610081 0.         0.44610081]
 [0.35464863 0.35464863 0.49844628 0.35464863 0.         0.35464863
  0.         0.49844628 0.        ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Carlos Delfino just dunked on Durant lmao
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'just', 'dunked', 'on', 'durant', 'lmao']
cosine_similarity: 0.9893665909767151
train_input: [0.4502681446556265, 0.9893666], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Carlos Delfino just jammed it on KD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.42567716 0.42567716 0.         0.30287281
  0.         0.42567716 0.42567716]
 [0.37930349 0.37930349 0.         0.         0.53309782 0.37930349
  0.53309782 0.         0.        ]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Carlos Delfino just jammed it on KD
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['carlos', 'delfino', 'just', 'jammed', 'it', 'on', 'kd']
cosine_similarity: 0.9594841599464417
train_input: [0.34464214103805474, 0.95948416], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Did Carlos Delfino just put KD on a poster
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.         0.42567716 0.42567716 0.30287281
  0.         0.42567716 0.         0.42567716]
 [0.33471228 0.33471228 0.47042643 0.         0.         0.33471228
  0.47042643 0.         0.47042643 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Did Carlos Delfino just put KD on a poster
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['did', 'carlos', 'delfino', 'just', 'put', 'kd', 'on', 'a', 'poster']
cosine_similarity: 0.9470809102058411
train_input: [0.3041257418754935, 0.9470809], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Durant must be injured if he let Carlos Delfino dunk on him
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.         0.30287281 0.42567716 0.
  0.42567716 0.42567716 0.         0.42567716]
 [0.33471228 0.33471228 0.47042643 0.33471228 0.         0.47042643
  0.         0.         0.47042643 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Durant must be injured if he let Carlos Delfino dunk on him
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['durant', 'must', 'be', 'injured', 'if', 'he', 'let', 'carlos', 'delfino', 'dunk', 'on', 'him']
cosine_similarity: 0.952934741973877
train_input: [0.3041257418754935, 0.95293474], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: How did Carlos Delfino just posterize Kevin Durant Lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.         0.33425073 0.46977774 0.33425073
  0.33425073 0.         0.         0.46977774]
 [0.30253071 0.30253071 0.42519636 0.30253071 0.         0.30253071
  0.30253071 0.42519636 0.42519636 0.        ]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739692
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: How did Carlos Delfino just posterize Kevin Durant Lol
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['how', 'did', 'carlos', 'delfino', 'just', 'posterize', 'kevin', 'durant', 'lol']
cosine_similarity: 0.9778879880905151
train_input: [0.5056055588739692, 0.977888], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Kevin Durant monster slam on Carlos delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.3174044  0.44610081 0.44610081 0.3174044
  0.         0.         0.44610081]
 [0.35464863 0.35464863 0.35464863 0.         0.         0.35464863
  0.49844628 0.49844628 0.        ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Kevin Durant monster slam on Carlos delfino
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['kevin', 'durant', 'monster', 'slam', 'on', 'carlos', 'delfino']
cosine_similarity: 0.958424985408783
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Kevin Durant you just got shitted on by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35409974 0.35409974 0.35409974 0.35409974 0.35409974 0.35409974
  0.         0.49767483]
 [0.35409974 0.35409974 0.35409974 0.35409974 0.35409974 0.35409974
  0.49767483 0.        ]]
pairwise_similarity: [[1.         0.75231976]
 [0.75231976 1.        ]]
cosine_similarity: 0.7523197619890014
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Kevin Durant you just got shitted on by Carlos Delfino
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['kevin', 'durant', 'you', 'just', 'got', 'shitted', 'on', 'by', 'carlos', 'delfino']
cosine_similarity: 0.987911581993103
train_input: [0.7523197619890014, 0.9879116], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Still sad that Kevin Durant got dunked on by Carlos Delfino of all people
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.         0.33425073 0.33425073 0.46977774
  0.33425073 0.         0.         0.46977774]
 [0.30253071 0.30253071 0.42519636 0.30253071 0.30253071 0.
  0.30253071 0.42519636 0.42519636 0.        ]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739692
word_to_vector_cosine_similarity: sentence1: Kevin Durant just got yammed on by Carlos Delfino, sentence2: Still sad that Kevin Durant got dunked on by Carlos Delfino of all people
After tokenization, sentence1: ['kevin', 'durant', 'just', 'got', 'yammed', 'on', 'by', 'carlos', 'delfino'], sentence2: ['still', 'sad', 'that', 'kevin', 'durant', 'got', 'dunked', 'on', 'by', 'carlos', 'delfino', 'of', 'all', 'people']
cosine_similarity: 0.9596658945083618
train_input: [0.5056055588739692, 0.9596659], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Carlos Delfino dunking on Kevin Durant was amazing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.37930349 0.53309782 0.         0.37930349
  0.53309782 0.        ]
 [0.47042643 0.33471228 0.33471228 0.         0.47042643 0.33471228
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Carlos Delfino dunking on Kevin Durant was amazing
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['carlos', 'delfino', 'dunking', 'on', 'kevin', 'durant', 'was', 'amazing']
cosine_similarity: 0.9678143262863159
train_input: [0.38087260847594373, 0.9678143], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Carlos Delfino just fucking jammed on Durant wtf lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.4090901  0.         0.
  0.4090901  0.         0.        ]
 [0.28986934 0.28986934 0.         0.28986934 0.40740124 0.40740124
  0.28986934 0.40740124 0.40740124]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Carlos Delfino just fucking jammed on Durant wtf lol
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'fucking', 'jammed', 'on', 'durant', 'wtf', 'lol']
cosine_similarity: 0.9356786012649536
train_input: [0.4743307064971939, 0.9356786], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Carlos Delfino just pizzad Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.4090901  0.4090901  0.
  0.        ]
 [0.35464863 0.35464863 0.         0.35464863 0.35464863 0.49844628
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Carlos Delfino just pizzad Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['carlos', 'delfino', 'just', 'kevin', 'durant']
cosine_similarity: 0.9585479497909546
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Damn carlos delfino just dunked on kevin durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.         0.4472136  0.4472136  0.4472136  0.4472136
  0.        ]
 [0.33425073 0.46977774 0.33425073 0.33425073 0.33425073 0.33425073
  0.46977774]]
pairwise_similarity: [[1.         0.74740735]
 [0.74740735 1.        ]]
cosine_similarity: 0.7474073540060464
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Damn carlos delfino just dunked on kevin durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['damn', 'carlos', 'delfino', 'just', 'dunked', 'on', 'kevin', 'durant']
cosine_similarity: 0.9826245307922363
train_input: [0.7474073540060464, 0.98262453], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: KD gets posterized by Carlos Delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: KD gets posterized by Carlos Delfino
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['kd', 'gets', 'posterized', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9381493926048279
train_input: [0.2523342014336961, 0.9381494], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: KD just got SMASHED on by Carlos Delfino hahaha
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.53309782 0.         0.
  0.37930349 0.         0.        ]
 [0.30287281 0.30287281 0.         0.         0.42567716 0.42567716
  0.30287281 0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: KD just got SMASHED on by Carlos Delfino hahaha
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['kd', 'just', 'got', 'smashed', 'on', 'by', 'carlos', 'delfino', 'hahaha']
cosine_similarity: 0.9063186645507812
train_input: [0.34464214103805474, 0.90631866], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Kd got dunk on like in that commercial but by Carlos delfino
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.49922133 0.49922133
  0.         0.49922133 0.         0.        ]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.         0.
  0.4078241  0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Kd got dunk on like in that commercial but by Carlos delfino
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['kd', 'got', 'dunk', 'on', 'like', 'in', 'that', 'commercial', 'but', 'by', 'carlos', 'delfino']
cosine_similarity: 0.9022307991981506
train_input: [0.20613696606828605, 0.9022308], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Lmao Carlos Delfino dunking over Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.         0.37930349 0.53309782
  0.        ]
 [0.37930349 0.37930349 0.         0.53309782 0.37930349 0.
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Lmao Carlos Delfino dunking over Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['lmao', 'carlos', 'delfino', 'dunking', 'over', 'durant']
cosine_similarity: 0.9851194024085999
train_input: [0.43161341897075145, 0.9851194], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Look at the Rockets Carlos Delfino throw it down on Kevin Durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.37930349 0.53309782 0.
  0.         0.         0.        ]
 [0.30287281 0.30287281 0.         0.30287281 0.         0.42567716
  0.42567716 0.42567716 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: Look at the Rockets Carlos Delfino throw it down on Kevin Durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['look', 'at', 'the', 'rockets', 'carlos', 'delfino', 'throw', 'it', 'down', 'on', 'kevin', 'durant']
cosine_similarity: 0.9350178241729736
train_input: [0.34464214103805474, 0.9350178], train_label: 1
TF_IDF_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: who saw carlos delfino get the and 1 dunk on kevin durant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.37930349 0.53309782
  0.         0.        ]
 [0.33471228 0.33471228 0.47042643 0.         0.33471228 0.
  0.47042643 0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Carlos Delfino just dunked over Durant, sentence2: who saw carlos delfino get the and 1 dunk on kevin durant
After tokenization, sentence1: ['carlos', 'delfino', 'just', 'dunked', 'over', 'durant'], sentence2: ['who', 'saw', 'carlos', 'delfino', 'get', 'the', 'and', 'dunk', 'on', 'kevin', 'durant']
cosine_similarity: 0.9320610165596008
train_input: [0.38087260847594373, 0.932061], train_label: 1
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Am I the only one who thinks Waka and Carmelo look alike
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.53404633 0.
  0.         0.        ]
 [0.47107781 0.         0.33517574 0.         0.         0.47107781
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Am I the only one who thinks Waka and Carmelo look alike
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['am', 'i', 'the', 'only', 'one', 'who', 'thinks', 'waka', 'and', 'carmelo', 'look', 'alike']
cosine_similarity: 0.8711732029914856
train_input: [0.1273595297947935, 0.8711732], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo Anthony shot 10 FOR 35
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.40993715 0.57615236 0.57615236
  0.        ]
 [0.49922133 0.49922133 0.35520009 0.35520009 0.         0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo Anthony shot 10 FOR 35
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['carmelo', 'anthony', 'shot', 'for']
cosine_similarity: 0.9350222945213318
train_input: [0.29121941856368966, 0.9350223], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo anthony is the only player in the who can score 36 by shooting 1035
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.40993715 0.57615236 0.57615236
  0.         0.         0.        ]
 [0.4078241  0.4078241  0.29017021 0.29017021 0.         0.
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo anthony is the only player in the who can score 36 by shooting 1035
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['carmelo', 'anthony', 'is', 'the', 'only', 'player', 'in', 'the', 'who', 'can', 'score', 'by', 'shooting']
cosine_similarity: 0.8928342461585999
train_input: [0.23790309463326234, 0.89283425], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo blew this game for us
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.53404633]
 [0.         0.6316672  0.44943642 0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo blew this game for us
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['carmelo', 'blew', 'this', 'game', 'for', 'us']
cosine_similarity: 0.8499060869216919
train_input: [0.17077611319011649, 0.8499061], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo gave this one away
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.53404633]
 [0.         0.6316672  0.44943642 0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo gave this one away
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['carmelo', 'gave', 'this', 'one', 'away']
cosine_similarity: 0.8740899562835693
train_input: [0.17077611319011649, 0.87408996], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo has missed 24 shots today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.53404633 0.
  0.         0.        ]
 [0.47107781 0.         0.33517574 0.         0.         0.47107781
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo has missed 24 shots today
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['carmelo', 'has', 'missed', 'shots', 'today']
cosine_similarity: 0.849716305732727
train_input: [0.1273595297947935, 0.8497163], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo what are you doing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.53404633]
 [0.         0.57973867 0.81480247 0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Carmelo what are you doing
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['carmelo', 'what', 'are', 'you', 'doing']
cosine_similarity: 0.8601498603820801
train_input: [0.22028815056182965, 0.86014986], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: If Carmelo Anthony was any more off the mark hed write a poem for Dzhokhar
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236
  0.         0.         0.        ]
 [0.29017021 0.29017021 0.4078241  0.4078241  0.         0.
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: If Carmelo Anthony was any more off the mark hed write a poem for Dzhokhar
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['if', 'carmelo', 'anthony', 'was', 'any', 'more', 'off', 'the', 'mark', 'hed', 'write', 'a', 'poem', 'for', 'dzhokhar']
cosine_similarity: 0.8915309309959412
train_input: [0.23790309463326234, 0.89153093], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Round of applause for Carmelo and his chuck ups today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.53404633
  0.         0.         0.        ]
 [0.         0.4261596  0.30321606 0.4261596  0.         0.
  0.4261596  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: Round of applause for Carmelo and his chuck ups today
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['round', 'of', 'applause', 'for', 'carmelo', 'and', 'his', 'chuck', 'ups', 'today']
cosine_similarity: 0.8871020078659058
train_input: [0.11521554337793122, 0.887102], train_label: 0
TF_IDF_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: this guy Carmelo shot 28
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.         0.53404633 0.53404633
  0.        ]
 [0.53404633 0.         0.37997836 0.53404633 0.         0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Kim Kardashian or Carmelo Anthony, sentence2: this guy Carmelo shot 28
After tokenization, sentence1: ['kim', 'kardashian', 'or', 'carmelo', 'anthony'], sentence2: ['this', 'guy', 'carmelo', 'shot']
cosine_similarity: 0.8929717540740967
train_input: [0.1443835552773867, 0.89297175], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.39166832 0.39166832 0.39166832 0.27867523
  0.39166832 0.         0.27867523]
 [0.         0.44832087 0.         0.         0.         0.44832087
  0.         0.63009934 0.44832087]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick']
cosine_similarity: 0.9653088450431824
train_input: [0.3748077700589726, 0.96530885], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: Central Michigan OT Eric Fisher to Kansas City
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.         0.40740124 0.28986934 0.28986934
  0.         0.28986934 0.40740124 0.         0.40740124]
 [0.         0.3174044  0.44610081 0.         0.3174044  0.3174044
  0.44610081 0.3174044  0.         0.44610081 0.        ]]
pairwise_similarity: [[1.         0.36802321]
 [0.36802321 1.        ]]
cosine_similarity: 0.36802320875611494
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: Central Michigan OT Eric Fisher to Kansas City
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['central', 'michigan', 'ot', 'eric', 'fisher', 'to', 'kansas', 'city']
cosine_similarity: 0.9484549164772034
train_input: [0.36802320875611494, 0.9484549], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: Central Michigan s 67 offensive tackle Eric Fisher was the No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.         0.28986934 0.40740124 0.28986934 0.28986934
  0.28986934 0.40740124 0.         0.40740124 0.        ]
 [0.         0.44610081 0.3174044  0.         0.3174044  0.3174044
  0.3174044  0.         0.44610081 0.         0.44610081]]
pairwise_similarity: [[1.         0.36802321]
 [0.36802321 1.        ]]
cosine_similarity: 0.36802320875611494
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: Central Michigan s 67 offensive tackle Eric Fisher was the No
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['central', 'michigan', 's', 'offensive', 'tackle', 'eric', 'fisher', 'was', 'the', 'no']
cosine_similarity: 0.9759248495101929
train_input: [0.36802320875611494, 0.97592485], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: Congrats to Eric Fisher 1 pick in 2013 NFLDraftThat s big for Central Michigan the MAC
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.         0.         0.30253071 0.         0.42519636
  0.30253071 0.30253071 0.         0.30253071 0.42519636 0.
  0.30253071]
 [0.         0.36439074 0.36439074 0.25926702 0.36439074 0.
  0.25926702 0.25926702 0.36439074 0.25926702 0.         0.36439074
  0.25926702]]
pairwise_similarity: [[1.         0.39218118]
 [0.39218118 1.        ]]
cosine_similarity: 0.392181175971253
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: Congrats to Eric Fisher 1 pick in 2013 NFLDraftThat s big for Central Michigan the MAC
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['congrats', 'to', 'eric', 'fisher', 'pick', 'in', 's', 'big', 'for', 'central', 'michigan', 'the', 'mac']
cosine_similarity: 0.9815515875816345
train_input: [0.392181175971253, 0.9815516], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: Eric Fisher straight outta Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.40740124 0.28986934 0.28986934 0.28986934
  0.40740124 0.         0.40740124 0.        ]
 [0.         0.35464863 0.         0.35464863 0.35464863 0.35464863
  0.         0.49844628 0.         0.49844628]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: Eric Fisher straight outta Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['eric', 'fisher', 'straight', 'outta', 'central', 'michigan']
cosine_similarity: 0.9445584416389465
train_input: [0.41120705506761857, 0.94455844], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: First pick overall in NFL draft was from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.30253071 0.30253071 0.42519636 0.42519636 0.30253071
  0.30253071 0.         0.30253071]
 [0.         0.37863221 0.37863221 0.         0.         0.37863221
  0.37863221 0.53215436 0.37863221]]
pairwise_similarity: [[1.         0.57273936]
 [0.57273936 1.        ]]
cosine_similarity: 0.5727393584196199
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: First pick overall in NFL draft was from Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['first', 'pick', 'overall', 'in', 'nfl', 'draft', 'was', 'from', 'central', 'michigan']
cosine_similarity: 0.9891225695610046
train_input: [0.5727393584196199, 0.98912257], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: The Kansas City Chiefs have selected Central Michigan OT Eric Fisher with the first o
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.         0.         0.40740124 0.28986934
  0.28986934 0.         0.28986934 0.40740124 0.         0.40740124
  0.        ]
 [0.         0.26844636 0.37729199 0.37729199 0.         0.26844636
  0.26844636 0.37729199 0.26844636 0.         0.37729199 0.
  0.37729199]]
pairwise_similarity: [[1.         0.31125747]
 [0.31125747 1.        ]]
cosine_similarity: 0.3112574675270537
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: The Kansas City Chiefs have selected Central Michigan OT Eric Fisher with the first o
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['the', 'kansas', 'city', 'chiefs', 'have', 'selected', 'central', 'michigan', 'ot', 'eric', 'fisher', 'with', 'the', 'first', 'o']
cosine_similarity: 0.9901632070541382
train_input: [0.3112574675270537, 0.9901632], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: The number one pick in the NFL draft is from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.30253071 0.30253071 0.42519636 0.42519636 0.30253071
  0.30253071 0.         0.30253071]
 [0.         0.37863221 0.37863221 0.         0.         0.37863221
  0.37863221 0.53215436 0.37863221]]
pairwise_similarity: [[1.         0.57273936]
 [0.57273936 1.        ]]
cosine_similarity: 0.5727393584196199
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: The number one pick in the NFL draft is from Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['the', 'number', 'one', 'pick', 'in', 'the', 'nfl', 'draft', 'is', 'from', 'central', 'michigan']
cosine_similarity: 0.9902293086051941
train_input: [0.5727393584196199, 0.9902293], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: big ass cracka from central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.         0.         0.26868528 0.         0.37762778
  0.37762778 0.37762778 0.26868528 0.37762778 0.37762778]
 [0.         0.49922133 0.49922133 0.35520009 0.49922133 0.
  0.         0.         0.35520009 0.         0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: big ass cracka from central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['big', 'ass', 'cracka', 'from', 'central', 'michigan']
cosine_similarity: 0.9295927882194519
train_input: [0.1908740661302035, 0.9295928], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: somebody from Central Michigan went first overall in the nfl draft and youre going there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.40740124 0.40740124 0.
  0.28986934 0.28986934 0.         0.40740124 0.         0.
  0.        ]
 [0.         0.26844636 0.26844636 0.         0.         0.37729199
  0.26844636 0.26844636 0.37729199 0.         0.37729199 0.37729199
  0.37729199]]
pairwise_similarity: [[1.         0.31125747]
 [0.31125747 1.        ]]
cosine_similarity: 0.3112574675270537
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan is the 1st pick in the NFL draft, sentence2: somebody from Central Michigan went first overall in the nfl draft and youre going there
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'pick', 'in', 'the', 'nfl', 'draft'], sentence2: ['somebody', 'from', 'central', 'michigan', 'went', 'first', 'overall', 'in', 'the', 'nfl', 'draft', 'and', 'youre', 'going', 'there']
cosine_similarity: 0.9799878001213074
train_input: [0.3112574675270537, 0.9799878], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: An Offensive Lineman from little ole Central Michigan goes 1 in the NFL Draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.42567716 0.         0.42567716 0.42567716 0.30287281
  0.         0.         0.30287281 0.         0.42567716 0.
  0.        ]
 [0.25948224 0.         0.36469323 0.         0.         0.25948224
  0.36469323 0.36469323 0.25948224 0.36469323 0.         0.36469323
  0.36469323]]
pairwise_similarity: [[1.         0.23577034]
 [0.23577034 1.        ]]
cosine_similarity: 0.23577033983032974
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: An Offensive Lineman from little ole Central Michigan goes 1 in the NFL Draft
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['an', 'offensive', 'lineman', 'from', 'little', 'ole', 'central', 'michigan', 'goes', 'in', 'the', 'nfl', 'draft']
cosine_similarity: 0.9789918661117554
train_input: [0.23577033983032974, 0.97899187], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: Central Michigan with the first over all pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.4078241  0.4078241  0.4078241  0.29017021
  0.4078241  0.        ]
 [0.50154891 0.         0.         0.         0.         0.50154891
  0.         0.70490949]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: Central Michigan with the first over all pick
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['central', 'michigan', 'with', 'the', 'first', 'over', 'all', 'pick']
cosine_similarity: 0.976241946220398
train_input: [0.2910691023819054, 0.97624195], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: Chiefs pick an offensive tackle from Central Michigan with the top pick in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.         0.42567716 0.42567716 0.42567716
  0.30287281 0.42567716 0.         0.         0.        ]
 [0.24377685 0.24377685 0.34261985 0.         0.         0.
  0.24377685 0.         0.34261985 0.68523971 0.34261985]]
pairwise_similarity: [[1.         0.22150013]
 [0.22150013 1.        ]]
cosine_similarity: 0.22150013430590973
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: Chiefs pick an offensive tackle from Central Michigan with the top pick in the draft
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['chiefs', 'pick', 'an', 'offensive', 'tackle', 'from', 'central', 'michigan', 'with', 'the', 'top', 'pick', 'in', 'the', 'draft']
cosine_similarity: 0.978994607925415
train_input: [0.22150013430590973, 0.9789946], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: Chiefs pick first for the first time and were happy with Eric fisher from central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.33425073 0.33425073 0.46977774 0.
  0.33425073 0.46977774 0.         0.        ]
 [0.30253071 0.30253071 0.30253071 0.30253071 0.         0.42519636
  0.30253071 0.         0.42519636 0.42519636]]
pairwise_similarity: [[1.         0.50560556]
 [0.50560556 1.        ]]
cosine_similarity: 0.5056055588739692
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: Chiefs pick first for the first time and were happy with Eric fisher from central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['chiefs', 'pick', 'first', 'for', 'the', 'first', 'time', 'and', 'were', 'happy', 'with', 'eric', 'fisher', 'from', 'central', 'michigan']
cosine_similarity: 0.9843065142631531
train_input: [0.5056055588739692, 0.9843065], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: Eric fisher left tackle from central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.3174044  0.3174044  0.44610081 0.
  0.3174044  0.44610081 0.        ]
 [0.35464863 0.         0.35464863 0.35464863 0.         0.49844628
  0.35464863 0.         0.49844628]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: Eric fisher left tackle from central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['eric', 'fisher', 'left', 'tackle', 'from', 'central', 'michigan']
cosine_similarity: 0.9858441948890686
train_input: [0.4502681446556265, 0.9858442], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: First Eric Fisher out Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.3174044  0.3174044  0.44610081 0.3174044
  0.44610081]
 [0.5        0.         0.5        0.5        0.         0.5
  0.        ]]
pairwise_similarity: [[1.        0.6348088]
 [0.6348088 1.       ]]
cosine_similarity: 0.6348087971775132
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: First Eric Fisher out Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['first', 'eric', 'fisher', 'out', 'central', 'michigan']
cosine_similarity: 0.9903877377510071
train_input: [0.6348087971775132, 0.99038774], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: KC Chiefs take Eric Fisher OT Central Michigan with the 1 overall pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.33425073 0.33425073 0.46977774 0.
  0.33425073 0.46977774 0.         0.         0.        ]
 [0.27840869 0.27840869 0.27840869 0.27840869 0.         0.39129369
  0.27840869 0.         0.39129369 0.39129369 0.39129369]]
pairwise_similarity: [[1.         0.46529153]
 [0.46529153 1.        ]]
cosine_similarity: 0.4652915323370137
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: KC Chiefs take Eric Fisher OT Central Michigan with the 1 overall pick
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['kc', 'chiefs', 'take', 'eric', 'fisher', 'ot', 'central', 'michigan', 'with', 'the', 'overall', 'pick']
cosine_similarity: 0.9882136583328247
train_input: [0.4652915323370137, 0.98821366], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: More on Chiefs taking Central Michigan OT Eric Fisher with the No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.33425073 0.33425073 0.46977774 0.33425073
  0.46977774 0.         0.        ]
 [0.33425073 0.33425073 0.33425073 0.33425073 0.         0.33425073
  0.         0.46977774 0.46977774]]
pairwise_similarity: [[1.         0.55861775]
 [0.55861775 1.        ]]
cosine_similarity: 0.5586177528223194
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: More on Chiefs taking Central Michigan OT Eric Fisher with the No
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['more', 'on', 'chiefs', 'taking', 'central', 'michigan', 'ot', 'eric', 'fisher', 'with', 'the', 'no']
cosine_similarity: 0.9907087683677673
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: OT Eric Fisher from Central Michigan is the No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.3174044  0.3174044  0.44610081 0.3174044
  0.44610081 0.        ]
 [0.4090901  0.         0.4090901  0.4090901  0.         0.4090901
  0.         0.57496187]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: OT Eric Fisher from Central Michigan is the No
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['ot', 'eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'no']
cosine_similarity: 0.9837036728858948
train_input: [0.5193879933129156, 0.9837037], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: With the 1 pick the Kansas City Chiefs select Eric Fisher Central Michigan University
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.         0.33425073 0.33425073 0.46977774
  0.         0.33425073 0.46977774 0.         0.         0.        ]
 [0.25926702 0.25926702 0.36439074 0.25926702 0.25926702 0.
  0.36439074 0.25926702 0.         0.36439074 0.36439074 0.36439074]]
pairwise_similarity: [[1.         0.43330095]
 [0.43330095 1.        ]]
cosine_similarity: 0.4333009465089471
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan goes number 1 to the Chiefs, sentence2: With the 1 pick the Kansas City Chiefs select Eric Fisher Central Michigan University
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'number', 'to', 'the', 'chiefs'], sentence2: ['with', 'the', 'pick', 'the', 'kansas', 'city', 'chiefs', 'select', 'eric', 'fisher', 'central', 'michigan', 'university']
cosine_similarity: 0.9849340319633484
train_input: [0.4333009465089471, 0.98493403], train_label: 1
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Central Michigan OT Eric Fisher to Kansas City
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.         0.
  0.35520009 0.49922133 0.         0.49922133]
 [0.29017021 0.4078241  0.         0.4078241  0.4078241  0.4078241
  0.29017021 0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Central Michigan OT Eric Fisher to Kansas City
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['central', 'michigan', 'ot', 'eric', 'fisher', 'to', 'kansas', 'city']
cosine_similarity: 0.9303202629089355
train_input: [0.20613696606828605, 0.93032026], train_label: 0
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Congratulations to Central Michigan University OT Eric Fisher to the Kansas City Chiefs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.49922133 0.
  0.         0.         0.35520009 0.49922133 0.         0.49922133
  0.        ]
 [0.23700504 0.33310232 0.33310232 0.33310232 0.         0.33310232
  0.33310232 0.33310232 0.23700504 0.         0.33310232 0.
  0.33310232]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Congratulations to Central Michigan University OT Eric Fisher to the Kansas City Chiefs
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['congratulations', 'to', 'central', 'michigan', 'university', 'ot', 'eric', 'fisher', 'to', 'the', 'kansas', 'city', 'chiefs']
cosine_similarity: 0.9551590085029602
train_input: [0.16836842163679844, 0.955159], train_label: 0
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Dam a 1 draft pick from central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.4090901  0.4090901  0.57496187 0.4090901 ]
 [0.4090901  0.57496187 0.4090901  0.4090901  0.         0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Dam a 1 draft pick from central Michigan
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['dam', 'a', 'draft', 'pick', 'from', 'central', 'michigan']
cosine_similarity: 0.9721935391426086
train_input: [0.6694188517266485, 0.97219354], train_label: 1
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Dude from central michigan went number 1 damnnnnnn
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.35520009 0.49922133
  0.         0.49922133 0.        ]
 [0.31779954 0.44665616 0.         0.44665616 0.31779954 0.
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Dude from central michigan went number 1 damnnnnnn
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['dude', 'from', 'central', 'michigan', 'went', 'number']
cosine_similarity: 0.9693211317062378
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Eric Fisher from Central Michigan goes No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.         0.35520009
  0.49922133 0.49922133]
 [0.35520009 0.         0.49922133 0.49922133 0.49922133 0.35520009
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Eric Fisher from Central Michigan goes No
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['eric', 'fisher', 'from', 'central', 'michigan', 'goes', 'no']
cosine_similarity: 0.9494834542274475
train_input: [0.2523342014336961, 0.94948345], train_label: 0
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Eric Fisher represents Central Michigan 1st overall in the NFL
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.53309782 0.         0.         0.37930349
  0.37930349 0.         0.53309782 0.        ]
 [0.39166832 0.27867523 0.         0.39166832 0.39166832 0.27867523
  0.27867523 0.39166832 0.         0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Eric Fisher represents Central Michigan 1st overall in the NFL
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl']
cosine_similarity: 0.9733038544654846
train_input: [0.31710746658027095, 0.97330385], train_label: 1
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Eric fisher central Michigan first pick of NFL draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4472136  0.4472136  0.         0.         0.4472136  0.4472136
  0.4472136 ]
 [0.33425073 0.33425073 0.46977774 0.46977774 0.33425073 0.33425073
  0.33425073]]
pairwise_similarity: [[1.         0.74740735]
 [0.74740735 1.        ]]
cosine_similarity: 0.7474073540060464
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Eric fisher central Michigan first pick of NFL draft
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['eric', 'fisher', 'central', 'michigan', 'first', 'pick', 'of', 'nfl', 'draft']
cosine_similarity: 0.9738940596580505
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Spotted and recruited a 1 draft pick to Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.4090901  0.57496187 0.4090901  0.
  0.        ]
 [0.35464863 0.35464863 0.35464863 0.         0.35464863 0.49844628
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Spotted and recruited a 1 draft pick to Central Michigan
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['spotted', 'and', 'recruited', 'a', 'draft', 'pick', 'to', 'central', 'michigan']
cosine_similarity: 0.9736371040344238
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: With the 1 pick the Kansas City Chiefs select Eric Fisher Central Michigan University
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.53309782 0.         0.
  0.         0.37930349 0.53309782 0.37930349 0.         0.        ]
 [0.24377685 0.34261985 0.34261985 0.         0.34261985 0.34261985
  0.34261985 0.24377685 0.         0.24377685 0.34261985 0.34261985]]
pairwise_similarity: [[1.         0.27739623]
 [0.27739623 1.        ]]
cosine_similarity: 0.27739622897624144
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: With the 1 pick the Kansas City Chiefs select Eric Fisher Central Michigan University
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['with', 'the', 'pick', 'the', 'kansas', 'city', 'chiefs', 'select', 'eric', 'fisher', 'central', 'michigan', 'university']
cosine_similarity: 0.9651405811309814
train_input: [0.27739622897624144, 0.9651406], train_label: 1
TF_IDF_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Wow Eric Fisher from Central Michigan on the Kansas City Chiefs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.         0.
  0.         0.35520009 0.49922133 0.49922133 0.        ]
 [0.26868528 0.37762778 0.37762778 0.         0.37762778 0.37762778
  0.37762778 0.26868528 0.         0.         0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: pick in the NFL draft is out of Central Michigan, sentence2: Wow Eric Fisher from Central Michigan on the Kansas City Chiefs
After tokenization, sentence1: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan'], sentence2: ['wow', 'eric', 'fisher', 'from', 'central', 'michigan', 'on', 'the', 'kansas', 'city', 'chiefs']
cosine_similarity: 0.9706606864929199
train_input: [0.1908740661302035, 0.9706607], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: A fellow CENTRAL MICHIGAN Grad Eric Fisher goes 1
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.3174044  0.         0.3174044  0.
  0.         0.3174044  0.44610081 0.44610081]
 [0.3174044  0.         0.3174044  0.44610081 0.3174044  0.44610081
  0.44610081 0.3174044  0.         0.        ]]
pairwise_similarity: [[1.         0.40298221]
 [0.40298221 1.        ]]
cosine_similarity: 0.40298220897396103
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: A fellow CENTRAL MICHIGAN Grad Eric Fisher goes 1
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['a', 'fellow', 'central', 'michigan', 'grad', 'eric', 'fisher', 'goes']
cosine_similarity: 0.965118408203125
train_input: [0.40298220897396103, 0.9651184], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: A walk on from Central Michigan was the number 1 draft pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.         0.4078241  0.4078241  0.29017021
  0.         0.4078241  0.         0.4078241  0.        ]
 [0.31779954 0.         0.44665616 0.         0.         0.31779954
  0.44665616 0.         0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: A walk on from Central Michigan was the number 1 draft pick
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['a', 'walk', 'on', 'from', 'central', 'michigan', 'was', 'the', 'number', 'draft', 'pick']
cosine_similarity: 0.910359799861908
train_input: [0.18443191662261305, 0.9103598], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: Congrats to Eric Fisher 1 pick in 2013 NFLDraftThat s big for Central Michigan the MAC
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.3174044  0.44610081 0.         0.3174044
  0.3174044  0.         0.3174044  0.         0.44610081 0.
  0.44610081]
 [0.35300279 0.35300279 0.25116439 0.         0.35300279 0.25116439
  0.25116439 0.35300279 0.25116439 0.35300279 0.         0.35300279
  0.        ]]
pairwise_similarity: [[1.         0.31888273]
 [0.31888273 1.        ]]
cosine_similarity: 0.3188827274930885
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: Congrats to Eric Fisher 1 pick in 2013 NFLDraftThat s big for Central Michigan the MAC
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['congrats', 'to', 'eric', 'fisher', 'pick', 'in', 's', 'big', 'for', 'central', 'michigan', 'the', 'mac']
cosine_similarity: 0.9244071245193481
train_input: [0.3188827274930885, 0.9244071], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: Congrats to Eric Fisher from Central Michigan going 1 overall in the
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.         0.3174044  0.3174044  0.
  0.3174044  0.44610081 0.         0.44610081]
 [0.3174044  0.         0.44610081 0.3174044  0.3174044  0.44610081
  0.3174044  0.         0.44610081 0.        ]]
pairwise_similarity: [[1.         0.40298221]
 [0.40298221 1.        ]]
cosine_similarity: 0.40298220897396103
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: Congrats to Eric Fisher from Central Michigan going 1 overall in the
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['congrats', 'to', 'eric', 'fisher', 'from', 'central', 'michigan', 'going', 'overall', 'in', 'the']
cosine_similarity: 0.9484491944313049
train_input: [0.40298220897396103, 0.9484492], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: Kansas City selects Central Michigan tackle Eric Fisher with the first pick in the NFL draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.         0.         0.3174044  0.3174044
  0.         0.3174044  0.         0.44610081 0.         0.44610081
  0.         0.        ]
 [0.23684101 0.         0.33287178 0.33287178 0.23684101 0.23684101
  0.33287178 0.23684101 0.33287178 0.         0.33287178 0.
  0.33287178 0.33287178]]
pairwise_similarity: [[1.         0.30069751]
 [0.30069751 1.        ]]
cosine_similarity: 0.30069751492635505
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: Kansas City selects Central Michigan tackle Eric Fisher with the first pick in the NFL draft
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['kansas', 'city', 'selects', 'central', 'michigan', 'tackle', 'eric', 'fisher', 'with', 'the', 'first', 'pick', 'in', 'the', 'nfl', 'draft']
cosine_similarity: 0.9705394506454468
train_input: [0.30069751492635505, 0.97053945], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: Salute to Eric Fisher representing Rochester and Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.3174044  0.3174044  0.3174044  0.44610081
  0.         0.         0.         0.44610081]
 [0.3174044  0.         0.3174044  0.3174044  0.3174044  0.
  0.44610081 0.44610081 0.44610081 0.        ]]
pairwise_similarity: [[1.         0.40298221]
 [0.40298221 1.        ]]
cosine_similarity: 0.40298220897396103
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: Salute to Eric Fisher representing Rochester and Central Michigan
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['salute', 'to', 'eric', 'fisher', 'representing', 'rochester', 'and', 'central', 'michigan']
cosine_similarity: 0.9693068265914917
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: a offensive tackle from central michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.4078241  0.4078241  0.29017021 0.
  0.4078241  0.4078241  0.        ]
 [0.40993715 0.         0.         0.         0.40993715 0.57615236
  0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: a offensive tackle from central michigan
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['a', 'offensive', 'tackle', 'from', 'central', 'michigan']
cosine_similarity: 0.9377028346061707
train_input: [0.23790309463326234, 0.93770283], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: central Michigan Eric Fisher is no 1 draft pick going to
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.44610081 0.         0.3174044  0.3174044  0.
  0.3174044  0.44610081 0.         0.44610081]
 [0.3174044  0.         0.44610081 0.3174044  0.3174044  0.44610081
  0.3174044  0.         0.44610081 0.        ]]
pairwise_similarity: [[1.         0.40298221]
 [0.40298221 1.        ]]
cosine_similarity: 0.40298220897396103
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: central Michigan Eric Fisher is no 1 draft pick going to
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['central', 'michigan', 'eric', 'fisher', 'is', 'no', 'draft', 'pick', 'going', 'to']
cosine_similarity: 0.9461337924003601
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: draft pick in the NFL from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.4078241  0.         0.4078241  0.4078241  0.29017021
  0.         0.4078241  0.         0.4078241 ]
 [0.35520009 0.         0.49922133 0.         0.         0.35520009
  0.49922133 0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: draft pick in the NFL from Central Michigan
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['draft', 'pick', 'in', 'the', 'nfl', 'from', 'central', 'michigan']
cosine_similarity: 0.9575810432434082
TF_IDF_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: pick in the draft came from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.29017021 0.4078241  0.         0.4078241 ]
 [0.49922133 0.35520009 0.         0.49922133 0.         0.
  0.35520009 0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Chiefs selected Eric Fisher OT from Central Michigan, sentence2: pick in the draft came from Central Michigan
After tokenization, sentence1: ['chiefs', 'selected', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan'], sentence2: ['pick', 'in', 'the', 'draft', 'came', 'from', 'central', 'michigan']
cosine_similarity: 0.937049925327301
train_input: [0.20613696606828605, 0.9370499], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: 1st overall pick in the NFL from central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.49922133 0.35520009
  0.         0.         0.        ]
 [0.44665616 0.31779954 0.         0.         0.         0.31779954
  0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: 1st overall pick in the NFL from central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['overall', 'pick', 'in', 'the', 'nfl', 'from', 'central', 'michigan']
cosine_similarity: 0.9514170289039612
train_input: [0.22576484600261604, 0.951417], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: And with the 1st pick from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.49922133 0.35520009
  0.        ]
 [0.57615236 0.40993715 0.         0.         0.         0.40993715
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: And with the 1st pick from Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['and', 'with', 'the', 'pick', 'from', 'central', 'michigan']
cosine_similarity: 0.9530016183853149
train_input: [0.29121941856368966, 0.9530016], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Central Michigan s Eric Fisher goes No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.4090901  0.         0.57496187 0.4090901 ]
 [0.4090901  0.4090901  0.4090901  0.57496187 0.         0.4090901 ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Central Michigan s Eric Fisher goes No
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['central', 'michigan', 's', 'eric', 'fisher', 'goes', 'no']
cosine_similarity: 0.9587664008140564
train_input: [0.6694188517266485, 0.9587664], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Chiefs have officially selected Eric Fisher from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.         0.4090901  0.4090901  0.57496187 0.4090901
  0.         0.        ]
 [0.3174044  0.44610081 0.3174044  0.3174044  0.         0.3174044
  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Chiefs have officially selected Eric Fisher from Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['chiefs', 'have', 'officially', 'selected', 'eric', 'fisher', 'from', 'central', 'michigan']
cosine_similarity: 0.9641687273979187
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: First pick of the NFL is from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.49922133 0.35520009 0.
  0.        ]
 [0.40993715 0.         0.         0.         0.40993715 0.57615236
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: First pick of the NFL is from Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['first', 'pick', 'of', 'the', 'nfl', 'is', 'from', 'central', 'michigan']
cosine_similarity: 0.9522852301597595
train_input: [0.29121941856368966, 0.95228523], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: He was the number one pick out if central michigan at that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.49922133 0.35520009 0.
  0.        ]
 [0.40993715 0.         0.         0.         0.40993715 0.57615236
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: He was the number one pick out if central michigan at that
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['he', 'was', 'the', 'number', 'one', 'pick', 'out', 'if', 'central', 'michigan', 'at', 'that']
cosine_similarity: 0.9234864711761475
train_input: [0.29121941856368966, 0.9234865], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Kansas City selects OT Eric Fisher From Central Michigan with 1st Pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.         0.4090901  0.4090901  0.57496187
  0.         0.4090901  0.         0.         0.        ]
 [0.35300279 0.25116439 0.35300279 0.25116439 0.25116439 0.
  0.35300279 0.25116439 0.35300279 0.35300279 0.35300279]]
pairwise_similarity: [[1.         0.41099546]
 [0.41099546 1.        ]]
cosine_similarity: 0.4109954639349511
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Kansas City selects OT Eric Fisher From Central Michigan with 1st Pick
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['kansas', 'city', 'selects', 'ot', 'eric', 'fisher', 'from', 'central', 'michigan', 'with', 'pick']
cosine_similarity: 0.9834396839141846
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Spent a semester at Central Michigan home of the 1 pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.49922133 0.35520009
  0.         0.         0.        ]
 [0.31779954 0.         0.         0.44665616 0.         0.31779954
  0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Spent a semester at Central Michigan home of the 1 pick
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['spent', 'a', 'semester', 'at', 'central', 'michigan', 'home', 'of', 'the', 'pick']
cosine_similarity: 0.9253794550895691
train_input: [0.22576484600261604, 0.92537946], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Tonight Eric Fisher and Central Michigan University stand at the top
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.4090901  0.57496187 0.4090901  0.
  0.         0.        ]
 [0.3174044  0.3174044  0.3174044  0.         0.3174044  0.44610081
  0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.51938799]
 [0.51938799 1.        ]]
cosine_similarity: 0.5193879933129156
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: Tonight Eric Fisher and Central Michigan University stand at the top
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['tonight', 'eric', 'fisher', 'and', 'central', 'michigan', 'university', 'stand', 'at', 'the', 'top']
cosine_similarity: 0.9761300086975098
TF_IDF_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: to the BCS and first pick of the draft goes to Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.49922133 0.49922133 0.
  0.49922133 0.35520009 0.        ]
 [0.44665616 0.31779954 0.44665616 0.         0.         0.44665616
  0.         0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Eric Fisher from Central Michigan huh, sentence2: to the BCS and first pick of the draft goes to Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'from', 'central', 'michigan', 'huh'], sentence2: ['to', 'the', 'bcs', 'and', 'first', 'pick', 'of', 'the', 'draft', 'goes', 'to', 'central', 'michigan']
cosine_similarity: 0.9400041699409485
train_input: [0.22576484600261604, 0.94000417], train_label: 0
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: 1st nfl draft pick is from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.         0.37930349 0.         0.53309782
  0.37930349 0.53309782]
 [0.47042643 0.33471228 0.47042643 0.33471228 0.47042643 0.
  0.33471228 0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: 1st nfl draft pick is from Central Michigan
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['nfl', 'draft', 'pick', 'is', 'from', 'central', 'michigan']
cosine_similarity: 0.9815264940261841
train_input: [0.38087260847594373, 0.9815265], train_label: 1
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: A football player got drafted first pick in the first rd from central michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.         0.37930349 0.53309782
  0.37930349 0.         0.         0.53309782]
 [0.27867523 0.39166832 0.39166832 0.39166832 0.27867523 0.
  0.27867523 0.39166832 0.39166832 0.        ]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: A football player got drafted first pick in the first rd from central michigan
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['a', 'football', 'player', 'got', 'drafted', 'first', 'pick', 'in', 'the', 'first', 'rd', 'from', 'central', 'michigan']
cosine_similarity: 0.9860436916351318
train_input: [0.31710746658027095, 0.9860437], train_label: 1
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: A guy from Central Michigan was no
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.49922133 0.49922133 0.49922133]
 [0.50154891 0.70490949 0.50154891 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: A guy from Central Michigan was no
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['a', 'guy', 'from', 'central', 'michigan', 'was', 'no']
cosine_similarity: 0.9351029992103577
train_input: [0.3563004293331381, 0.935103], train_label: 0
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Central Michigan guy is the 1 draft pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.37930349 0.53309782 0.37930349
  0.53309782]
 [0.37930349 0.53309782 0.53309782 0.37930349 0.         0.37930349
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Central Michigan guy is the 1 draft pick
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['central', 'michigan', 'guy', 'is', 'the', 'draft', 'pick']
cosine_similarity: 0.9897850751876831
train_input: [0.43161341897075145, 0.9897851], train_label: 1
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Central Michigan with the first round pick NFLDraft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.         0.53309782 0.37930349 0.
  0.53309782]
 [0.37930349 0.37930349 0.53309782 0.         0.37930349 0.53309782
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Central Michigan with the first round pick NFLDraft
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['central', 'michigan', 'with', 'the', 'first', 'round', 'pick', 'nfldraft']
cosine_similarity: 0.9832035899162292
train_input: [0.43161341897075145, 0.9832036], train_label: 1
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Chiefs make Central Michigan s Eric Fisher the No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.         0.         0.35520009
  0.49922133 0.49922133 0.49922133]
 [0.31779954 0.44665616 0.44665616 0.44665616 0.44665616 0.31779954
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Chiefs make Central Michigan s Eric Fisher the No
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['chiefs', 'make', 'central', 'michigan', 's', 'eric', 'fisher', 'the', 'no']
cosine_similarity: 0.9481322765350342
train_input: [0.22576484600261604, 0.9481323], train_label: 0
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Chiefs pick an offensive tackle from Central Michigan with the top pick in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.         0.37930349 0.         0.53309782
  0.37930349 0.         0.53309782]
 [0.26820807 0.37695709 0.37695709 0.26820807 0.37695709 0.
  0.53641614 0.37695709 0.        ]]
pairwise_similarity: [[1.         0.40692903]
 [0.40692903 1.        ]]
cosine_similarity: 0.4069290338737718
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Chiefs pick an offensive tackle from Central Michigan with the top pick in the draft
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['chiefs', 'pick', 'an', 'offensive', 'tackle', 'from', 'central', 'michigan', 'with', 'the', 'top', 'pick', 'in', 'the', 'draft']
cosine_similarity: 0.9850031137466431
train_input: [0.4069290338737718, 0.9850031], train_label: 1
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Never thought I would see the day where the 1 pick was from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.37930349 0.53309782 0.37930349 0.
  0.53309782]
 [0.37930349 0.53309782 0.37930349 0.         0.37930349 0.53309782
  0.        ]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: Never thought I would see the day where the 1 pick was from Central Michigan
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['never', 'thought', 'i', 'would', 'see', 'the', 'day', 'where', 'the', 'pick', 'was', 'from', 'central', 'michigan']
cosine_similarity: 0.959608793258667
train_input: [0.43161341897075145, 0.9596088], train_label: 1
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: That tackle from central michigan is a stud
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: That tackle from central michigan is a stud
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['that', 'tackle', 'from', 'central', 'michigan', 'is', 'a', 'stud']
cosine_similarity: 0.9765709638595581
train_input: [0.29121941856368966, 0.97657096], train_label: 0
TF_IDF_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: a Central Michigan player is No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133]
 [0.50154891 0.50154891 0.         0.         0.70490949 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: The first overall pick is from Central Michigan University, sentence2: a Central Michigan player is No
After tokenization, sentence1: ['the', 'first', 'overall', 'pick', 'is', 'from', 'central', 'michigan', 'university'], sentence2: ['a', 'central', 'michigan', 'player', 'is', 'no']
cosine_similarity: 0.9183995127677917
train_input: [0.3563004293331381, 0.9183995], train_label: 0
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: 1st pick The Kansas City Chiefs select Eric Fisher from Central Michigan University
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.         0.         0.53309782 0.
  0.         0.         0.37930349 0.37930349 0.53309782 0.
  0.        ]
 [0.32412345 0.2306165  0.32412345 0.32412345 0.         0.32412345
  0.32412345 0.32412345 0.2306165  0.2306165  0.         0.32412345
  0.32412345]]
pairwise_similarity: [[1.         0.26242094]
 [0.26242094 1.        ]]
cosine_similarity: 0.26242093626195995
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: 1st pick The Kansas City Chiefs select Eric Fisher from Central Michigan University
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['pick', 'the', 'kansas', 'city', 'chiefs', 'select', 'eric', 'fisher', 'from', 'central', 'michigan', 'university']
cosine_similarity: 0.9684016108512878
train_input: [0.26242093626195995, 0.9684016], train_label: 1
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: Central Michigan produces the first pick in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.4090901  0.4090901  0.         0.57496187]
 [0.4090901  0.4090901  0.4090901  0.4090901  0.57496187 0.        ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: Central Michigan produces the first pick in the draft
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['central', 'michigan', 'produces', 'the', 'first', 'pick', 'in', 'the', 'draft']
cosine_similarity: 0.9802625179290771
train_input: [0.6694188517266485, 0.9802625], train_label: 1
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: Chiefs pick first for the first time and were happy with Eric fisher from central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.         0.         0.
  0.37930349 0.37930349 0.53309782 0.        ]
 [0.27867523 0.39166832 0.         0.39166832 0.39166832 0.39166832
  0.27867523 0.27867523 0.         0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: Chiefs pick first for the first time and were happy with Eric fisher from central Michigan
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['chiefs', 'pick', 'first', 'for', 'the', 'first', 'time', 'and', 'were', 'happy', 'with', 'eric', 'fisher', 'from', 'central', 'michigan']
cosine_similarity: 0.9707297682762146
train_input: [0.31710746658027095, 0.97072977], train_label: 1
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: Kansas City Chiefs take Central Michigan s Eric Fisher with No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.         0.
  0.         0.35520009 0.49922133 0.49922133]
 [0.29017021 0.4078241  0.4078241  0.         0.4078241  0.4078241
  0.4078241  0.29017021 0.         0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: Kansas City Chiefs take Central Michigan s Eric Fisher with No
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['kansas', 'city', 'chiefs', 'take', 'central', 'michigan', 's', 'eric', 'fisher', 'with', 'no']
cosine_similarity: 0.9556688666343689
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: So to central Michigan university 1 NFL draft pick Eric Fisher
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.         0.         0.4090901  0.
  0.4090901  0.57496187 0.        ]
 [0.28986934 0.28986934 0.40740124 0.40740124 0.28986934 0.40740124
  0.28986934 0.         0.40740124]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: So to central Michigan university 1 NFL draft pick Eric Fisher
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['so', 'to', 'central', 'michigan', 'university', 'nfl', 'draft', 'pick', 'eric', 'fisher']
cosine_similarity: 0.9691444039344788
train_input: [0.4743307064971939, 0.9691444], train_label: 1
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: The kcchiefs took an Offensive Lineman from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.35520009 0.
  0.49922133 0.49922133 0.        ]
 [0.31779954 0.         0.44665616 0.44665616 0.31779954 0.44665616
  0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: The kcchiefs took an Offensive Lineman from Central Michigan
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['the', 'took', 'an', 'offensive', 'lineman', 'from', 'central', 'michigan']
cosine_similarity: 0.9705145955085754
train_input: [0.22576484600261604, 0.9705146], train_label: 0
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: Watching The Draft Eric Fisher From Central Michigan Went 1st
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.37930349 0.         0.         0.37930349
  0.53309782 0.53309782 0.         0.        ]
 [0.39166832 0.27867523 0.27867523 0.39166832 0.39166832 0.27867523
  0.         0.         0.39166832 0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: Watching The Draft Eric Fisher From Central Michigan Went 1st
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['watching', 'the', 'draft', 'eric', 'fisher', 'from', 'central', 'michigan', 'went']
cosine_similarity: 0.9752575755119324
train_input: [0.31710746658027095, 0.9752576], train_label: 1
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: With the 1st pick in the 2013 NFL Draft chiefs select Eric Fisher OT Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.4090901  0.         0.4090901  0.
  0.         0.4090901  0.         0.         0.4090901  0.57496187
  0.        ]
 [0.3158336  0.3158336  0.22471821 0.3158336  0.22471821 0.3158336
  0.3158336  0.22471821 0.3158336  0.3158336  0.22471821 0.
  0.3158336 ]]
pairwise_similarity: [[1.         0.36771998]
 [0.36771998 1.        ]]
cosine_similarity: 0.3677199804697471
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: With the 1st pick in the 2013 NFL Draft chiefs select Eric Fisher OT Central Michigan
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'chiefs', 'select', 'eric', 'fisher', 'ot', 'central', 'michigan']
cosine_similarity: 0.9856497049331665
train_input: [0.3677199804697471, 0.9856497], train_label: 1
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: With the 1st pick the Kansas City Chiefs select Eric Fisher OTCentral Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.         0.49922133 0.
  0.         0.         0.35520009 0.         0.35520009 0.49922133
  0.        ]
 [0.33310232 0.         0.33310232 0.33310232 0.         0.33310232
  0.33310232 0.33310232 0.23700504 0.33310232 0.23700504 0.
  0.33310232]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: With the 1st pick the Kansas City Chiefs select Eric Fisher OTCentral Michigan
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['with', 'the', 'pick', 'the', 'kansas', 'city', 'chiefs', 'select', 'eric', 'fisher', 'michigan']
cosine_similarity: 0.977103054523468
train_input: [0.16836842163679844, 0.97710305], train_label: 1
TF_IDF_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: pick in the NFL draft is out of Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.4090901  0.         0.4090901  0.57496187]
 [0.4090901  0.4090901  0.4090901  0.57496187 0.4090901  0.        ]]
pairwise_similarity: [[1.         0.66941885]
 [0.66941885 1.        ]]
cosine_similarity: 0.6694188517266485
word_to_vector_cosine_similarity: sentence1: First round draft pick from Central Michigan, sentence2: pick in the NFL draft is out of Central Michigan
After tokenization, sentence1: ['first', 'round', 'draft', 'pick', 'from', 'central', 'michigan'], sentence2: ['pick', 'in', 'the', 'nfl', 'draft', 'is', 'out', 'of', 'central', 'michigan']
cosine_similarity: 0.9806597232818604
train_input: [0.6694188517266485, 0.9806597], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: A central Michigan player drafted first overall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.         0.39166832 0.39166832 0.27867523
  0.39166832 0.27867523 0.         0.39166832]
 [0.         0.37930349 0.53309782 0.         0.         0.37930349
  0.         0.37930349 0.53309782 0.        ]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: A central Michigan player drafted first overall
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['a', 'central', 'michigan', 'player', 'drafted', 'first', 'overall']
cosine_similarity: 0.9682331681251526
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Central Michigan OT Eric Fisher selected No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.28986934 0.28986934 0.40740124
  0.         0.40740124 0.40740124 0.        ]
 [0.         0.35464863 0.35464863 0.35464863 0.35464863 0.
  0.49844628 0.         0.         0.49844628]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Central Michigan OT Eric Fisher selected No
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['central', 'michigan', 'ot', 'eric', 'fisher', 'selected', 'no']
cosine_similarity: 0.9483892917633057
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Central Michigan s own Eric Fisher 1 overall NFL draft pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44554752 0.31701073 0.         0.31701073 0.31701073 0.31701073
  0.31701073 0.31701073 0.         0.44554752]
 [0.         0.31701073 0.44554752 0.31701073 0.31701073 0.31701073
  0.31701073 0.31701073 0.44554752 0.        ]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380571
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Central Michigan s own Eric Fisher 1 overall NFL draft pick
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['central', 'michigan', 's', 'own', 'eric', 'fisher', 'overall', 'nfl', 'draft', 'pick']
cosine_similarity: 0.9725077748298645
train_input: [0.6029748160380571, 0.9725078], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: First Eric Fisher out Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.28986934 0.28986934 0.40740124
  0.40740124 0.40740124]
 [0.         0.5        0.5        0.5        0.5        0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376658
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: First Eric Fisher out Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['first', 'eric', 'fisher', 'out', 'central', 'michigan']
cosine_similarity: 0.97471684217453
train_input: [0.5797386715376658, 0.97471684], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: OT from Central Michigan 1st overall pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.28986934 0.28986934 0.40740124 0.40740124 0.28986934 0.40740124
  0.         0.28986934 0.         0.40740124]
 [0.35464863 0.35464863 0.         0.         0.35464863 0.
  0.49844628 0.35464863 0.49844628 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: OT from Central Michigan 1st overall pick
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['ot', 'from', 'central', 'michigan', 'overall', 'pick']
cosine_similarity: 0.9669479131698608
train_input: [0.41120705506761857, 0.9669479], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Tackle from central Michigan goes number 1
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.37762778 0.37762778 0.         0.26868528
  0.37762778 0.         0.37762778 0.37762778 0.        ]
 [0.         0.35520009 0.         0.         0.49922133 0.35520009
  0.         0.49922133 0.         0.         0.49922133]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Tackle from central Michigan goes number 1
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['tackle', 'from', 'central', 'michigan', 'goes', 'number']
cosine_similarity: 0.9575304985046387
train_input: [0.1908740661302035, 0.9575305], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: The Chiefs take OT Eric Fisher from Central Michigan with 1 overall pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.42519636 0.30253071 0.         0.30253071 0.30253071 0.30253071
  0.42519636 0.         0.30253071 0.         0.42519636]
 [0.         0.30253071 0.42519636 0.30253071 0.30253071 0.30253071
  0.         0.42519636 0.30253071 0.42519636 0.        ]]
pairwise_similarity: [[1.         0.45762416]
 [0.45762416 1.        ]]
cosine_similarity: 0.4576241622696326
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: The Chiefs take OT Eric Fisher from Central Michigan with 1 overall pick
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['the', 'chiefs', 'take', 'ot', 'eric', 'fisher', 'from', 'central', 'michigan', 'with', 'overall', 'pick']
cosine_similarity: 0.9766672253608704
train_input: [0.4576241622696326, 0.9766672], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: The first ever number 1 overall pick out of Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.39166832 0.39166832 0.27867523 0.39166832
  0.         0.27867523 0.         0.39166832]
 [0.         0.37930349 0.         0.         0.37930349 0.
  0.53309782 0.37930349 0.53309782 0.        ]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: The first ever number 1 overall pick out of Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['the', 'first', 'ever', 'number', 'overall', 'pick', 'out', 'of', 'central', 'michigan']
cosine_similarity: 0.9479655623435974
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: With the first pick of the 2013 NFLDraft the Chiefs select OT Eric Fisher Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.         0.28986934 0.         0.28986934 0.28986934
  0.28986934 0.40740124 0.         0.         0.40740124 0.
  0.40740124 0.        ]
 [0.         0.35300279 0.25116439 0.35300279 0.25116439 0.25116439
  0.25116439 0.         0.35300279 0.35300279 0.         0.35300279
  0.         0.35300279]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.2912194185636897
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: With the first pick of the 2013 NFLDraft the Chiefs select OT Eric Fisher Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['with', 'the', 'first', 'pick', 'of', 'the', 'nfldraft', 'the', 'chiefs', 'select', 'ot', 'eric', 'fisher', 'central', 'michigan']
cosine_similarity: 0.9792464971542358
train_input: [0.2912194185636897, 0.9792465], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: except Central Michigan is FAR from a top team in the country
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.37762778 0.         0.37762778
  0.26868528 0.37762778 0.37762778 0.37762778 0.        ]
 [0.         0.35520009 0.49922133 0.         0.49922133 0.
  0.35520009 0.         0.         0.         0.49922133]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: except Central Michigan is FAR from a top team in the country
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['except', 'central', 'michigan', 'is', 'far', 'from', 'a', 'top', 'team', 'in', 'the', 'country']
cosine_similarity: 0.9446876645088196
train_input: [0.1908740661302035, 0.94468766], train_label: 0
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: 1 overall pick in the NFLDraft the kcchiefs pick Central Michigan OT Eric Fisher
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.         0.         0.         0.5        0.
  0.5        0.         0.5       ]
 [0.24342027 0.3421187  0.3421187  0.3421187  0.24342027 0.3421187
  0.24342027 0.3421187  0.48684054]]
pairwise_similarity: [[1.         0.60855067]
 [0.60855067 1.        ]]
cosine_similarity: 0.608550673173113
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: 1 overall pick in the NFLDraft the kcchiefs pick Central Michigan OT Eric Fisher
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['overall', 'pick', 'in', 'the', 'nfldraft', 'the', 'pick', 'central', 'michigan', 'ot', 'eric', 'fisher']
cosine_similarity: 0.9575903415679932
train_input: [0.608550673173113, 0.95759034], train_label: 1
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: And the number 1 pick overall is Eric Fisher out of Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.44832087 0.         0.63009934
  0.         0.44832087]
 [0.30287281 0.42567716 0.42567716 0.30287281 0.42567716 0.
  0.42567716 0.30287281]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: And the number 1 pick overall is Eric Fisher out of Central Michigan
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['and', 'the', 'number', 'pick', 'overall', 'is', 'eric', 'fisher', 'out', 'of', 'central', 'michigan']
cosine_similarity: 0.9635905623435974
train_input: [0.4073526042885674, 0.96359056], train_label: 1
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Central Michigan s Eric Fisher is NFL s number 1 draft pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.44832087 0.
  0.         0.63009934 0.44832087]
 [0.27867523 0.39166832 0.39166832 0.39166832 0.27867523 0.39166832
  0.39166832 0.         0.27867523]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Central Michigan s Eric Fisher is NFL s number 1 draft pick
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['central', 'michigan', 's', 'eric', 'fisher', 'is', 'nfl', 's', 'number', 'draft', 'pick']
cosine_similarity: 0.9606961607933044
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Congrats to Eric Fisher from Central Michigan to be drafted 1 pick to Kansas City
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.         0.
  0.         0.44832087 0.63009934 0.44832087]
 [0.25948224 0.36469323 0.36469323 0.36469323 0.36469323 0.36469323
  0.36469323 0.25948224 0.         0.25948224]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Congrats to Eric Fisher from Central Michigan to be drafted 1 pick to Kansas City
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['congrats', 'to', 'eric', 'fisher', 'from', 'central', 'michigan', 'to', 'be', 'drafted', 'pick', 'to', 'kansas', 'city']
cosine_similarity: 0.9620428681373596
train_input: [0.3489939079552687, 0.96204287], train_label: 1
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Congrats to the 1 overall pick from central Michigan Eric Fisher
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.44832087 0.63009934
  0.         0.44832087]
 [0.30287281 0.42567716 0.42567716 0.42567716 0.30287281 0.
  0.42567716 0.30287281]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Congrats to the 1 overall pick from central Michigan Eric Fisher
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['congrats', 'to', 'the', 'overall', 'pick', 'from', 'central', 'michigan', 'eric', 'fisher']
cosine_similarity: 0.9640498161315918
train_input: [0.4073526042885674, 0.9640498], train_label: 1
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Evidence 1 that you dont pick a player from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.44832087 0.63009934 0.44832087
  0.        ]
 [0.33471228 0.47042643 0.47042643 0.33471228 0.         0.33471228
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Evidence 1 that you dont pick a player from Central Michigan
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['evidence', 'that', 'you', 'dont', 'pick', 'a', 'player', 'from', 'central', 'michigan']
cosine_similarity: 0.9775306582450867
train_input: [0.4501755023269898, 0.97753066], train_label: 0
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: MT MACSports Central Michigan OT Eric Fisher No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.44832087 0.
  0.44832087 0.63009934]
 [0.30287281 0.42567716 0.42567716 0.42567716 0.30287281 0.42567716
  0.30287281 0.        ]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: MT MACSports Central Michigan OT Eric Fisher No
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['mt', 'central', 'michigan', 'ot', 'eric', 'fisher', 'no']
cosine_similarity: 0.9338091015815735
train_input: [0.4073526042885674, 0.9338091], train_label: 0
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Pick 1 Eric Fisher OT Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.         0.         0.5        0.5        0.5       ]
 [0.35464863 0.49844628 0.49844628 0.35464863 0.35464863 0.35464863]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: Pick 1 Eric Fisher OT Central Michigan
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['pick', 'eric', 'fisher', 'ot', 'central', 'michigan']
cosine_similarity: 0.9225212931632996
train_input: [0.7092972666062737, 0.9225213], train_label: 1
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: So the KCChiefs pick Eric Fisher from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.         0.44832087 0.63009934
  0.44832087]
 [0.33471228 0.47042643 0.47042643 0.47042643 0.33471228 0.
  0.33471228]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: So the KCChiefs pick Eric Fisher from Central Michigan
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['so', 'the', 'pick', 'eric', 'fisher', 'from', 'central', 'michigan']
cosine_similarity: 0.9725664854049683
TF_IDF_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: the Central Michigan OL said as he goes first in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.         0.57615236
  0.57615236 0.        ]
 [0.31779954 0.44665616 0.44665616 0.31779954 0.44665616 0.
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: A OT FROM CENTRAL MICHIGAN IS THE no 1 pick, sentence2: the Central Michigan OL said as he goes first in the draft
After tokenization, sentence1: ['a', 'ot', 'from', 'central', 'michigan', 'is', 'the', 'no', 'pick'], sentence2: ['the', 'central', 'michigan', 'ol', 'said', 'as', 'he', 'goes', 'first', 'in', 'the', 'draft']
cosine_similarity: 0.9724839925765991
train_input: [0.2605556710562624, 0.972484], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Central Michigan and the MAC provide the first pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.37762778 0.37762778 0.         0.26868528
  0.37762778 0.37762778 0.         0.         0.37762778]
 [0.         0.35520009 0.         0.         0.49922133 0.35520009
  0.         0.         0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Central Michigan and the MAC provide the first pick
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['central', 'michigan', 'and', 'the', 'mac', 'provide', 'the', 'first', 'pick']
cosine_similarity: 0.9529933333396912
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Eric Fisher Central Michigan trending right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.28986934 0.28986934 0.40740124
  0.40740124 0.40740124 0.         0.        ]
 [0.         0.35464863 0.35464863 0.35464863 0.35464863 0.
  0.         0.         0.49844628 0.49844628]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Eric Fisher Central Michigan trending right now
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['eric', 'fisher', 'central', 'michigan', 'trending', 'right', 'now']
cosine_similarity: 0.9570068120956421
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: How cool is it that an OL from Central Michigan is the No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.37762778 0.37762778 0.26868528
  0.37762778 0.         0.37762778 0.37762778]
 [0.         0.40993715 0.57615236 0.         0.         0.40993715
  0.         0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: How cool is it that an OL from Central Michigan is the No
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['how', 'cool', 'is', 'it', 'that', 'an', 'ol', 'from', 'central', 'michigan', 'is', 'the', 'no']
cosine_similarity: 0.9184051156044006
train_input: [0.2202881505618297, 0.9184051], train_label: 0
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Kansas city gets a OT from central michigan In the first round
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.37762778 0.37762778 0.
  0.         0.26868528 0.37762778 0.         0.37762778 0.37762778
  0.        ]
 [0.         0.29017021 0.4078241  0.         0.         0.4078241
  0.4078241  0.29017021 0.         0.4078241  0.         0.
  0.4078241 ]]
pairwise_similarity: [[1.         0.15592893]
 [0.15592893 1.        ]]
cosine_similarity: 0.15592892548708362
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Kansas city gets a OT from central michigan In the first round
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['kansas', 'city', 'gets', 'a', 'ot', 'from', 'central', 'michigan', 'in', 'the', 'first', 'round']
cosine_similarity: 0.957697868347168
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: OT Eric Fisher from Central Michigan is the No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.28986934 0.28986934 0.40740124
  0.         0.40740124 0.40740124]
 [0.         0.4090901  0.4090901  0.4090901  0.4090901  0.
  0.57496187 0.         0.        ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.474330706497194
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: OT Eric Fisher from Central Michigan is the No
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['ot', 'eric', 'fisher', 'from', 'central', 'michigan', 'is', 'the', 'no']
cosine_similarity: 0.969992458820343
train_input: [0.474330706497194, 0.96999246], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: SO to Eric Fisher from Central Michigan for being the number 1 pick of the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.         0.28986934 0.28986934 0.28986934
  0.40740124 0.         0.40740124 0.         0.40740124]
 [0.         0.3174044  0.44610081 0.3174044  0.3174044  0.3174044
  0.         0.44610081 0.         0.44610081 0.        ]]
pairwise_similarity: [[1.         0.36802321]
 [0.36802321 1.        ]]
cosine_similarity: 0.368023208756115
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: SO to Eric Fisher from Central Michigan for being the number 1 pick of the draft
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['so', 'to', 'eric', 'fisher', 'from', 'central', 'michigan', 'for', 'being', 'the', 'number', 'pick', 'of', 'the', 'draft']
cosine_similarity: 0.9581961035728455
train_input: [0.368023208756115, 0.9581961], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Salute to Eric Fisher representing Rochester and Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.28986934 0.28986934 0.28986934 0.40740124
  0.40740124 0.         0.40740124 0.         0.        ]
 [0.         0.3174044  0.3174044  0.3174044  0.3174044  0.
  0.         0.44610081 0.         0.44610081 0.44610081]]
pairwise_similarity: [[1.         0.36802321]
 [0.36802321 1.        ]]
cosine_similarity: 0.368023208756115
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: Salute to Eric Fisher representing Rochester and Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['salute', 'to', 'eric', 'fisher', 'representing', 'rochester', 'and', 'central', 'michigan']
cosine_similarity: 0.957109808921814
train_input: [0.368023208756115, 0.9571098], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: The Kansas City Chiefs Select Eric Fisher OT From Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.         0.         0.28986934 0.28986934
  0.         0.28986934 0.40740124 0.         0.40740124 0.40740124
  0.        ]
 [0.         0.26844636 0.37729199 0.37729199 0.26844636 0.26844636
  0.37729199 0.26844636 0.         0.37729199 0.         0.
  0.37729199]]
pairwise_similarity: [[1.         0.31125747]
 [0.31125747 1.        ]]
cosine_similarity: 0.31125746752705374
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: The Kansas City Chiefs Select Eric Fisher OT From Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['the', 'kansas', 'city', 'chiefs', 'select', 'eric', 'fisher', 'ot', 'from', 'central', 'michigan']
cosine_similarity: 0.9725770354270935
train_input: [0.31125746752705374, 0.97257704], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: The number one pick comes from Central Michigan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.37762778 0.37762778 0.26868528
  0.37762778 0.         0.37762778 0.         0.37762778]
 [0.         0.35520009 0.49922133 0.         0.         0.35520009
  0.         0.49922133 0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: The number one pick comes from Central Michigan
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['the', 'number', 'one', 'pick', 'comes', 'from', 'central', 'michigan']
cosine_similarity: 0.9400324821472168
train_input: [0.1908740661302035, 0.9400325], train_label: 1
TF_IDF_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: You draft OT s from Central Michigan in the 3rd round
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.         0.26868528 0.         0.37762778 0.37762778
  0.26868528 0.37762778 0.         0.37762778 0.37762778 0.        ]
 [0.         0.44665616 0.31779954 0.44665616 0.         0.
  0.31779954 0.         0.44665616 0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Eric Fisher represents Central Michigan 1st overall in the NFL, sentence2: You draft OT s from Central Michigan in the 3rd round
After tokenization, sentence1: ['eric', 'fisher', 'represents', 'central', 'michigan', 'overall', 'in', 'the', 'nfl'], sentence2: ['you', 'draft', 'ot', 's', 'from', 'central', 'michigan', 'in', 'the', 'round']
cosine_similarity: 0.9506889581680298
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Chael Sonnen Come to the WWE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.44665616 0.44665616 0.31779954
  0.44665616 0.        ]
 [0.40993715 0.57615236 0.         0.         0.         0.40993715
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Chael Sonnen Come to the WWE
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['chael', 'sonnen', 'come', 'to', 'the', 'wwe']
cosine_similarity: 0.9137142896652222
train_input: [0.2605556710562624, 0.9137143], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Chael Sonnen got his ass whopped
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.31779954 0.44665616 0.        ]
 [0.49922133 0.35520009 0.         0.49922133 0.         0.
  0.35520009 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Chael Sonnen got his ass whopped
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['chael', 'sonnen', 'got', 'his', 'ass', 'whopped']
cosine_similarity: 0.9472644329071045
train_input: [0.22576484600261604, 0.94726443], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: I now baptize Chael Sonnen as the Charles Barkley of the UFC
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.31779954 0.         0.44665616 0.44665616
  0.44665616 0.31779954 0.44665616 0.        ]
 [0.44665616 0.44665616 0.31779954 0.44665616 0.         0.
  0.         0.31779954 0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: I now baptize Chael Sonnen as the Charles Barkley of the UFC
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['i', 'now', 'baptize', 'chael', 'sonnen', 'as', 'the', 'charles', 'barkley', 'of', 'the', 'ufc']
cosine_similarity: 0.9164907336235046
train_input: [0.20199309249791833, 0.91649073], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: I told these mfs that Jone Jones was gone fuck Chael Sonnen up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.47042643 0.         0.47042643 0.
  0.33471228 0.         0.33471228 0.47042643 0.        ]
 [0.27867523 0.39166832 0.         0.39166832 0.         0.39166832
  0.27867523 0.39166832 0.27867523 0.         0.39166832]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: I told these mfs that Jone Jones was gone fuck Chael Sonnen up
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['i', 'told', 'these', 'mfs', 'that', 'jone', 'jones', 'was', 'gone', 'fuck', 'chael', 'sonnen', 'up']
cosine_similarity: 0.9348785877227783
train_input: [0.27982806524328774, 0.9348786], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Jon Jones beat Chael Sonnen in round 1
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.49844628 0.35464863 0.35464863 0.
  0.35464863 0.49844628]
 [0.49844628 0.35464863 0.         0.35464863 0.35464863 0.49844628
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Jon Jones beat Chael Sonnen in round 1
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['jon', 'jones', 'beat', 'chael', 'sonnen', 'in', 'round']
cosine_similarity: 0.9714155197143555
train_input: [0.5031026124151313, 0.9714155], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Jon Jones finished Chael Sonnen in the first round
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.         0.49844628 0.35464863 0.35464863 0.
  0.35464863 0.49844628]
 [0.35464863 0.49844628 0.         0.35464863 0.35464863 0.49844628
  0.35464863 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Jon Jones finished Chael Sonnen in the first round
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['jon', 'jones', 'finished', 'chael', 'sonnen', 'in', 'the', 'first', 'round']
cosine_similarity: 0.9462823271751404
train_input: [0.5031026124151313, 0.9462823], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Jon Jones practically bent Chael Sonnen over and had his way with him
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.49844628 0.35464863 0.35464863 0.
  0.35464863 0.49844628 0.        ]
 [0.44610081 0.3174044  0.         0.3174044  0.3174044  0.44610081
  0.3174044  0.         0.44610081]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Jon Jones practically bent Chael Sonnen over and had his way with him
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['jon', 'jones', 'practically', 'bent', 'chael', 'sonnen', 'over', 'and', 'had', 'his', 'way', 'with', 'him']
cosine_similarity: 0.9398519396781921
train_input: [0.4502681446556265, 0.93985194], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Vince McMahon will be calling Chael Sonnen in the morning folks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.         0.         0.31779954 0.44665616 0.        ]
 [0.4078241  0.29017021 0.4078241  0.         0.         0.
  0.4078241  0.4078241  0.29017021 0.         0.4078241 ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Vince McMahon will be calling Chael Sonnen in the morning folks
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['vince', 'mcmahon', 'will', 'be', 'calling', 'chael', 'sonnen', 'in', 'the', 'morning', 'folks']
cosine_similarity: 0.9169830083847046
train_input: [0.18443191662261305, 0.916983], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Yes JonnyBones you absolutely chael sonnen chael sonnen
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.44665616 0.44665616 0.
  0.31779954 0.44665616 0.        ]
 [0.37662308 0.53594084 0.         0.         0.         0.37662308
  0.53594084 0.         0.37662308]]
pairwise_similarity: [[1.        0.3406435]
 [0.3406435 1.       ]]
cosine_similarity: 0.3406435041305656
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: Yes JonnyBones you absolutely chael sonnen chael sonnen
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['yes', 'you', 'absolutely', 'chael', 'sonnen', 'chael', 'sonnen']
cosine_similarity: 0.9387997388839722
train_input: [0.3406435041305656, 0.93879974], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: does anyone know where i can watch the jon joneschael sonnen fight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.         0.         0.44665616 0.31779954 0.44665616
  0.         0.         0.31779954 0.44665616 0.        ]
 [0.         0.4078241  0.4078241  0.         0.29017021 0.
  0.4078241  0.4078241  0.29017021 0.         0.4078241 ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Chael sonnen fucked up Jon jones toe, sentence2: does anyone know where i can watch the jon joneschael sonnen fight
After tokenization, sentence1: ['chael', 'sonnen', 'fucked', 'up', 'jon', 'jones', 'toe'], sentence2: ['does', 'anyone', 'know', 'where', 'i', 'can', 'watch', 'the', 'jon', 'sonnen', 'fight']
cosine_similarity: 0.8747532367706299
train_input: [0.18443191662261305, 0.87475324], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: 401 on barca to win the champions league
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.33471228 0.33471228 0.47042643 0.47042643
  0.47042643 0.33471228]
 [0.53309782 0.53309782 0.37930349 0.37930349 0.         0.
  0.         0.37930349]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: 401 on barca to win the champions league
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['on', 'barca', 'to', 'win', 'the', 'champions', 'league']
cosine_similarity: 0.9917062520980835
train_input: [0.3808726084759436, 0.99170625], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: Dortmund to win champions league and all German final
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.         0.         0.33471228 0.47042643
  0.47042643 0.47042643 0.33471228]
 [0.33471228 0.47042643 0.47042643 0.47042643 0.33471228 0.
  0.         0.         0.33471228]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: Dortmund to win champions league and all German final
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['dortmund', 'to', 'win', 'champions', 'league', 'and', 'all', 'german', 'final']
cosine_similarity: 0.9919995069503784
train_input: [0.3360969272762574, 0.9919995], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: First the Jews now the champions league the Germans are taking over the world
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.31779954 0.44665616 0.44665616
  0.44665616 0.         0.44665616 0.        ]
 [0.31779954 0.44665616 0.44665616 0.31779954 0.         0.
  0.         0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: First the Jews now the champions league the Germans are taking over the world
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['first', 'the', 'jews', 'now', 'the', 'champions', 'league', 'the', 'germans', 'are', 'taking', 'over', 'the', 'world']
cosine_similarity: 0.957311749458313
train_input: [0.20199309249791833, 0.95731175], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: Good Night everyone i enjoyed the Champions League football 2nyt
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.         0.         0.31779954
  0.44665616 0.         0.44665616 0.44665616 0.44665616]
 [0.4078241  0.29017021 0.4078241  0.4078241  0.4078241  0.29017021
  0.         0.4078241  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: Good Night everyone i enjoyed the Champions League football 2nyt
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['good', 'night', 'everyone', 'i', 'enjoyed', 'the', 'champions', 'league', 'football']
cosine_similarity: 0.9564138650894165
train_input: [0.18443191662261305, 0.95641387], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: It s crazy to think that Dortmund is still undefeated in the champions league
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.31779954 0.44665616 0.44665616
  0.44665616 0.         0.         0.44665616]
 [0.31779954 0.44665616 0.44665616 0.31779954 0.         0.
  0.         0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: It s crazy to think that Dortmund is still undefeated in the champions league
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['it', 's', 'crazy', 'to', 'think', 'that', 'dortmund', 'is', 'still', 'undefeated', 'in', 'the', 'champions', 'league']
cosine_similarity: 0.9730566740036011
train_input: [0.20199309249791833, 0.9730567], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: Miracle is when u see an ElClasico final of the champions league
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.31779954 0.44665616 0.
  0.44665616 0.44665616 0.44665616]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.         0.49922133
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: Miracle is when u see an ElClasico final of the champions league
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['miracle', 'is', 'when', 'u', 'see', 'an', 'elclasico', 'final', 'of', 'the', 'champions', 'league']
cosine_similarity: 0.9789332747459412
train_input: [0.22576484600261604, 0.9789333], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: So where are arsenal in the champions league
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.31779954 0.44665616 0.44665616 0.44665616
  0.44665616]
 [0.70490949 0.50154891 0.50154891 0.         0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: So where are arsenal in the champions league
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['so', 'where', 'are', 'arsenal', 'in', 'the', 'champions', 'league']
cosine_similarity: 0.9846494197845459
train_input: [0.31878402175377923, 0.9846494], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: The champions league final could be a bit awkward for Gotze
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.31779954 0.         0.         0.31779954
  0.44665616 0.44665616 0.44665616 0.44665616]
 [0.44665616 0.44665616 0.31779954 0.44665616 0.44665616 0.31779954
  0.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: The champions league final could be a bit awkward for Gotze
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['the', 'champions', 'league', 'final', 'could', 'be', 'a', 'bit', 'awkward', 'for', 'gotze']
cosine_similarity: 0.9833667874336243
train_input: [0.20199309249791833, 0.9833668], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: The germansa have invaded the champions league
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.31779954 0.44665616 0.44665616
  0.44665616 0.44665616]
 [0.40993715 0.57615236 0.57615236 0.40993715 0.         0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: The germansa have invaded the champions league
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['the', 'have', 'invaded', 'the', 'champions', 'league']
cosine_similarity: 0.9637098908424377
train_input: [0.2605556710562624, 0.9637099], train_label: 0
TF_IDF_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: but we won the champions league after that what did you last win
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.33471228 0.47042643 0.47042643 0.47042643
  0.33471228 0.        ]
 [0.37930349 0.53309782 0.37930349 0.         0.         0.
  0.37930349 0.53309782]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Odd on Real Madrid to win the champions league, sentence2: but we won the champions league after that what did you last win
After tokenization, sentence1: ['odd', 'on', 'real', 'madrid', 'to', 'win', 'the', 'champions', 'league'], sentence2: ['but', 'we', 'won', 'the', 'champions', 'league', 'after', 'that', 'what', 'did', 'you', 'last', 'win']
cosine_similarity: 0.9559069871902466
train_input: [0.3808726084759436, 0.955907], train_label: 0
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Chance Warmack can finally afford a big enough jersey
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.         0.         0.31779954 0.         0.
  0.44665616 0.44665616 0.44665616 0.31779954]
 [0.         0.44665616 0.44665616 0.31779954 0.44665616 0.44665616
  0.         0.         0.         0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Chance Warmack can finally afford a big enough jersey
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['chance', 'warmack', 'can', 'finally', 'afford', 'a', 'big', 'enough', 'jersey']
cosine_similarity: 0.9477797150611877
train_input: [0.20199309249791833, 0.9477797], train_label: 0
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Chance Warmack is about to MAUL some defensive lineman
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.         0.         0.44665616
  0.44665616 0.44665616 0.31779954]
 [0.         0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.         0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Chance Warmack is about to MAUL some defensive lineman
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['chance', 'warmack', 'is', 'about', 'to', 'maul', 'some', 'defensive', 'lineman']
cosine_similarity: 0.9581646919250488
train_input: [0.22576484600261604, 0.9581647], train_label: 0
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Chance Warmack is like having a bulldozer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.         0.31779954 0.         0.         0.44665616
  0.44665616 0.44665616 0.31779954]
 [0.         0.49922133 0.35520009 0.49922133 0.49922133 0.
  0.         0.         0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Chance Warmack is like having a bulldozer
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['chance', 'warmack', 'is', 'like', 'having', 'a', 'bulldozer']
cosine_similarity: 0.9450790286064148
train_input: [0.22576484600261604, 0.945079], train_label: 0
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Chance Warmack protect this man Locker
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.         0.44665616 0.
  0.44665616 0.44665616 0.31779954]
 [0.         0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.         0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Chance Warmack protect this man Locker
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['chance', 'warmack', 'protect', 'this', 'man', 'locker']
cosine_similarity: 0.9720094203948975
train_input: [0.22576484600261604, 0.9720094], train_label: 0
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Congrats to Chance Warmack on being drafted by the Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.33471228 0.         0.         0.47042643 0.47042643
  0.33471228 0.33471228]
 [0.         0.37930349 0.53309782 0.53309782 0.         0.
  0.37930349 0.37930349]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Congrats to Chance Warmack on being drafted by the Titans
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['congrats', 'to', 'chance', 'warmack', 'on', 'being', 'drafted', 'by', 'the', 'titans']
cosine_similarity: 0.9775193333625793
train_input: [0.3808726084759436, 0.97751933], train_label: 1
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: I think chance warmack is going to be a monster for years to come
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.         0.         0.         0.44665616
  0.44665616 0.         0.44665616 0.31779954 0.        ]
 [0.         0.29017021 0.4078241  0.4078241  0.4078241  0.
  0.         0.4078241  0.         0.29017021 0.4078241 ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: I think chance warmack is going to be a monster for years to come
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['i', 'think', 'chance', 'warmack', 'is', 'going', 'to', 'be', 'a', 'monster', 'for', 'years', 'to', 'come']
cosine_similarity: 0.942540168762207
train_input: [0.18443191662261305, 0.94254017], train_label: 0
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Mel Kiper nailed Chance Warmack to the Bills
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.         0.31779954 0.         0.         0.
  0.44665616 0.44665616 0.44665616 0.31779954]
 [0.         0.44665616 0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.         0.         0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Mel Kiper nailed Chance Warmack to the Bills
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['mel', 'kiper', 'nailed', 'chance', 'warmack', 'to', 'the', 'bills']
cosine_similarity: 0.9618451595306396
train_input: [0.20199309249791833, 0.96184516], train_label: 0
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Titans get Andy Levitre and Chance Warmack in offseason
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.         0.33471228 0.         0.         0.47042643
  0.47042643 0.33471228 0.33471228]
 [0.         0.47042643 0.33471228 0.47042643 0.47042643 0.
  0.         0.33471228 0.33471228]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Titans get Andy Levitre and Chance Warmack in offseason
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['titans', 'get', 'andy', 'and', 'chance', 'warmack', 'in', 'offseason']
cosine_similarity: 0.970172107219696
train_input: [0.3360969272762574, 0.9701721], train_label: 1
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Welcome to the Titans Alabama guard Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47042643 0.         0.33471228 0.         0.47042643 0.47042643
  0.33471228 0.33471228 0.        ]
 [0.         0.47042643 0.33471228 0.47042643 0.         0.
  0.33471228 0.33471228 0.47042643]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: Welcome to the Titans Alabama guard Chance Warmack
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['welcome', 'to', 'the', 'titans', 'alabama', 'guard', 'chance', 'warmack']
cosine_similarity: 0.9615318179130554
train_input: [0.3360969272762574, 0.9615318], train_label: 1
TF_IDF_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: With pick 110 the Titans select OG Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49844628 0.         0.35464863 0.35464863 0.         0.
  0.49844628 0.35464863 0.35464863]
 [0.         0.44610081 0.3174044  0.3174044  0.44610081 0.44610081
  0.         0.3174044  0.3174044 ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: OG Chance Warmack taken 10th by the Titans, sentence2: With pick 110 the Titans select OG Chance Warmack
After tokenization, sentence1: ['og', 'chance', 'warmack', 'taken', 'by', 'the', 'titans'], sentence2: ['with', 'pick', 'the', 'titans', 'select', 'og', 'chance', 'warmack']
cosine_similarity: 0.9819015860557556
train_input: [0.4502681446556265, 0.9819016], train_label: 1
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack 10pick in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.57615236 0.40993715]
 [0.57615236 0.40993715 0.57615236 0.         0.         0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack 10pick in the draft
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'in', 'the', 'draft']
cosine_similarity: 0.967864990234375
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack GAlabama just got snatched up by the Tennessee Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.57615236 0.
  0.         0.57615236 0.         0.40993715]
 [0.26868528 0.37762778 0.37762778 0.37762778 0.         0.37762778
  0.37762778 0.         0.37762778 0.26868528]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack GAlabama just got snatched up by the Tennessee Titans
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'just', 'got', 'snatched', 'up', 'by', 'the', 'tennessee', 'titans']
cosine_similarity: 0.9560720324516296
train_input: [0.2202881505618297, 0.95607203], train_label: 1
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack is a TENNESSEE TITAN
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.63009934 0.         0.44832087 0.44832087]
 [0.44832087 0.         0.63009934 0.44832087 0.44832087]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack is a TENNESSEE TITAN
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'is', 'a', 'tennessee', 'titan']
cosine_similarity: 0.9549809098243713
train_input: [0.6029748160380572, 0.9549809], train_label: 1
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack is gonna look huge in that 1 jersey
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.         0.57615236
  0.57615236 0.40993715]
 [0.31779954 0.44665616 0.44665616 0.44665616 0.44665616 0.
  0.         0.31779954]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack is gonna look huge in that 1 jersey
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'is', 'gonna', 'look', 'huge', 'in', 'that', 'jersey']
cosine_similarity: 0.9655290842056274
train_input: [0.2605556710562624, 0.9655291], train_label: 0
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack looks looks like a big fluffy teddy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.         0.57615236
  0.         0.57615236 0.40993715]
 [0.33310232 0.23700504 0.33310232 0.33310232 0.66620463 0.
  0.33310232 0.         0.23700504]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858148
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack looks looks like a big fluffy teddy
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'looks', 'looks', 'like', 'a', 'big', 'fluffy', 'teddy']
cosine_similarity: 0.9356462955474854
train_input: [0.19431434016858148, 0.9356463], train_label: 0
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack selected by Titans with No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.57615236 0.         0.40993715]
 [0.40993715 0.         0.57615236 0.         0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack selected by Titans with No
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'selected', 'by', 'titans', 'with', 'no']
cosine_similarity: 0.9750443696975708
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack was only a 3 star recruit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.57615236 0.40993715]
 [0.40993715 0.         0.57615236 0.57615236 0.         0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Chance Warmack was only a 3 star recruit
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'was', 'only', 'a', 'star', 'recruit']
cosine_similarity: 0.9761457443237305
train_input: [0.3360969272762575, 0.97614574], train_label: 0
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: I think chance warmack is going to be a monster for years to come
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.57615236 0.
  0.57615236 0.40993715 0.        ]
 [0.29017021 0.4078241  0.4078241  0.4078241  0.         0.4078241
  0.         0.29017021 0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: I think chance warmack is going to be a monster for years to come
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['i', 'think', 'chance', 'warmack', 'is', 'going', 'to', 'be', 'a', 'monster', 'for', 'years', 'to', 'come']
cosine_similarity: 0.9640869498252869
train_input: [0.23790309463326234, 0.96408695], train_label: 0
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Looks like Chance Warmack may need to invest in a longer jersey in the nfl
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.         0.
  0.         0.57615236 0.         0.57615236 0.40993715]
 [0.25136004 0.35327777 0.35327777 0.35327777 0.35327777 0.35327777
  0.35327777 0.         0.35327777 0.         0.25136004]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: Looks like Chance Warmack may need to invest in a longer jersey in the nfl
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['looks', 'like', 'chance', 'warmack', 'may', 'need', 'to', 'invest', 'in', 'a', 'longer', 'jersey', 'in', 'the', 'nfl']
cosine_similarity: 0.9772056937217712
train_input: [0.20608363501393823, 0.9772057], train_label: 0
TF_IDF_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: this kid Chance Warmack that just went to the titans looks just like you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.         0.57615236
  0.57615236 0.         0.40993715 0.        ]
 [0.2248583  0.632061   0.3160305  0.3160305  0.3160305  0.
  0.         0.3160305  0.2248583  0.3160305 ]]
pairwise_similarity: [[1.         0.18435554]
 [0.18435554 1.        ]]
cosine_similarity: 0.18435554192630063
word_to_vector_cosine_similarity: sentence1: SO to the newest Titan Chance Warmack, sentence2: this kid Chance Warmack that just went to the titans looks just like you
After tokenization, sentence1: ['so', 'to', 'the', 'newest', 'titan', 'chance', 'warmack'], sentence2: ['this', 'kid', 'chance', 'warmack', 'that', 'just', 'went', 'to', 'the', 'titans', 'looks', 'just', 'like', 'you']
cosine_similarity: 0.9564746618270874
train_input: [0.18435554192630063, 0.95647466], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: Chance Warmack is by far the best player in this draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.         0.26868528 0.         0.         0.37762778
  0.37762778 0.         0.37762778 0.37762778 0.37762778 0.26868528]
 [0.         0.44665616 0.31779954 0.44665616 0.44665616 0.
  0.         0.44665616 0.         0.         0.         0.31779954]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: Chance Warmack is by far the best player in this draft
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'is', 'by', 'far', 'the', 'best', 'player', 'in', 'this', 'draft']
cosine_similarity: 0.9532691240310669
train_input: [0.1707761131901165, 0.9532691], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: Chance Warmack needs a bigger jersey
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.         0.26868528 0.         0.         0.37762778
  0.37762778 0.37762778 0.37762778 0.37762778 0.26868528]
 [0.         0.49922133 0.35520009 0.49922133 0.49922133 0.
  0.         0.         0.         0.         0.35520009]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: Chance Warmack needs a bigger jersey
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'needs', 'a', 'bigger', 'jersey']
cosine_similarity: 0.9319278001785278
train_input: [0.1908740661302035, 0.9319278], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: First pick for the titans chance warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.40740124 0.28986934 0.40740124 0.40740124
  0.28986934 0.28986934]
 [0.         0.5        0.         0.5        0.         0.
  0.5        0.5       ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: First pick for the titans chance warmack
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['first', 'pick', 'for', 'the', 'titans', 'chance', 'warmack']
cosine_similarity: 0.9842495918273926
train_input: [0.5797386715376657, 0.9842496], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: I just hope the get Chance Warmack a Jersey that fits
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.         0.         0.
  0.37762778 0.37762778 0.37762778 0.37762778 0.37762778 0.26868528]
 [0.         0.31779954 0.44665616 0.44665616 0.44665616 0.44665616
  0.         0.         0.         0.         0.         0.31779954]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: I just hope the get Chance Warmack a Jersey that fits
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['i', 'just', 'hope', 'the', 'get', 'chance', 'warmack', 'a', 'jersey', 'that', 'fits']
cosine_similarity: 0.934228241443634
train_input: [0.1707761131901165, 0.93422824], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: I wanted the cowboys to pick up Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.         0.39166832 0.27867523 0.39166832
  0.39166832 0.39166832 0.         0.27867523]
 [0.         0.37930349 0.53309782 0.         0.37930349 0.
  0.         0.         0.53309782 0.37930349]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: I wanted the cowboys to pick up Chance Warmack
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['i', 'wanted', 'the', 'cowboys', 'to', 'pick', 'up', 'chance', 'warmack']
cosine_similarity: 0.9488083720207214
train_input: [0.31710746658027095, 0.9488084], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: Mel Kiper nailed Chance Warmack to the Bills
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.         0.26868528 0.         0.         0.
  0.37762778 0.37762778 0.37762778 0.37762778 0.37762778 0.26868528]
 [0.         0.44665616 0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.         0.         0.         0.         0.31779954]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: Mel Kiper nailed Chance Warmack to the Bills
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['mel', 'kiper', 'nailed', 'chance', 'warmack', 'to', 'the', 'bills']
cosine_similarity: 0.9564381241798401
train_input: [0.1707761131901165, 0.9564381], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: My baby Chance Warmack going to the TENNESSEE TITANS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.         0.28986934 0.         0.40740124 0.40740124
  0.40740124 0.28986934 0.28986934 0.28986934]
 [0.         0.49844628 0.35464863 0.49844628 0.         0.
  0.         0.35464863 0.35464863 0.35464863]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: My baby Chance Warmack going to the TENNESSEE TITANS
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['my', 'baby', 'chance', 'warmack', 'going', 'to', 'the', 'tennessee', 'titans']
cosine_similarity: 0.9624316096305847
train_input: [0.41120705506761857, 0.9624316], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: TEN takes Chance Warmack with 10th overall
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.28986934 0.28986934 0.28986934 0.40740124 0.40740124 0.
  0.40740124 0.40740124 0.28986934]
 [0.4090901  0.4090901  0.4090901  0.         0.         0.57496187
  0.         0.         0.4090901 ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: TEN takes Chance Warmack with 10th overall
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['ten', 'takes', 'chance', 'warmack', 'with', 'overall']
cosine_similarity: 0.9554311037063599
train_input: [0.4743307064971939, 0.9554311], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: That Chance Warmack highlight film is case 1 why the Bears should not pick Teo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.         0.         0.27867523 0.         0.
  0.39166832 0.27867523 0.39166832 0.39166832 0.         0.39166832
  0.27867523]
 [0.         0.39166832 0.39166832 0.27867523 0.39166832 0.39166832
  0.         0.27867523 0.         0.         0.39166832 0.
  0.27867523]]
pairwise_similarity: [[1.         0.23297965]
 [0.23297965 1.        ]]
cosine_similarity: 0.2329796548048752
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: That Chance Warmack highlight film is case 1 why the Bears should not pick Teo
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['that', 'chance', 'warmack', 'highlight', 'film', 'is', 'case', 'why', 'the', 'bears', 'should', 'not', 'pick', 'teo']
cosine_similarity: 0.9489032626152039
train_input: [0.2329796548048752, 0.94890326], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: Two Alabama players gone in the First round Chance Warmack goes to the Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.         0.27867523 0.         0.         0.39166832
  0.39166832 0.         0.         0.39166832 0.39166832 0.27867523
  0.27867523]
 [0.         0.39166832 0.27867523 0.39166832 0.39166832 0.
  0.         0.39166832 0.39166832 0.         0.         0.27867523
  0.27867523]]
pairwise_similarity: [[1.         0.23297965]
 [0.23297965 1.        ]]
cosine_similarity: 0.2329796548048752
word_to_vector_cosine_similarity: sentence1: With the 10th overall pick Tennessee Titans selects G Chance Warmack, sentence2: Two Alabama players gone in the First round Chance Warmack goes to the Titans
After tokenization, sentence1: ['with', 'the', 'overall', 'pick', 'tennessee', 'titans', 'selects', 'g', 'chance', 'warmack'], sentence2: ['two', 'alabama', 'players', 'gone', 'in', 'the', 'first', 'round', 'chance', 'warmack', 'goes', 'to', 'the', 'titans']
cosine_similarity: 0.9651528596878052
train_input: [0.2329796548048752, 0.96515286], train_label: 1
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Bro you would look just like chance Warmack with the shmedium jersey too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.         0.         0.
  0.         0.44665616 0.44665616 0.         0.44665616 0.31779954]
 [0.37762778 0.26868528 0.         0.37762778 0.37762778 0.37762778
  0.37762778 0.         0.         0.37762778 0.         0.26868528]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Bro you would look just like chance Warmack with the shmedium jersey too
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['bro', 'you', 'would', 'look', 'just', 'like', 'chance', 'warmack', 'with', 'the', 'jersey', 'too']
cosine_similarity: 0.923761248588562
train_input: [0.1707761131901165, 0.92376125], train_label: 0
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Chance Warmack look like the dude from Remember the Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.         0.47042643 0.         0.         0.47042643
  0.         0.47042643 0.33471228 0.33471228]
 [0.30287281 0.42567716 0.         0.42567716 0.42567716 0.
  0.42567716 0.         0.30287281 0.30287281]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Chance Warmack look like the dude from Remember the Titans
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'look', 'like', 'the', 'dude', 'from', 'remember', 'the', 'titans']
cosine_similarity: 0.9519740343093872
train_input: [0.30412574187549346, 0.95197403], train_label: 0
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Chance Warmack was responsible for Alabama rushing for 350 in the SEC title game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.31779954 0.         0.44665616 0.44665616
  0.         0.44665616 0.         0.         0.44665616 0.
  0.31779954]
 [0.35327777 0.35327777 0.25136004 0.35327777 0.         0.
  0.35327777 0.         0.35327777 0.35327777 0.         0.35327777
  0.25136004]]
pairwise_similarity: [[1.         0.15976421]
 [0.15976421 1.        ]]
cosine_similarity: 0.1597642092414444
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Chance Warmack was responsible for Alabama rushing for 350 in the SEC title game
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'was', 'responsible', 'for', 'alabama', 'rushing', 'for', 'in', 'the', 'sec', 'title', 'game']
cosine_similarity: 0.9587275981903076
train_input: [0.1597642092414444, 0.9587276], train_label: 0
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Couldnt be happier with Chance Warmack joining the Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.         0.47042643 0.47042643
  0.33471228 0.33471228]
 [0.37930349 0.         0.53309782 0.53309782 0.         0.
  0.37930349 0.37930349]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Couldnt be happier with Chance Warmack joining the Titans
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['couldnt', 'be', 'happier', 'with', 'chance', 'warmack', 'joining', 'the', 'titans']
cosine_similarity: 0.9623234868049622
train_input: [0.3808726084759436, 0.9623235], train_label: 1
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Dee Milliner Chance Warmack and Jonathan Cooper
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.44665616 0.         0.
  0.44665616 0.44665616 0.44665616 0.31779954]
 [0.31779954 0.44665616 0.44665616 0.         0.44665616 0.44665616
  0.         0.         0.         0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Dee Milliner Chance Warmack and Jonathan Cooper
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['dee', 'milliner', 'chance', 'warmack', 'and', 'jonathan', 'cooper']
cosine_similarity: 0.8796640634536743
train_input: [0.20199309249791833, 0.87966406], train_label: 0
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: I love chance warmack for doing the Saldivar during the natty
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.         0.44665616
  0.44665616 0.         0.44665616 0.31779954]
 [0.31779954 0.44665616 0.         0.44665616 0.44665616 0.
  0.         0.44665616 0.         0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: I love chance warmack for doing the Saldivar during the natty
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['i', 'love', 'chance', 'warmack', 'for', 'doing', 'the', 'saldivar', 'during', 'the', 'natty']
cosine_similarity: 0.958854615688324
train_input: [0.20199309249791833, 0.9588546], train_label: 0
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: The Tennessee Titans select from Alabama Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.47042643 0.47042643 0.47042643 0.
  0.         0.33471228 0.33471228]
 [0.47042643 0.33471228 0.         0.         0.         0.47042643
  0.47042643 0.33471228 0.33471228]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: The Tennessee Titans select from Alabama Chance Warmack
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['the', 'tennessee', 'titans', 'select', 'from', 'alabama', 'chance', 'warmack']
cosine_similarity: 0.9743630886077881
train_input: [0.3360969272762574, 0.9743631], train_label: 1
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Titans have selected OG Chance Warmack with the 10 pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35464863 0.49844628 0.         0.35464863 0.49844628
  0.         0.35464863 0.35464863]
 [0.44610081 0.3174044  0.         0.44610081 0.3174044  0.
  0.44610081 0.3174044  0.3174044 ]]
pairwise_similarity: [[1.         0.45026814]
 [0.45026814 1.        ]]
cosine_similarity: 0.4502681446556265
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Titans have selected OG Chance Warmack with the 10 pick
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['titans', 'have', 'selected', 'og', 'chance', 'warmack', 'with', 'the', 'pick']
cosine_similarity: 0.9763008952140808
train_input: [0.4502681446556265, 0.9763009], train_label: 1
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Welcome to the titansgang chance warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.44665616 0.
  0.31779954 0.        ]
 [0.40993715 0.         0.         0.         0.         0.57615236
  0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: Welcome to the titansgang chance warmack
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['welcome', 'to', 'the', 'chance', 'warmack']
cosine_similarity: 0.9646245241165161
TF_IDF_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: With the 10th pick in the 2013 NFLDraft the Titans select Chance Warmack OG Alabama
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.35464863 0.49844628 0.
  0.         0.35464863 0.49844628 0.         0.35464863 0.35464863]
 [0.35300279 0.35300279 0.35300279 0.25116439 0.         0.35300279
  0.35300279 0.25116439 0.         0.35300279 0.25116439 0.25116439]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.35630042933313805
word_to_vector_cosine_similarity: sentence1: Great pick for the Titans RollTide Chance Warmack, sentence2: With the 10th pick in the 2013 NFLDraft the Titans select Chance Warmack OG Alabama
After tokenization, sentence1: ['great', 'pick', 'for', 'the', 'titans', 'rolltide', 'chance', 'warmack'], sentence2: ['with', 'the', 'pick', 'in', 'the', 'nfldraft', 'the', 'titans', 'select', 'chance', 'warmack', 'og', 'alabama']
cosine_similarity: 0.9814708232879639
train_input: [0.35630042933313805, 0.9814708], train_label: 1
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance Warmack had a good interview on Midday180
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.         0.49922133
  0.49922133 0.35520009]
 [0.35520009 0.49922133 0.         0.49922133 0.49922133 0.
  0.         0.35520009]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance Warmack had a good interview on Midday180
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'had', 'a', 'good', 'interview', 'on']
cosine_similarity: 0.9420875310897827
train_input: [0.2523342014336961, 0.94208753], train_label: 0
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance Warmack is a damn bulldozer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.35520009]
 [0.57615236 0.40993715 0.57615236 0.         0.         0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance Warmack is a damn bulldozer
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'is', 'a', 'damn', 'bulldozer']
cosine_similarity: 0.9202808737754822
train_input: [0.29121941856368966, 0.9202809], train_label: 0
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance Warmack looks like a bad mother fucker
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.         0.49922133 0.         0.
  0.         0.49922133 0.49922133 0.35520009]
 [0.4078241  0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.4078241  0.         0.         0.29017021]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance Warmack looks like a bad mother fucker
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'looks', 'like', 'a', 'bad', 'mother', 'fucker']
cosine_similarity: 0.9036517143249512
train_input: [0.20613696606828605, 0.9036517], train_label: 0
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance Warmack reminds me of a black SumnerPaschke
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133 0.35520009]
 [0.49922133 0.35520009 0.         0.         0.49922133 0.49922133
  0.         0.35520009]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance Warmack reminds me of a black SumnerPaschke
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'reminds', 'me', 'of', 'a', 'black']
cosine_similarity: 0.8975352048873901
train_input: [0.2523342014336961, 0.8975352], train_label: 0
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance warmack is a fucking monster
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.49922133 0.         0.49922133 0.49922133
  0.35520009]
 [0.40993715 0.57615236 0.         0.57615236 0.         0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Chance warmack is a fucking monster
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['chance', 'warmack', 'is', 'a', 'fucking', 'monster']
cosine_similarity: 0.9136013388633728
train_input: [0.29121941856368966, 0.91360134], train_label: 0
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Damn wanted Dallas to get Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.49922133 0.49922133 0.49922133
  0.         0.35520009]
 [0.35520009 0.49922133 0.49922133 0.         0.         0.
  0.49922133 0.35520009]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Damn wanted Dallas to get Chance Warmack
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['damn', 'wanted', 'dallas', 'to', 'get', 'chance', 'warmack']
cosine_similarity: 0.9555308818817139
train_input: [0.2523342014336961, 0.9555309], train_label: 0
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Ill tolerate the Chance Warmack pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.         0.37930349 0.53309782 0.
  0.37930349]
 [0.37930349 0.         0.53309782 0.37930349 0.         0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Ill tolerate the Chance Warmack pick
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['ill', 'tolerate', 'the', 'chance', 'warmack', 'pick']
cosine_similarity: 0.9532186388969421
train_input: [0.43161341897075145, 0.95321864], train_label: 0
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Looks like Chance Warmack may need to invest in a longer jersey in the nfl
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.         0.
  0.         0.         0.         0.49922133 0.49922133 0.35520009]
 [0.25136004 0.         0.35327777 0.35327777 0.35327777 0.35327777
  0.35327777 0.35327777 0.35327777 0.         0.         0.25136004]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: Looks like Chance Warmack may need to invest in a longer jersey in the nfl
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['looks', 'like', 'chance', 'warmack', 'may', 'need', 'to', 'invest', 'in', 'a', 'longer', 'jersey', 'in', 'the', 'nfl']
cosine_similarity: 0.9341511130332947
train_input: [0.17856621555757476, 0.9341511], train_label: 0
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: My baby Chance Warmack going to the TENNESSEE TITANS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.         0.53309782 0.53309782 0.
  0.37930349 0.37930349]
 [0.47042643 0.33471228 0.47042643 0.         0.         0.47042643
  0.33471228 0.33471228]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: My baby Chance Warmack going to the TENNESSEE TITANS
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['my', 'baby', 'chance', 'warmack', 'going', 'to', 'the', 'tennessee', 'titans']
cosine_similarity: 0.9697358012199402
train_input: [0.38087260847594373, 0.9697358], train_label: 1
TF_IDF_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: That s Chance Warmack not LeeAnn Womack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.49922133 0.49922133 0.35520009
  0.        ]
 [0.40993715 0.         0.57615236 0.         0.         0.40993715
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Happy with Titans pick Chance Warmack, sentence2: That s Chance Warmack not LeeAnn Womack
After tokenization, sentence1: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack'], sentence2: ['that', 's', 'chance', 'warmack', 'not', 'leeann', 'womack']
cosine_similarity: 0.938770055770874
train_input: [0.29121941856368966, 0.93877006], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack comes to the Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891]
 [0.         0.40993715 0.57615236 0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack comes to the Titans
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['chance', 'warmack', 'comes', 'to', 'the', 'titans']
cosine_similarity: 0.9726442694664001
train_input: [0.4112070550676187, 0.97264427], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is a beast no problem with him at the titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.70490949 0.50154891 0.         0.         0.50154891]
 [0.49922133 0.         0.35520009 0.49922133 0.49922133 0.35520009]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is a beast no problem with him at the titans
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['chance', 'warmack', 'is', 'a', 'beast', 'no', 'problem', 'with', 'him', 'at', 'the', 'titans']
cosine_similarity: 0.9647884964942932
train_input: [0.3563004293331381, 0.9647885], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is a damn bulldozer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.         0.50154891 0.         0.50154891]
 [0.         0.57615236 0.40993715 0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is a damn bulldozer
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['chance', 'warmack', 'is', 'a', 'damn', 'bulldozer']
cosine_similarity: 0.9469571113586426
train_input: [0.4112070550676187, 0.9469571], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is a good pick for the Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.         0.50154891]
 [0.         0.35520009 0.49922133 0.49922133 0.49922133 0.35520009]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is a good pick for the Titans
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['chance', 'warmack', 'is', 'a', 'good', 'pick', 'for', 'the', 'titans']
cosine_similarity: 0.9792641997337341
train_input: [0.3563004293331381, 0.9792642], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is an absolute animal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.70490949 0.50154891 0.50154891]
 [0.57615236 0.57615236 0.         0.40993715 0.40993715]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is an absolute animal
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['chance', 'warmack', 'is', 'an', 'absolute', 'animal']
cosine_similarity: 0.9357994198799133
train_input: [0.4112070550676187, 0.9357994], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is such a fucking awesome pickup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.70490949 0.50154891 0.         0.         0.50154891]
 [0.49922133 0.         0.35520009 0.49922133 0.49922133 0.35520009]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Chance Warmack is such a fucking awesome pickup
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['chance', 'warmack', 'is', 'such', 'a', 'fucking', 'awesome', 'pickup']
cosine_similarity: 0.9491318464279175
train_input: [0.3563004293331381, 0.94913185], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Congrats to Chance Warmack for being drafted by The Tennessee Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.         0.
  0.50154891]
 [0.         0.31779954 0.44665616 0.44665616 0.44665616 0.44665616
  0.31779954]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: Congrats to Chance Warmack for being drafted by The Tennessee Titans
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['congrats', 'to', 'chance', 'warmack', 'for', 'being', 'drafted', 'by', 'the', 'tennessee', 'titans']
cosine_similarity: 0.944603681564331
train_input: [0.31878402175377923, 0.9446037], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: I want a hug off Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891]
 [0.         0.40993715 0.57615236 0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: I want a hug off Chance Warmack
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['i', 'want', 'a', 'hug', 'off', 'chance', 'warmack']
cosine_similarity: 0.969738245010376
train_input: [0.4112070550676187, 0.96973825], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: That Chance Warmack highlight film is case 1 why the Bears should not pick Teo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.70490949 0.         0.50154891 0.         0.
  0.         0.         0.50154891]
 [0.37762778 0.         0.37762778 0.26868528 0.37762778 0.37762778
  0.37762778 0.37762778 0.26868528]]
pairwise_similarity: [[1.         0.26951761]
 [0.26951761 1.        ]]
cosine_similarity: 0.26951761324603224
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: That Chance Warmack highlight film is case 1 why the Bears should not pick Teo
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['that', 'chance', 'warmack', 'highlight', 'film', 'is', 'case', 'why', 'the', 'bears', 'should', 'not', 'pick', 'teo']
cosine_similarity: 0.9585065245628357
train_input: [0.26951761324603224, 0.9585065], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: That s Chance Warmack not LeeAnn Womack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.        ]
 [0.         0.40993715 0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Chance Warmack please show the belly, sentence2: That s Chance Warmack not LeeAnn Womack
After tokenization, sentence1: ['chance', 'warmack', 'please', 'show', 'the', 'belly'], sentence2: ['that', 's', 'chance', 'warmack', 'not', 'leeann', 'womack']
cosine_similarity: 0.9360389113426208
train_input: [0.4112070550676187, 0.9360389], train_label: 0
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Chance Warmack I will personally come and pick you up from the airport
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.         0.27867523 0.         0.39166832 0.39166832
  0.         0.27867523 0.39166832 0.39166832 0.27867523]
 [0.         0.47042643 0.33471228 0.47042643 0.         0.
  0.47042643 0.33471228 0.         0.         0.33471228]]
pairwise_similarity: [[1.         0.27982807]
 [0.27982807 1.        ]]
cosine_similarity: 0.27982806524328774
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Chance Warmack I will personally come and pick you up from the airport
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['chance', 'warmack', 'i', 'will', 'personally', 'come', 'and', 'pick', 'you', 'up', 'from', 'the', 'airport']
cosine_similarity: 0.9470700621604919
train_input: [0.27982806524328774, 0.94707006], train_label: 0
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Chance Warmack doesnt realize jerseys are suppose to cover the complete torso
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.         0.         0.37762778
  0.         0.37762778 0.37762778 0.         0.37762778 0.
  0.37762778 0.         0.26868528]
 [0.         0.25136004 0.35327777 0.35327777 0.35327777 0.
  0.35327777 0.         0.         0.35327777 0.         0.35327777
  0.         0.35327777 0.25136004]]
pairwise_similarity: [[1.         0.13507348]
 [0.13507348 1.        ]]
cosine_similarity: 0.1350734836708713
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Chance Warmack doesnt realize jerseys are suppose to cover the complete torso
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['chance', 'warmack', 'doesnt', 'realize', 'jerseys', 'are', 'suppose', 'to', 'cover', 'the', 'complete', 'torso']
cosine_similarity: 0.9380736351013184
train_input: [0.1350734836708713, 0.93807364], train_label: 0
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Chance Warmack is my favorite player in the draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.27867523 0.         0.39166832 0.39166832
  0.         0.39166832 0.39166832 0.27867523]
 [0.         0.37930349 0.37930349 0.53309782 0.         0.
  0.53309782 0.         0.         0.37930349]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Chance Warmack is my favorite player in the draft
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['chance', 'warmack', 'is', 'my', 'favorite', 'player', 'in', 'the', 'draft']
cosine_similarity: 0.9840742945671082
train_input: [0.31710746658027095, 0.9840743], train_label: 0
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Chance Warmack ran a 549 40
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.         0.         0.26868528 0.37762778 0.37762778
  0.37762778 0.         0.37762778 0.37762778 0.26868528]
 [0.         0.49922133 0.49922133 0.35520009 0.         0.
  0.         0.49922133 0.         0.         0.35520009]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Chance Warmack ran a 549 40
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['chance', 'warmack', 'ran', 'a']
cosine_similarity: 0.8944761753082275
train_input: [0.1908740661302035, 0.8944762], train_label: 0
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: First pick for the titans chance warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.40740124 0.40740124 0.28986934 0.40740124
  0.28986934 0.28986934]
 [0.         0.5        0.         0.         0.5        0.
  0.5        0.5       ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: First pick for the titans chance warmack
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['first', 'pick', 'for', 'the', 'titans', 'chance', 'warmack']
cosine_similarity: 0.9873512983322144
train_input: [0.5797386715376657, 0.9873513], train_label: 1
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Lets hope the Titans give Chance Warmack a jersey that fits
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39166832 0.27867523 0.39166832 0.         0.         0.
  0.         0.39166832 0.39166832 0.39166832 0.27867523 0.27867523]
 [0.         0.30287281 0.         0.42567716 0.42567716 0.42567716
  0.42567716 0.         0.         0.         0.30287281 0.30287281]]
pairwise_similarity: [[1.         0.25320945]
 [0.25320945 1.        ]]
cosine_similarity: 0.2532094495161745
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Lets hope the Titans give Chance Warmack a jersey that fits
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['lets', 'hope', 'the', 'titans', 'give', 'chance', 'warmack', 'a', 'jersey', 'that', 'fits']
cosine_similarity: 0.9761418104171753
train_input: [0.2532094495161745, 0.9761418], train_label: 0
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Love the Chance Warmack pick for the Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.40740124 0.         0.40740124 0.28986934
  0.40740124 0.28986934 0.28986934]
 [0.         0.4090901  0.         0.57496187 0.         0.4090901
  0.         0.4090901  0.4090901 ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Love the Chance Warmack pick for the Titans
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['love', 'the', 'chance', 'warmack', 'pick', 'for', 'the', 'titans']
cosine_similarity: 0.9816252589225769
train_input: [0.4743307064971939, 0.98162526], train_label: 1
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Titans 10 choice guard Chance Warmack will go to war twice a year with JJ Watt
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.39166832 0.27867523 0.         0.39166832 0.
  0.         0.39166832 0.39166832 0.39166832 0.27867523 0.
  0.         0.27867523 0.         0.        ]
 [0.32412345 0.         0.2306165  0.32412345 0.         0.32412345
  0.32412345 0.         0.         0.         0.2306165  0.32412345
  0.32412345 0.2306165  0.32412345 0.32412345]]
pairwise_similarity: [[1.         0.19280132]
 [0.19280132 1.        ]]
cosine_similarity: 0.1928013231703809
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: Titans 10 choice guard Chance Warmack will go to war twice a year with JJ Watt
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['titans', 'choice', 'guard', 'chance', 'warmack', 'will', 'go', 'to', 'war', 'twice', 'a', 'year', 'with', 'jj', 'watt']
cosine_similarity: 0.9787316918373108
train_input: [0.1928013231703809, 0.9787317], train_label: 1
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: With the 10th pick in the NFLDraft the Titans select Alabama guard Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31701073 0.         0.31701073 0.44554752 0.         0.44554752
  0.         0.31701073 0.31701073 0.31701073 0.31701073]
 [0.2895694  0.40697968 0.2895694  0.         0.40697968 0.
  0.40697968 0.2895694  0.2895694  0.2895694  0.2895694 ]]
pairwise_similarity: [[1.         0.55077963]
 [0.55077963 1.        ]]
cosine_similarity: 0.5507796339456278
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: With the 10th pick in the NFLDraft the Titans select Alabama guard Chance Warmack
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['with', 'the', 'pick', 'in', 'the', 'nfldraft', 'the', 'titans', 'select', 'alabama', 'guard', 'chance', 'warmack']
cosine_similarity: 0.9917353391647339
train_input: [0.5507796339456278, 0.99173534], train_label: 1
TF_IDF_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: select DJ Fluker after Titans select G Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40740124 0.28986934 0.         0.40740124 0.         0.40740124
  0.40740124 0.28986934 0.28986934 0.28986934]
 [0.         0.30218978 0.42471719 0.         0.42471719 0.
  0.         0.60437955 0.30218978 0.30218978]]
pairwise_similarity: [[1.         0.43797775]
 [0.43797775 1.        ]]
cosine_similarity: 0.43797774789091437
word_to_vector_cosine_similarity: sentence1: with the 10th pick in the nfl draft the titans select chance warmack G, sentence2: select DJ Fluker after Titans select G Chance Warmack
After tokenization, sentence1: ['with', 'the', 'pick', 'in', 'the', 'nfl', 'draft', 'the', 'titans', 'select', 'chance', 'warmack', 'g'], sentence2: ['select', 'dj', 'fluker', 'after', 'titans', 'select', 'g', 'chance', 'warmack']
cosine_similarity: 0.9537193775177002
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack had a good interview on Midday180
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.36499647 0.36499647 0.         0.         0.36499647 0.
  0.         0.36499647 0.36499647 0.36499647 0.36499647 0.25969799]
 [0.         0.         0.47107781 0.47107781 0.         0.47107781
  0.47107781 0.         0.         0.         0.         0.33517574]]
pairwise_similarity: [[1.         0.08704447]
 [0.08704447 1.        ]]
cosine_similarity: 0.08704446792504215
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack had a good interview on Midday180
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['chance', 'warmack', 'had', 'a', 'good', 'interview', 'on']
cosine_similarity: 0.9077404737472534
train_input: [0.08704446792504215, 0.9077405], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack is on his way to town
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.36499647 0.36499647 0.         0.36499647 0.36499647 0.36499647
  0.36499647 0.36499647 0.         0.25969799 0.        ]
 [0.         0.         0.53404633 0.         0.         0.
  0.         0.         0.53404633 0.37997836 0.53404633]]
pairwise_similarity: [[1.         0.09867962]
 [0.09867962 1.        ]]
cosine_similarity: 0.09867961797986954
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack is on his way to town
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['chance', 'warmack', 'is', 'on', 'his', 'way', 'to', 'town']
cosine_similarity: 0.9487188458442688
train_input: [0.09867961797986954, 0.94871885], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack is the 10th pick in the first round to the Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.28986934 0.40740124 0.         0.40740124 0.28986934 0.
  0.40740124 0.40740124 0.28986934 0.28986934]
 [0.35464863 0.         0.49844628 0.         0.35464863 0.49844628
  0.         0.         0.35464863 0.35464863]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack is the 10th pick in the first round to the Titans
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['chance', 'warmack', 'is', 'the', 'pick', 'in', 'the', 'first', 'round', 'to', 'the', 'titans']
cosine_similarity: 0.9637619256973267
train_input: [0.41120705506761857, 0.9637619], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack looks like a bad mother fucker
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.36499647 0.36499647 0.         0.         0.         0.36499647
  0.         0.         0.         0.36499647 0.36499647 0.36499647
  0.36499647 0.25969799]
 [0.         0.         0.39204401 0.39204401 0.39204401 0.
  0.39204401 0.39204401 0.39204401 0.         0.         0.
  0.         0.27894255]]
pairwise_similarity: [[1.         0.07244082]
 [0.07244082 1.        ]]
cosine_similarity: 0.07244081925041984
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack looks like a bad mother fucker
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['chance', 'warmack', 'looks', 'like', 'a', 'bad', 'mother', 'fucker']
cosine_similarity: 0.8864284753799438
train_input: [0.07244081925041984, 0.8864285], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack was responsible for Alabama rushing for 350 in the SEC title game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.         0.26868528 0.         0.         0.37762778
  0.37762778 0.         0.         0.         0.37762778 0.37762778
  0.37762778 0.         0.26868528]
 [0.         0.35327777 0.25136004 0.35327777 0.35327777 0.
  0.         0.35327777 0.35327777 0.35327777 0.         0.
  0.         0.35327777 0.25136004]]
pairwise_similarity: [[1.         0.13507348]
 [0.13507348 1.        ]]
cosine_similarity: 0.1350734836708713
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack was responsible for Alabama rushing for 350 in the SEC title game
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['chance', 'warmack', 'was', 'responsible', 'for', 'alabama', 'rushing', 'for', 'in', 'the', 'sec', 'title', 'game']
cosine_similarity: 0.962517261505127
train_input: [0.1350734836708713, 0.96251726], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack was sporting a sweet pocket square
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.36499647 0.36499647 0.         0.36499647 0.36499647 0.
  0.36499647 0.         0.         0.         0.36499647 0.36499647
  0.25969799]
 [0.         0.         0.4261596  0.         0.         0.4261596
  0.         0.4261596  0.4261596  0.4261596  0.         0.
  0.30321606]]
pairwise_similarity: [[1.        0.0787446]
 [0.0787446 1.       ]]
cosine_similarity: 0.07874460345594507
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack was sporting a sweet pocket square
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['chance', 'warmack', 'was', 'sporting', 'a', 'sweet', 'pocket', 'square']
cosine_similarity: 0.9115474820137024
train_input: [0.07874460345594507, 0.9115475], train_label: 0
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack will make an impact on the Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.37762778 0.         0.37762778 0.         0.
  0.37762778 0.37762778 0.37762778 0.26868528 0.26868528]
 [0.         0.         0.49922133 0.         0.49922133 0.49922133
  0.         0.         0.         0.35520009 0.35520009]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Chance Warmack will make an impact on the Titans
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['chance', 'warmack', 'will', 'make', 'an', 'impact', 'on', 'the', 'titans']
cosine_similarity: 0.9454880356788635
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Titans lucked out with Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.37762778 0.         0.37762778 0.         0.37762778
  0.37762778 0.37762778 0.26868528 0.26868528]
 [0.         0.         0.57615236 0.         0.57615236 0.
  0.         0.         0.40993715 0.40993715]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Titans lucked out with Chance Warmack
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['titans', 'lucked', 'out', 'with', 'chance', 'warmack']
cosine_similarity: 0.9524014592170715
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Titans take a chance on Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.37762778 0.         0.37762778 0.37762778 0.37762778
  0.37762778 0.26868528 0.26868528]
 [0.         0.         0.89331232 0.         0.         0.
  0.         0.31779954 0.31779954]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Titans take a chance on Chance Warmack
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['titans', 'take', 'a', 'chance', 'on', 'chance', 'warmack']
cosine_similarity: 0.9010803699493408
train_input: [0.1707761131901165, 0.90108037], train_label: 1
TF_IDF_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Watching the NFL Draft and the highlights of the Gaurd Chance Warmack from Alabama
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37762778 0.26868528 0.         0.         0.         0.37762778
  0.         0.         0.37762778 0.37762778 0.37762778 0.37762778
  0.26868528 0.        ]
 [0.         0.26868528 0.37762778 0.37762778 0.37762778 0.
  0.37762778 0.37762778 0.         0.         0.         0.
  0.26868528 0.37762778]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.14438355527738672
word_to_vector_cosine_similarity: sentence1: With the 10th pick the Tennessee Titans selectChance Warmack guard from Alabama, sentence2: Watching the NFL Draft and the highlights of the Gaurd Chance Warmack from Alabama
After tokenization, sentence1: ['with', 'the', 'pick', 'the', 'tennessee', 'titans', 'warmack', 'guard', 'from', 'alabama'], sentence2: ['watching', 'the', 'nfl', 'draft', 'and', 'the', 'highlights', 'of', 'the', 'gaurd', 'chance', 'warmack', 'from', 'alabama']
cosine_similarity: 0.981635570526123
train_input: [0.14438355527738672, 0.9816356], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: 10th pick goes to Alabama OL Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53309782 0.         0.37930349 0.53309782 0.
  0.         0.37930349 0.37930349]
 [0.42567716 0.         0.42567716 0.30287281 0.         0.42567716
  0.42567716 0.30287281 0.30287281]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: 10th pick goes to Alabama OL Chance Warmack
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['pick', 'goes', 'to', 'alabama', 'ol', 'chance', 'warmack']
cosine_similarity: 0.9747180342674255
train_input: [0.34464214103805474, 0.97471803], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Chance Warmack drafted 1st round 10th overall by the Tennessee Titans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.35520009 0.49922133 0.
  0.         0.49922133 0.         0.         0.         0.35520009]
 [0.35327777 0.         0.35327777 0.25136004 0.         0.35327777
  0.35327777 0.         0.35327777 0.35327777 0.35327777 0.25136004]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Chance Warmack drafted 1st round 10th overall by the Tennessee Titans
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['chance', 'warmack', 'drafted', 'round', 'overall', 'by', 'the', 'tennessee', 'titans']
cosine_similarity: 0.9319254159927368
train_input: [0.17856621555757476, 0.9319254], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Chance Warmack s affinity for belly shirts is interesting
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.         0.35520009 0.49922133 0.
  0.49922133 0.         0.35520009]
 [0.         0.44665616 0.44665616 0.31779954 0.         0.44665616
  0.         0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Chance Warmack s affinity for belly shirts is interesting
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['chance', 'warmack', 's', 'affinity', 'for', 'belly', 'shirts', 'is', 'interesting']
cosine_similarity: 0.9631083607673645
train_input: [0.22576484600261604, 0.96310836], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Chance Warmack to the Titans ROLLTIDEROLL
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.49922133 0.         0.
  0.35520009]
 [0.         0.40993715 0.         0.         0.57615236 0.57615236
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Chance Warmack to the Titans ROLLTIDEROLL
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['chance', 'warmack', 'to', 'the', 'titans', 'rolltideroll']
cosine_similarity: 0.928455650806427
train_input: [0.29121941856368966, 0.92845565], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Congrats to the newest Tennessee Titan 10 pick Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57496187 0.4090901  0.4090901  0.         0.4090901
  0.         0.         0.4090901 ]
 [0.40740124 0.         0.28986934 0.28986934 0.40740124 0.28986934
  0.40740124 0.40740124 0.28986934]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Congrats to the newest Tennessee Titan 10 pick Chance Warmack
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['congrats', 'to', 'the', 'newest', 'tennessee', 'titan', 'pick', 'chance', 'warmack']
cosine_similarity: 0.9767150282859802
train_input: [0.4743307064971939, 0.976715], train_label: 1
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Happy with Titans pick Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.37930349 0.53309782 0.         0.37930349 0.
  0.37930349]
 [0.         0.37930349 0.         0.53309782 0.37930349 0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Happy with Titans pick Chance Warmack
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['happy', 'with', 'titans', 'pick', 'chance', 'warmack']
cosine_similarity: 0.9728891849517822
train_input: [0.43161341897075145, 0.9728892], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Imma need Chance Warmack to put his stomach away
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.35520009 0.49922133 0.         0.
  0.49922133 0.         0.35520009]
 [0.         0.44665616 0.31779954 0.         0.44665616 0.44665616
  0.         0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Imma need Chance Warmack to put his stomach away
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['imma', 'need', 'chance', 'warmack', 'to', 'put', 'his', 'stomach', 'away']
cosine_similarity: 0.9417768120765686
train_input: [0.22576484600261604, 0.9417768], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Is that Chance Warmack or Sean Kingston
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.         0.49922133 0.
  0.35520009]
 [0.         0.40993715 0.         0.57615236 0.         0.57615236
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Is that Chance Warmack or Sean Kingston
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['is', 'that', 'chance', 'warmack', 'or', 'sean', 'kingston']
cosine_similarity: 0.9619557857513428
train_input: [0.29121941856368966, 0.9619558], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Watching the NFL Draft and the highlights of the Gaurd Chance Warmack from Alabama
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.35520009 0.49922133 0.         0.
  0.         0.         0.49922133 0.35520009 0.        ]
 [0.         0.37762778 0.26868528 0.         0.37762778 0.37762778
  0.37762778 0.37762778 0.         0.26868528 0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: Watching the NFL Draft and the highlights of the Gaurd Chance Warmack from Alabama
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['watching', 'the', 'nfl', 'draft', 'and', 'the', 'highlights', 'of', 'the', 'gaurd', 'chance', 'warmack', 'from', 'alabama']
cosine_similarity: 0.9629490375518799
train_input: [0.1908740661302035, 0.96294904], train_label: 0
TF_IDF_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: that s no shit the Titans just got a mauler up front in Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.         0.         0.
  0.49922133 0.         0.         0.35520009]
 [0.         0.29017021 0.         0.4078241  0.4078241  0.4078241
  0.         0.4078241  0.4078241  0.29017021]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Congrats to Chance Warmack on the top 15 pick, sentence2: that s no shit the Titans just got a mauler up front in Chance Warmack
After tokenization, sentence1: ['congrats', 'to', 'chance', 'warmack', 'on', 'the', 'top', 'pick'], sentence2: ['that', 's', 'no', 'shit', 'the', 'titans', 'just', 'got', 'a', 'mauler', 'up', 'front', 'in', 'chance', 'warmack']
cosine_similarity: 0.9599167704582214
train_input: [0.20613696606828605, 0.9599168], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Chance Warmack is my favorite lineman of all time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.         0.         0.57615236
  0.40993715]
 [0.         0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Chance Warmack is my favorite lineman of all time
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['chance', 'warmack', 'is', 'my', 'favorite', 'lineman', 'of', 'all', 'time']
cosine_similarity: 0.952860951423645
train_input: [0.29121941856368966, 0.95286095], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Chance Warmack protect this man Locker
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.         0.         0.57615236
  0.40993715]
 [0.         0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Chance Warmack protect this man Locker
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['chance', 'warmack', 'protect', 'this', 'man', 'locker']
cosine_similarity: 0.9642969965934753
train_input: [0.29121941856368966, 0.964297], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: I hope Chance Warmack covers that belly in the NFL
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.         0.40993715 0.         0.         0.
  0.57615236 0.40993715]
 [0.         0.44665616 0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.31779954]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: I hope Chance Warmack covers that belly in the NFL
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['i', 'hope', 'chance', 'warmack', 'covers', 'that', 'belly', 'in', 'the', 'nfl']
cosine_similarity: 0.9619190096855164
train_input: [0.2605556710562624, 0.961919], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Is Alabama s Chance Warmack the nation s best college football player
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.         0.         0.40993715 0.         0.
  0.         0.         0.57615236 0.40993715]
 [0.         0.37762778 0.37762778 0.26868528 0.37762778 0.37762778
  0.37762778 0.37762778 0.         0.26868528]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Is Alabama s Chance Warmack the nation s best college football player
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['is', 'alabama', 's', 'chance', 'warmack', 'the', 'nation', 's', 'best', 'college', 'football', 'player']
cosine_similarity: 0.9454291462898254
train_input: [0.2202881505618297, 0.94542915], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: OG Chance Warmack to the Titans rounds out the top 10 of the
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.         0.         0.5        0.5       ]
 [0.35464863 0.35464863 0.49844628 0.49844628 0.35464863 0.35464863]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: OG Chance Warmack to the Titans rounds out the top 10 of the
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['og', 'chance', 'warmack', 'to', 'the', 'titans', 'rounds', 'out', 'the', 'top', 'of', 'the']
cosine_similarity: 0.9751184582710266
train_input: [0.7092972666062737, 0.97511846], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Pop the champagne my Titans landed Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.         0.44832087 0.         0.         0.44832087
  0.44832087]
 [0.         0.47042643 0.33471228 0.47042643 0.47042643 0.33471228
  0.33471228]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Pop the champagne my Titans landed Chance Warmack
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['pop', 'the', 'champagne', 'my', 'titans', 'landed', 'chance', 'warmack']
cosine_similarity: 0.9596559405326843
train_input: [0.4501755023269898, 0.95965594], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Titans pick 10 OG Chance Warmack BAMA RTR
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.         0.5        0.         0.         0.
  0.5        0.5       ]
 [0.28986934 0.40740124 0.28986934 0.40740124 0.40740124 0.40740124
  0.28986934 0.28986934]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: Titans pick 10 OG Chance Warmack BAMA RTR
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['titans', 'pick', 'og', 'chance', 'warmack', 'bama', 'rtr']
cosine_similarity: 0.8684351444244385
train_input: [0.5797386715376657, 0.86843514], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: With the 10th overall pick of the 2013 nfl draft the Tennessee Titans select Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.         0.         0.44832087 0.         0.
  0.         0.         0.         0.         0.44832087 0.44832087]
 [0.         0.32412345 0.32412345 0.2306165  0.32412345 0.32412345
  0.32412345 0.32412345 0.32412345 0.32412345 0.2306165  0.2306165 ]]
pairwise_similarity: [[1.         0.31017058]
 [0.31017058 1.        ]]
cosine_similarity: 0.31017057717950425
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: With the 10th overall pick of the 2013 nfl draft the Tennessee Titans select Chance Warmack
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['with', 'the', 'overall', 'pick', 'of', 'the', 'nfl', 'draft', 'the', 'tennessee', 'titans', 'select', 'chance', 'warmack']
cosine_similarity: 0.9638282060623169
train_input: [0.31017057717950425, 0.9638282], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: With the 10th overall pick of the draft titans select G Chance Warmack out of Alabama
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.         0.         0.44832087 0.         0.
  0.         0.         0.44832087 0.44832087]
 [0.         0.36469323 0.36469323 0.25948224 0.36469323 0.36469323
  0.36469323 0.36469323 0.25948224 0.25948224]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: With the 10th overall pick of the draft titans select G Chance Warmack out of Alabama
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['with', 'the', 'overall', 'pick', 'of', 'the', 'draft', 'titans', 'select', 'g', 'chance', 'warmack', 'out', 'of', 'alabama']
cosine_similarity: 0.9661434888839722
train_input: [0.3489939079552687, 0.9661435], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: With the 10th pick in the NFLDraft the Tennessee Titans take Chance Warmack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.63009934 0.         0.44832087 0.         0.         0.
  0.44832087 0.44832087]
 [0.         0.42567716 0.30287281 0.42567716 0.42567716 0.42567716
  0.30287281 0.30287281]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Chance Warmack to The Titans at 10, sentence2: With the 10th pick in the NFLDraft the Tennessee Titans take Chance Warmack
After tokenization, sentence1: ['chance', 'warmack', 'to', 'the', 'titans', 'at'], sentence2: ['with', 'the', 'pick', 'in', 'the', 'nfldraft', 'the', 'tennessee', 'titans', 'take', 'chance', 'warmack']
cosine_similarity: 0.9801801443099976
train_input: [0.4073526042885674, 0.98018014], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Chandler Parsons acting like a grown man out there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.57615236 0.
  0.40993715 0.57615236]
 [0.44665616 0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.31779954 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Chandler Parsons acting like a grown man out there
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['chandler', 'parsons', 'acting', 'like', 'a', 'grown', 'man', 'out', 'there']
cosine_similarity: 0.969774603843689
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Chandler Parsons is a beast white boy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.57615236 0.40993715 0.57615236
  0.        ]
 [0.49922133 0.49922133 0.35520009 0.         0.35520009 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Chandler Parsons is a beast white boy
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['chandler', 'parsons', 'is', 'a', 'beast', 'white', 'boy']
cosine_similarity: 0.9378114342689514
train_input: [0.29121941856368966, 0.93781143], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Chandler Parsons is a cold ass honkey
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.57615236 0.40993715
  0.57615236]
 [0.49922133 0.35520009 0.49922133 0.49922133 0.         0.35520009
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Chandler Parsons is a cold ass honkey
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['chandler', 'parsons', 'is', 'a', 'cold', 'ass', 'honkey']
cosine_similarity: 0.9403524398803711
train_input: [0.29121941856368966, 0.94035244], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Chandler Parsons is to the Rockets as James Harden was to the Thunder
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.40993715 0.
  0.         0.57615236]
 [0.31779954 0.44665616 0.44665616 0.         0.31779954 0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Chandler Parsons is to the Rockets as James Harden was to the Thunder
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['chandler', 'parsons', 'is', 'to', 'the', 'rockets', 'as', 'james', 'harden', 'was', 'to', 'the', 'thunder']
cosine_similarity: 0.9585800170898438
train_input: [0.2605556710562624, 0.95858], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: I am a big Chandler Parsons fan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.40993715 0.57615236]
 [0.57615236 0.40993715 0.57615236 0.         0.40993715 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: I am a big Chandler Parsons fan
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['i', 'am', 'a', 'big', 'chandler', 'parsons', 'fan']
cosine_similarity: 0.9337157607078552
train_input: [0.3360969272762575, 0.93371576], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Looks like were getting a double dose of Chandler Parsons tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.         0.
  0.57615236 0.40993715 0.         0.57615236]
 [0.26868528 0.37762778 0.37762778 0.37762778 0.37762778 0.37762778
  0.         0.26868528 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Looks like were getting a double dose of Chandler Parsons tonight
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['looks', 'like', 'were', 'getting', 'a', 'double', 'dose', 'of', 'chandler', 'parsons', 'tonight']
cosine_similarity: 0.9604979157447815
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Was so wrong about Chandler Parsons
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.40993715 0.57615236 0.        ]
 [0.50154891 0.         0.50154891 0.         0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Was so wrong about Chandler Parsons
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['was', 'so', 'wrong', 'about', 'chandler', 'parsons']
cosine_similarity: 0.9700477123260498
train_input: [0.4112070550676187, 0.9700477], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: What player is SUPPOSED to be guarding Chandler Parsons
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.40993715 0.         0.
  0.57615236]
 [0.35520009 0.49922133 0.         0.35520009 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: What player is SUPPOSED to be guarding Chandler Parsons
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['what', 'player', 'is', 'supposed', 'to', 'be', 'guarding', 'chandler', 'parsons']
cosine_similarity: 0.9818027019500732
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: What s with all the Chandler Parsons talk
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.40993715 0.         0.57615236]
 [0.50154891 0.         0.50154891 0.70490949 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: What s with all the Chandler Parsons talk
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['what', 's', 'with', 'all', 'the', 'chandler', 'parsons', 'talk']
cosine_similarity: 0.952131450176239
train_input: [0.4112070550676187, 0.95213145], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Ya and chandler parsons is on fire
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.40993715 0.57615236 0.        ]
 [0.50154891 0.         0.50154891 0.         0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Chandler Parsons is not tryna lose, sentence2: Ya and chandler parsons is on fire
After tokenization, sentence1: ['chandler', 'parsons', 'is', 'not', 'tryna', 'lose'], sentence2: ['ya', 'and', 'chandler', 'parsons', 'is', 'on', 'fire']
cosine_similarity: 0.9423871040344238
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Bruhhhhhhhhh listening to Charles Barkley talk will never ever ever EVER
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.70490949 0.         0.        ]
 [0.35520009 0.49922133 0.35520009 0.         0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Bruhhhhhhhhh listening to Charles Barkley talk will never ever ever EVER
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['listening', 'to', 'charles', 'barkley', 'talk', 'will', 'never', 'ever', 'ever', 'ever']
cosine_similarity: 0.9404982328414917
train_input: [0.3563004293331381, 0.94049823], train_label: 0
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Charles Barkley be trying Kenny Smith lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.
  0.        ]
 [0.31779954 0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Charles Barkley be trying Kenny Smith lol
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['charles', 'barkley', 'be', 'trying', 'kenny', 'smith', 'lol']
cosine_similarity: 0.9506388306617737
train_input: [0.31878402175377923, 0.95063883], train_label: 0
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Charles Barkley cant make up his mind on what to say
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Charles Barkley cant make up his mind on what to say
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['charles', 'barkley', 'cant', 'make', 'up', 'his', 'mind', 'on', 'what', 'to', 'say']
cosine_similarity: 0.9505283236503601
train_input: [0.3563004293331381, 0.9505283], train_label: 1
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Charles Barkley might be ignorant but he tells the truth
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Charles Barkley might be ignorant but he tells the truth
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['charles', 'barkley', 'might', 'be', 'ignorant', 'but', 'he', 'tells', 'the', 'truth']
cosine_similarity: 0.9325374364852905
train_input: [0.3563004293331381, 0.93253744], train_label: 0
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Charles Barkley on NBAonTNT says that Seattle was the toughest place to play in his career
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.70490949 0.         0.
  0.         0.         0.         0.        ]
 [0.25136004 0.35327777 0.25136004 0.         0.35327777 0.35327777
  0.35327777 0.35327777 0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.25213871]
 [0.25213871 1.        ]]
cosine_similarity: 0.2521387069452626
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Charles Barkley on NBAonTNT says that Seattle was the toughest place to play in his career
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['charles', 'barkley', 'on', 'nbaontnt', 'says', 'that', 'seattle', 'was', 'the', 'toughest', 'place', 'to', 'play', 'in', 'his', 'career']
cosine_similarity: 0.9300110936164856
train_input: [0.2521387069452626, 0.9300111], train_label: 0
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Dawgg I got tears in my eyes laughin at charles barkley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.
  0.         0.        ]
 [0.29017021 0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Dawgg I got tears in my eyes laughin at charles barkley
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['dawgg', 'i', 'got', 'tears', 'in', 'my', 'eyes', 'laughin', 'at', 'charles', 'barkley']
cosine_similarity: 0.9058516621589661
train_input: [0.2910691023819054, 0.90585166], train_label: 0
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: They on dirt with Charles Barkley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949]
 [0.50154891 0.50154891 0.70490949 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: They on dirt with Charles Barkley
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['they', 'on', 'dirt', 'with', 'charles', 'barkley']
cosine_similarity: 0.9282701015472412
train_input: [0.5031026124151314, 0.9282701], train_label: 0
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Was that Charles Barkley s porky pig impression
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: Was that Charles Barkley s porky pig impression
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['was', 'that', 'charles', 'barkley', 's', 'porky', 'pig', 'impression']
cosine_similarity: 0.9316074252128601
train_input: [0.3563004293331381, 0.9316074], train_label: 0
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: When I see a bad outfit Ill channel my inner Charles Barkley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.         0.50154891 0.70490949 0.
  0.         0.        ]
 [0.4078241  0.29017021 0.4078241  0.29017021 0.         0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: When I see a bad outfit Ill channel my inner Charles Barkley
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['when', 'i', 'see', 'a', 'bad', 'outfit', 'ill', 'channel', 'my', 'inner', 'charles', 'barkley']
cosine_similarity: 0.9594321250915527
train_input: [0.2910691023819054, 0.9594321], train_label: 0
TF_IDF_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: charles barkley also said carmelo anthony is the best scorer on the planet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.         0.         0.50154891 0.70490949
  0.         0.         0.        ]
 [0.37762778 0.26868528 0.37762778 0.37762778 0.26868528 0.
  0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.26951761]
 [0.26951761 1.        ]]
cosine_similarity: 0.26951761324603224
word_to_vector_cosine_similarity: sentence1: Charles Barkley still a dumbass to me, sentence2: charles barkley also said carmelo anthony is the best scorer on the planet
After tokenization, sentence1: ['charles', 'barkley', 'still', 'a', 'dumbass', 'to', 'me'], sentence2: ['charles', 'barkley', 'also', 'said', 'carmelo', 'anthony', 'is', 'the', 'best', 'scorer', 'on', 'the', 'planet']
cosine_similarity: 0.9273493885993958
train_input: [0.26951761324603224, 0.9273494], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Chicago we need this so let s make the most of it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.
  0.53404633]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Chicago we need this so let s make the most of it
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['chicago', 'we', 'need', 'this', 'so', 'let', 's', 'make', 'the', 'most', 'of', 'it']
cosine_similarity: 0.8738870620727539
train_input: [0.1443835552773867, 0.87388706], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Going out to chicago to support my brother at his wrestling tourney
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.         0.         0.53404633]
 [0.4261596  0.         0.30321606 0.         0.4261596  0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Going out to chicago to support my brother at his wrestling tourney
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['going', 'out', 'to', 'chicago', 'to', 'support', 'my', 'brother', 'at', 'his', 'wrestling', 'tourney']
cosine_similarity: 0.923102855682373
train_input: [0.11521554337793122, 0.92310286], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Have you given any thought to attending Pittcon 2014 in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.53404633 0.37997836 0.53404633 0.
  0.         0.         0.53404633]
 [0.4261596  0.4261596  0.         0.30321606 0.         0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Have you given any thought to attending Pittcon 2014 in Chicago
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['have', 'you', 'given', 'any', 'thought', 'to', 'attending', 'in', 'chicago']
cosine_similarity: 0.9166479110717773
train_input: [0.11521554337793122, 0.9166479], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: HelloBleached is playing tonight subtchicago TONIGHT
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.5        0.         0.         0.
  0.         0.5       ]
 [0.         0.         0.         0.37796447 0.37796447 0.37796447
  0.75592895 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: HelloBleached is playing tonight subtchicago TONIGHT
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['is', 'playing', 'tonight', 'tonight']
cosine_similarity: 0.9278340935707092
train_input: [0.0, 0.9278341], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: I expect to see you patrolling in Chicago asap
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.         0.53404633 0.
  0.53404633]
 [0.53404633 0.         0.37997836 0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: I expect to see you patrolling in Chicago asap
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['i', 'expect', 'to', 'see', 'you', 'patrolling', 'in', 'chicago', 'asap']
cosine_similarity: 0.9114717841148376
train_input: [0.1443835552773867, 0.9114718], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: It s spring in chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.53404633]
 [0.         0.57973867 0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: It s spring in chicago
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['it', 's', 'spring', 'in', 'chicago']
cosine_similarity: 0.9166005849838257
train_input: [0.22028815056182965, 0.9166006], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Oh I left out Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]
 [0.         0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Oh I left out Chicago
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['oh', 'i', 'left', 'out', 'chicago']
cosine_similarity: 0.92323899269104
train_input: [0.17077611319011649, 0.923239], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Think AileybayRae is the best bartender in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.53404633 0.37997836 0.53404633
  0.         0.53404633]
 [0.47107781 0.47107781 0.47107781 0.         0.33517574 0.
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: Think AileybayRae is the best bartender in Chicago
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['think', 'is', 'the', 'best', 'bartender', 'in', 'chicago']
cosine_similarity: 0.9176381826400757
train_input: [0.1273595297947935, 0.9176382], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: You looked beautiful in Chicago this morning
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.53404633]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: You looked beautiful in Chicago this morning
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['you', 'looked', 'beautiful', 'in', 'chicago', 'this', 'morning']
cosine_similarity: 0.9106646776199341
train_input: [0.1443835552773867, 0.9106647], train_label: 0
TF_IDF_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: justinbieber are you ever going to perform at Chicago Illinois
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.
  0.         0.53404633]
 [0.         0.33517574 0.         0.47107781 0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Flight cancelled in Chicago yesterday, sentence2: justinbieber are you ever going to perform at Chicago Illinois
After tokenization, sentence1: ['flight', 'cancelled', 'in', 'chicago', 'yesterday'], sentence2: ['justinbieber', 'are', 'you', 'ever', 'going', 'to', 'perform', 'at', 'chicago', 'illinois']
cosine_similarity: 0.9195122718811035
train_input: [0.1273595297947935, 0.9195123], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: After School draws in crowds in Japan and China with MIXXO
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.         0.         0.47107781 0.
  0.         0.         0.47107781 0.47107781]
 [0.         0.30321606 0.4261596  0.4261596  0.         0.4261596
  0.4261596  0.4261596  0.         0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: After School draws in crowds in Japan and China with MIXXO
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['after', 'school', 'draws', 'in', 'crowds', 'in', 'japan', 'and', 'china', 'with', 'mixxo']
cosine_similarity: 0.8966661691665649
train_input: [0.10163066979112656, 0.89666617], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: And if you know youre using china phone to tweet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.         0.47107781
  0.         0.         0.47107781 0.        ]
 [0.         0.30321606 0.         0.4261596  0.4261596  0.
  0.4261596  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: And if you know youre using china phone to tweet
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['and', 'if', 'you', 'know', 'youre', 'using', 'china', 'phone', 'to', 'tweet']
cosine_similarity: 0.8235321044921875
train_input: [0.10163066979112656, 0.8235321], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: CYD China Yuchai Appoints New Director
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.         0.         0.47107781
  0.         0.47107781 0.47107781 0.        ]
 [0.         0.4261596  0.30321606 0.4261596  0.4261596  0.
  0.4261596  0.         0.         0.4261596 ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: CYD China Yuchai Appoints New Director
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['cyd', 'china', 'appoints', 'new', 'director']
cosine_similarity: 0.7256899476051331
train_input: [0.10163066979112656, 0.72568995], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: In honor of my Japan China trip and to take care of my Asian items better
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.         0.         0.33517574 0.47107781
  0.         0.         0.         0.47107781 0.         0.47107781]
 [0.         0.36499647 0.36499647 0.36499647 0.25969799 0.
  0.36499647 0.36499647 0.36499647 0.         0.36499647 0.        ]]
pairwise_similarity: [[1.         0.08704447]
 [0.08704447 1.        ]]
cosine_similarity: 0.08704446792504217
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: In honor of my Japan China trip and to take care of my Asian items better
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['in', 'honor', 'of', 'my', 'japan', 'china', 'trip', 'and', 'to', 'take', 'care', 'of', 'my', 'asian', 'items', 'better']
cosine_similarity: 0.889657199382782
train_input: [0.08704446792504217, 0.8896572], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: Love love PinkMartini thank you China
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.         0.
  0.47107781 0.47107781]
 [0.         0.27894255 0.         0.78408803 0.39204401 0.39204401
  0.         0.        ]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536716
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: Love love PinkMartini thank you China
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['love', 'love', 'thank', 'you', 'china']
cosine_similarity: 0.8458718061447144
train_input: [0.09349477497536716, 0.8458718], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: Package tour groups from China
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.         0.         0.35520009
  0.49922133]
 [0.         0.40993715 0.         0.57615236 0.57615236 0.40993715
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: Package tour groups from China
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['package', 'tour', 'groups', 'from', 'china']
cosine_similarity: 0.9146462678909302
train_input: [0.29121941856368966, 0.91464627], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: SEUNGRI IS GOING FOR GDS WORLD TOUR IN CHINA
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53309782 0.37930349 0.53309782 0.         0.         0.
  0.37930349 0.37930349]
 [0.         0.33471228 0.         0.47042643 0.47042643 0.47042643
  0.33471228 0.33471228]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: SEUNGRI IS GOING FOR GDS WORLD TOUR IN CHINA
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['seungri', 'is', 'going', 'for', 'gds', 'world', 'tour', 'in', 'china']
cosine_similarity: 0.9577029943466187
train_input: [0.38087260847594373, 0.957703], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: Why we cant use twitter in China
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.47107781 0.         0.
  0.47107781]
 [0.         0.44943642 0.         0.         0.6316672  0.6316672
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: Why we cant use twitter in China
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['why', 'we', 'cant', 'use', 'twitter', 'in', 'china']
cosine_similarity: 0.866047203540802
train_input: [0.15064018498706508, 0.8660472], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: You sound like some science professor who s from China or something
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.         0.
  0.         0.47107781 0.47107781]
 [0.         0.33517574 0.         0.47107781 0.47107781 0.47107781
  0.47107781 0.         0.        ]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: You sound like some science professor who s from China or something
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['you', 'sound', 'like', 'some', 'science', 'professor', 'who', 's', 'from', 'china', 'or', 'something']
cosine_similarity: 0.855741560459137
train_input: [0.11234277891542777, 0.85574156], train_label: 0
TF_IDF_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: just read about the china border encroachment on fb
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.         0.         0.47107781
  0.         0.         0.47107781 0.47107781]
 [0.         0.4261596  0.30321606 0.4261596  0.4261596  0.
  0.4261596  0.4261596  0.         0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: 5 GDragon s 1st World Tour in China, sentence2: just read about the china border encroachment on fb
After tokenization, sentence1: ['gdragon', 's', 'world', 'tour', 'in', 'china'], sentence2: ['just', 'read', 'about', 'the', 'china', 'border', 'encroachment', 'on', 'fb']
cosine_similarity: 0.8809570074081421
train_input: [0.10163066979112656, 0.880957], train_label: 0
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Barkley should be a terrific fit for Chip Kelly s offense
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.4090901  0.57496187 0.4090901  0.4090901  0.4090901
  0.        ]
 [0.49844628 0.35464863 0.         0.35464863 0.35464863 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Barkley should be a terrific fit for Chip Kelly s offense
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['barkley', 'should', 'be', 'a', 'terrific', 'fit', 'for', 'chip', 'kelly', 's', 'offense']
cosine_similarity: 0.9805934429168701
train_input: [0.5803329846765685, 0.98059344], train_label: 0
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Chip Kelly and the damn Eagles traded up to steal Matt Barkley from the Chiefs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35520009 0.         0.49922133 0.
  0.49922133 0.35520009 0.         0.49922133 0.         0.        ]
 [0.35327777 0.35327777 0.25136004 0.35327777 0.         0.35327777
  0.         0.25136004 0.35327777 0.         0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.17856622]
 [0.17856622 1.        ]]
cosine_similarity: 0.17856621555757476
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Chip Kelly and the damn Eagles traded up to steal Matt Barkley from the Chiefs
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['chip', 'kelly', 'and', 'the', 'damn', 'eagles', 'traded', 'up', 'to', 'steal', 'matt', 'barkley', 'from', 'the', 'chiefs']
cosine_similarity: 0.9678118228912354
train_input: [0.17856621555757476, 0.9678118], train_label: 0
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Chip Kelly is taking this draft by the horns
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.49922133 0.         0.35520009
  0.49922133 0.        ]
 [0.35520009 0.         0.49922133 0.         0.49922133 0.35520009
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Chip Kelly is taking this draft by the horns
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['chip', 'kelly', 'is', 'taking', 'this', 'draft', 'by', 'the', 'horns']
cosine_similarity: 0.9660210609436035
train_input: [0.2523342014336961, 0.96602106], train_label: 0
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Chip Kelly on Matt Barkley in 2011
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35520009 0.49922133 0.49922133 0.35520009
  0.         0.49922133]
 [0.49922133 0.49922133 0.35520009 0.         0.         0.35520009
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Chip Kelly on Matt Barkley in 2011
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['chip', 'kelly', 'on', 'matt', 'barkley', 'in']
cosine_similarity: 0.9345464706420898
train_input: [0.2523342014336961, 0.9345465], train_label: 0
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: I dont see MattBarkley fitting into Chip Kelly s explosive spread offense
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.         0.         0.53309782 0.
  0.37930349 0.         0.37930349 0.        ]
 [0.27867523 0.         0.39166832 0.39166832 0.         0.39166832
  0.27867523 0.39166832 0.27867523 0.39166832]]
pairwise_similarity: [[1.         0.31710747]
 [0.31710747 1.        ]]
cosine_similarity: 0.31710746658027095
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: I dont see MattBarkley fitting into Chip Kelly s explosive spread offense
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['i', 'dont', 'see', 'fitting', 'into', 'chip', 'kelly', 's', 'explosive', 'spread', 'offense']
cosine_similarity: 0.9716947674751282
train_input: [0.31710746658027095, 0.97169477], train_label: 1
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Matt Barkley playing under Chip Kelly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.35520009 0.
  0.49922133 0.        ]
 [0.49922133 0.35520009 0.         0.         0.35520009 0.49922133
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Matt Barkley playing under Chip Kelly
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['matt', 'barkley', 'playing', 'under', 'chip', 'kelly']
cosine_similarity: 0.936161994934082
train_input: [0.2523342014336961, 0.936162], train_label: 0
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Matt Barkley will never be able to get to the line fast enough for chip Kelly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35520009 0.49922133 0.         0.49922133
  0.35520009 0.         0.         0.49922133]
 [0.4078241  0.4078241  0.29017021 0.         0.4078241  0.
  0.29017021 0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: Matt Barkley will never be able to get to the line fast enough for chip Kelly
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['matt', 'barkley', 'will', 'never', 'be', 'able', 'to', 'get', 'to', 'the', 'line', 'fast', 'enough', 'for', 'chip', 'kelly']
cosine_similarity: 0.9735748171806335
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: People keep labeling Chip Kelly s offense
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.53309782 0.37930349 0.         0.37930349
  0.        ]
 [0.37930349 0.         0.         0.37930349 0.53309782 0.37930349
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: People keep labeling Chip Kelly s offense
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['people', 'keep', 'labeling', 'chip', 'kelly', 's', 'offense']
cosine_similarity: 0.9540039896965027
train_input: [0.43161341897075145, 0.954004], train_label: 0
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: holy shit Chip Kelly picked Barkley
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.         0.35520009
  0.49922133 0.         0.        ]
 [0.44665616 0.31779954 0.         0.         0.44665616 0.31779954
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: holy shit Chip Kelly picked Barkley
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['holy', 'shit', 'chip', 'kelly', 'picked', 'barkley']
cosine_similarity: 0.9528451561927795
train_input: [0.22576484600261604, 0.95284516], train_label: 0
TF_IDF_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: it s gonna be a high powered offense the way chip Kelly runs things
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.53309782 0.         0.         0.37930349
  0.37930349 0.         0.         0.         0.        ]
 [0.25948224 0.         0.         0.36469323 0.36469323 0.25948224
  0.25948224 0.36469323 0.36469323 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.29526756]
 [0.29526756 1.        ]]
cosine_similarity: 0.2952675553824053
word_to_vector_cosine_similarity: sentence1: He doesnt fit the Chip Kelly offense, sentence2: it s gonna be a high powered offense the way chip Kelly runs things
After tokenization, sentence1: ['he', 'doesnt', 'fit', 'the', 'chip', 'kelly', 'offense'], sentence2: ['it', 's', 'gonna', 'be', 'a', 'high', 'powered', 'offense', 'the', 'way', 'chip', 'kelly', 'runs', 'things']
cosine_similarity: 0.9743450880050659
train_input: [0.2952675553824053, 0.9743451], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman is such a joke
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.44665616]
 [0.50154891 0.50154891 0.70490949 0.         0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman is such a joke
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['chris', 'berman', 'is', 'such', 'a', 'joke']
cosine_similarity: 0.9547234773635864
train_input: [0.31878402175377923, 0.9547235], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman is sucking my will to live
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.44665616 0.        ]
 [0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman is sucking my will to live
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['chris', 'berman', 'is', 'sucking', 'my', 'will', 'to', 'live']
cosine_similarity: 0.956373393535614
train_input: [0.2605556710562624, 0.9563734], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman just compared offensive linemen to square rectangle Legos So that s a thing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.44665616 0.         0.
  0.         0.44665616 0.44665616 0.         0.         0.44665616
  0.         0.        ]
 [0.23700504 0.23700504 0.33310232 0.         0.33310232 0.33310232
  0.33310232 0.         0.         0.33310232 0.33310232 0.
  0.33310232 0.33310232]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman just compared offensive linemen to square rectangle Legos So that s a thing
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['chris', 'berman', 'just', 'compared', 'offensive', 'linemen', 'to', 'square', 'rectangle', 'legos', 'so', 'that', 's', 'a', 'thing']
cosine_similarity: 0.9591614603996277
train_input: [0.15064018498706508, 0.95916146], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman just said this draft is like building a LEGO fort
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.         0.         0.44665616
  0.         0.         0.         0.44665616 0.44665616 0.
  0.44665616]
 [0.25136004 0.35327777 0.25136004 0.35327777 0.35327777 0.
  0.35327777 0.35327777 0.35327777 0.         0.         0.35327777
  0.        ]]
pairwise_similarity: [[1.         0.15976421]
 [0.15976421 1.        ]]
cosine_similarity: 0.1597642092414444
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman just said this draft is like building a LEGO fort
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['chris', 'berman', 'just', 'said', 'this', 'draft', 'is', 'like', 'building', 'a', 'lego', 'fort']
cosine_similarity: 0.9699669480323792
train_input: [0.1597642092414444, 0.96996695], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman s Lego kit analogy is the stupidest thing I have ever heard
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.31779954 0.         0.44665616 0.
  0.         0.44665616 0.44665616 0.44665616 0.         0.        ]
 [0.37762778 0.26868528 0.26868528 0.37762778 0.         0.37762778
  0.37762778 0.         0.         0.         0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman s Lego kit analogy is the stupidest thing I have ever heard
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['chris', 'berman', 's', 'lego', 'kit', 'analogy', 'is', 'the', 'stupidest', 'thing', 'i', 'have', 'ever', 'heard']
cosine_similarity: 0.9535578489303589
train_input: [0.1707761131901165, 0.95355785], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman should never try to use an analogie again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.31779954 0.44665616 0.44665616 0.44665616
  0.44665616 0.         0.        ]
 [0.49922133 0.35520009 0.35520009 0.         0.         0.
  0.         0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Chris Berman should never try to use an analogie again
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['chris', 'berman', 'should', 'never', 'try', 'to', 'use', 'an', 'again']
cosine_similarity: 0.9135169386863708
train_input: [0.22576484600261604, 0.91351694], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Did Chris Berman really just try to sell us on the Jaguars vs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.         0.44665616 0.
  0.44665616 0.44665616 0.         0.         0.44665616 0.
  0.        ]
 [0.25136004 0.25136004 0.35327777 0.35327777 0.         0.35327777
  0.         0.         0.35327777 0.35327777 0.         0.35327777
  0.35327777]]
pairwise_similarity: [[1.         0.15976421]
 [0.15976421 1.        ]]
cosine_similarity: 0.1597642092414444
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Did Chris Berman really just try to sell us on the Jaguars vs
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['did', 'chris', 'berman', 'really', 'just', 'try', 'to', 'sell', 'us', 'on', 'the', 'jaguars', 'vs']
cosine_similarity: 0.9483054280281067
train_input: [0.1597642092414444, 0.9483054], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Gotta love Chris Berman with his Legos analogy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.31779954 0.         0.44665616 0.
  0.         0.44665616 0.44665616 0.44665616]
 [0.44665616 0.31779954 0.31779954 0.44665616 0.         0.44665616
  0.44665616 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Gotta love Chris Berman with his Legos analogy
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['gotta', 'love', 'chris', 'berman', 'with', 'his', 'legos', 'analogy']
cosine_similarity: 0.9611865282058716
train_input: [0.20199309249791833, 0.9611865], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Was Chris Berman just hitting on Barry Sanders
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.31779954 0.         0.44665616 0.
  0.44665616 0.44665616 0.         0.44665616]
 [0.44665616 0.31779954 0.31779954 0.44665616 0.         0.44665616
  0.         0.         0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: Was Chris Berman just hitting on Barry Sanders
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['was', 'chris', 'berman', 'just', 'hitting', 'on', 'barry', 'sanders']
cosine_similarity: 0.9604264497756958
train_input: [0.20199309249791833, 0.96042645], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: What the HELL was Chris Berman just talking about with Leggos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.         0.44665616 0.         0.
  0.44665616 0.44665616 0.44665616 0.        ]
 [0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.         0.         0.         0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Chris Berman is the Michael Jordan of sports metaphors, sentence2: What the HELL was Chris Berman just talking about with Leggos
After tokenization, sentence1: ['chris', 'berman', 'is', 'the', 'michael', 'jordan', 'of', 'sports', 'metaphors'], sentence2: ['what', 'the', 'hell', 'was', 'chris', 'berman', 'just', 'talking', 'about', 'with', 'leggos']
cosine_similarity: 0.9534315466880798
train_input: [0.20199309249791833, 0.95343155], train_label: 0
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Chris Berman building a Legos fort
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.35520009 0.49922133
  0.         0.        ]
 [0.35520009 0.         0.         0.49922133 0.35520009 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Chris Berman building a Legos fort
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['chris', 'berman', 'building', 'a', 'legos', 'fort']
cosine_similarity: 0.8653111457824707
train_input: [0.2523342014336961, 0.86531115], train_label: 0
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Chris Berman compares football to building a LEGO fort
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.35520009 0.
  0.49922133 0.         0.         0.        ]
 [0.29017021 0.         0.         0.4078241  0.29017021 0.4078241
  0.         0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Chris Berman compares football to building a LEGO fort
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['chris', 'berman', 'compares', 'football', 'to', 'building', 'a', 'lego', 'fort']
cosine_similarity: 0.9159044027328491
train_input: [0.20613696606828605, 0.9159044], train_label: 0
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Chris Berman is my favorite human being
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.49922133 0.
  0.        ]
 [0.40993715 0.         0.         0.40993715 0.         0.57615236
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Chris Berman is my favorite human being
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['chris', 'berman', 'is', 'my', 'favorite', 'human', 'being']
cosine_similarity: 0.962365984916687
train_input: [0.29121941856368966, 0.962366], train_label: 0
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Chris Berman is trending and reading those mentions is fantastic
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.49922133 0.
  0.         0.         0.        ]
 [0.31779954 0.         0.         0.31779954 0.         0.44665616
  0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Chris Berman is trending and reading those mentions is fantastic
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['chris', 'berman', 'is', 'trending', 'and', 'reading', 'those', 'mentions', 'is', 'fantastic']
cosine_similarity: 0.9503676891326904
train_input: [0.22576484600261604, 0.9503677], train_label: 0
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Hmm Rich Eisen or Chris Berman
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.         0.         0.35520009 0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Hmm Rich Eisen or Chris Berman
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['hmm', 'rich', 'eisen', 'or', 'chris', 'berman']
cosine_similarity: 0.9203519821166992
train_input: [0.2523342014336961, 0.920352], train_label: 0
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: I see Chris Berman insists on being Captain Obvious
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.35520009 0.49922133
  0.         0.        ]
 [0.35520009 0.         0.         0.49922133 0.35520009 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: I see Chris Berman insists on being Captain Obvious
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['i', 'see', 'chris', 'berman', 'insists', 'on', 'being', 'captain', 'obvious']
cosine_similarity: 0.9772689342498779
train_input: [0.2523342014336961, 0.97726893], train_label: 1
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Not sure if I can take Chris Berman for the 3 hours
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.49922133 0.
  0.        ]
 [0.40993715 0.         0.         0.40993715 0.         0.57615236
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Not sure if I can take Chris Berman for the 3 hours
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['not', 'sure', 'if', 'i', 'can', 'take', 'chris', 'berman', 'for', 'the', 'hours']
cosine_similarity: 0.9473822116851807
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Sigh Chris Berman ruined that joke
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.         0.         0.35520009 0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Sigh Chris Berman ruined that joke
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['sigh', 'chris', 'berman', 'ruined', 'that', 'joke']
cosine_similarity: 0.9614709615707397
train_input: [0.2523342014336961, 0.96147096], train_label: 1
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Sure Chris Berman the NFL is just like Legos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.49922133 0.
  0.         0.         0.         0.        ]
 [0.29017021 0.         0.         0.29017021 0.         0.4078241
  0.4078241  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: Sure Chris Berman the NFL is just like Legos
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['sure', 'chris', 'berman', 'the', 'nfl', 'is', 'just', 'like', 'legos']
cosine_similarity: 0.9678807854652405
train_input: [0.20613696606828605, 0.9678808], train_label: 0
TF_IDF_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: for the NFLDraft TV coverage I can have either Chris Berman or Deion Sanders
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.35520009 0.         0.
  0.49922133 0.         0.         0.        ]
 [0.29017021 0.         0.         0.29017021 0.4078241  0.4078241
  0.         0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: I blame Chris Berman dude is brutal, sentence2: for the NFLDraft TV coverage I can have either Chris Berman or Deion Sanders
After tokenization, sentence1: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal'], sentence2: ['for', 'the', 'nfldraft', 'tv', 'coverage', 'i', 'can', 'have', 'either', 'chris', 'berman', 'or', 'deion', 'sanders']
cosine_similarity: 0.9472895860671997
train_input: [0.20613696606828605, 0.9472896], train_label: 0
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Also I started typing that before Chris Berman said it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.         0.70490949 0.        ]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Also I started typing that before Chris Berman said it
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['also', 'i', 'started', 'typing', 'that', 'before', 'chris', 'berman', 'said', 'it']
cosine_similarity: 0.969562292098999
train_input: [0.3563004293331381, 0.9695623], train_label: 0
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Chris Berman is comparing the NFLDraft to legos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.         0.         0.70490949]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Chris Berman is comparing the NFLDraft to legos
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['chris', 'berman', 'is', 'comparing', 'the', 'nfldraft', 'to', 'legos']
cosine_similarity: 0.941887378692627
train_input: [0.3563004293331381, 0.9418874], train_label: 0
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Chris Berman just proposed to had wedding sex with AND had breakup sex with Tim Tebow
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.         0.
  0.         0.         0.70490949 0.        ]
 [0.21440614 0.30134034 0.21440614 0.30134034 0.30134034 0.60268068
  0.30134034 0.30134034 0.         0.30134034]]
pairwise_similarity: [[1.         0.21507033]
 [0.21507033 1.        ]]
cosine_similarity: 0.21507032570577075
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Chris Berman just proposed to had wedding sex with AND had breakup sex with Tim Tebow
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['chris', 'berman', 'just', 'proposed', 'to', 'had', 'wedding', 'sex', 'with', 'and', 'had', 'breakup', 'sex', 'with', 'tim', 'tebow']
cosine_similarity: 0.9791229367256165
train_input: [0.21507032570577075, 0.97912294], train_label: 0
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: First draft completely Chris Bermanfree
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.6316672 ]
 [0.         0.53404633 0.37997836 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: First draft completely Chris Bermanfree
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['first', 'draft', 'completely', 'chris']
cosine_similarity: 0.9547457695007324
train_input: [0.17077611319011649, 0.95474577], train_label: 0
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Glad to see Chris Berman in TimTebow s corner
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.         0.         0.70490949]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Glad to see Chris Berman in TimTebow s corner
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['glad', 'to', 'see', 'chris', 'berman', 'in', 'timtebow', 's', 'corner']
cosine_similarity: 0.9705122113227844
train_input: [0.3563004293331381, 0.9705122], train_label: 0
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: I could carry groceries in the bags under Chris Bermans eyes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.         0.         0.44943642 0.
  0.         0.6316672 ]
 [0.4261596  0.         0.4261596  0.4261596  0.30321606 0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: I could carry groceries in the bags under Chris Bermans eyes
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['i', 'could', 'carry', 'groceries', 'in', 'the', 'bags', 'under', 'chris', 'eyes']
cosine_similarity: 0.9518160223960876
train_input: [0.1362763414390864, 0.951816], train_label: 0
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: I dont think anyone is more annoying than Chris Berman
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.50154891 0.         0.         0.70490949]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: I dont think anyone is more annoying than Chris Berman
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['i', 'dont', 'think', 'anyone', 'is', 'more', 'annoying', 'than', 'chris', 'berman']
cosine_similarity: 0.9680095911026001
train_input: [0.3563004293331381, 0.9680096], train_label: 1
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Not sure if I can take Chris Berman for the 3 hours
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.         0.70490949]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Not sure if I can take Chris Berman for the 3 hours
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['not', 'sure', 'if', 'i', 'can', 'take', 'chris', 'berman', 'for', 'the', 'hours']
cosine_similarity: 0.9791029691696167
train_input: [0.4112070550676187, 0.97910297], train_label: 1
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Think of the size of the pee Chris Berman is taking right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.         0.         0.
  0.         0.70490949]
 [0.29017021 0.29017021 0.4078241  0.4078241  0.4078241  0.4078241
  0.4078241  0.        ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Think of the size of the pee Chris Berman is taking right now
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['think', 'of', 'the', 'size', 'of', 'the', 'pee', 'chris', 'berman', 'is', 'taking', 'right', 'now']
cosine_similarity: 0.9768285155296326
train_input: [0.2910691023819054, 0.9768285], train_label: 0
TF_IDF_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Who is Chris Berman s hair guy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.         0.70490949]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Someone get Chris Berman off the TV, sentence2: Who is Chris Berman s hair guy
After tokenization, sentence1: ['someone', 'get', 'chris', 'berman', 'off', 'the', 'tv'], sentence2: ['who', 'is', 'chris', 'berman', 's', 'hair', 'guy']
cosine_similarity: 0.9667282700538635
train_input: [0.4112070550676187, 0.96672827], train_label: 0
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Am I the only person who cannot stand Chris Berman
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Am I the only person who cannot stand Chris Berman
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['am', 'i', 'the', 'only', 'person', 'who', 'cannot', 'stand', 'chris', 'berman']
cosine_similarity: 0.9703613519668579
train_input: [0.3360969272762575, 0.97036135], train_label: 1
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Chris Berman explaining lego forts at the NFL draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.
  0.         0.57615236 0.57615236]
 [0.29017021 0.29017021 0.4078241  0.4078241  0.4078241  0.4078241
  0.4078241  0.         0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Chris Berman explaining lego forts at the NFL draft
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['chris', 'berman', 'explaining', 'lego', 'forts', 'at', 'the', 'nfl', 'draft']
cosine_similarity: 0.9315266609191895
train_input: [0.23790309463326234, 0.93152666], train_label: 0
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Chris Berman is awful right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.57615236]
 [0.57615236 0.40993715 0.40993715 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Chris Berman is awful right now
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['chris', 'berman', 'is', 'awful', 'right', 'now']
cosine_similarity: 0.9907299280166626
train_input: [0.3360969272762575, 0.9907299], train_label: 1
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Chris Berman talking about the CFL is awesome
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.40993715 0.         0.57615236
  0.57615236]
 [0.49922133 0.35520009 0.49922133 0.35520009 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Chris Berman talking about the CFL is awesome
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['chris', 'berman', 'talking', 'about', 'the', 'cfl', 'is', 'awesome']
cosine_similarity: 0.9826409816741943
train_input: [0.29121941856368966, 0.982641], train_label: 0
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Chris Berman with the best metaphor for building a team ever
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.         0.
  0.57615236 0.57615236]
 [0.31779954 0.44665616 0.44665616 0.31779954 0.44665616 0.44665616
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Chris Berman with the best metaphor for building a team ever
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['chris', 'berman', 'with', 'the', 'best', 'metaphor', 'for', 'building', 'a', 'team', 'ever']
cosine_similarity: 0.9631488919258118
train_input: [0.2605556710562624, 0.9631489], train_label: 0
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Dear espn Please find something else for Chris Berman to do
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Dear espn Please find something else for Chris Berman to do
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['dear', 'espn', 'please', 'find', 'something', 'else', 'for', 'chris', 'berman', 'to', 'do']
cosine_similarity: 0.9414852261543274
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Ive defended Chris Berman for a while now but
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Ive defended Chris Berman for a while now but
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['ive', 'defended', 'chris', 'berman', 'for', 'a', 'while', 'now', 'but']
cosine_similarity: 0.9744253158569336
train_input: [0.3360969272762575, 0.9744253], train_label: 0
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: What the hell sort of Legos did Chris Berman play with as a kid
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.
  0.         0.         0.57615236 0.57615236]
 [0.26868528 0.26868528 0.37762778 0.37762778 0.37762778 0.37762778
  0.37762778 0.37762778 0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: What the hell sort of Legos did Chris Berman play with as a kid
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['what', 'the', 'hell', 'sort', 'of', 'legos', 'did', 'chris', 'berman', 'play', 'with', 'as', 'a', 'kid']
cosine_similarity: 0.9709095358848572
train_input: [0.2202881505618297, 0.97090954], train_label: 0
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Why does Chris Berman keep talking about Legos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Why does Chris Berman keep talking about Legos
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['why', 'does', 'chris', 'berman', 'keep', 'talking', 'about', 'legos']
cosine_similarity: 0.9682594537734985
train_input: [0.29121941856368966, 0.96825945], train_label: 0
TF_IDF_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Why is Chris Berman still employed in the television business
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.57615236
  0.57615236]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Yep Chris Berman is still the worst, sentence2: Why is Chris Berman still employed in the television business
After tokenization, sentence1: ['yep', 'chris', 'berman', 'is', 'still', 'the', 'worst'], sentence2: ['why', 'is', 'chris', 'berman', 'still', 'employed', 'in', 'the', 'television', 'business']
cosine_similarity: 0.975639283657074
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: According to chris berman this is the NFL draft of squares and rectangles
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.
  0.57615236 0.         0.        ]
 [0.4078241  0.29017021 0.29017021 0.4078241  0.         0.4078241
  0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: According to chris berman this is the NFL draft of squares and rectangles
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['according', 'to', 'chris', 'berman', 'this', 'is', 'the', 'nfl', 'draft', 'of', 'squares', 'and', 'rectangles']
cosine_similarity: 0.9721246361732483
train_input: [0.23790309463326234, 0.97212464], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Already tired of Chris Berman s act
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.57615236 0.        ]
 [0.57615236 0.40993715 0.40993715 0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Already tired of Chris Berman s act
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['already', 'tired', 'of', 'chris', 'berman', 's', 'act']
cosine_similarity: 0.9314934611320496
train_input: [0.3360969272762575, 0.93149346], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Chris Berman is a fucking train wreck at this draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236
  0.         0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.         0.
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Chris Berman is a fucking train wreck at this draft
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['chris', 'berman', 'is', 'a', 'fucking', 'train', 'wreck', 'at', 'this', 'draft']
cosine_similarity: 0.9563245177268982
train_input: [0.2605556710562624, 0.9563245], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Chris Berman is suggesting that Tim Tebow head to the CFL
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.57615236 0.57615236
  0.         0.         0.        ]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.         0.
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Chris Berman is suggesting that Tim Tebow head to the CFL
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['chris', 'berman', 'is', 'suggesting', 'that', 'tim', 'tebow', 'head', 'to', 'the', 'cfl']
cosine_similarity: 0.965171217918396
train_input: [0.23790309463326234, 0.9651712], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Chris Berman just suggested Tim Tebow to go to Canada
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.57615236 0.57615236
  0.         0.         0.        ]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.         0.
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Chris Berman just suggested Tim Tebow to go to Canada
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['chris', 'berman', 'just', 'suggested', 'tim', 'tebow', 'to', 'go', 'to', 'canada']
cosine_similarity: 0.9284334182739258
train_input: [0.23790309463326234, 0.9284334], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Chris Berman wants Shane Lechler in the HOF after Ray Guy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236 0.         0.         0.        ]
 [0.26868528 0.26868528 0.37762778 0.37762778 0.37762778 0.
  0.         0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Chris Berman wants Shane Lechler in the HOF after Ray Guy
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['chris', 'berman', 'wants', 'shane', 'lechler', 'in', 'the', 'hof', 'after', 'ray', 'guy']
cosine_similarity: 0.9543765783309937
train_input: [0.2202881505618297, 0.9543766], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Ill always love Chris Berman for saying The Raiders the way he does
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236 0.         0.         0.        ]
 [0.26868528 0.26868528 0.37762778 0.37762778 0.37762778 0.
  0.         0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Ill always love Chris Berman for saying The Raiders the way he does
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['ill', 'always', 'love', 'chris', 'berman', 'for', 'saying', 'the', 'raiders', 'the', 'way', 'he', 'does']
cosine_similarity: 0.9446421265602112
train_input: [0.2202881505618297, 0.9446421], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Someone needs to create a Chris Berman app
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.
  0.57615236]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.         0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: Someone needs to create a Chris Berman app
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['someone', 'needs', 'to', 'create', 'a', 'chris', 'berman', 'app']
cosine_similarity: 0.9414438009262085
train_input: [0.29121941856368966, 0.9414438], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: What the hell sort of Legos did Chris Berman play with as a kid
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.
  0.57615236 0.         0.57615236 0.        ]
 [0.26868528 0.26868528 0.37762778 0.37762778 0.37762778 0.37762778
  0.         0.37762778 0.         0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: What the hell sort of Legos did Chris Berman play with as a kid
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['what', 'the', 'hell', 'sort', 'of', 'legos', 'did', 'chris', 'berman', 'play', 'with', 'as', 'a', 'kid']
cosine_similarity: 0.9669294953346252
train_input: [0.2202881505618297, 0.9669295], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: is chris berman high as shit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.57615236 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chris Berman Master of the Plug, sentence2: is chris berman high as shit
After tokenization, sentence1: ['chris', 'berman', 'master', 'of', 'the', 'plug'], sentence2: ['is', 'chris', 'berman', 'high', 'as', 'shit']
cosine_similarity: 0.956406831741333
train_input: [0.3360969272762575, 0.95640683], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Chris Berman Where bad puns go to live
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.         0.
  0.57615236]
 [0.49922133 0.35520009 0.35520009 0.         0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Chris Berman Where bad puns go to live
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['chris', 'berman', 'where', 'bad', 'puns', 'go', 'to', 'live']
cosine_similarity: 0.9187909364700317
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Chris Berman just keeps answering his own questions
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.57615236
  0.         0.57615236]
 [0.44665616 0.31779954 0.31779954 0.44665616 0.44665616 0.
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Chris Berman just keeps answering his own questions
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['chris', 'berman', 'just', 'keeps', 'answering', 'his', 'own', 'questions']
cosine_similarity: 0.9455710649490356
train_input: [0.2605556710562624, 0.94557106], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Chris Berman s lego fort analogy is the highlight of the Draft so far
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.
  0.         0.         0.57615236 0.57615236]
 [0.37762778 0.26868528 0.26868528 0.37762778 0.37762778 0.37762778
  0.37762778 0.37762778 0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Chris Berman s lego fort analogy is the highlight of the Draft so far
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['chris', 'berman', 's', 'lego', 'fort', 'analogy', 'is', 'the', 'highlight', 'of', 'the', 'draft', 'so', 'far']
cosine_similarity: 0.9055306315422058
train_input: [0.2202881505618297, 0.90553063], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Frankly if I never heard Chris Berman again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Frankly if I never heard Chris Berman again
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['frankly', 'if', 'i', 'never', 'heard', 'chris', 'berman', 'again']
cosine_similarity: 0.9026803970336914
train_input: [0.3360969272762575, 0.9026804], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Good gawd Chris Berman is a tool
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236
  0.        ]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Good gawd Chris Berman is a tool
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['good', 'gawd', 'chris', 'berman', 'is', 'a', 'tool']
cosine_similarity: 0.9168985486030579
train_input: [0.29121941856368966, 0.91689855], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: I feel like Chris Berman just uttered the longest sentence in the history of television
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.         0.         0.57615236 0.         0.         0.        ]
 [0.23700504 0.23700504 0.33310232 0.33310232 0.33310232 0.
  0.33310232 0.33310232 0.         0.33310232 0.33310232 0.33310232]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858148
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: I feel like Chris Berman just uttered the longest sentence in the history of television
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['i', 'feel', 'like', 'chris', 'berman', 'just', 'uttered', 'the', 'longest', 'sentence', 'in', 'the', 'history', 'of', 'television']
cosine_similarity: 0.8977752327919006
train_input: [0.19431434016858148, 0.89777523], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: I swear to God Chris Berman is just reading ad libs out loud
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.57615236
  0.         0.         0.57615236 0.         0.        ]
 [0.35327777 0.25136004 0.25136004 0.35327777 0.35327777 0.
  0.35327777 0.35327777 0.         0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: I swear to God Chris Berman is just reading ad libs out loud
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['i', 'swear', 'to', 'god', 'chris', 'berman', 'is', 'just', 'reading', 'ad', 'libs', 'out', 'loud']
cosine_similarity: 0.901496171951294
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: I think it s time to officially put Chris Berman out to pasture
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.57615236
  0.         0.        ]
 [0.31779954 0.31779954 0.         0.44665616 0.44665616 0.
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: I think it s time to officially put Chris Berman out to pasture
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['i', 'think', 'it', 's', 'time', 'to', 'officially', 'put', 'chris', 'berman', 'out', 'to', 'pasture']
cosine_similarity: 0.8800205588340759
train_input: [0.2605556710562624, 0.88002056], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Is Chris Berman having a stroke on air
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.57615236
  0.        ]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.         0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Is Chris Berman having a stroke on air
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['is', 'chris', 'berman', 'having', 'a', 'stroke', 'on', 'air']
cosine_similarity: 0.9108294248580933
train_input: [0.29121941856368966, 0.9108294], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Stop talking about legos Chris Berman
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.44832087 0.63009934 0.         0.        ]
 [0.37930349 0.37930349 0.37930349 0.         0.53309782 0.53309782]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Chris Berman rambling on about Legos, sentence2: Stop talking about legos Chris Berman
After tokenization, sentence1: ['chris', 'berman', 'rambling', 'on', 'about', 'legos'], sentence2: ['stop', 'talking', 'about', 'legos', 'chris', 'berman']
cosine_similarity: 0.9694089889526367
train_input: [0.5101490193104813, 0.969409], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman is like totally annoying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.49922133 0.49922133 0.
  0.49922133 0.        ]
 [0.49922133 0.35520009 0.35520009 0.         0.         0.49922133
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman is like totally annoying
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['chris', 'berman', 'is', 'like', 'totally', 'annoying']
cosine_similarity: 0.9570542573928833
train_input: [0.2523342014336961, 0.95705426], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman is still doing this
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]
 [0.50154891 0.50154891 0.70490949 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman is still doing this
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['chris', 'berman', 'is', 'still', 'doing', 'this']
cosine_similarity: 0.9576101899147034
train_input: [0.3563004293331381, 0.9576102], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman is talking about legos at the NFL Draft haha
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.         0.49922133 0.         0.        ]
 [0.29017021 0.29017021 0.4078241  0.         0.         0.4078241
  0.4078241  0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman is talking about legos at the NFL Draft haha
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['chris', 'berman', 'is', 'talking', 'about', 'legos', 'at', 'the', 'nfl', 'draft', 'haha']
cosine_similarity: 0.9450494050979614
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman just compared choosing players for the draft to buildiing a fort with Legos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.35520009 0.         0.
  0.49922133 0.         0.49922133 0.         0.         0.49922133
  0.        ]
 [0.23700504 0.33310232 0.33310232 0.23700504 0.33310232 0.33310232
  0.         0.33310232 0.         0.33310232 0.33310232 0.
  0.33310232]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman just compared choosing players for the draft to buildiing a fort with Legos
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['chris', 'berman', 'just', 'compared', 'choosing', 'players', 'for', 'the', 'draft', 'to', 'a', 'fort', 'with', 'legos']
cosine_similarity: 0.9388311505317688
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman needs to learn to finish a thought
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.         0.49922133 0.
  0.49922133 0.         0.        ]
 [0.31779954 0.31779954 0.         0.44665616 0.         0.44665616
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman needs to learn to finish a thought
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['chris', 'berman', 'needs', 'to', 'learn', 'to', 'finish', 'a', 'thought']
cosine_similarity: 0.9377756118774414
train_input: [0.22576484600261604, 0.9377756], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman s voice never gets less annoying in the off season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.49922133 0.49922133 0.
  0.49922133 0.         0.        ]
 [0.44665616 0.31779954 0.31779954 0.         0.         0.44665616
  0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman s voice never gets less annoying in the off season
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['chris', 'berman', 's', 'voice', 'never', 'gets', 'less', 'annoying', 'in', 'the', 'off', 'season']
cosine_similarity: 0.9550596475601196
train_input: [0.22576484600261604, 0.95505965], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman sounds like he s hammered
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133 0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Chris Berman sounds like he s hammered
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['chris', 'berman', 'sounds', 'like', 'he', 's', 'hammered']
cosine_similarity: 0.9785754084587097
train_input: [0.2523342014336961, 0.9785754], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: So glad I dont have to listen to Chris Berman this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.         0.49922133 0.        ]
 [0.31779954 0.31779954 0.44665616 0.         0.         0.44665616
  0.44665616 0.         0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: So glad I dont have to listen to Chris Berman this year
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['so', 'glad', 'i', 'dont', 'have', 'to', 'listen', 'to', 'chris', 'berman', 'this', 'year']
cosine_similarity: 0.9358598589897156
train_input: [0.22576484600261604, 0.93585986], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Subject yourself to Chris Berman or Michael Irvin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Subject yourself to Chris Berman or Michael Irvin
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['subject', 'yourself', 'to', 'chris', 'berman', 'or', 'michael', 'irvin']
cosine_similarity: 0.9291059374809265
train_input: [0.2523342014336961, 0.92910594], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Thank you soo much Chris Berman
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.        ]
 [0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman is a fat fuck loser, sentence2: Thank you soo much Chris Berman
After tokenization, sentence1: ['chris', 'berman', 'is', 'a', 'fat', 'fuck', 'loser'], sentence2: ['thank', 'you', 'soo', 'much', 'chris', 'berman']
cosine_similarity: 0.9241596460342407
train_input: [0.29121941856368966, 0.92415965], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: 1500ESPNReusse is Chris Berman to ESPN the Sid to StarTribune
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.57615236
  0.         0.        ]
 [0.44665616 0.31779954 0.31779954 0.44665616 0.         0.
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: 1500ESPNReusse is Chris Berman to ESPN the Sid to StarTribune
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['is', 'chris', 'berman', 'to', 'espn', 'the', 'sid', 'to']
cosine_similarity: 0.9632101655006409
train_input: [0.2605556710562624, 0.96321017], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Btw Chris Berman needs to hang up his mic
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.44832087 0.         0.63009934 0.
  0.44832087]
 [0.33471228 0.47042643 0.33471228 0.47042643 0.         0.47042643
  0.33471228]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Btw Chris Berman needs to hang up his mic
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['btw', 'chris', 'berman', 'needs', 'to', 'hang', 'up', 'his', 'mic']
cosine_similarity: 0.9682875871658325
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Chris Berman comparing drafting offensive linemen with the top picks to BUILDING LEGOS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.57615236
  0.         0.         0.57615236 0.         0.        ]
 [0.25136004 0.35327777 0.25136004 0.35327777 0.35327777 0.
  0.35327777 0.35327777 0.         0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Chris Berman comparing drafting offensive linemen with the top picks to BUILDING LEGOS
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['chris', 'berman', 'comparing', 'drafting', 'offensive', 'linemen', 'with', 'the', 'top', 'picks', 'to', 'building', 'legos']
cosine_similarity: 0.9198697805404663
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Chris Berman has forced me to watch the draft on mute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.57615236 0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Chris Berman has forced me to watch the draft on mute
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['chris', 'berman', 'has', 'forced', 'me', 'to', 'watch', 'the', 'draft', 'on', 'mute']
cosine_similarity: 0.9702306389808655
train_input: [0.2605556710562624, 0.97023064], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Chris berman is rambling about legos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]
 [0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Chris berman is rambling about legos
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['chris', 'berman', 'is', 'rambling', 'about', 'legos']
cosine_similarity: 0.919549822807312
train_input: [0.3360969272762575, 0.9195498], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: I cant believe Chris Berman was right
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.57615236 0.        ]
 [0.57615236 0.40993715 0.40993715 0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: I cant believe Chris Berman was right
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['i', 'cant', 'believe', 'chris', 'berman', 'was', 'right']
cosine_similarity: 0.9667815566062927
train_input: [0.3360969272762575, 0.96678156], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Proving once again why Chris Berman is a douchebag
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.57615236 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: Proving once again why Chris Berman is a douchebag
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['proving', 'once', 'again', 'why', 'chris', 'berman', 'is', 'a', 'douchebag']
cosine_similarity: 0.9573006629943848
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: and Chris Berman is fumbling over his words more often than normal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.57615236 0.
  0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.         0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: and Chris Berman is fumbling over his words more often than normal
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['and', 'chris', 'berman', 'is', 'fumbling', 'over', 'his', 'words', 'more', 'often', 'than', 'normal']
cosine_similarity: 0.9444971084594727
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: can you take Chris Berman s spot as host of the draft next year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236
  0.         0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.         0.
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: can you take Chris Berman s spot as host of the draft next year
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['can', 'you', 'take', 'chris', 'berman', 's', 'spot', 'as', 'host', 'of', 'the', 'draft', 'next', 'year']
cosine_similarity: 0.9460753202438354
train_input: [0.2605556710562624, 0.9460753], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: is there anything worse than Chris Berman
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.        ]
 [0.50154891 0.50154891 0.         0.         0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Chris Berman needs to get laid, sentence2: is there anything worse than Chris Berman
After tokenization, sentence1: ['chris', 'berman', 'needs', 'to', 'get', 'laid'], sentence2: ['is', 'there', 'anything', 'worse', 'than', 'chris', 'berman']
cosine_similarity: 0.950441837310791
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Chris Berman got me wanting to play with legos now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.42567716 0.42567716 0.         0.30287281
  0.42567716 0.         0.42567716 0.        ]
 [0.33471228 0.33471228 0.         0.         0.47042643 0.33471228
  0.         0.47042643 0.         0.47042643]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Chris Berman got me wanting to play with legos now
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['chris', 'berman', 'got', 'me', 'wanting', 'to', 'play', 'with', 'legos', 'now']
cosine_similarity: 0.9485167264938354
train_input: [0.3041257418754935, 0.9485167], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Chris Berman is a fucking train wreck at this draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.42567716 0.30287281 0.         0.42567716
  0.42567716 0.42567716 0.         0.        ]
 [0.33471228 0.33471228 0.         0.33471228 0.47042643 0.
  0.         0.         0.47042643 0.47042643]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.3041257418754935
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Chris Berman is a fucking train wreck at this draft
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['chris', 'berman', 'is', 'a', 'fucking', 'train', 'wreck', 'at', 'this', 'draft']
cosine_similarity: 0.9499155282974243
train_input: [0.3041257418754935, 0.9499155], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Did Chris Berman really just compare the NFL draft to a Lego Fort
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.         0.44610081 0.         0.3174044
  0.         0.         0.         0.44610081 0.3174044  0.
  0.44610081]
 [0.25116439 0.25116439 0.35300279 0.         0.35300279 0.25116439
  0.35300279 0.35300279 0.35300279 0.         0.25116439 0.35300279
  0.        ]]
pairwise_similarity: [[1.         0.31888273]
 [0.31888273 1.        ]]
cosine_similarity: 0.3188827274930885
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Did Chris Berman really just compare the NFL draft to a Lego Fort
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['did', 'chris', 'berman', 'really', 'just', 'compare', 'the', 'nfl', 'draft', 'to', 'a', 'lego', 'fort']
cosine_similarity: 0.9733549356460571
train_input: [0.3188827274930885, 0.97335494], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Does Chris Berman have any idea what he is saying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.29017021 0.4078241  0.         0.4078241  0.
  0.4078241  0.4078241  0.         0.4078241 ]
 [0.35520009 0.35520009 0.         0.49922133 0.         0.49922133
  0.         0.         0.49922133 0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Does Chris Berman have any idea what he is saying
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['does', 'chris', 'berman', 'have', 'any', 'idea', 'what', 'he', 'is', 'saying']
cosine_similarity: 0.9364619851112366
train_input: [0.20613696606828605, 0.936462], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Frankly if I never heard Chris Berman again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.29017021 0.4078241  0.4078241  0.         0.
  0.4078241  0.4078241  0.4078241 ]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.57615236
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Frankly if I never heard Chris Berman again
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['frankly', 'if', 'i', 'never', 'heard', 'chris', 'berman', 'again']
cosine_similarity: 0.9119246602058411
train_input: [0.23790309463326234, 0.91192466], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: I blame Chris Berman dude is brutal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.         0.         0.29017021 0.4078241  0.4078241
  0.         0.4078241  0.4078241  0.4078241 ]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.         0.
  0.49922133 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: I blame Chris Berman dude is brutal
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['i', 'blame', 'chris', 'berman', 'dude', 'is', 'brutal']
cosine_similarity: 0.9200225472450256
train_input: [0.20613696606828605, 0.92002255], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: MANTI TAEOS GIRLFRIEND IS SITTING NEXT TO CHRIS BERMAN RIGHT NOW
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.29017021 0.4078241  0.4078241  0.         0.4078241
  0.         0.4078241  0.         0.         0.         0.4078241 ]
 [0.29017021 0.29017021 0.         0.         0.4078241  0.
  0.4078241  0.         0.4078241  0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.        0.1683975]
 [0.1683975 1.       ]]
cosine_similarity: 0.16839750037215276
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: MANTI TAEOS GIRLFRIEND IS SITTING NEXT TO CHRIS BERMAN RIGHT NOW
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['manti', 'girlfriend', 'is', 'sitting', 'next', 'to', 'chris', 'berman', 'right', 'now']
cosine_similarity: 0.9471138119697571
train_input: [0.16839750037215276, 0.9471138], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Only person worse than the losers wearing jerseys at the NFL draft is Chris Berman
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.3174044  0.3174044  0.44610081 0.3174044  0.         0.44610081
  0.         0.3174044  0.         0.44610081 0.         0.        ]
 [0.26844636 0.26844636 0.         0.26844636 0.37729199 0.
  0.37729199 0.26844636 0.37729199 0.         0.37729199 0.37729199]]
pairwise_similarity: [[1.         0.34082422]
 [0.34082422 1.        ]]
cosine_similarity: 0.3408242166238352
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Only person worse than the losers wearing jerseys at the NFL draft is Chris Berman
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['only', 'person', 'worse', 'than', 'the', 'losers', 'wearing', 'jerseys', 'at', 'the', 'nfl', 'draft', 'is', 'chris', 'berman']
cosine_similarity: 0.9745402932167053
train_input: [0.3408242166238352, 0.9745403], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Why is chris berman talking about legos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30287281 0.30287281 0.42567716 0.42567716 0.30287281 0.42567716
  0.         0.42567716]
 [0.44832087 0.44832087 0.         0.         0.44832087 0.
  0.63009934 0.        ]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: Why is chris berman talking about legos
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['why', 'is', 'chris', 'berman', 'talking', 'about', 'legos']
cosine_similarity: 0.9525768756866455
train_input: [0.4073526042885674, 0.9525769], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: You can say it Chris berman
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.29017021 0.29017021 0.4078241  0.4078241  0.4078241  0.4078241
  0.         0.4078241 ]
 [0.50154891 0.50154891 0.         0.         0.         0.
  0.70490949 0.        ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Chris Berman is comparing the NFL Draft to Legos toys, sentence2: You can say it Chris berman
After tokenization, sentence1: ['chris', 'berman', 'is', 'comparing', 'the', 'nfl', 'draft', 'to', 'legos', 'toys'], sentence2: ['you', 'can', 'say', 'it', 'chris', 'berman']
cosine_similarity: 0.9116078019142151
train_input: [0.2910691023819054, 0.9116078], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Chris Berman and Jon Gruden are the most entertaining aspects of this draft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.
  0.         0.57615236 0.57615236]
 [0.4078241  0.29017021 0.29017021 0.4078241  0.4078241  0.4078241
  0.4078241  0.         0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Chris Berman and Jon Gruden are the most entertaining aspects of this draft
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['chris', 'berman', 'and', 'jon', 'gruden', 'are', 'the', 'most', 'entertaining', 'aspects', 'of', 'this', 'draft']
cosine_similarity: 0.9681105613708496
train_input: [0.23790309463326234, 0.96811056], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Chris Berman has finally given me the initiative I need to buy NFL Network
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.
  0.         0.         0.         0.57615236 0.57615236]
 [0.25136004 0.35327777 0.25136004 0.35327777 0.35327777 0.35327777
  0.35327777 0.35327777 0.35327777 0.         0.        ]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Chris Berman has finally given me the initiative I need to buy NFL Network
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['chris', 'berman', 'has', 'finally', 'given', 'me', 'the', 'initiative', 'i', 'need', 'to', 'buy', 'nfl', 'network']
cosine_similarity: 0.9652412533760071
train_input: [0.20608363501393823, 0.96524125], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Chris Berman is so fucking annoying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.57615236]
 [0.57615236 0.40993715 0.40993715 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Chris Berman is so fucking annoying
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['chris', 'berman', 'is', 'so', 'fucking', 'annoying']
cosine_similarity: 0.943345844745636
train_input: [0.3360969272762575, 0.94334584], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Chris Berman reliving his childhood vicariously through the Eagles right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.
  0.57615236 0.57615236 0.        ]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.4078241  0.4078241
  0.         0.         0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Chris Berman reliving his childhood vicariously through the Eagles right now
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['chris', 'berman', 'reliving', 'his', 'childhood', 'vicariously', 'through', 'the', 'eagles', 'right', 'now']
cosine_similarity: 0.940617024898529
train_input: [0.23790309463326234, 0.940617], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Does Chris Berman get on anyone else s nerves
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Does Chris Berman get on anyone else s nerves
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['does', 'chris', 'berman', 'get', 'on', 'anyone', 'else', 's', 'nerves']
cosine_similarity: 0.9607111811637878
train_input: [0.3360969272762575, 0.9607112], train_label: 1
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Im beginning to wish Chris Berman had more airtime
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.40993715 0.         0.57615236
  0.57615236 0.        ]
 [0.44665616 0.44665616 0.31779954 0.31779954 0.44665616 0.
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Im beginning to wish Chris Berman had more airtime
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['im', 'beginning', 'to', 'wish', 'chris', 'berman', 'had', 'more', 'airtime']
cosine_similarity: 0.9720239639282227
train_input: [0.2605556710562624, 0.97202396], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: TF is Chris Berman talking about right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: TF is Chris Berman talking about right now
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['tf', 'is', 'chris', 'berman', 'talking', 'about', 'right', 'now']
cosine_similarity: 0.9488362669944763
train_input: [0.29121941856368966, 0.94883627], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Why is Chris Berman so incredible on live television
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: Why is Chris Berman so incredible on live television
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['why', 'is', 'chris', 'berman', 'so', 'incredible', 'on', 'live', 'television']
cosine_similarity: 0.9531262516975403
train_input: [0.29121941856368966, 0.95312625], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: and over on ESPN Chris Berman is talking about legos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: and over on ESPN Chris Berman is talking about legos
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['and', 'over', 'on', 'espn', 'chris', 'berman', 'is', 'talking', 'about', 'legos']
cosine_similarity: 0.9594083428382874
train_input: [0.29121941856368966, 0.95940834], train_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: what the fuck is chris berman saying about legos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.57615236]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Berman used to be tolerable, sentence2: what the fuck is chris berman saying about legos
After tokenization, sentence1: ['chris', 'berman', 'used', 'to', 'be', 'tolerable'], sentence2: ['what', 'the', 'fuck', 'is', 'chris', 'berman', 'saying', 'about', 'legos']
cosine_similarity: 0.9633428454399109
train_input: [0.29121941856368966, 0.96334285], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard I award you the 19th Century Ignoramus Award for today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.49922133 0.35520009 0.         0.35520009
  0.49922133 0.         0.49922133 0.        ]
 [0.33310232 0.66620463 0.         0.23700504 0.33310232 0.23700504
  0.         0.33310232 0.         0.33310232]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard I award you the 19th Century Ignoramus Award for today
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['chris', 'broussard', 'i', 'award', 'you', 'the', 'century', 'ignoramus', 'award', 'for', 'today']
cosine_similarity: 0.9314907193183899
train_input: [0.16836842163679844, 0.9314907], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard is the truth if he did that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.49922133 0.         0.49922133
  0.        ]
 [0.         0.40993715 0.40993715 0.         0.57615236 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard is the truth if he did that
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['chris', 'broussard', 'is', 'the', 'truth', 'if', 'he', 'did', 'that']
cosine_similarity: 0.9704235196113586
train_input: [0.29121941856368966, 0.9704235], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard needs to cut his own mic before he bounces a check
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.35520009 0.         0.35520009 0.
  0.49922133 0.49922133 0.         0.        ]
 [0.4078241  0.         0.29017021 0.4078241  0.29017021 0.4078241
  0.         0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard needs to cut his own mic before he bounces a check
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['chris', 'broussard', 'needs', 'to', 'cut', 'his', 'own', 'mic', 'before', 'he', 'bounces', 'a', 'check']
cosine_similarity: 0.9500805139541626
train_input: [0.20613696606828605, 0.9500805], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard said he doesnt agree with homosexuality since he is of Christian religion
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.35520009 0.35520009 0.         0.49922133
  0.         0.         0.49922133 0.         0.        ]
 [0.37762778 0.         0.26868528 0.26868528 0.37762778 0.
  0.37762778 0.37762778 0.         0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard said he doesnt agree with homosexuality since he is of Christian religion
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['chris', 'broussard', 'said', 'he', 'doesnt', 'agree', 'with', 'homosexuality', 'since', 'he', 'is', 'of', 'christian', 'religion']
cosine_similarity: 0.9683898091316223
train_input: [0.1908740661302035, 0.9683898], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard will be dropped by espn
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.49922133 0.         0.
  0.49922133]
 [0.         0.40993715 0.40993715 0.         0.57615236 0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Chris Broussard will be dropped by espn
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['chris', 'broussard', 'will', 'be', 'dropped', 'by', 'espn']
cosine_similarity: 0.9508103728294373
train_input: [0.29121941856368966, 0.9508104], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Did Chris Broussard get fired by ESPN yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.49922133 0.         0.
  0.         0.49922133]
 [0.         0.35520009 0.35520009 0.         0.49922133 0.49922133
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Did Chris Broussard get fired by ESPN yet
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['did', 'chris', 'broussard', 'get', 'fired', 'by', 'espn', 'yet']
cosine_similarity: 0.9392383694648743
train_input: [0.2523342014336961, 0.93923837], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: I dont disagree with Chris Broussard comments
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.         0.49922133 0.
  0.         0.49922133]
 [0.         0.35520009 0.35520009 0.49922133 0.         0.49922133
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: I dont disagree with Chris Broussard comments
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['i', 'dont', 'disagree', 'with', 'chris', 'broussard', 'comments']
cosine_similarity: 0.9399272203445435
train_input: [0.2523342014336961, 0.9399272], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Seriously how does Chris Broussard have a job
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.49922133 0.         0.
  0.49922133 0.        ]
 [0.         0.35520009 0.35520009 0.         0.49922133 0.49922133
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: Seriously how does Chris Broussard have a job
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['seriously', 'how', 'does', 'chris', 'broussard', 'have', 'a', 'job']
cosine_similarity: 0.9645504951477051
train_input: [0.2523342014336961, 0.9645505], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: it s like Chris Broussard is very religious
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.49922133 0.         0.49922133
  0.        ]
 [0.         0.40993715 0.40993715 0.         0.57615236 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: it s like Chris Broussard is very religious
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['it', 's', 'like', 'chris', 'broussard', 'is', 'very', 'religious']
cosine_similarity: 0.9748884439468384
train_input: [0.29121941856368966, 0.97488844], train_label: 0
TF_IDF_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: what Jason Collins said or what Chris Broussard said
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.         0.49922133 0.
  0.49922133 0.        ]
 [0.         0.26868528 0.26868528 0.37762778 0.         0.37762778
  0.         0.75525556]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Damn Chris Broussard is a brave man, sentence2: what Jason Collins said or what Chris Broussard said
After tokenization, sentence1: ['damn', 'chris', 'broussard', 'is', 'a', 'brave', 'man'], sentence2: ['what', 'jason', 'collins', 'said', 'or', 'what', 'chris', 'broussard', 'said']
cosine_similarity: 0.9550272822380066
train_input: [0.1908740661302035, 0.9550273], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: A closed minded Christian is one the hardest people to talk to
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.53404633 0.         0.        ]
 [0.30321606 0.         0.4261596  0.         0.4261596  0.4261596
  0.         0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: A closed minded Christian is one the hardest people to talk to
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['a', 'closed', 'minded', 'christian', 'is', 'one', 'the', 'hardest', 'people', 'to', 'talk', 'to']
cosine_similarity: 0.989086389541626
train_input: [0.11521554337793122, 0.9890864], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: And by the way u cant claim to be a christian and gay
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]
 [0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: And by the way u cant claim to be a christian and gay
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['and', 'by', 'the', 'way', 'u', 'cant', 'claim', 'to', 'be', 'a', 'christian', 'and', 'gay']
cosine_similarity: 0.9895655512809753
train_input: [0.3360969272762575, 0.98956555], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: Christian Wade has to be involved
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]
 [0.44943642 0.         0.         0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: Christian Wade has to be involved
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['christian', 'wade', 'has', 'to', 'be', 'involved']
cosine_similarity: 0.9689818620681763
train_input: [0.17077611319011649, 0.96898186], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: He s a Christian too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633]
 [1.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: He s a Christian too
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['he', 's', 'a', 'christian', 'too']
cosine_similarity: 0.9277485013008118
train_input: [0.37997836159100784, 0.9277485], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: I do have a problem w him saying Jason Collins is not a Christian
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.         0.53404633
  0.         0.        ]
 [0.33517574 0.         0.47107781 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: I do have a problem w him saying Jason Collins is not a Christian
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['i', 'do', 'have', 'a', 'problem', 'w', 'him', 'saying', 'jason', 'collins', 'is', 'not', 'a', 'christian']
cosine_similarity: 0.9782514572143555
train_input: [0.1273595297947935, 0.97825146], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: KING Idolatry in Christian Hip Hop
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.         0.53404633]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: KING Idolatry in Christian Hip Hop
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['king', 'idolatry', 'in', 'christian', 'hip', 'hop']
cosine_similarity: 0.9055190086364746
train_input: [0.1273595297947935, 0.905519], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: Or any other Christian American
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.53404633 0.53404633]
 [0.81480247 0.57973867 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: Or any other Christian American
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['or', 'any', 'other', 'christian', 'american']
cosine_similarity: 0.9682617783546448
train_input: [0.22028815056182965, 0.9682618], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: Shame is a true Christian virtue
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633 0.         0.
  0.        ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: Shame is a true Christian virtue
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['shame', 'is', 'a', 'true', 'christian', 'virtue']
cosine_similarity: 0.9578208327293396
train_input: [0.1443835552773867, 0.95782083], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: Tebow does it to Christian churches
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.53404633
  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: Tebow does it to Christian churches
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['tebow', 'does', 'it', 'to', 'christian', 'churches']
cosine_similarity: 0.9650598764419556
train_input: [0.1443835552773867, 0.9650599], train_label: 0
TF_IDF_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: That s not very Christian like
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.53404633]
 [0.57973867 0.         0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: If you claim to be a Christian your duty is obedience, sentence2: That s not very Christian like
After tokenization, sentence1: ['if', 'you', 'claim', 'to', 'be', 'a', 'christian', 'your', 'duty', 'is', 'obedience'], sentence2: ['that', 's', 'not', 'very', 'christian', 'like']
cosine_similarity: 0.9795597195625305
train_input: [0.22028815056182965, 0.9795597], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Christine Michael from Texas AM totes the rock
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.         0.         0.44665616]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Christine Michael from Texas AM totes the rock
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['christine', 'michael', 'from', 'texas', 'am', 'totes', 'the', 'rock']
cosine_similarity: 0.946425199508667
train_input: [0.22576484600261604, 0.9464252], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Christine Michael selected by Seahawks at No
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.47042643 0.33471228 0.
  0.47042643]
 [0.44832087 0.44832087 0.         0.         0.44832087 0.63009934
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Christine Michael selected by Seahawks at No
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['christine', 'michael', 'selected', 'by', 'seahawks', 'at', 'no']
cosine_similarity: 0.959186851978302
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Christine Michael the explosive type of back to complement Lynch and Turbin
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.         0.         0.31779954 0.44665616
  0.44665616 0.44665616 0.         0.         0.44665616]
 [0.29017021 0.4078241  0.4078241  0.4078241  0.29017021 0.
  0.         0.         0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Christine Michael the explosive type of back to complement Lynch and Turbin
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['christine', 'michael', 'the', 'explosive', 'type', 'of', 'back', 'to', 'complement', 'lynch', 'and', 'turbin']
cosine_similarity: 0.9520527720451355
train_input: [0.18443191662261305, 0.9520528], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Christine Michael to backup Marshawn Lynch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.         0.31779954 0.44665616
  0.44665616 0.44665616 0.44665616]
 [0.49922133 0.35520009 0.49922133 0.49922133 0.35520009 0.
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Christine Michael to backup Marshawn Lynch
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['christine', 'michael', 'to', 'backup', 'marshawn', 'lynch']
cosine_similarity: 0.915814220905304
train_input: [0.22576484600261604, 0.9158142], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Seahawks Christine Michael RB Texas A M
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.35464863 0.
  0.49844628]
 [0.4090901  0.4090901  0.         0.4090901  0.4090901  0.57496187
  0.        ]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Seahawks Christine Michael RB Texas A M
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['seahawks', 'christine', 'michael', 'rb', 'texas', 'a', 'm']
cosine_similarity: 0.932473361492157
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Texas A M RB Christine Michael taken by the Seahawks at
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35464863 0.35464863 0.49844628 0.35464863 0.35464863 0.
  0.         0.49844628]
 [0.35464863 0.35464863 0.         0.35464863 0.35464863 0.49844628
  0.49844628 0.        ]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151313
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: Texas A M RB Christine Michael taken by the Seahawks at
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['texas', 'a', 'm', 'rb', 'christine', 'michael', 'taken', 'by', 'the', 'seahawks', 'at']
cosine_similarity: 0.9722923636436462
train_input: [0.5031026124151313, 0.97229236], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: With the 62nd pick Christine Michael
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.31779954 0.44665616 0.         0.44665616
  0.44665616 0.44665616]
 [0.57615236 0.40993715 0.40993715 0.         0.57615236 0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: With the 62nd pick Christine Michael
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['with', 'the', 'pick', 'christine', 'michael']
cosine_similarity: 0.9764052033424377
train_input: [0.2605556710562624, 0.9764052], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: at the Seahawks take with the 62 pick Christine Michael
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33471228 0.33471228 0.47042643 0.         0.47042643
  0.33471228 0.47042643]
 [0.53309782 0.37930349 0.37930349 0.         0.53309782 0.
  0.37930349 0.        ]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.3808726084759436
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: at the Seahawks take with the 62 pick Christine Michael
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['at', 'the', 'seahawks', 'take', 'with', 'the', 'pick', 'christine', 'michael']
cosine_similarity: 0.9836679100990295
train_input: [0.3808726084759436, 0.9836679], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: take Christine Michael RB from Texas A M
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.33471228 0.47042643 0.
  0.47042643]
 [0.44832087 0.44832087 0.         0.44832087 0.         0.63009934
  0.        ]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269897
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: take Christine Michael RB from Texas A M
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['take', 'christine', 'michael', 'rb', 'from', 'texas', 'a', 'm']
cosine_similarity: 0.9591694474220276
train_input: [0.4501755023269897, 0.95916945], train_label: 0
TF_IDF_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: waste no time picking Christine Michael RB Texas A M
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.47042643 0.         0.33471228 0.47042643
  0.         0.         0.47042643 0.        ]
 [0.30287281 0.30287281 0.         0.42567716 0.30287281 0.
  0.42567716 0.42567716 0.         0.42567716]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: Wanna see the new Seahawks RB Christine Michael, sentence2: waste no time picking Christine Michael RB Texas A M
After tokenization, sentence1: ['wanna', 'see', 'the', 'new', 'seahawks', 'rb', 'christine', 'michael'], sentence2: ['waste', 'no', 'time', 'picking', 'christine', 'michael', 'rb', 'texas', 'a', 'm']
cosine_similarity: 0.9424508213996887
train_input: [0.30412574187549346, 0.9424508], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Babyyyyyy Ciara is giving life in this video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133 0.35520009]
 [0.49922133 0.35520009 0.         0.         0.49922133 0.49922133
  0.         0.35520009]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Babyyyyyy Ciara is giving life in this video
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['ciara', 'is', 'giving', 'life', 'in', 'this', 'video']
cosine_similarity: 0.97653728723526
train_input: [0.2523342014336961, 0.9765373], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Body Party by Ciara is my new shit I cant stop listening to it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33517574 0.47107781 0.47107781 0.         0.47107781
  0.         0.         0.         0.         0.47107781]
 [0.39204401 0.27894255 0.         0.         0.39204401 0.
  0.39204401 0.39204401 0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536716
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Body Party by Ciara is my new shit I cant stop listening to it
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['body', 'party', 'by', 'ciara', 'is', 'my', 'new', 'shit', 'i', 'cant', 'stop', 'listening', 'to', 'it']
cosine_similarity: 0.9671695828437805
train_input: [0.09349477497536716, 0.9671696], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Future Ciara was so cute in her video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.         0.53309782 0.37930349 0.53309782 0.37930349]
 [0.44832087 0.63009934 0.         0.44832087 0.         0.44832087]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Future Ciara was so cute in her video
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['future', 'ciara', 'was', 'so', 'cute', 'in', 'her', 'video']
cosine_similarity: 0.9735358953475952
train_input: [0.5101490193104813, 0.9735359], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Ill give Ash a lap dance to Ciara x Body Party that s my song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35520009 0.35520009 0.49922133 0.
  0.         0.49922133 0.         0.         0.49922133]
 [0.37762778 0.37762778 0.26868528 0.26868528 0.         0.37762778
  0.37762778 0.         0.37762778 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Ill give Ash a lap dance to Ciara x Body Party that s my song
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['ill', 'give', 'ash', 'a', 'lap', 'dance', 'to', 'ciara', 'x', 'body', 'party', 'that', 's', 'my', 'song']
cosine_similarity: 0.9685417413711548
train_input: [0.1908740661302035, 0.96854174], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Just watched the music video to Body Party by Ciara and she killed it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133 0.         0.         0.35520009 0.        ]
 [0.37762778 0.26868528 0.         0.         0.37762778 0.37762778
  0.         0.37762778 0.37762778 0.26868528 0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Just watched the music video to Body Party by Ciara and she killed it
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['just', 'watched', 'the', 'music', 'video', 'to', 'body', 'party', 'by', 'ciara', 'and', 'she', 'killed', 'it']
cosine_similarity: 0.9714018106460571
train_input: [0.1908740661302035, 0.9714018], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Okay so Ciara s my new girlfriend
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.         0.47107781]
 [0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Okay so Ciara s my new girlfriend
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['okay', 'so', 'ciara', 's', 'my', 'new', 'girlfriend']
cosine_similarity: 0.9549760222434998
train_input: [0.1273595297947935, 0.954976], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Was washing dishes when a new Ciara music video came on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.         0.49922133 0.49922133
  0.         0.         0.35520009 0.        ]
 [0.4078241  0.29017021 0.         0.4078241  0.         0.
  0.4078241  0.4078241  0.29017021 0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Was washing dishes when a new Ciara music video came on
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['was', 'washing', 'dishes', 'when', 'a', 'new', 'ciara', 'music', 'video', 'came', 'on']
cosine_similarity: 0.9718791842460632
train_input: [0.20613696606828605, 0.9718792], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Watching the Ciara vid for the first time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.47107781 0.         0.
  0.47107781 0.        ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: Watching the Ciara vid for the first time
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['watching', 'the', 'ciara', 'vid', 'for', 'the', 'first', 'time']
cosine_similarity: 0.9689384698867798
train_input: [0.1273595297947935, 0.96893847], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: When Ciara does the butterfly in her body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.35520009 0.49922133 0.         0.49922133
  0.49922133 0.         0.35520009]
 [0.44665616 0.44665616 0.31779954 0.         0.44665616 0.
  0.         0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: When Ciara does the butterfly in her body party video
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['when', 'ciara', 'does', 'the', 'butterfly', 'in', 'her', 'body', 'party', 'video']
cosine_similarity: 0.9782235026359558
train_input: [0.22576484600261604, 0.9782235], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: fuck Ciara all on my man for
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.         0.47107781 0.47107781 0.
  0.47107781]
 [0.44943642 0.         0.6316672  0.         0.         0.6316672
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Ciara s little dance for Future in her video, sentence2: fuck Ciara all on my man for
After tokenization, sentence1: ['ciara', 's', 'little', 'dance', 'for', 'future', 'in', 'her', 'video'], sentence2: ['fuck', 'ciara', 'all', 'on', 'my', 'man', 'for']
cosine_similarity: 0.9705919027328491
train_input: [0.15064018498706508, 0.9705919], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Ciara got a video for body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.         0.57615236
  0.40993715]
 [0.49922133 0.35520009 0.49922133 0.         0.49922133 0.
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Ciara got a video for body party
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['ciara', 'got', 'a', 'video', 'for', 'body', 'party']
cosine_similarity: 0.9520316123962402
train_input: [0.29121941856368966, 0.9520316], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Ciara look so good in this Body Party video I love the way she dance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.         0.
  0.57615236 0.         0.57615236 0.40993715 0.        ]
 [0.35327777 0.25136004 0.35327777 0.35327777 0.35327777 0.35327777
  0.         0.35327777 0.         0.25136004 0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Ciara look so good in this Body Party video I love the way she dance
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['ciara', 'look', 'so', 'good', 'in', 'this', 'body', 'party', 'video', 'i', 'love', 'the', 'way', 'she', 'dance']
cosine_similarity: 0.9374581575393677
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Ciara took over twitter with this body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.         0.57615236 0.
  0.         0.40993715]
 [0.44665616 0.31779954 0.         0.44665616 0.         0.44665616
  0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Ciara took over twitter with this body party video
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['ciara', 'took', 'over', 'twitter', 'with', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9508160948753357
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: I like fell in love with Ciara s video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.57615236 0.57615236
  0.40993715]
 [0.35520009 0.49922133 0.49922133 0.49922133 0.         0.
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: I like fell in love with Ciara s video
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['i', 'like', 'fell', 'in', 'love', 'with', 'ciara', 's', 'video']
cosine_similarity: 0.947382390499115
train_input: [0.29121941856368966, 0.9473824], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: I think Ciara Future are the cutest
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.
  0.53404633]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: I think Ciara Future are the cutest
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['i', 'think', 'ciara', 'future', 'are', 'the', 'cutest']
cosine_similarity: 0.9037783145904541
train_input: [0.1443835552773867, 0.9037783], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Lmao damn BowWowYMCMB just disappeared when ciara video came on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.         0.         0.
  0.         0.57615236 0.57615236 0.40993715]
 [0.37762778 0.37762778 0.26868528 0.37762778 0.37762778 0.37762778
  0.37762778 0.         0.         0.26868528]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Lmao damn BowWowYMCMB just disappeared when ciara video came on
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['lmao', 'damn', 'just', 'disappeared', 'when', 'ciara', 'video', 'came', 'on']
cosine_similarity: 0.9214072227478027
train_input: [0.2202881505618297, 0.9214072], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Omg i love ciara s new video body party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.         0.
  0.         0.57615236 0.40993715]
 [0.4078241  0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.4078241  0.         0.29017021]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: Omg i love ciara s new video body party
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['omg', 'i', 'love', 'ciara', 's', 'new', 'video', 'body', 'party']
cosine_similarity: 0.9562399387359619
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: ciara can dance her ass OFF
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]
 [0.6316672  0.44943642 0.6316672  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: ciara can dance her ass OFF
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['ciara', 'can', 'dance', 'her', 'ass', 'off']
cosine_similarity: 0.8935775756835938
train_input: [0.17077611319011649, 0.8935776], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: oh shit know ciara got the D after that video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.         0.57615236
  0.         0.40993715]
 [0.31779954 0.44665616 0.44665616 0.         0.44665616 0.
  0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: oh shit know ciara got the D after that video
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['oh', 'shit', 'know', 'ciara', 'got', 'the', 'd', 'after', 'that', 'video']
cosine_similarity: 0.9281796813011169
train_input: [0.2605556710562624, 0.9281797], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: the day before Body Party dropped all the Ciara fans hated her for her tracklist
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.         0.         0.
  0.53404633 0.         0.53404633 0.         0.53404633]
 [0.36499647 0.25969799 0.36499647 0.36499647 0.36499647 0.36499647
  0.         0.36499647 0.         0.36499647 0.        ]]
pairwise_similarity: [[1.         0.09867962]
 [0.09867962 1.        ]]
cosine_similarity: 0.09867961797986956
word_to_vector_cosine_similarity: sentence1: Ciara on Ride music video, sentence2: the day before Body Party dropped all the Ciara fans hated her for her tracklist
After tokenization, sentence1: ['ciara', 'on', 'ride', 'music', 'video'], sentence2: ['the', 'day', 'before', 'body', 'party', 'dropped', 'all', 'the', 'ciara', 'fans', 'hated', 'her', 'for', 'her', 'tracklist']
cosine_similarity: 0.9072520732879639
train_input: [0.09867961797986956, 0.9072521], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: All Ciara videos are in Atlanta
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.53404633 0.        ]
 [0.6316672  0.         0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: All Ciara videos are in Atlanta
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['all', 'ciara', 'videos', 'are', 'in', 'atlanta']
cosine_similarity: 0.9544251561164856
train_input: [0.17077611319011649, 0.95442516], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: But I Thought KeriHilson took Ciara Place
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.         0.53404633
  0.         0.        ]
 [0.         0.33517574 0.47107781 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: But I Thought KeriHilson took Ciara Place
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['but', 'i', 'thought', 'took', 'ciara', 'place']
cosine_similarity: 0.9706352949142456
train_input: [0.1273595297947935, 0.9706353], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Ciara BodyParty Video Is Forsure For Da Haters
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.         0.
  0.53404633 0.53404633 0.        ]
 [0.         0.4261596  0.30321606 0.4261596  0.4261596  0.4261596
  0.         0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Ciara BodyParty Video Is Forsure For Da Haters
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['ciara', 'bodyparty', 'video', 'is', 'forsure', 'for', 'da', 'haters']
cosine_similarity: 0.9475710988044739
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Ciara Future are actually cute together
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.         0.         0.53404633
  0.53404633]
 [0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Ciara Future are actually cute together
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['ciara', 'future', 'are', 'actually', 'cute', 'together']
cosine_similarity: 0.9545491933822632
train_input: [0.1443835552773867, 0.9545492], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Ciara Video With Future That s Cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.         0.53404633 0.53404633
  0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Ciara Video With Future That s Cute
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['ciara', 'video', 'with', 'future', 'that', 's', 'cute']
cosine_similarity: 0.9711340069770813
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: DAMN Ciara BAD n dat body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.44832087 0.         0.         0.44832087
  0.63009934 0.        ]
 [0.42567716 0.30287281 0.30287281 0.42567716 0.42567716 0.30287281
  0.         0.42567716]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: DAMN Ciara BAD n dat body party video
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['damn', 'ciara', 'bad', 'n', 'dat', 'body', 'party', 'video']
cosine_similarity: 0.9692961573600769
train_input: [0.4073526042885674, 0.96929616], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: I aint feeling this whole future and Ciara thang at all
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.         0.         0.53404633
  0.53404633 0.        ]
 [0.47107781 0.         0.33517574 0.47107781 0.47107781 0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: I aint feeling this whole future and Ciara thang at all
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['i', 'aint', 'feeling', 'this', 'whole', 'future', 'and', 'ciara', 'thang', 'at', 'all']
cosine_similarity: 0.9785784482955933
train_input: [0.1273595297947935, 0.97857845], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: I mean this lil Ciara song got potential
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.         0.         0.53404633
  0.         0.53404633 0.        ]
 [0.         0.30321606 0.4261596  0.4261596  0.4261596  0.
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: I mean this lil Ciara song got potential
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['i', 'mean', 'this', 'lil', 'ciara', 'song', 'got', 'potential']
cosine_similarity: 0.9814409613609314
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Was washing dishes when a new Ciara music video came on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.         0.
  0.53404633 0.53404633 0.         0.        ]
 [0.         0.39204401 0.27894255 0.39204401 0.39204401 0.39204401
  0.         0.         0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Was washing dishes when a new Ciara music video came on
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['was', 'washing', 'dishes', 'when', 'a', 'new', 'ciara', 'music', 'video', 'came', 'on']
cosine_similarity: 0.9762519001960754
train_input: [0.1059921313509325, 0.9762519], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Why isnt Ciara A Megastar Yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.         0.53404633 0.53404633]
 [0.         0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Ciara body party is the shit, sentence2: Why isnt Ciara A Megastar Yet
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'the', 'shit'], sentence2: ['why', 'isnt', 'ciara', 'a', 'megastar', 'yet']
cosine_similarity: 0.9508479237556458
train_input: [0.17077611319011649, 0.9508479], train_label: 0
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara Need To Get Off My BabyDaddy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.39204401 0.27894255 0.39204401 0.39204401 0.39204401
  0.         0.39204401 0.39204401]
 [0.6316672  0.         0.44943642 0.         0.         0.
  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara Need To Get Off My BabyDaddy
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['ciara', 'need', 'to', 'get', 'off', 'my', 'babydaddy']
cosine_similarity: 0.964158296585083
train_input: [0.12536693798731732, 0.9641583], train_label: 0
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara did it to me with this song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39204401 0.27894255 0.         0.39204401 0.39204401 0.39204401
  0.39204401 0.         0.39204401]
 [0.         0.44943642 0.6316672  0.         0.         0.
  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara did it to me with this song
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['ciara', 'did', 'it', 'to', 'me', 'with', 'this', 'song']
cosine_similarity: 0.9752597212791443
train_input: [0.12536693798731732, 0.9752597], train_label: 0
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara look sexy af in this body party video lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.3174044  0.3174044  0.44610081 0.44610081 0.44610081
  0.         0.         0.3174044  0.         0.3174044 ]
 [0.40740124 0.28986934 0.28986934 0.         0.         0.
  0.40740124 0.40740124 0.28986934 0.40740124 0.28986934]]
pairwise_similarity: [[1.         0.36802321]
 [0.36802321 1.        ]]
cosine_similarity: 0.36802320875611494
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara look sexy af in this body party video lol
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['ciara', 'look', 'sexy', 'af', 'in', 'this', 'body', 'party', 'video', 'lol']
cosine_similarity: 0.9783259034156799
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara needs to teach me how to dance like her lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39204401 0.27894255 0.         0.39204401 0.39204401 0.39204401
  0.         0.         0.         0.39204401 0.         0.39204401]
 [0.         0.30321606 0.4261596  0.         0.         0.
  0.4261596  0.4261596  0.4261596  0.         0.4261596  0.        ]]
pairwise_similarity: [[1.         0.08457986]
 [0.08457986 1.        ]]
cosine_similarity: 0.0845798608014702
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara needs to teach me how to dance like her lol
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['ciara', 'needs', 'to', 'teach', 'me', 'how', 'to', 'dance', 'like', 'her', 'lol']
cosine_similarity: 0.9691665768623352
train_input: [0.0845798608014702, 0.9691666], train_label: 0
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara video for Body Party with her bf Future is actually kinda cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.33425073 0.33425073 0.         0.46977774
  0.33425073 0.46977774 0.         0.33425073 0.33425073]
 [0.39129369 0.39129369 0.27840869 0.27840869 0.39129369 0.
  0.27840869 0.         0.39129369 0.27840869 0.27840869]]
pairwise_similarity: [[1.         0.46529153]
 [0.46529153 1.        ]]
cosine_similarity: 0.4652915323370137
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara video for Body Party with her bf Future is actually kinda cute
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['ciara', 'video', 'for', 'body', 'party', 'with', 'her', 'bf', 'future', 'is', 'actually', 'kinda', 'cute']
cosine_similarity: 0.9906142354011536
train_input: [0.4652915323370137, 0.99061424], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara will get hot for like a second
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39204401 0.27894255 0.39204401 0.39204401 0.         0.39204401
  0.         0.39204401 0.         0.39204401]
 [0.         0.37997836 0.         0.         0.53404633 0.
  0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Ciara will get hot for like a second
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['ciara', 'will', 'get', 'hot', 'for', 'like', 'a', 'second']
cosine_similarity: 0.9797125458717346
train_input: [0.1059921313509325, 0.97971255], train_label: 0
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: I love ciara and future together
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.4078241  0.29017021 0.4078241  0.
  0.4078241  0.4078241 ]
 [0.         0.50154891 0.         0.50154891 0.         0.70490949
  0.         0.        ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: I love ciara and future together
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['i', 'love', 'ciara', 'and', 'future', 'together']
cosine_similarity: 0.9721991419792175
train_input: [0.2910691023819054, 0.97219914], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Im in love with Ciara Body Party Video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33425073 0.33425073 0.46977774 0.46977774 0.33425073 0.
  0.33425073 0.33425073]
 [0.37863221 0.37863221 0.         0.         0.37863221 0.53215436
  0.37863221 0.37863221]]
pairwise_similarity: [[1.         0.63279046]
 [0.63279046 1.        ]]
cosine_similarity: 0.6327904583679949
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Im in love with Ciara Body Party Video
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['im', 'in', 'love', 'with', 'ciara', 'body', 'party', 'video']
cosine_similarity: 0.9886704683303833
train_input: [0.6327904583679949, 0.98867047], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Im watching Ride by Ciara ft
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4078241  0.29017021 0.4078241  0.         0.4078241  0.29017021
  0.4078241  0.         0.4078241  0.        ]
 [0.         0.35520009 0.         0.49922133 0.         0.35520009
  0.         0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: Im watching Ride by Ciara ft
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['im', 'watching', 'ride', 'by', 'ciara', 'ft']
cosine_similarity: 0.9466896057128906
train_input: [0.20613696606828605, 0.9466896], train_label: 0
TF_IDF_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: remember wen perolawiberg use to dance like Ciara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.39204401 0.27894255 0.         0.39204401 0.39204401 0.39204401
  0.         0.39204401 0.         0.         0.         0.39204401
  0.        ]
 [0.         0.27894255 0.39204401 0.         0.         0.
  0.39204401 0.         0.39204401 0.39204401 0.39204401 0.
  0.39204401]]
pairwise_similarity: [[1.         0.07780894]
 [0.07780894 1.        ]]
cosine_similarity: 0.07780894359285004
word_to_vector_cosine_similarity: sentence1: Im so feeling Ciara video Body Party with future in it, sentence2: remember wen perolawiberg use to dance like Ciara
After tokenization, sentence1: ['im', 'so', 'feeling', 'ciara', 'video', 'body', 'party', 'with', 'future', 'in', 'it'], sentence2: ['remember', 'wen', 'use', 'to', 'dance', 'like', 'ciara']
cosine_similarity: 0.9711740016937256
train_input: [0.07780894359285004, 0.971174], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: All I got is one word for Ciara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.6316672  0.        ]
 [0.         0.44943642 0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: All I got is one word for Ciara
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['all', 'i', 'got', 'is', 'one', 'word', 'for', 'ciara']
cosine_similarity: 0.9548740983009338
train_input: [0.20199309249791833, 0.9548741], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Ciara Beyonce s body is like
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.50154891 0.         0.70490949]
 [0.57615236 0.40993715 0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Ciara Beyonce s body is like
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['ciara', 'beyonce', 's', 'body', 'is', 'like']
cosine_similarity: 0.9613629579544067
train_input: [0.4112070550676187, 0.96136296], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Future Ciara are cute together
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672 ]
 [0.         0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Future Ciara are cute together
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['future', 'ciara', 'are', 'cute', 'together']
cosine_similarity: 0.9252715706825256
train_input: [0.20199309249791833, 0.9252716], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Future always by Ciara side
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.6316672 ]
 [0.         0.57973867 0.81480247 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Future always by Ciara side
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['future', 'always', 'by', 'ciara', 'side']
cosine_similarity: 0.9466215372085571
train_input: [0.2605556710562624, 0.94662154], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Got damn that Ciara is Sexxyy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.6316672 ]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Got damn that Ciara is Sexxyy
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['got', 'damn', 'that', 'ciara', 'is', 'sexxyy']
cosine_similarity: 0.8789888024330139
train_input: [0.17077611319011649, 0.8789888], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: I didnt even know Ciara and Future were in a relationship
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.
  0.6316672 ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: I didnt even know Ciara and Future were in a relationship
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['i', 'didnt', 'even', 'know', 'ciara', 'and', 'future', 'were', 'in', 'a', 'relationship']
cosine_similarity: 0.9562298655509949
train_input: [0.15064018498706508, 0.95622987], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Jeepers but Ciara is crazy sexy cool
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.
  0.6316672 ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Jeepers but Ciara is crazy sexy cool
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['jeepers', 'but', 'ciara', 'is', 'crazy', 'sexy', 'cool']
cosine_similarity: 0.9007327556610107
train_input: [0.15064018498706508, 0.90073276], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Okay yall Ciara did that
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672  0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: Okay yall Ciara did that
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['okay', 'yall', 'ciara', 'did', 'that']
cosine_similarity: 0.8888048529624939
train_input: [0.17077611319011649, 0.88880485], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: The ciara song is playing in my head and im bouncing in this chair
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.         0.44943642 0.         0.
  0.         0.         0.6316672 ]
 [0.         0.39204401 0.39204401 0.27894255 0.39204401 0.39204401
  0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: The ciara song is playing in my head and im bouncing in this chair
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['the', 'ciara', 'song', 'is', 'playing', 'in', 'my', 'head', 'and', 'im', 'bouncing', 'in', 'this', 'chair']
cosine_similarity: 0.9509023427963257
train_input: [0.12536693798731732, 0.95090234], train_label: 0
TF_IDF_cosine_similarity: sentence1: ciara made a video to body part, sentence2: eff the Ciara or Rihanna question
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.6316672 ]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: ciara made a video to body part, sentence2: eff the Ciara or Rihanna question
After tokenization, sentence1: ['ciara', 'made', 'a', 'video', 'to', 'body', 'part'], sentence2: ['eff', 'the', 'ciara', 'or', 'rihanna', 'question']
cosine_similarity: 0.9503560066223145
train_input: [0.17077611319011649, 0.950356], train_label: 0
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Ciara Body Parts that chick is BADD
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.57973867 0.81480247 0.        ]
 [0.47107781 0.47107781 0.47107781 0.33517574 0.         0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Ciara Body Parts that chick is BADD
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['ciara', 'body', 'parts', 'that', 'chick', 'is', 'badd']
cosine_similarity: 0.9476739168167114
train_input: [0.19431434016858146, 0.9476739], train_label: 0
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Ciara body party do something to me I just be want dance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.81480247 0.         0.
  0.        ]
 [0.4261596  0.30321606 0.4261596  0.         0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Ciara body party do something to me I just be want dance
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['ciara', 'body', 'party', 'do', 'something', 'to', 'me', 'i', 'just', 'be', 'want', 'dance']
cosine_similarity: 0.9682366251945496
train_input: [0.17578607839334617, 0.9682366], train_label: 0
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Ciara s Body Party video is pretty nice
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.81480247 0.         0.         0.
  0.        ]
 [0.4261596  0.30321606 0.         0.4261596  0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Ciara s Body Party video is pretty nice
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['ciara', 's', 'body', 'party', 'video', 'is', 'pretty', 'nice']
cosine_similarity: 0.9608710408210754
train_input: [0.17578607839334617, 0.96087104], train_label: 0
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Ciara will get hot for like a second
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.        ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Ciara will get hot for like a second
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['ciara', 'will', 'get', 'hot', 'for', 'like', 'a', 'second']
cosine_similarity: 0.9709988236427307
train_input: [0.22028815056182965, 0.9709988], train_label: 0
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: I give Ciara ah 10
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57973867 0.81480247]
 [0.6316672  0.6316672  0.44943642 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: I give Ciara ah 10
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['i', 'give', 'ciara', 'ah']
cosine_similarity: 0.9295936822891235
train_input: [0.2605556710562624, 0.9295937], train_label: 0
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: I will repeat Ciara and future are so cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.         0.70710678 0.        ]
 [0.40993715 0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: I will repeat Ciara and future are so cute
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['i', 'will', 'repeat', 'ciara', 'and', 'future', 'are', 'so', 'cute']
cosine_similarity: 0.9915553331375122
train_input: [0.5797386715376657, 0.99155533], train_label: 1
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Kelly Rowland and Ciara need to make a sexy song together
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.         0.
  0.         0.        ]
 [0.27894255 0.         0.39204401 0.39204401 0.39204401 0.39204401
  0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.16171378]
 [0.16171378 1.        ]]
cosine_similarity: 0.16171378066252898
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Kelly Rowland and Ciara need to make a sexy song together
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['kelly', 'rowland', 'and', 'ciara', 'need', 'to', 'make', 'a', 'sexy', 'song', 'together']
cosine_similarity: 0.9768590927124023
train_input: [0.16171378066252898, 0.9768591], train_label: 0
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Yes for Ciara making the countdown
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.81480247 0.         0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: Yes for Ciara making the countdown
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['yes', 'for', 'ciara', 'making', 'the', 'countdown']
cosine_similarity: 0.9723555445671082
train_input: [0.22028815056182965, 0.97235554], train_label: 0
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: i need a lapdance to this ciara song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.        ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: i need a lapdance to this ciara song
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['i', 'need', 'a', 'lapdance', 'to', 'this', 'ciara', 'song']
cosine_similarity: 0.9652778506278992
train_input: [0.22028815056182965, 0.96527785], train_label: 0
TF_IDF_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: you mad your not in that video with Ciara cause future is
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.70710678 0.70710678 0.         0.        ]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.50232878]
 [0.50232878 1.        ]]
cosine_similarity: 0.5023287782256717
word_to_vector_cosine_similarity: sentence1: So is Ciara and Future together, sentence2: you mad your not in that video with Ciara cause future is
After tokenization, sentence1: ['so', 'is', 'ciara', 'and', 'future', 'together'], sentence2: ['you', 'mad', 'your', 'not', 'in', 'that', 'video', 'with', 'ciara', 'cause', 'future', 'is']
cosine_similarity: 0.9853286147117615
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ciara Future showed out for this Body Party video lol I love it bro
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.44832087 0.         0.63009934 0.
  0.         0.44832087 0.         0.        ]
 [0.25948224 0.36469323 0.25948224 0.36469323 0.         0.36469323
  0.36469323 0.25948224 0.36469323 0.36469323]]
pairwise_similarity: [[1.         0.34899391]
 [0.34899391 1.        ]]
cosine_similarity: 0.3489939079552687
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ciara Future showed out for this Body Party video lol I love it bro
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['ciara', 'future', 'showed', 'out', 'for', 'this', 'body', 'party', 'video', 'lol', 'i', 'love', 'it', 'bro']
cosine_similarity: 0.9755367636680603
train_input: [0.3489939079552687, 0.97553676], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ciara be dancin her ass off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.         0.53404633 0.53404633]
 [0.6316672  0.         0.44943642 0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ciara be dancin her ass off
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['ciara', 'be', 'dancin', 'her', 'ass', 'off']
cosine_similarity: 0.9055163264274597
train_input: [0.17077611319011649, 0.9055163], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ciara be having the best videos
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ciara be having the best videos
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['ciara', 'be', 'having', 'the', 'best', 'videos']
cosine_similarity: 0.957179605960846
train_input: [0.1443835552773867, 0.9571796], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ciara took over twitter with this body party video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.44832087 0.         0.
  0.        ]
 [0.33471228 0.33471228 0.         0.33471228 0.47042643 0.47042643
  0.47042643]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ciara took over twitter with this body party video
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['ciara', 'took', 'over', 'twitter', 'with', 'this', 'body', 'party', 'video']
cosine_similarity: 0.9743337035179138
train_input: [0.4501755023269898, 0.9743337], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Dang Ciara doin em like that now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.         0.         0.53404633
  0.         0.53404633]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Dang Ciara doin em like that now
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['dang', 'ciara', 'doin', 'em', 'like', 'that', 'now']
cosine_similarity: 0.9369035959243774
train_input: [0.1273595297947935, 0.9369036], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: I was waiting for Ciara body party and its
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.44832087 0.        ]
 [0.44832087 0.44832087 0.         0.44832087 0.63009934]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: I was waiting for Ciara body party and its
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['i', 'was', 'waiting', 'for', 'ciara', 'body', 'party', 'and', 'its']
cosine_similarity: 0.9744511246681213
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ok 1future and ciara I love the video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.53404633 0.        ]
 [0.47107781 0.         0.33517574 0.         0.47107781 0.47107781
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Ok 1future and ciara I love the video
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['ok', 'and', 'ciara', 'i', 'love', 'the', 'video']
cosine_similarity: 0.9691590666770935
train_input: [0.1273595297947935, 0.96915907], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: So is ciara and rebecca and Mike mairs lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.
  0.53404633 0.        ]
 [0.         0.33517574 0.         0.47107781 0.47107781 0.47107781
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: So is ciara and rebecca and Mike mairs lol
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['so', 'is', 'ciara', 'and', 'rebecca', 'and', 'mike', 'lol']
cosine_similarity: 0.9648478031158447
train_input: [0.1273595297947935, 0.9648478], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Why isnt Ciara A Megastar Yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]
 [0.         0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: Why isnt Ciara A Megastar Yet
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['why', 'isnt', 'ciara', 'a', 'megastar', 'yet']
cosine_similarity: 0.9550360441207886
train_input: [0.17077611319011649, 0.95503604], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: im guessing Future still aint seen Ciara toes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.         0.53404633 0.
  0.         0.53404633 0.         0.        ]
 [0.39204401 0.         0.27894255 0.39204401 0.         0.39204401
  0.39204401 0.         0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Ciara Body Party is growing on me, sentence2: im guessing Future still aint seen Ciara toes
After tokenization, sentence1: ['ciara', 'body', 'party', 'is', 'growing', 'on', 'me'], sentence2: ['im', 'guessing', 'future', 'still', 'aint', 'seen', 'ciara', 'toes']
cosine_similarity: 0.9239364862442017
train_input: [0.1059921313509325, 0.9239365], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Am so diggin that body party vid ciara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70819948 0.35409974 0.         0.35409974 0.         0.49767483]
 [0.37930349 0.37930349 0.53309782 0.37930349 0.53309782 0.        ]]
pairwise_similarity: [[1.         0.53724508]
 [0.53724508 1.        ]]
cosine_similarity: 0.5372450751597531
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Am so diggin that body party vid ciara
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['am', 'so', 'diggin', 'that', 'body', 'party', 'vid', 'ciara']
cosine_similarity: 0.9537434577941895
train_input: [0.5372450751597531, 0.95374346], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Ciara in that video tho smh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81564821 0.29017021 0.4078241  0.         0.         0.29017021]
 [0.         0.40993715 0.         0.57615236 0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Ciara in that video tho smh
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['ciara', 'in', 'that', 'video', 'tho', 'smh']
cosine_similarity: 0.9520443081855774
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Hollldddd on My boyfriend and Ciara go out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.78408803 0.         0.27894255 0.         0.39204401 0.39204401]
 [0.         0.6316672  0.44943642 0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Hollldddd on My boyfriend and Ciara go out
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['on', 'my', 'boyfriend', 'and', 'ciara', 'go', 'out']
cosine_similarity: 0.9581791758537292
train_input: [0.12536693798731732, 0.9581792], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I Cant Believe Future Ciara Really Go Together
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.78408803 0.27894255 0.         0.39204401 0.
  0.39204401]
 [0.53404633 0.         0.37997836 0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I Cant Believe Future Ciara Really Go Together
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['i', 'cant', 'believe', 'future', 'ciara', 'really', 'go', 'together']
cosine_similarity: 0.9218491315841675
train_input: [0.1059921313509325, 0.92184913], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I asked who the guy in Ciara s video was
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.81564821 0.29017021 0.         0.4078241  0.29017021]
 [0.57615236 0.         0.40993715 0.57615236 0.         0.40993715]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I asked who the guy in Ciara s video was
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['i', 'asked', 'who', 'the', 'guy', 'in', 'ciara', 's', 'video', 'was']
cosine_similarity: 0.9683162569999695
train_input: [0.23790309463326234, 0.96831626], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I knw da way ciara dance she cuts up in da bed
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.78408803 0.27894255 0.         0.         0.
  0.         0.39204401 0.39204401 0.        ]
 [0.32433627 0.         0.23076793 0.32433627 0.64867255 0.32433627
  0.32433627 0.         0.         0.32433627]]
pairwise_similarity: [[1.         0.06437099]
 [0.06437099 1.        ]]
cosine_similarity: 0.06437099366532754
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I knw da way ciara dance she cuts up in da bed
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['i', 'knw', 'da', 'way', 'ciara', 'dance', 'she', 'cuts', 'up', 'in', 'da', 'bed']
cosine_similarity: 0.9407610893249512
train_input: [0.06437099366532754, 0.9407611], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I like Ciara and future as a couple
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.78408803 0.27894255 0.         0.         0.         0.39204401
  0.39204401]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: I like Ciara and future as a couple
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['i', 'like', 'ciara', 'and', 'future', 'as', 'a', 'couple']
cosine_similarity: 0.9553729891777039
train_input: [0.1059921313509325, 0.955373], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Really diggin ciara body part video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70819948 0.35409974 0.         0.49767483 0.         0.35409974]
 [0.37930349 0.37930349 0.53309782 0.         0.53309782 0.37930349]]
pairwise_similarity: [[1.         0.53724508]
 [0.53724508 1.        ]]
cosine_similarity: 0.5372450751597531
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: Really diggin ciara body part video
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['really', 'diggin', 'ciara', 'body', 'part', 'video']
cosine_similarity: 0.9645302891731262
train_input: [0.5372450751597531, 0.9645303], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: WUNM is Ciara Hendrix DOWNE though
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.78408803 0.27894255 0.         0.         0.39204401 0.39204401
  0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.
  0.53404633]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: WUNM is Ciara Hendrix DOWNE though
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['is', 'ciara', 'hendrix', 'downe', 'though']
cosine_similarity: 0.8472021222114563
train_input: [0.1059921313509325, 0.8472021], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: why is Ciara gettin it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.78408803 0.27894255 0.         0.39204401 0.39204401]
 [0.         0.57973867 0.81480247 0.         0.        ]]
pairwise_similarity: [[1.         0.16171378]
 [0.16171378 1.        ]]
cosine_similarity: 0.16171378066252898
word_to_vector_cosine_similarity: sentence1: Ciara body in the Body Party video, sentence2: why is Ciara gettin it
After tokenization, sentence1: ['ciara', 'body', 'in', 'the', 'body', 'party', 'video'], sentence2: ['why', 'is', 'ciara', 'gettin', 'it']
cosine_similarity: 0.9258499145507812
train_input: [0.16171378066252898, 0.9258499], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Ciara Future showed out for this Body Party video lol I love it bro
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.         0.5        0.         0.         0.
  0.5        0.         0.5       ]
 [0.26844636 0.37729199 0.26844636 0.37729199 0.37729199 0.37729199
  0.26844636 0.37729199 0.26844636]]
pairwise_similarity: [[1.         0.53689271]
 [0.53689271 1.        ]]
cosine_similarity: 0.5368927118515179
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Ciara Future showed out for this Body Party video lol I love it bro
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['ciara', 'future', 'showed', 'out', 'for', 'this', 'body', 'party', 'video', 'lol', 'i', 'love', 'it', 'bro']
cosine_similarity: 0.9862993359565735
train_input: [0.5368927118515179, 0.98629934], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Ciara is just the baddest female dancer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.         0.         0.
  0.53404633 0.53404633]
 [0.47107781 0.         0.33517574 0.47107781 0.47107781 0.47107781
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Ciara is just the baddest female dancer
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['ciara', 'is', 'just', 'the', 'baddest', 'female', 'dancer']
cosine_similarity: 0.9545043706893921
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Everytime I watch Ciara video I be all over here dancing and stuff
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.         0.57615236 0.
  0.40993715 0.        ]
 [0.         0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Everytime I watch Ciara video I be all over here dancing and stuff
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['everytime', 'i', 'watch', 'ciara', 'video', 'i', 'be', 'all', 'over', 'here', 'dancing', 'and', 'stuff']
cosine_similarity: 0.9852930307388306
train_input: [0.2605556710562624, 0.98529303], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: I knw da way ciara dance she cuts up in da bed
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.         0.         0.
  0.         0.53404633 0.53404633 0.        ]
 [0.32433627 0.         0.23076793 0.32433627 0.64867255 0.32433627
  0.32433627 0.         0.         0.32433627]]
pairwise_similarity: [[1.         0.08768682]
 [0.08768682 1.        ]]
cosine_similarity: 0.08768681980142445
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: I knw da way ciara dance she cuts up in da bed
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['i', 'knw', 'da', 'way', 'ciara', 'dance', 'she', 'cuts', 'up', 'in', 'da', 'bed']
cosine_similarity: 0.9518298506736755
train_input: [0.08768681980142445, 0.95182985], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: I really do like Ciara new video she just left me wanting more though
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.         0.         0.
  0.57615236 0.         0.40993715 0.        ]
 [0.         0.26868528 0.37762778 0.37762778 0.37762778 0.37762778
  0.         0.37762778 0.26868528 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: I really do like Ciara new video she just left me wanting more though
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['i', 'really', 'do', 'like', 'ciara', 'new', 'video', 'she', 'just', 'left', 'me', 'wanting', 'more', 'though']
cosine_similarity: 0.9724331498146057
train_input: [0.2202881505618297, 0.97243315], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Is how Ciara wining that waist for future
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.53404633 0.
  0.        ]
 [0.         0.37997836 0.53404633 0.         0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Is how Ciara wining that waist for future
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['is', 'how', 'ciara', 'wining', 'that', 'waist', 'for', 'future']
cosine_similarity: 0.9631844162940979
train_input: [0.1443835552773867, 0.9631844], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: My new song body party by ciara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.         0.44832087 0.         0.63009934]
 [0.37930349 0.37930349 0.53309782 0.37930349 0.53309782 0.        ]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: My new song body party by ciara
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['my', 'new', 'song', 'body', 'party', 'by', 'ciara']
cosine_similarity: 0.9778669476509094
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Only if oomf can dance like Ciara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.         0.         0.53404633
  0.53404633]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: Only if oomf can dance like Ciara
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['only', 'if', 'oomf', 'can', 'dance', 'like', 'ciara']
cosine_similarity: 0.9577100872993469
train_input: [0.1443835552773867, 0.9577101], train_label: 0
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: See Ciara wanna watch that shit too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.53404633 0.
  0.        ]
 [0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: See Ciara wanna watch that shit too
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['see', 'ciara', 'wanna', 'watch', 'that', 'shit', 'too']
cosine_similarity: 0.956189751625061
train_input: [0.1443835552773867, 0.95618975], train_label: 1
TF_IDF_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: im in love with Ciara s video to Body Party
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.5        0.         0.         0.5        0.5       ]
 [0.35464863 0.35464863 0.49844628 0.49844628 0.35464863 0.35464863]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062737
word_to_vector_cosine_similarity: sentence1: Ciara in that Body Party video though, sentence2: im in love with Ciara s video to Body Party
After tokenization, sentence1: ['ciara', 'in', 'that', 'body', 'party', 'video', 'though'], sentence2: ['im', 'in', 'love', 'with', 'ciara', 's', 'video', 'to', 'body', 'party']
cosine_similarity: 0.9835255742073059
train_input: [0.7092972666062737, 0.9835256], train_label: 1
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Aye that bitch Ciara bad asf yo that new video is HOT
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.         0.57615236 0.40993715
  0.57615236 0.         0.         0.40993715 0.        ]
 [0.35327777 0.35327777 0.35327777 0.35327777 0.         0.25136004
  0.         0.35327777 0.35327777 0.25136004 0.35327777]]
pairwise_similarity: [[1.         0.20608364]
 [0.20608364 1.        ]]
cosine_similarity: 0.20608363501393823
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Aye that bitch Ciara bad asf yo that new video is HOT
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['aye', 'that', 'bitch', 'ciara', 'bad', 'asf', 'yo', 'that', 'new', 'video', 'is', 'hot']
cosine_similarity: 0.9525507688522339
train_input: [0.20608363501393823, 0.95255077], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: But I Thought KeriHilson took Ciara Place
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.
  0.         0.53404633]
 [0.         0.33517574 0.         0.47107781 0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: But I Thought KeriHilson took Ciara Place
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['but', 'i', 'thought', 'took', 'ciara', 'place']
cosine_similarity: 0.9596412181854248
train_input: [0.1273595297947935, 0.9596412], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Ciara Beyonce s body is like
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.         0.57615236]
 [0.57615236 0.40993715 0.40993715 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Ciara Beyonce s body is like
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['ciara', 'beyonce', 's', 'body', 'is', 'like']
cosine_similarity: 0.9754902720451355
train_input: [0.3360969272762575, 0.9754903], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Ciara s new music video lawwwwdddd
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.         0.         0.
  0.40993715]
 [0.         0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Ciara s new music video lawwwwdddd
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['ciara', 's', 'new', 'music', 'video']
cosine_similarity: 0.9187174439430237
train_input: [0.29121941856368966, 0.91871744], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Ciara video on so it s a MUST that I watch it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.57615236 0.40993715 0.        ]
 [0.         0.50154891 0.         0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Ciara video on so it s a MUST that I watch it
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['ciara', 'video', 'on', 'so', 'it', 's', 'a', 'must', 'that', 'i', 'watch', 'it']
cosine_similarity: 0.975785493850708
train_input: [0.4112070550676187, 0.9757855], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Future Ciara was so cute in her video
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.         0.57615236 0.40993715]
 [0.         0.40993715 0.57615236 0.57615236 0.         0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Future Ciara was so cute in her video
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['future', 'ciara', 'was', 'so', 'cute', 'in', 'her', 'video']
cosine_similarity: 0.9785537719726562
train_input: [0.3360969272762575, 0.9785538], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: I fucks with Ciara new jawn
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.         0.
  0.53404633]
 [0.         0.37997836 0.53404633 0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: I fucks with Ciara new jawn
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['i', 'fucks', 'with', 'ciara', 'new', 'jawn']
cosine_similarity: 0.9432598948478699
train_input: [0.1443835552773867, 0.9432599], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Ill do the dance ciara did for future in her video on set lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.         0.         0.57615236
  0.         0.         0.         0.40993715]
 [0.         0.26868528 0.37762778 0.37762778 0.37762778 0.
  0.37762778 0.37762778 0.37762778 0.26868528]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Ill do the dance ciara did for future in her video on set lol
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['ill', 'do', 'the', 'dance', 'ciara', 'did', 'for', 'future', 'in', 'her', 'video', 'on', 'set', 'lol']
cosine_similarity: 0.9813213348388672
train_input: [0.2202881505618297, 0.98132133], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Just watched the music video to Body Party by Ciara and she killed it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.         0.
  0.         0.44832087 0.        ]
 [0.27867523 0.27867523 0.         0.39166832 0.39166832 0.39166832
  0.39166832 0.27867523 0.39166832]]
pairwise_similarity: [[1.         0.37480777]
 [0.37480777 1.        ]]
cosine_similarity: 0.3748077700589726
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: Just watched the music video to Body Party by Ciara and she killed it
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['just', 'watched', 'the', 'music', 'video', 'to', 'body', 'party', 'by', 'ciara', 'and', 'she', 'killed', 'it']
cosine_similarity: 0.9893815517425537
train_input: [0.3748077700589726, 0.98938155], train_label: 0
TF_IDF_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: So Future and Ciara go together
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.53404633]
 [0.         0.57973867 0.81480247 0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Why is hitmansteviej in the Ciara Body video, sentence2: So Future and Ciara go together
After tokenization, sentence1: ['why', 'is', 'in', 'the', 'ciara', 'body', 'video'], sentence2: ['so', 'future', 'and', 'ciara', 'go', 'together']
cosine_similarity: 0.9560664892196655
train_input: [0.22028815056182965, 0.9560665], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Break time and Cinderella is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.         0.30321606 0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]
 [0.         0.6316672  0.44943642 0.         0.         0.
  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Break time and Cinderella is on
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['break', 'time', 'and', 'cinderella', 'is', 'on']
cosine_similarity: 0.9335117936134338
train_input: [0.1362763414390864, 0.9335118], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Cinderella Peter Pan and the Lion King are all on TV today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.30321606 0.4261596  0.4261596  0.         0.
  0.4261596  0.         0.         0.         0.         0.4261596 ]
 [0.         0.27894255 0.         0.         0.39204401 0.39204401
  0.         0.39204401 0.39204401 0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.08457986]
 [0.08457986 1.        ]]
cosine_similarity: 0.0845798608014702
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Cinderella Peter Pan and the Lion King are all on TV today
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['cinderella', 'peter', 'pan', 'and', 'the', 'lion', 'king', 'are', 'all', 'on', 'tv', 'today']
cosine_similarity: 0.9363649487495422
train_input: [0.0845798608014702, 0.93636495], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Cinderella is on and Peter Pan is on next
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.30321606 0.4261596  0.4261596  0.4261596  0.
  0.         0.4261596 ]
 [0.         0.44943642 0.         0.         0.         0.6316672
  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Cinderella is on and Peter Pan is on next
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next']
cosine_similarity: 0.9281203746795654
train_input: [0.1362763414390864, 0.9281204], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Currently watching Cinderella then Peter Pan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.         0.         0.31779954]
 [0.         0.35520009 0.         0.49922133 0.         0.
  0.49922133 0.49922133 0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Currently watching Cinderella then Peter Pan
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['currently', 'watching', 'cinderella', 'then', 'peter', 'pan']
cosine_similarity: 0.9189439415931702
train_input: [0.22576484600261604, 0.91894394], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Im really watching Cinderella the cartoon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.         0.31779954 0.44665616 0.44665616 0.
  0.44665616 0.         0.31779954]
 [0.         0.49922133 0.35520009 0.         0.         0.49922133
  0.         0.49922133 0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Im really watching Cinderella the cartoon
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['im', 'really', 'watching', 'cinderella', 'the', 'cartoon']
cosine_similarity: 0.9403252601623535
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Its beautiful outside BUT Cinderella is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.         0.30321606 0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]
 [0.         0.6316672  0.44943642 0.         0.         0.
  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Its beautiful outside BUT Cinderella is on
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['its', 'beautiful', 'outside', 'but', 'cinderella', 'is', 'on']
cosine_similarity: 0.9315084218978882
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Loving the fact that Cinderella is on right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.30321606 0.4261596  0.4261596  0.         0.4261596
  0.         0.         0.4261596 ]
 [0.         0.37997836 0.         0.         0.53404633 0.
  0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Loving the fact that Cinderella is on right now
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['loving', 'the', 'fact', 'that', 'cinderella', 'is', 'on', 'right', 'now']
cosine_similarity: 0.9338958263397217
train_input: [0.11521554337793122, 0.9338958], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Spending the rest of my day at home watching cinderella peterpan thelionking
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.         0.44665616 0.
  0.44665616 0.         0.         0.         0.         0.31779954]
 [0.         0.26868528 0.         0.37762778 0.         0.37762778
  0.         0.37762778 0.37762778 0.37762778 0.37762778 0.26868528]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.1707761131901165
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Spending the rest of my day at home watching cinderella peterpan thelionking
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['spending', 'the', 'rest', 'of', 'my', 'day', 'at', 'home', 'watching', 'cinderella', 'peterpan', 'thelionking']
cosine_similarity: 0.9424846768379211
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: The original Cinderella is on which calls for a study break
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4261596  0.         0.         0.30321606 0.4261596  0.4261596
  0.4261596  0.         0.         0.4261596 ]
 [0.         0.47107781 0.47107781 0.33517574 0.         0.
  0.         0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: The original Cinderella is on which calls for a study break
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['the', 'original', 'cinderella', 'is', 'on', 'which', 'calls', 'for', 'a', 'study', 'break']
cosine_similarity: 0.9155946969985962
train_input: [0.10163066979112656, 0.9155947], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Watching Cinderella with my little girl
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44665616 0.31779954 0.44665616 0.44665616 0.         0.
  0.44665616 0.31779954]
 [0.         0.40993715 0.         0.         0.57615236 0.57615236
  0.         0.40993715]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Watching Cinderella on ABCFamily Love the Disney classics, sentence2: Watching Cinderella with my little girl
After tokenization, sentence1: ['watching', 'cinderella', 'on', 'abcfamily', 'love', 'the', 'disney', 'classics'], sentence2: ['watching', 'cinderella', 'with', 'my', 'little', 'girl']
cosine_similarity: 0.9363842606544495
train_input: [0.2605556710562624, 0.93638426], train_label: 1
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: I am certainly not watching Cinderella and wishing for a
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.47107781 0.47107781 0.47107781
  0.         0.        ]
 [0.         0.53404633 0.37997836 0.         0.         0.
  0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: I am certainly not watching Cinderella and wishing for a
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['i', 'am', 'certainly', 'not', 'watching', 'cinderella', 'and', 'wishing', 'for', 'a']
cosine_similarity: 0.9786465167999268
train_input: [0.1273595297947935, 0.9786465], train_label: 0
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: I am now watching Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.47107781 0.47107781 0.        ]
 [0.         0.57973867 0.         0.         0.         0.81480247]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: I am now watching Cinderella
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['i', 'am', 'now', 'watching', 'cinderella']
cosine_similarity: 0.9634702801704407
train_input: [0.19431434016858146, 0.9634703], train_label: 1
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: I forgot how much I hated the car from Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.         0.         0.47107781
  0.47107781 0.47107781]
 [0.         0.53404633 0.37997836 0.53404633 0.53404633 0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: I forgot how much I hated the car from Cinderella
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['i', 'forgot', 'how', 'much', 'i', 'hated', 'the', 'car', 'from', 'cinderella']
cosine_similarity: 0.978287398815155
train_input: [0.1273595297947935, 0.9782874], train_label: 0
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: I wish I was Cinderella for a day
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.         0.47107781 0.47107781 0.47107781
  0.        ]
 [0.         0.44943642 0.6316672  0.         0.         0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: I wish I was Cinderella for a day
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['i', 'wish', 'i', 'was', 'cinderella', 'for', 'a', 'day']
cosine_similarity: 0.9755240082740784
train_input: [0.15064018498706508, 0.975524], train_label: 0
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: Nothing like laying in bed watching Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.         0.         0.47107781
  0.47107781 0.47107781 0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.47107781 0.
  0.         0.         0.47107781]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: Nothing like laying in bed watching Cinderella
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['nothing', 'like', 'laying', 'in', 'bed', 'watching', 'cinderella']
cosine_similarity: 0.9668244123458862
train_input: [0.11234277891542777, 0.9668244], train_label: 1
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: Omg my favorite movie Cinderella is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.         0.         0.         0.47107781
  0.47107781 0.47107781]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633 0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: Omg my favorite movie Cinderella is on
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['omg', 'my', 'favorite', 'movie', 'cinderella', 'is', 'on']
cosine_similarity: 0.9450674057006836
train_input: [0.1273595297947935, 0.9450674], train_label: 1
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: The REAL Cinderella movie never fails to put a smile on my face
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.         0.         0.         0.
  0.47107781 0.47107781 0.         0.47107781]
 [0.         0.30321606 0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.         0.4261596  0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: The REAL Cinderella movie never fails to put a smile on my face
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['the', 'real', 'cinderella', 'movie', 'never', 'fails', 'to', 'put', 'a', 'smile', 'on', 'my', 'face']
cosine_similarity: 0.977623701095581
train_input: [0.10163066979112656, 0.9776237], train_label: 0
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: Totally watching Cinderella right and Im just saying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.         0.         0.         0.47107781
  0.         0.47107781 0.47107781 0.         0.        ]
 [0.         0.27894255 0.39204401 0.39204401 0.39204401 0.
  0.39204401 0.         0.         0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536716
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: Totally watching Cinderella right and Im just saying
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['totally', 'watching', 'cinderella', 'right', 'and', 'im', 'just', 'saying']
cosine_similarity: 0.9663956165313721
train_input: [0.09349477497536716, 0.9663956], train_label: 1
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: can i just be Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.         0.47107781 0.47107781 0.47107781]
 [0.         0.57973867 0.81480247 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: can i just be Cinderella
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['can', 'i', 'just', 'be', 'cinderella']
cosine_similarity: 0.961484968662262
train_input: [0.19431434016858146, 0.96148497], train_label: 0
TF_IDF_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: cinderella is on aw i love princess movies so much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.33517574 0.         0.         0.
  0.47107781 0.47107781 0.47107781]
 [0.47107781 0.         0.33517574 0.47107781 0.47107781 0.47107781
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: I rushed taking a shower bc Cinderella is on, sentence2: cinderella is on aw i love princess movies so much
After tokenization, sentence1: ['i', 'rushed', 'taking', 'a', 'shower', 'bc', 'cinderella', 'is', 'on'], sentence2: ['cinderella', 'is', 'on', 'aw', 'i', 'love', 'princess', 'movies', 'so', 'much']
cosine_similarity: 0.9536960124969482
train_input: [0.11234277891542777, 0.953696], train_label: 1
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Aint nobody got time to watch the Miami Heat when Cinderella is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.57615236 0.57615236
  0.         0.         0.40993715]
 [0.4078241  0.29017021 0.4078241  0.4078241  0.         0.
  0.4078241  0.4078241  0.29017021]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Aint nobody got time to watch the Miami Heat when Cinderella is on
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['aint', 'nobody', 'got', 'time', 'to', 'watch', 'the', 'miami', 'heat', 'when', 'cinderella', 'is', 'on']
cosine_similarity: 0.9852507710456848
train_input: [0.23790309463326234, 0.9852508], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: CINDERELLA PETER PAN AND THE LION KING ARE BACK TO BACK ON ABC FAMILY
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.53404633 0.
  0.         0.         0.         0.53404633]
 [0.39204401 0.27894255 0.39204401 0.         0.         0.39204401
  0.39204401 0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: CINDERELLA PETER PAN AND THE LION KING ARE BACK TO BACK ON ABC FAMILY
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['cinderella', 'peter', 'pan', 'and', 'the', 'lion', 'king', 'are', 'back', 'to', 'back', 'on', 'abc', 'family']
cosine_similarity: 0.9703859090805054
train_input: [0.1059921313509325, 0.9703859], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Cinderella Peter Pan and Lion King all back to back to back
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.         0.53404633]
 [0.33517574 0.         0.         0.47107781 0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Cinderella Peter Pan and Lion King all back to back to back
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['cinderella', 'peter', 'pan', 'and', 'lion', 'king', 'all', 'back', 'to', 'back', 'to', 'back']
cosine_similarity: 0.97103351354599
train_input: [0.1273595297947935, 0.9710335], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Cinderella is on and I feel like a kid again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.         0.
  0.53404633]
 [0.37997836 0.53404633 0.         0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Cinderella is on and I feel like a kid again
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['cinderella', 'is', 'on', 'and', 'i', 'feel', 'like', 'a', 'kid', 'again']
cosine_similarity: 0.9904122352600098
train_input: [0.1443835552773867, 0.99041224], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Even though the movie Cinderella is animated the Prince is still pretty handsome
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.53404633 0.
  0.         0.         0.53404633]
 [0.4261596  0.30321606 0.4261596  0.         0.         0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Even though the movie Cinderella is animated the Prince is still pretty handsome
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['even', 'though', 'the', 'movie', 'cinderella', 'is', 'animated', 'the', 'prince', 'is', 'still', 'pretty', 'handsome']
cosine_similarity: 0.9694123268127441
train_input: [0.11521554337793122, 0.9694123], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Having my brother record Peter Pan Cinderella and the lion king for me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.53404633 0.
  0.         0.         0.         0.         0.53404633]
 [0.36499647 0.25969799 0.36499647 0.         0.         0.36499647
  0.36499647 0.36499647 0.36499647 0.36499647 0.        ]]
pairwise_similarity: [[1.         0.09867962]
 [0.09867962 1.        ]]
cosine_similarity: 0.09867961797986956
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: Having my brother record Peter Pan Cinderella and the lion king for me
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['having', 'my', 'brother', 'record', 'peter', 'pan', 'cinderella', 'and', 'the', 'lion', 'king', 'for', 'me']
cosine_similarity: 0.9614434242248535
train_input: [0.09867961797986956, 0.9614434], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: How ironic and Cinderella is on TV too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.53404633]
 [0.44943642 0.         0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: How ironic and Cinderella is on TV too
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['how', 'ironic', 'and', 'cinderella', 'is', 'on', 'tv', 'too']
cosine_similarity: 0.9888932704925537
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: aw A Cinderella Story is my fave
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.53404633 0.
  0.53404633]
 [0.53404633 0.37997836 0.53404633 0.         0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: aw A Cinderella Story is my fave
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['aw', 'a', 'cinderella', 'story', 'is', 'my', 'fave']
cosine_similarity: 0.9457398653030396
train_input: [0.1443835552773867, 0.94573987], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: cinderella is on abc family my favorite princess i
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.         0.53404633 0.53404633
  0.         0.53404633]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.         0.
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: cinderella is on abc family my favorite princess i
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['cinderella', 'is', 'on', 'abc', 'family', 'my', 'favorite', 'princess', 'i']
cosine_similarity: 0.9693742394447327
train_input: [0.1273595297947935, 0.96937424], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: watching cinderella with the famm
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]
 [0.44943642 0.6316672  0.         0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Im not home to watch it, sentence2: watching cinderella with the famm
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'im', 'not', 'home', 'to', 'watch', 'it'], sentence2: ['watching', 'cinderella', 'with', 'the', 'famm']
cosine_similarity: 0.9422718286514282
train_input: [0.17077611319011649, 0.9422718], train_label: 0
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: CINDERELLA IS ON OH MY CHILDHOOD RIGHT IN THE FEELS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.57615236 0.         0.40993715
  0.57615236]
 [0.49922133 0.35520009 0.49922133 0.         0.49922133 0.35520009
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: CINDERELLA IS ON OH MY CHILDHOOD RIGHT IN THE FEELS
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['cinderella', 'is', 'on', 'oh', 'my', 'childhood', 'right', 'in', 'the', 'feels']
cosine_similarity: 0.9629460573196411
train_input: [0.29121941856368966, 0.96294606], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Cinderella I bet he has your ugly step sisters on the side
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.53404633 0.         0.
  0.         0.53404633]
 [0.47107781 0.33517574 0.         0.         0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Cinderella I bet he has your ugly step sisters on the side
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['cinderella', 'i', 'bet', 'he', 'has', 'your', 'ugly', 'step', 'sisters', 'on', 'the', 'side']
cosine_similarity: 0.94891357421875
train_input: [0.1273595297947935, 0.9489136], train_label: 0
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: I had a dream about Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.53404633]
 [0.57973867 0.81480247 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: I had a dream about Cinderella
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['i', 'had', 'a', 'dream', 'about', 'cinderella']
cosine_similarity: 0.9488440752029419
train_input: [0.22028815056182965, 0.9488441], train_label: 0
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Im so watching Cinderella right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5 0.5 0.5 0.5]
 [0.5 0.5 0.5 0.5]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 1.0
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Im so watching Cinderella right now
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['im', 'so', 'watching', 'cinderella', 'right', 'now']
cosine_similarity: 1.0
train_input: [1.0, 1.0], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Is Cinderella really on TV right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.40993715 0.         0.57615236]
 [0.40993715 0.         0.57615236 0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Is Cinderella really on TV right now
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['is', 'cinderella', 'really', 'on', 'tv', 'right', 'now']
cosine_similarity: 0.9721778631210327
train_input: [0.3360969272762575, 0.97217786], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: My girl Cinderella is on abc family so you know Im not studying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.         0.40993715 0.
  0.57615236 0.         0.57615236]
 [0.4078241  0.29017021 0.4078241  0.4078241  0.29017021 0.4078241
  0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: My girl Cinderella is on abc family so you know Im not studying
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['my', 'girl', 'cinderella', 'is', 'on', 'abc', 'family', 'so', 'you', 'know', 'im', 'not', 'studying']
cosine_similarity: 0.9828171730041504
train_input: [0.23790309463326234, 0.9828172], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Should I pretend like Im not watching Cinderella on abc family
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44832087 0.         0.44832087 0.         0.
  0.63009934 0.44832087]
 [0.42567716 0.30287281 0.42567716 0.30287281 0.42567716 0.42567716
  0.         0.30287281]]
pairwise_similarity: [[1.        0.4073526]
 [0.4073526 1.       ]]
cosine_similarity: 0.4073526042885674
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Should I pretend like Im not watching Cinderella on abc family
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['should', 'i', 'pretend', 'like', 'im', 'not', 'watching', 'cinderella', 'on', 'abc', 'family']
cosine_similarity: 0.9802407622337341
train_input: [0.4073526042885674, 0.98024076], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Watching Cinderella is making me emotional
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.57615236 0.40993715]
 [0.40993715 0.57615236 0.         0.57615236 0.         0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Watching Cinderella is making me emotional
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['watching', 'cinderella', 'is', 'making', 'me', 'emotional']
cosine_similarity: 0.9503389000892639
train_input: [0.3360969272762575, 0.9503389], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Why yes I do want to spend my day watching Cinderella with the fam
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.57615236 0.57615236 0.
  0.         0.40993715 0.        ]
 [0.29017021 0.4078241  0.4078241  0.         0.         0.4078241
  0.4078241  0.29017021 0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Why yes I do want to spend my day watching Cinderella with the fam
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['why', 'yes', 'i', 'do', 'want', 'to', 'spend', 'my', 'day', 'watching', 'cinderella', 'with', 'the', 'fam']
cosine_similarity: 0.9714053869247437
train_input: [0.23790309463326234, 0.9714054], train_label: 1
TF_IDF_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Writer is making me watch a movie called Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.         0.         0.53404633
  0.         0.53404633 0.        ]
 [0.4261596  0.30321606 0.         0.4261596  0.4261596  0.
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Im so watching Cinderella right now, sentence2: Writer is making me watch a movie called Cinderella
After tokenization, sentence1: ['im', 'so', 'watching', 'cinderella', 'right', 'now'], sentence2: ['writer', 'is', 'making', 'me', 'watch', 'a', 'movie', 'called', 'cinderella']
cosine_similarity: 0.9346837401390076
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: How can I study when Cinderella is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.        ]
 [0.57973867 0.         0.         0.81480247]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: How can I study when Cinderella is on
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['how', 'can', 'i', 'study', 'when', 'cinderella', 'is', 'on']
cosine_similarity: 0.9581466913223267
train_input: [0.2605556710562624, 0.9581467], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: I had a dream that I watched Cinderella last night
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: I had a dream that I watched Cinderella last night
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['i', 'had', 'a', 'dream', 'that', 'i', 'watched', 'cinderella', 'last', 'night']
cosine_similarity: 0.9631653428077698
train_input: [0.17077611319011649, 0.96316534], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: I love cinderella but all this singing is too much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.        ]
 [0.44943642 0.         0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: I love cinderella but all this singing is too much
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['i', 'love', 'cinderella', 'but', 'all', 'this', 'singing', 'is', 'too', 'much']
cosine_similarity: 0.9745453596115112
train_input: [0.20199309249791833, 0.97454536], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: I love that Cinderella is on so much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672 ]
 [0.57973867 0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: I love that Cinderella is on so much
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['i', 'love', 'that', 'cinderella', 'is', 'on', 'so', 'much']
cosine_similarity: 0.9801387190818787
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: I wake up to find my mom watching Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: I wake up to find my mom watching Cinderella
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['i', 'wake', 'up', 'to', 'find', 'my', 'mom', 'watching', 'cinderella']
cosine_similarity: 0.9544748067855835
train_input: [0.17077611319011649, 0.9544748], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: Watching Cinderella as I write my paper
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: Watching Cinderella as I write my paper
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['watching', 'cinderella', 'as', 'i', 'write', 'my', 'paper']
cosine_similarity: 0.9482242465019226
train_input: [0.17077611319011649, 0.94822425], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: Watching Cinderella with my boy Steven
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.         0.        ]
 [0.53404633 0.37997836 0.         0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: Watching Cinderella with my boy Steven
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['watching', 'cinderella', 'with', 'my', 'boy', 'steven']
cosine_similarity: 0.969512939453125
train_input: [0.17077611319011649, 0.96951294], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: can i just be Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672 ]
 [0.57973867 0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: can i just be Cinderella
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['can', 'i', 'just', 'be', 'cinderella']
cosine_similarity: 0.9479812979698181
train_input: [0.2605556710562624, 0.9479813], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: cinddyyrelaa cinderella all I hear is CINDARELLA
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44943642 0.6316672  0.         0.6316672 ]
 [0.53404633 0.53404633 0.37997836 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: cinddyyrelaa cinderella all I hear is CINDARELLA
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['cinderella', 'all', 'i', 'hear', 'is', 'cindarella']
cosine_similarity: 0.9562711715698242
train_input: [0.17077611319011649, 0.9562712], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: cinderella is on tv bye im watching it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.6316672  0.
  0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.         0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Cinderella is on she s my favorite princess, sentence2: cinderella is on tv bye im watching it
After tokenization, sentence1: ['cinderella', 'is', 'on', 'she', 's', 'my', 'favorite', 'princess'], sentence2: ['cinderella', 'is', 'on', 'tv', 'bye', 'im', 'watching', 'it']
cosine_similarity: 0.9638471007347107
train_input: [0.15064018498706508, 0.9638471], train_label: 1
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Am I really watching Cinderella right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.49922133 0.         0.35520009
  0.        ]
 [0.         0.40993715 0.         0.         0.57615236 0.40993715
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Am I really watching Cinderella right now
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['am', 'i', 'really', 'watching', 'cinderella', 'right', 'now']
cosine_similarity: 0.9585115909576416
train_input: [0.29121941856368966, 0.9585116], train_label: 0
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Cinderella Peter Pan and the lion kingdont bother call me today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.47107781 0.         0.
  0.47107781 0.         0.         0.47107781 0.        ]
 [0.         0.39204401 0.27894255 0.         0.39204401 0.39204401
  0.         0.39204401 0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536718
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Cinderella Peter Pan and the lion kingdont bother call me today
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['cinderella', 'peter', 'pan', 'and', 'the', 'lion', 'bother', 'call', 'me', 'today']
cosine_similarity: 0.9696851968765259
train_input: [0.09349477497536718, 0.9696852], train_label: 0
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Cinderella bout to watch me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.47107781 0.47107781 0.47107781
  0.        ]
 [0.         0.6316672  0.44943642 0.         0.         0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.1506401849870651
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Cinderella bout to watch me
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['cinderella', 'bout', 'to', 'watch', 'me']
cosine_similarity: 0.9319645166397095
train_input: [0.1506401849870651, 0.9319645], train_label: 0
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Cinderella was the original Started from the bottom story
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.49922133 0.35520009 0.49922133 0.
  0.        ]
 [0.         0.40993715 0.         0.40993715 0.         0.57615236
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Cinderella was the original Started from the bottom story
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['cinderella', 'was', 'the', 'original', 'started', 'from', 'the', 'bottom', 'story']
cosine_similarity: 0.985835075378418
train_input: [0.29121941856368966, 0.9858351], train_label: 0
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: I wonder what mascara Cinderella uses
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.47107781 0.47107781
  0.         0.        ]
 [0.         0.37997836 0.         0.53404633 0.         0.
  0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.12735952979479354
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: I wonder what mascara Cinderella uses
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['i', 'wonder', 'what', 'mascara', 'cinderella', 'uses']
cosine_similarity: 0.940326988697052
train_input: [0.12735952979479354, 0.940327], train_label: 0
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: If NiallOfficial was the Prince I would be Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.47107781 0.
  0.47107781]
 [0.         0.44943642 0.         0.6316672  0.         0.6316672
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.1506401849870651
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: If NiallOfficial was the Prince I would be Cinderella
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['if', 'niallofficial', 'was', 'the', 'prince', 'i', 'would', 'be', 'cinderella']
cosine_similarity: 0.9615436792373657
train_input: [0.1506401849870651, 0.9615437], train_label: 0
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Is it bad that I find the prince on Cinderella cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.         0.47107781 0.47107781
  0.         0.47107781]
 [0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.12735952979479354
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Is it bad that I find the prince on Cinderella cute
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['is', 'it', 'bad', 'that', 'i', 'find', 'the', 'prince', 'on', 'cinderella', 'cute']
cosine_similarity: 0.98043292760849
train_input: [0.12735952979479354, 0.9804329], train_label: 0
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: My dog is literally sitting next to me watching cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.         0.47107781 0.         0.47107781
  0.47107781 0.         0.        ]
 [0.         0.33517574 0.47107781 0.         0.47107781 0.
  0.         0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542778
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: My dog is literally sitting next to me watching cinderella
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['my', 'dog', 'is', 'literally', 'sitting', 'next', 'to', 'me', 'watching', 'cinderella']
cosine_similarity: 0.9645480513572693
train_input: [0.11234277891542778, 0.96454805], train_label: 0
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Watching Cinderella on ABCFamily I want a
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.47107781 0.47107781 0.47107781
  0.         0.        ]
 [0.         0.53404633 0.37997836 0.         0.         0.
  0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.12735952979479354
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Watching Cinderella on ABCFamily I want a
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['watching', 'cinderella', 'on', 'abcfamily', 'i', 'want', 'a']
cosine_similarity: 0.9597221612930298
train_input: [0.12735952979479354, 0.95972216], train_label: 1
TF_IDF_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Why is my mom watching Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.47107781 0.47107781
  0.        ]
 [0.         0.44943642 0.         0.6316672  0.         0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.1506401849870651
word_to_vector_cosine_similarity: sentence1: The original Cinderella is on ABC Family right now, sentence2: Why is my mom watching Cinderella
After tokenization, sentence1: ['the', 'original', 'cinderella', 'is', 'on', 'abc', 'family', 'right', 'now'], sentence2: ['why', 'is', 'my', 'mom', 'watching', 'cinderella']
cosine_similarity: 0.9621634483337402
train_input: [0.1506401849870651, 0.96216345], train_label: 0
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: And then along came Cinderella and Panda
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.23076793 0.97300882 0.        ]
 [0.6316672  0.44943642 0.         0.6316672 ]]
pairwise_similarity: [[1.         0.10371551]
 [0.10371551 1.        ]]
cosine_similarity: 0.10371551133313005
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: And then along came Cinderella and Panda
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['and', 'then', 'along', 'came', 'cinderella', 'and', 'panda']
cosine_similarity: 0.8916537761688232
train_input: [0.10371551133313005, 0.8916538], train_label: 0
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella Peter Pan and the lion kingdont bother call me today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.23076793 0.         0.         0.97300882 0.
  0.         0.        ]
 [0.39204401 0.27894255 0.39204401 0.39204401 0.         0.39204401
  0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.06437099]
 [0.06437099 1.        ]]
cosine_similarity: 0.06437099366532754
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella Peter Pan and the lion kingdont bother call me today
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['cinderella', 'peter', 'pan', 'and', 'the', 'lion', 'bother', 'call', 'me', 'today']
cosine_similarity: 0.9028179049491882
train_input: [0.06437099366532754, 0.9028179], train_label: 0
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella and Peter Pan is on today and Im going to Lou Lous
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.         0.         0.         0.         0.97300882
  0.         0.         0.        ]
 [0.25969799 0.36499647 0.36499647 0.36499647 0.36499647 0.
  0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.05992997]
 [0.05992997 1.        ]]
cosine_similarity: 0.05992996822422367
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella and Peter Pan is on today and Im going to Lou Lous
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['cinderella', 'and', 'peter', 'pan', 'is', 'on', 'today', 'and', 'im', 'going', 'to', 'lou', 'lous']
cosine_similarity: 0.9246086478233337
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella is my most favorite movie ever
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.         0.         0.97300882]
 [0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.10371551]
 [0.10371551 1.        ]]
cosine_similarity: 0.10371551133313005
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella is my most favorite movie ever
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['cinderella', 'is', 'my', 'most', 'favorite', 'movie', 'ever']
cosine_similarity: 0.9171460866928101
train_input: [0.10371551133313005, 0.9171461], train_label: 1
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella is on one of my favorites
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.         0.97300882]
 [0.57973867 0.81480247 0.        ]]
pairwise_similarity: [[1.         0.13378509]
 [0.13378509 1.        ]]
cosine_similarity: 0.1337850929463124
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella is on one of my favorites
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['cinderella', 'is', 'on', 'one', 'of', 'my', 'favorites']
cosine_similarity: 0.9255927801132202
train_input: [0.1337850929463124, 0.9255928], train_label: 1
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella is on then peter pan comes on then lion king
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.         0.         0.         0.97300882 0.
  0.        ]
 [0.30321606 0.4261596  0.4261596  0.4261596  0.         0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.06997254]
 [0.06997254 1.        ]]
cosine_similarity: 0.06997254341808112
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Cinderella is on then peter pan comes on then lion king
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['cinderella', 'is', 'on', 'then', 'peter', 'pan', 'comes', 'on', 'then', 'lion', 'king']
cosine_similarity: 0.9189383387565613
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: I love that Cinderella is on so much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.         0.97300882]
 [0.57973867 0.81480247 0.        ]]
pairwise_similarity: [[1.         0.13378509]
 [0.13378509 1.        ]]
cosine_similarity: 0.1337850929463124
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: I love that Cinderella is on so much
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['i', 'love', 'that', 'cinderella', 'is', 'on', 'so', 'much']
cosine_similarity: 0.9499318599700928
train_input: [0.1337850929463124, 0.94993186], train_label: 1
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Im 19 and watching Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.23076793 0.         0.97300882 0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.08768682]
 [0.08768682 1.        ]]
cosine_similarity: 0.08768681980142445
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Im 19 and watching Cinderella
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['im', 'and', 'watching', 'cinderella']
cosine_similarity: 0.9143344759941101
train_input: [0.08768681980142445, 0.9143345], train_label: 0
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Oh my god Cinderella is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.         0.         0.97300882]
 [0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.10371551]
 [0.10371551 1.        ]]
cosine_similarity: 0.10371551133313005
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: Oh my god Cinderella is on
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['oh', 'my', 'god', 'cinderella', 'is', 'on']
cosine_similarity: 0.9550929665565491
train_input: [0.10371551133313005, 0.95509297], train_label: 1
TF_IDF_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: making my dad and grandfather watch Cinderella right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.         0.         0.         0.97300882 0.
  0.        ]
 [0.30321606 0.4261596  0.4261596  0.4261596  0.         0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.06997254]
 [0.06997254 1.        ]]
cosine_similarity: 0.06997254341808112
word_to_vector_cosine_similarity: sentence1: CINDERELLA IS ON OMG OMG OMG, sentence2: making my dad and grandfather watch Cinderella right now
After tokenization, sentence1: ['cinderella', 'is', 'on', 'omg', 'omg', 'omg'], sentence2: ['making', 'my', 'dad', 'and', 'grandfather', 'watch', 'cinderella', 'right', 'now']
cosine_similarity: 0.9017934203147888
train_input: [0.06997254341808112, 0.9017934], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Cinderella is giving me life right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.         0.53404633
  0.53404633]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Cinderella is giving me life right now
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['cinderella', 'is', 'giving', 'me', 'life', 'right', 'now']
cosine_similarity: 0.8815606832504272
train_input: [0.1443835552773867, 0.8815607], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Cinderella is my favorite Disney princess
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.53404633]
 [0.37997836 0.53404633 0.         0.53404633 0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Cinderella is my favorite Disney princess
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['cinderella', 'is', 'my', 'favorite', 'disney', 'princess']
cosine_similarity: 0.9101775884628296
train_input: [0.1443835552773867, 0.9101776], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: I LOVE Cinderella especially fairlygodmother so cute
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.         0.
  0.53404633 0.53404633]
 [0.33517574 0.47107781 0.         0.47107781 0.47107781 0.47107781
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: I LOVE Cinderella especially fairlygodmother so cute
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['i', 'love', 'cinderella', 'especially', 'so', 'cute']
cosine_similarity: 0.8834466934204102
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Not even Cinderella is beating me at this dance
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]
 [0.6316672  0.44943642 0.6316672  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Not even Cinderella is beating me at this dance
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['not', 'even', 'cinderella', 'is', 'beating', 'me', 'at', 'this', 'dance']
cosine_similarity: 0.8981171250343323
train_input: [0.17077611319011649, 0.8981171], train_label: 0
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Nothing like laying in bed watching Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.57615236 0.         0.         0.57615236
  0.40993715]
 [0.49922133 0.35520009 0.         0.49922133 0.49922133 0.
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Nothing like laying in bed watching Cinderella
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['nothing', 'like', 'laying', 'in', 'bed', 'watching', 'cinderella']
cosine_similarity: 0.9398399591445923
train_input: [0.29121941856368966, 0.93983996], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Watching Cinderella and eating spaghetti
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5 0.5 0.5 0.5]
 [0.5 0.5 0.5 0.5]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 1.0
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Watching Cinderella and eating spaghetti
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['watching', 'cinderella', 'and', 'eating', 'spaghetti']
cosine_similarity: 1.0
train_input: [1.0, 1.0], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Watching Cinderella is making me emotional
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.57615236 0.40993715]
 [0.40993715 0.         0.57615236 0.57615236 0.         0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Watching Cinderella is making me emotional
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['watching', 'cinderella', 'is', 'making', 'me', 'emotional']
cosine_similarity: 0.9021226167678833
train_input: [0.3360969272762575, 0.9021226], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Watching Cinderella while still in my pajamas
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.57615236 0.40993715]
 [0.50154891 0.         0.70490949 0.         0.50154891]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: Watching Cinderella while still in my pajamas
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['watching', 'cinderella', 'while', 'still', 'in', 'my', 'pajamas']
cosine_similarity: 0.9484935998916626
train_input: [0.4112070550676187, 0.9484936], train_label: 1
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: haha wana watch Cinderella and pizza
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.53404633 0.
  0.         0.53404633]
 [0.33517574 0.         0.47107781 0.47107781 0.         0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: haha wana watch Cinderella and pizza
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['haha', 'wana', 'watch', 'cinderella', 'and', 'pizza']
cosine_similarity: 0.9125970005989075
TF_IDF_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: why is the cat in cinderella called lucifer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37997836 0.53404633 0.         0.53404633
  0.53404633]
 [0.53404633 0.53404633 0.37997836 0.         0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Watching Cinderella and eating spaghetti, sentence2: why is the cat in cinderella called lucifer
After tokenization, sentence1: ['watching', 'cinderella', 'and', 'eating', 'spaghetti'], sentence2: ['why', 'is', 'the', 'cat', 'in', 'cinderella', 'called', 'lucifer']
cosine_similarity: 0.9137136936187744
train_input: [0.1443835552773867, 0.9137137], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Cinderella on abc family is so well remastered
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Cinderella on abc family is so well remastered
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['cinderella', 'on', 'abc', 'family', 'is', 'so', 'well', 'remastered']
cosine_similarity: 0.9667052626609802
train_input: [0.17077611319011649, 0.96670526], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Holy shit the Cinderella Disney movie is from 1950
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.         0.
  0.6316672  0.6316672 ]
 [0.4261596  0.30321606 0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Holy shit the Cinderella Disney movie is from 1950
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['holy', 'shit', 'the', 'cinderella', 'disney', 'movie', 'is', 'from']
cosine_similarity: 0.9645495414733887
train_input: [0.1362763414390864, 0.96454954], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: I am now watching Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.        ]
 [0.57973867 0.         0.         0.81480247]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: I am now watching Cinderella
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['i', 'am', 'now', 'watching', 'cinderella']
cosine_similarity: 0.9685671329498291
train_input: [0.2605556710562624, 0.96856713], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Im really watching Cinderella the cartoon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.6316672
  0.        ]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.         0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Im really watching Cinderella the cartoon
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['im', 'really', 'watching', 'cinderella', 'the', 'cartoon']
cosine_similarity: 0.9623674750328064
train_input: [0.15064018498706508, 0.9623675], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: The cat s name is Lucifer on Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672 ]
 [0.6316672  0.44943642 0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: The cat s name is Lucifer on Cinderella
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['the', 'cat', 's', 'name', 'is', 'lucifer', 'on', 'cinderella']
cosine_similarity: 0.9624602794647217
train_input: [0.20199309249791833, 0.9624603], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: The fact that Cinderella is on TV right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: The fact that Cinderella is on TV right now
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['the', 'fact', 'that', 'cinderella', 'is', 'on', 'tv', 'right', 'now']
cosine_similarity: 0.970268189907074
train_input: [0.17077611319011649, 0.9702682], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Was out and about half the day and now watching Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672  0.        ]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Was out and about half the day and now watching Cinderella
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['was', 'out', 'and', 'about', 'half', 'the', 'day', 'and', 'now', 'watching', 'cinderella']
cosine_similarity: 0.9711434245109558
train_input: [0.17077611319011649, 0.9711434], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Watching Cinderella on ABCFamily I want a
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.70490949 0.50154891 0.        ]
 [0.57615236 0.40993715 0.         0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: Watching Cinderella on ABCFamily I want a
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['watching', 'cinderella', 'on', 'abcfamily', 'i', 'want', 'a']
cosine_similarity: 0.967190682888031
train_input: [0.4112070550676187, 0.9671907], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: am I actually watching Cinderella right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: am I actually watching Cinderella right now
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['am', 'i', 'actually', 'watching', 'cinderella', 'right', 'now']
cosine_similarity: 0.9697406888008118
train_input: [0.17077611319011649, 0.9697407], train_label: 0
TF_IDF_cosine_similarity: sentence1: I want my Cinderella story, sentence2: the orginal cinderella is the best one
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672 ]
 [0.6316672  0.44943642 0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: I want my Cinderella story, sentence2: the orginal cinderella is the best one
After tokenization, sentence1: ['i', 'want', 'my', 'cinderella', 'story'], sentence2: ['the', 'orginal', 'cinderella', 'is', 'the', 'best', 'one']
cosine_similarity: 0.9629219174385071
train_input: [0.20199309249791833, 0.9629219], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Cinderella Peter Pan and the lion kingdont bother call me today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.         0.57735027 0.57735027
  0.        ]
 [0.42567716 0.30287281 0.42567716 0.42567716 0.30287281 0.30287281
  0.42567716]]
pairwise_similarity: [[1.         0.52459109]
 [0.52459109 1.        ]]
cosine_similarity: 0.5245910904457137
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Cinderella Peter Pan and the lion kingdont bother call me today
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['cinderella', 'peter', 'pan', 'and', 'the', 'lion', 'bother', 'call', 'me', 'today']
cosine_similarity: 0.9768144488334656
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Cinderella Peter Pan in one day
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.         0.57735027 0.57735027]
 [0.44832087 0.63009934 0.44832087 0.44832087]]
pairwise_similarity: [[1.         0.77651453]
 [0.77651453 1.        ]]
cosine_similarity: 0.7765145304745156
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Cinderella Peter Pan in one day
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['cinderella', 'peter', 'pan', 'in', 'one', 'day']
cosine_similarity: 0.9733842015266418
train_input: [0.7765145304745156, 0.9733842], train_label: 1
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Cinderella is my 2nd favorite Disney movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.         0.6316672
  0.6316672 ]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.47107781 0.
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Cinderella is my 2nd favorite Disney movie
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['cinderella', 'is', 'my', 'favorite', 'disney', 'movie']
cosine_similarity: 0.9417595863342285
train_input: [0.15064018498706508, 0.9417596], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Cinderella is on and then freaking Peter Pan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.         0.57735027 0.57735027]
 [0.44832087 0.63009934 0.44832087 0.44832087]]
pairwise_similarity: [[1.         0.77651453]
 [0.77651453 1.        ]]
cosine_similarity: 0.7765145304745156
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Cinderella is on and then freaking Peter Pan
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['cinderella', 'is', 'on', 'and', 'then', 'freaking', 'peter', 'pan']
cosine_similarity: 0.9901230335235596
train_input: [0.7765145304745156, 0.99012303], train_label: 1
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Every time I watch Cinderella my hatred for the step mother and sisters increases tremendously
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.6316672
  0.         0.         0.         0.         0.        ]
 [0.24395573 0.34287126 0.34287126 0.34287126 0.         0.
  0.34287126 0.34287126 0.34287126 0.34287126 0.34287126]]
pairwise_similarity: [[1.         0.10964259]
 [0.10964259 1.        ]]
cosine_similarity: 0.10964258683453854
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Every time I watch Cinderella my hatred for the step mother and sisters increases tremendously
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['every', 'time', 'i', 'watch', 'cinderella', 'my', 'hatred', 'for', 'the', 'step', 'mother', 'and', 'sisters', 'increases', 'tremendously']
cosine_similarity: 0.9327538013458252
train_input: [0.10964258683453854, 0.9327538], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Gus from Cinderella is my spirit animal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Gus from Cinderella is my spirit animal
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['gus', 'from', 'cinderella', 'is', 'my', 'spirit', 'animal']
cosine_similarity: 0.972404956817627
train_input: [0.17077611319011649, 0.97240496], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: If you dont like Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: If you dont like Cinderella
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['if', 'you', 'dont', 'like', 'cinderella']
cosine_similarity: 0.9168555736541748
train_input: [0.20199309249791833, 0.9168556], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Watching Cinderella and eating animal crackers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.6316672
  0.        ]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.         0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Watching Cinderella and eating animal crackers
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['watching', 'cinderella', 'and', 'eating', 'animal', 'crackers']
cosine_similarity: 0.880839467048645
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Why is my mom watching Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.44943642 0.6316672  0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: Why is my mom watching Cinderella
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['why', 'is', 'my', 'mom', 'watching', 'cinderella']
cosine_similarity: 0.948088526725769
train_input: [0.20199309249791833, 0.9480885], train_label: 0
TF_IDF_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: thanks for updating me on Cinderella
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.         0.        ]
 [0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Cinderella is on and Peter Pan is on next, sentence2: thanks for updating me on Cinderella
After tokenization, sentence1: ['cinderella', 'is', 'on', 'and', 'peter', 'pan', 'is', 'on', 'next'], sentence2: ['thanks', 'for', 'updating', 'me', 'on', 'cinderella']
cosine_similarity: 0.949171781539917
train_input: [0.20199309249791833, 0.9491718], train_label: 0
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: Btw cole summer is a dope song
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.6316672  0.6316672  0.
  0.44943642]
 [0.47107781 0.47107781 0.47107781 0.         0.         0.47107781
  0.33517574]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: Btw cole summer is a dope song
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['btw', 'cole', 'summer', 'is', 'a', 'dope', 'song']
cosine_similarity: 0.9541974067687988
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: I fuck with Cole Summer too hard
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.6316672  0.         0.6316672  0.44943642]
 [0.53404633 0.53404633 0.         0.53404633 0.         0.37997836]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: I fuck with Cole Summer too hard
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['i', 'fuck', 'with', 'cole', 'summer', 'too', 'hard']
cosine_similarity: 0.9833471775054932
train_input: [0.17077611319011649, 0.9833472], train_label: 0
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: It s going to be a Cole Cole Summer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.6316672  0.6316672  0.44943642]
 [0.8523192  0.4261596  0.         0.         0.30321606]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: It s going to be a Cole Cole Summer
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['it', 's', 'going', 'to', 'be', 'a', 'cole', 'cole', 'summer']
cosine_similarity: 0.978431224822998
train_input: [0.1362763414390864, 0.9784312], train_label: 1
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: Man this JColeNC Cole Summer is too nice
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.6316672  0.         0.         0.
  0.44943642]
 [0.47107781 0.         0.         0.47107781 0.47107781 0.47107781
  0.33517574]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: Man this JColeNC Cole Summer is too nice
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['man', 'this', 'cole', 'summer', 'is', 'too', 'nice']
cosine_similarity: 0.9766921401023865
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: This Cole Summer is just what the doctor ordered
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.6316672  0.6316672  0.         0.
  0.44943642]
 [0.47107781 0.47107781 0.         0.         0.47107781 0.47107781
  0.33517574]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: This Cole Summer is just what the doctor ordered
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['this', 'cole', 'summer', 'is', 'just', 'what', 'the', 'doctor', 'ordered']
cosine_similarity: 0.9659400582313538
train_input: [0.15064018498706508, 0.96594006], train_label: 1
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: What dont I know lol Your heard Cole Summer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.6316672  0.         0.6316672  0.
  0.         0.44943642]
 [0.4261596  0.4261596  0.         0.4261596  0.         0.4261596
  0.4261596  0.30321606]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: What dont I know lol Your heard Cole Summer
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['what', 'dont', 'i', 'know', 'lol', 'your', 'heard', 'cole', 'summer']
cosine_similarity: 0.9693692922592163
train_input: [0.1362763414390864, 0.9693693], train_label: 0
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: Yo Cole lowkey snappin on the end of this Cole Summer track
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.6316672  0.6316672  0.         0.
  0.44943642 0.         0.        ]
 [0.64867255 0.32433627 0.         0.         0.32433627 0.32433627
  0.23076793 0.32433627 0.32433627]]
pairwise_similarity: [[1.         0.10371551]
 [0.10371551 1.        ]]
cosine_similarity: 0.10371551133313005
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: Yo Cole lowkey snappin on the end of this Cole Summer track
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['yo', 'cole', 'lowkey', 'snappin', 'on', 'the', 'end', 'of', 'this', 'cole', 'summer', 'track']
cosine_similarity: 0.9701453447341919
train_input: [0.10371551133313005, 0.97014534], train_label: 0
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: cole summer i predict a cold winter
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.6316672  0.6316672  0.         0.44943642
  0.        ]
 [0.47107781 0.47107781 0.         0.         0.47107781 0.33517574
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: cole summer i predict a cold winter
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['cole', 'summer', 'i', 'predict', 'a', 'cold', 'winter']
cosine_similarity: 0.9802924990653992
train_input: [0.15064018498706508, 0.9802925], train_label: 0
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: lol no it s from Cole Summer
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.6316672  0.         0.44943642]
 [0.6316672  0.         0.         0.6316672  0.44943642]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: lol no it s from Cole Summer
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['lol', 'no', 'it', 's', 'from', 'cole', 'summer']
cosine_similarity: 0.9604175090789795
train_input: [0.20199309249791833, 0.9604175], train_label: 0
TF_IDF_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: so the song is called cole summer let me borrow the link real quick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.6316672  0.6316672  0.
  0.         0.         0.         0.         0.44943642]
 [0.34287126 0.34287126 0.34287126 0.         0.         0.34287126
  0.34287126 0.34287126 0.34287126 0.34287126 0.24395573]]
pairwise_similarity: [[1.         0.10964259]
 [0.10964259 1.        ]]
cosine_similarity: 0.10964258683453854
word_to_vector_cosine_similarity: sentence1: Its gonna be a hotCole summer, sentence2: so the song is called cole summer let me borrow the link real quick
After tokenization, sentence1: ['its', 'gonna', 'be', 'a', 'summer'], sentence2: ['so', 'the', 'song', 'is', 'called', 'cole', 'summer', 'let', 'me', 'borrow', 'the', 'link', 'real', 'quick']
cosine_similarity: 0.9760776162147522
train_input: [0.10964258683453854, 0.9760776], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: But can he convince Congress
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.6316672 ]
 [0.57973867 0.81480247 0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: But can he convince Congress
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['but', 'can', 'he', 'convince', 'congress']
cosine_similarity: 0.9720867872238159
train_input: [0.2605556710562624, 0.9720868], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Congress willing to work whim
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.         0.         0.        ]
 [0.37997836 0.         0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Congress willing to work whim
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['congress', 'willing', 'to', 'work', 'whim']
cosine_similarity: 0.9309521913528442
train_input: [0.17077611319011649, 0.9309522], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: I have noticed congress go quiet on questions of rulegoverence
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.         0.6316672
  0.6316672 ]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.47107781 0.
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: I have noticed congress go quiet on questions of rulegoverence
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['i', 'have', 'noticed', 'congress', 'go', 'quiet', 'on', 'questions', 'of']
cosine_similarity: 0.9833690524101257
train_input: [0.15064018498706508, 0.98336905], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: I know it s not the members of Congress
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: I know it s not the members of Congress
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['i', 'know', 'it', 's', 'not', 'the', 'members', 'of', 'congress']
cosine_similarity: 0.9816961288452148
train_input: [0.20199309249791833, 0.9816961], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Immigration reform will pass Congress
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Immigration reform will pass Congress
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['immigration', 'reform', 'will', 'pass', 'congress']
cosine_similarity: 0.8820180892944336
train_input: [0.17077611319011649, 0.8820181], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Of Congress President says It s not my job to get them to behave
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.6316672
  0.        ]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.         0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Of Congress President says It s not my job to get them to behave
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['of', 'congress', 'president', 'says', 'it', 's', 'not', 'my', 'job', 'to', 'get', 'them', 'to', 'behave']
cosine_similarity: 0.9808108806610107
train_input: [0.15064018498706508, 0.9808109], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: POTUS says Congress needs to start thinking long term
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.6316672
  0.         0.         0.         0.        ]
 [0.25969799 0.36499647 0.36499647 0.36499647 0.         0.
  0.36499647 0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: POTUS says Congress needs to start thinking long term
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['potus', 'says', 'congress', 'needs', 'to', 'start', 'thinking', 'long', 'term']
cosine_similarity: 0.9753879308700562
train_input: [0.11671773546032795, 0.97538793], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Why congressgovt is failing to bring truth to the people
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57735027 0.         0.         0.         0.57735027
  0.57735027 0.        ]
 [0.4472136  0.         0.4472136  0.4472136  0.4472136  0.
  0.         0.4472136 ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Why congressgovt is failing to bring truth to the people
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['why', 'is', 'failing', 'to', 'bring', 'truth', 'to', 'the', 'people']
cosine_similarity: 0.9765830636024475
train_input: [0.0, 0.97658306], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Why isnt Obamacare good enough for you and Congress
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: Why isnt Obamacare good enough for you and Congress
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['why', 'isnt', 'obamacare', 'good', 'enough', 'for', 'you', 'and', 'congress']
cosine_similarity: 0.9647513031959534
train_input: [0.17077611319011649, 0.9647513], train_label: 0
TF_IDF_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: congress should be doing something and not reinstating the airports to make their lives better
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44943642 0.         0.         0.
  0.         0.6316672  0.6316672 ]
 [0.39204401 0.39204401 0.27894255 0.39204401 0.39204401 0.39204401
  0.39204401 0.         0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Sad Congress not on samepage, sentence2: congress should be doing something and not reinstating the airports to make their lives better
After tokenization, sentence1: ['sad', 'congress', 'not', 'on'], sentence2: ['congress', 'should', 'be', 'doing', 'something', 'and', 'not', 'reinstating', 'the', 'airports', 'to', 'make', 'their', 'lives', 'better']
cosine_similarity: 0.9645107984542847
train_input: [0.12536693798731732, 0.9645108], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Cant believe cordarelle patterson is still available
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.40993715 0.57615236 0.57615236]
 [0.57615236 0.57615236 0.40993715 0.40993715 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Cant believe cordarelle patterson is still available
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['cant', 'believe', 'patterson', 'is', 'still', 'available']
cosine_similarity: 0.948245108127594
train_input: [0.3360969272762575, 0.9482451], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: If the Colts dont get Cordarelle Patterson w their next pick theyll regret it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.         0.40993715 0.         0.
  0.57615236 0.         0.57615236]
 [0.4078241  0.29017021 0.4078241  0.29017021 0.4078241  0.4078241
  0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: If the Colts dont get Cordarelle Patterson w their next pick theyll regret it
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['if', 'the', 'colts', 'dont', 'get', 'patterson', 'w', 'their', 'next', 'pick', 'theyll', 'regret', 'it']
cosine_similarity: 0.9620649218559265
train_input: [0.23790309463326234, 0.9620649], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: If the Texans dont pick Cordarelle Patterson I might freak out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.         0.57615236
  0.         0.57615236]
 [0.31779954 0.44665616 0.44665616 0.31779954 0.44665616 0.
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: If the Texans dont pick Cordarelle Patterson I might freak out
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['if', 'the', 'texans', 'dont', 'pick', 'patterson', 'i', 'might', 'freak', 'out']
cosine_similarity: 0.9576336741447449
train_input: [0.2605556710562624, 0.9576337], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Not so much with Cordarelle Patterson at 29
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.57615236]
 [0.70490949 0.50154891 0.50154891 0.         0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Not so much with Cordarelle Patterson at 29
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['not', 'so', 'much', 'with', 'patterson', 'at']
cosine_similarity: 0.9628867506980896
train_input: [0.4112070550676187, 0.96288675], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: The Vikings just gave up 4 picks for Cordarelle Patterson
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.         0.57615236
  0.         0.57615236]
 [0.31779954 0.44665616 0.44665616 0.31779954 0.44665616 0.
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: The Vikings just gave up 4 picks for Cordarelle Patterson
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['the', 'vikings', 'just', 'gave', 'up', 'picks', 'for', 'patterson']
cosine_similarity: 0.9774495363235474
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Vikings picked cordarelle patterson with their 3rd first round pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.
  0.57615236 0.         0.57615236]
 [0.4078241  0.29017021 0.29017021 0.4078241  0.4078241  0.4078241
  0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Vikings picked cordarelle patterson with their 3rd first round pick
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['vikings', 'picked', 'patterson', 'with', 'their', 'first', 'round', 'pick']
cosine_similarity: 0.9720372557640076
train_input: [0.23790309463326234, 0.97203726], train_label: 1
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Where the hell did they get Cordarelle Patterson s hat
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.40993715 0.57615236
  0.57615236]
 [0.35520009 0.49922133 0.49922133 0.49922133 0.35520009 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Where the hell did they get Cordarelle Patterson s hat
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['where', 'the', 'hell', 'did', 'they', 'get', 'patterson', 's', 'hat']
cosine_similarity: 0.9561693072319031
train_input: [0.29121941856368966, 0.9561693], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Yall see Cordarelle Patterson out there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.        ]
 [0.50154891 0.50154891 0.         0.         0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: Yall see Cordarelle Patterson out there
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['yall', 'see', 'patterson', 'out', 'there']
cosine_similarity: 0.9494580626487732
train_input: [0.4112070550676187, 0.94945806], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: cordarelle Patterson has the same hairdo as his mom
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.40993715 0.57615236 0.57615236]
 [0.40993715 0.57615236 0.57615236 0.40993715 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: cordarelle Patterson has the same hairdo as his mom
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['patterson', 'has', 'the', 'same', 'hairdo', 'as', 'his', 'mom']
cosine_similarity: 0.9446614384651184
train_input: [0.3360969272762575, 0.94466144], train_label: 0
TF_IDF_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: wHo the fcc did Cordarelle Patterson hair
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.40993715 0.57615236
  0.57615236]
 [0.35520009 0.49922133 0.49922133 0.49922133 0.35520009 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Welcome to the squad cordarelle Patterson, sentence2: wHo the fcc did Cordarelle Patterson hair
After tokenization, sentence1: ['welcome', 'to', 'the', 'squad', 'patterson'], sentence2: ['who', 'the', 'fcc', 'did', 'patterson', 'hair']
cosine_similarity: 0.9555901288986206
train_input: [0.29121941856368966, 0.9555901], train_label: 0
TF_IDF_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho Kroos Pirlo and Bale
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.53404633]
 [0.53404633 0.37997836 0.53404633 0.         0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho Kroos Pirlo and Bale
After tokenization, sentence1: ['phillipe', 'coutinho', 'is', 'a', 'proper', 'player'], sentence2: ['coutinho', 'kroos', 'pirlo', 'and', 'bale']
cosine_similarity: 0.8626506328582764
train_input: [0.1443835552773867, 0.86265063], train_label: 0
TF_IDF_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho Sturridge or Henderson for man of the match
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.53404633 0.        ]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.         0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho Sturridge or Henderson for man of the match
After tokenization, sentence1: ['phillipe', 'coutinho', 'is', 'a', 'proper', 'player'], sentence2: ['coutinho', 'sturridge', 'or', 'henderson', 'for', 'man', 'of', 'the', 'match']
cosine_similarity: 0.9615972638130188
train_input: [0.1273595297947935, 0.96159726], train_label: 0
TF_IDF_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho gem of a player
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.40993715 0.57615236]
 [0.50154891 0.70490949 0.         0.50154891 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho gem of a player
After tokenization, sentence1: ['phillipe', 'coutinho', 'is', 'a', 'proper', 'player'], sentence2: ['coutinho', 'gem', 'of', 'a', 'player']
cosine_similarity: 0.9684357643127441
train_input: [0.4112070550676187, 0.96843576], train_label: 1
TF_IDF_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho has to be the best buy Liverpool made
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37997836 0.         0.53404633 0.53404633
  0.53404633]
 [0.53404633 0.53404633 0.37997836 0.53404633 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho has to be the best buy Liverpool made
After tokenization, sentence1: ['phillipe', 'coutinho', 'is', 'a', 'proper', 'player'], sentence2: ['coutinho', 'has', 'to', 'be', 'the', 'best', 'buy', 'liverpool', 'made']
cosine_similarity: 0.9309288859367371
train_input: [0.1443835552773867, 0.9309289], train_label: 0
TF_IDF_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho is a super player
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.40993715 0.57615236 0.        ]
 [0.50154891 0.         0.50154891 0.         0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Phillipe Coutinho is a proper player, sentence2: Coutinho is a super player
After tokenization, sentence1: ['phillipe', 'coutinho', 'is', 'a', 'proper', 'player'], sentence2: ['coutinho', 'is', 'a', 'super', 'player']
cosine_similarity: 0.9625721573829651
train_input: [0.4112070550676187, 0.96257216], train_label: 1
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A Walk to Remember is on and Im in town and Im upset
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.49922133 0.35520009 0.         0.49922133
  0.         0.35520009]
 [0.         0.75525556 0.         0.26868528 0.37762778 0.
  0.37762778 0.26868528]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A Walk to Remember is on and Im in town and Im upset
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'on', 'and', 'im', 'in', 'town', 'and', 'im', 'upset']
cosine_similarity: 0.9772884845733643
dev_input: [0.1908740661302035, 0.9772885], dev_label: 0
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A Walk to Remember is the cutest thing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.49922133 0.35520009 0.         0.49922133
  0.35520009]
 [0.57615236 0.         0.         0.40993715 0.57615236 0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A Walk to Remember is the cutest thing
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'the', 'cutest', 'thing']
cosine_similarity: 0.9919882416725159
dev_input: [0.29121941856368966, 0.99198824], dev_label: 1
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A walk to remember is on ABC family youre welcome
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.35520009 0.         0.        ]
 [0.44665616 0.         0.44665616 0.         0.31779954 0.
  0.31779954 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A walk to remember is on ABC family youre welcome
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'on', 'abc', 'family', 'youre', 'welcome']
cosine_similarity: 0.9850156307220459
dev_input: [0.22576484600261604, 0.98501563], dev_label: 0
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A walk to remember is so amazing and inspiring
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.35520009]
 [0.57615236 0.         0.57615236 0.         0.40993715 0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A walk to remember is so amazing and inspiring
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'so', 'amazing', 'and', 'inspiring']
cosine_similarity: 0.9902955293655396
dev_input: [0.29121941856368966, 0.9902955], dev_label: 1
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: BUT GUYS ITS ON MY FAVE PART OF A WALK TO REMEMBER
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.         0.49922133 0.35520009 0.49922133
  0.35520009]
 [0.         0.57615236 0.57615236 0.         0.40993715 0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: BUT GUYS ITS ON MY FAVE PART OF A WALK TO REMEMBER
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['but', 'guys', 'its', 'on', 'my', 'fave', 'part', 'of', 'a', 'walk', 'to', 'remember']
cosine_similarity: 0.9889701008796692
dev_input: [0.29121941856368966, 0.9889701], dev_label: 0
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Day is made A Walk to Remember is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.49922133 0.35520009 0.49922133 0.35520009]
 [0.70490949 0.         0.         0.50154891 0.         0.50154891]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Day is made A Walk to Remember is on
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['day', 'is', 'made', 'a', 'walk', 'to', 'remember', 'is', 'on']
cosine_similarity: 0.9902157187461853
dev_input: [0.3563004293331381, 0.9902157], dev_label: 0
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: I have to watch A Walk to Remember every time it shows
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.35520009 0.         0.         0.49922133
  0.35520009 0.        ]
 [0.         0.         0.35520009 0.49922133 0.49922133 0.
  0.35520009 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: I have to watch A Walk to Remember every time it shows
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['i', 'have', 'to', 'watch', 'a', 'walk', 'to', 'remember', 'every', 'time', 'it', 'shows']
cosine_similarity: 0.980711817741394
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Oh god a walk to remember is on bring on the water works
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.49922133 0.         0.35520009
  0.49922133 0.35520009 0.         0.        ]
 [0.4078241  0.         0.4078241  0.         0.4078241  0.29017021
  0.         0.29017021 0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Oh god a walk to remember is on bring on the water works
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['oh', 'god', 'a', 'walk', 'to', 'remember', 'is', 'on', 'bring', 'on', 'the', 'water', 'works']
cosine_similarity: 0.9855803847312927
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: The only Nicholas Sparks movie I genuinely like is A Walk To Remember
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.         0.49922133 0.         0.
  0.35520009 0.         0.49922133 0.35520009]
 [0.         0.4078241  0.4078241  0.         0.4078241  0.4078241
  0.29017021 0.4078241  0.         0.29017021]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: The only Nicholas Sparks movie I genuinely like is A Walk To Remember
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['the', 'only', 'nicholas', 'sparks', 'movie', 'i', 'genuinely', 'like', 'is', 'a', 'walk', 'to', 'remember']
cosine_similarity: 0.9909297227859497
dev_input: [0.20613696606828605, 0.9909297], dev_label: 0
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Watching A Walk To Remember for the millionth time and for the millionth time I will cry
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.         0.35520009 0.         0.49922133
  0.35520009 0.        ]
 [0.         0.         0.632061   0.2248583  0.632061   0.
  0.2248583  0.3160305 ]]
pairwise_similarity: [[1.         0.15973938]
 [0.15973938 1.        ]]
cosine_similarity: 0.15973937686327605
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Watching A Walk To Remember for the millionth time and for the millionth time I will cry
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['watching', 'a', 'walk', 'to', 'remember', 'for', 'the', 'millionth', 'time', 'and', 'for', 'the', 'millionth', 'time', 'i', 'will', 'cry']
cosine_similarity: 0.9798288941383362
dev_input: [0.15973937686327605, 0.9798289], dev_label: 0
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: A Walk to Remember is on tv right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.49922133 0.35520009 0.         0.
  0.35520009]
 [0.         0.         0.         0.40993715 0.57615236 0.57615236
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: A Walk to Remember is on tv right now
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'on', 'tv', 'right', 'now']
cosine_similarity: 0.9940584897994995
dev_input: [0.29121941856368966, 0.9940585], dev_label: 0
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: I never even seen a walk to remember
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.49922133 0.35520009 0.         0.35520009]
 [0.         0.         0.         0.50154891 0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: I never even seen a walk to remember
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['i', 'never', 'even', 'seen', 'a', 'walk', 'to', 'remember']
cosine_similarity: 0.9818761944770813
dev_input: [0.3563004293331381, 0.9818762], dev_label: 0
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: I want a love like Jamie and Landon on A Walk To Remember
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37930349 0.         0.53309782 0.53309782
  0.37930349 0.37930349 0.        ]
 [0.42567716 0.42567716 0.30287281 0.42567716 0.         0.
  0.30287281 0.30287281 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: I want a love like Jamie and Landon on A Walk To Remember
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['i', 'want', 'a', 'love', 'like', 'jamie', 'and', 'landon', 'on', 'a', 'walk', 'to', 'remember']
cosine_similarity: 0.9859643578529358
dev_input: [0.34464214103805474, 0.98596436], dev_label: 0
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: No matter how many times I watch a walk to remember it always makes me cry
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.         0.49922133 0.49922133 0.35520009
  0.         0.35520009 0.        ]
 [0.         0.44665616 0.44665616 0.         0.         0.31779954
  0.44665616 0.31779954 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: No matter how many times I watch a walk to remember it always makes me cry
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['no', 'matter', 'how', 'many', 'times', 'i', 'watch', 'a', 'walk', 'to', 'remember', 'it', 'always', 'makes', 'me', 'cry']
cosine_similarity: 0.9805065989494324
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: THE GUY IN A WALK TO REMEMBER IS SO CUTE IM PISSING
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.49922133 0.49922133 0.
  0.49922133 0.35520009 0.35520009]
 [0.44665616 0.44665616 0.44665616 0.         0.         0.44665616
  0.         0.31779954 0.31779954]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: THE GUY IN A WALK TO REMEMBER IS SO CUTE IM PISSING
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['the', 'guy', 'in', 'a', 'walk', 'to', 'remember', 'is', 'so', 'cute', 'im', 'pissing']
cosine_similarity: 0.9813199639320374
dev_input: [0.22576484600261604, 0.98131996], dev_label: 0
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: The part on A Walk To Remember when they are looking at the stars
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.49922133 0.49922133 0.35520009 0.
  0.35520009]
 [0.         0.57615236 0.         0.         0.40993715 0.57615236
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: The part on A Walk To Remember when they are looking at the stars
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['the', 'part', 'on', 'a', 'walk', 'to', 'remember', 'when', 'they', 'are', 'looking', 'at', 'the', 'stars']
cosine_similarity: 0.9928199648857117
dev_input: [0.29121941856368966, 0.99281996], dev_label: 0
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: Turned on the tv and A Walk to Remember is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.49922133 0.35520009 0.         0.
  0.35520009]
 [0.         0.         0.         0.40993715 0.57615236 0.57615236
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: Turned on the tv and A Walk to Remember is on
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['turned', 'on', 'the', 'tv', 'and', 'a', 'walk', 'to', 'remember', 'is', 'on']
cosine_similarity: 0.9940493106842041
dev_input: [0.29121941856368966, 0.9940493], dev_label: 0
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: Watching A Walk To Remember is seriously making me ball right now it s so perfect
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.49922133 0.         0.49922133
  0.35520009 0.         0.         0.35520009 0.        ]
 [0.37762778 0.         0.37762778 0.         0.37762778 0.
  0.26868528 0.37762778 0.37762778 0.26868528 0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: Watching A Walk To Remember is seriously making me ball right now it s so perfect
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['watching', 'a', 'walk', 'to', 'remember', 'is', 'seriously', 'making', 'me', 'ball', 'right', 'now', 'it', 's', 'so', 'perfect']
cosine_similarity: 0.9878873229026794
dev_input: [0.1908740661302035, 0.9878873], dev_label: 1
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: When a walk to remember is on tv
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.49922133 0.35520009 0.         0.35520009]
 [0.         0.         0.         0.50154891 0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: When a walk to remember is on tv
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['when', 'a', 'walk', 'to', 'remember', 'is', 'on', 'tv']
cosine_similarity: 0.9957384467124939
dev_input: [0.3563004293331381, 0.99573845], dev_label: 0
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: a walk to remember is the only movie I like better than the book
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.4090901  0.4090901  0.57496187 0.4090901
  0.4090901 ]
 [0.49844628 0.49844628 0.35464863 0.35464863 0.         0.35464863
  0.35464863]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: a walk to remember is the only movie I like better than the book
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'the', 'only', 'movie', 'i', 'like', 'better', 'than', 'the', 'book']
cosine_similarity: 0.9932776689529419
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: AAP in that Adidas Commercial lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.57735027 0.57735027 0.        ]
 [0.44832087 0.44832087 0.44832087 0.63009934]]
pairwise_similarity: [[1.         0.77651453]
 [0.77651453 1.        ]]
cosine_similarity: 0.7765145304745156
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: AAP in that Adidas Commercial lol
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['aap', 'in', 'that', 'adidas', 'commercial', 'lol']
cosine_similarity: 0.9831011295318604
dev_input: [0.7765145304745156, 0.9831011], dev_label: 1
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: AAP was nice on that Adidas commercial
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.57735027 0.57735027 0.        ]
 [0.44832087 0.44832087 0.44832087 0.63009934]]
pairwise_similarity: [[1.         0.77651453]
 [0.77651453 1.        ]]
cosine_similarity: 0.7765145304745156
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: AAP was nice on that Adidas commercial
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['aap', 'was', 'nice', 'on', 'that', 'adidas', 'commercial']
cosine_similarity: 0.9807596206665039
dev_input: [0.7765145304745156, 0.9807596], dev_label: 1
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Dammnnnnnn that ASAP commercial for Adidas is sweet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.        ]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Dammnnnnnn that ASAP commercial for Adidas is sweet
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['that', 'asap', 'commercial', 'for', 'adidas', 'is', 'sweet']
cosine_similarity: 0.9391748905181885
dev_input: [0.3563004293331381, 0.9391749], dev_label: 0
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Rocky and john wall in that new quickaintfair Adidas commercial
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.         0.
  0.         0.        ]
 [0.         0.29017021 0.29017021 0.4078241  0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Rocky and john wall in that new quickaintfair Adidas commercial
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['rocky', 'and', 'john', 'wall', 'in', 'that', 'new', 'quickaintfair', 'adidas', 'commercial']
cosine_similarity: 0.9532425999641418
dev_input: [0.2910691023819054, 0.9532426], dev_label: 0
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Rocky killed that Adidas commercial too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.        ]
 [0.         0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Rocky killed that Adidas commercial too
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['rocky', 'killed', 'that', 'adidas', 'commercial', 'too']
cosine_similarity: 0.9264121055603027
dev_input: [0.4112070550676187, 0.9264121], dev_label: 0
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That AAP Rocky Adidas commerical is hard af
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.
  0.        ]
 [0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That AAP Rocky Adidas commerical is hard af
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['that', 'aap', 'rocky', 'adidas', 'commerical', 'is', 'hard', 'af']
cosine_similarity: 0.9520277380943298
dev_input: [0.31878402175377923, 0.95202774], dev_label: 1
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That Adidas ASAP Rocky commercial dope af
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891 0.
  0.        ]
 [0.         0.31779954 0.44665616 0.44665616 0.31779954 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That Adidas ASAP Rocky commercial dope af
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['that', 'adidas', 'asap', 'rocky', 'commercial', 'dope', 'af']
cosine_similarity: 0.8890411853790283
dev_input: [0.31878402175377923, 0.8890412], dev_label: 0
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That Adidas commercial with ASAP Rocky goes hard
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.        ]
 [0.         0.31779954 0.44665616 0.31779954 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That Adidas commercial with ASAP Rocky goes hard
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['that', 'adidas', 'commercial', 'with', 'asap', 'rocky', 'goes', 'hard']
cosine_similarity: 0.9248361587524414
dev_input: [0.31878402175377923, 0.92483616], dev_label: 0
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: The new Asap rocky Adidas commercial is fresh as shit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.         0.        ]
 [0.         0.29017021 0.4078241  0.29017021 0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: The new Asap rocky Adidas commercial is fresh as shit
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['the', 'new', 'asap', 'rocky', 'adidas', 'commercial', 'is', 'fresh', 'as', 'shit']
cosine_similarity: 0.9488048553466797
dev_input: [0.2910691023819054, 0.94880486], dev_label: 0
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: and Adidas commercial that was legit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.        ]
 [0.         0.50154891 0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: and Adidas commercial that was legit
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['and', 'adidas', 'commercial', 'that', 'was', 'legit']
cosine_similarity: 0.9446026682853699
dev_input: [0.5031026124151314, 0.94460267], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: About to get amanda bynes nudes tatted on my forearm
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: About to get amanda bynes nudes tatted on my forearm
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['about', 'to', 'get', 'amanda', 'bynes', 'nudes', 'tatted', 'on', 'my', 'forearm']
cosine_similarity: 0.9650211334228516
dev_input: [0.2523342014336961, 0.96502113], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Can someone explain what the fuck happened to Amanda Bynes face
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.         0.4090901  0.4090901
  0.        ]
 [0.35464863 0.35464863 0.         0.49844628 0.35464863 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Can someone explain what the fuck happened to Amanda Bynes face
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['can', 'someone', 'explain', 'what', 'the', 'fuck', 'happened', 'to', 'amanda', 'bynes', 'face']
cosine_similarity: 0.9859800934791565
dev_input: [0.5803329846765685, 0.9859801], dev_label: 1
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Dude Amanda bynes is like cracked out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.         0.49922133
  0.49922133 0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Dude Amanda bynes is like cracked out
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['dude', 'amanda', 'bynes', 'is', 'like', 'cracked', 'out']
cosine_similarity: 0.9614703059196472
dev_input: [0.2523342014336961, 0.9614703], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: For the love of God someone please 5150 Amanda Bynes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.49922133 0.49922133 0.49922133
  0.         0.        ]
 [0.49922133 0.35520009 0.35520009 0.         0.         0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: For the love of God someone please 5150 Amanda Bynes
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['for', 'the', 'love', 'of', 'god', 'someone', 'please', 'amanda', 'bynes']
cosine_similarity: 0.9606016874313354
dev_input: [0.2523342014336961, 0.9606017], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Hoes confused as to why everyone is infatuated with Amanda Bynes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.         0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Hoes confused as to why everyone is infatuated with Amanda Bynes
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['hoes', 'confused', 'as', 'to', 'why', 'everyone', 'is', 'infatuated', 'with', 'amanda', 'bynes']
cosine_similarity: 0.9732469916343689
dev_input: [0.2523342014336961, 0.973247], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: If Amanda Bynes can make it through the year I think we all can
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: If Amanda Bynes can make it through the year I think we all can
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['if', 'amanda', 'bynes', 'can', 'make', 'it', 'through', 'the', 'year', 'i', 'think', 'we', 'all', 'can']
cosine_similarity: 0.9652775526046753
dev_input: [0.2523342014336961, 0.96527755], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Im going to call the police on Amanda Bynes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Im going to call the police on Amanda Bynes
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['im', 'going', 'to', 'call', 'the', 'police', 'on', 'amanda', 'bynes']
cosine_similarity: 0.9701753854751587
dev_input: [0.2523342014336961, 0.9701754], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Is amanda bynes on crack or something
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]
 [0.50154891 0.50154891 0.70490949 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Is amanda bynes on crack or something
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['is', 'amanda', 'bynes', 'on', 'crack', 'or', 'something']
cosine_similarity: 0.9750398993492126
dev_input: [0.3563004293331381, 0.9750399], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: amanda bynes what the actual fuck happened to you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37930349 0.37930349 0.53309782 0.53309782 0.37930349
  0.        ]
 [0.53309782 0.37930349 0.37930349 0.         0.         0.37930349
  0.53309782]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: amanda bynes what the actual fuck happened to you
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['amanda', 'bynes', 'what', 'the', 'actual', 'fuck', 'happened', 'to', 'you']
cosine_similarity: 0.9834297895431519
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: wow what the hell has happened to amanda bynes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: wow what the hell has happened to amanda bynes
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['wow', 'what', 'the', 'hell', 'has', 'happened', 'to', 'amanda', 'bynes']
cosine_similarity: 0.9739439487457275
dev_input: [0.2523342014336961, 0.97394395], dev_label: 1
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: Colorado Mandala is NOW AVAILABLE for purchase on Amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.
  0.6316672 ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: Colorado Mandala is NOW AVAILABLE for purchase on Amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['colorado', 'mandala', 'is', 'now', 'available', 'for', 'purchase', 'on', 'amazon']
cosine_similarity: 0.9594444036483765
dev_input: [0.15064018498706508, 0.9594444], dev_label: 0
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: I am going to deliver the camera tomorrow when I purchased it in the Amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.
  0.6316672  0.        ]
 [0.         0.30321606 0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: I am going to deliver the camera tomorrow when I purchased it in the Amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['i', 'am', 'going', 'to', 'deliver', 'the', 'camera', 'tomorrow', 'when', 'i', 'purchased', 'it', 'in', 'the', 'amazon']
cosine_similarity: 0.9169626832008362
dev_input: [0.1362763414390864, 0.9169627], dev_label: 0
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: I got it from amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.6316672 ]
 [0.         0.57973867 0.81480247 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: I got it from amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['i', 'got', 'it', 'from', 'amazon']
cosine_similarity: 0.9146691560745239
dev_input: [0.2605556710562624, 0.91466916], dev_label: 0
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: I hope to win a 50 Amazon Gift Code
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.         0.
  0.6316672  0.        ]
 [0.4261596  0.         0.30321606 0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: I hope to win a 50 Amazon Gift Code
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['i', 'hope', 'to', 'win', 'a', 'amazon', 'gift', 'code']
cosine_similarity: 0.9539032578468323
dev_input: [0.1362763414390864, 0.95390326], dev_label: 0
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: Learn to Publish Your Hot Selling eBooks to Amazon Kindle
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.
  0.         0.6316672  0.        ]
 [0.         0.27894255 0.39204401 0.39204401 0.39204401 0.39204401
  0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: Learn to Publish Your Hot Selling eBooks to Amazon Kindle
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['learn', 'to', 'publish', 'your', 'hot', 'selling', 'ebooks', 'to', 'amazon', 'kindle']
cosine_similarity: 0.9446346759796143
dev_input: [0.12536693798731732, 0.9446347], dev_label: 0
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: May s prize on My Question of the Day is a 50 Amazon gift card
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.         0.
  0.         0.         0.6316672 ]
 [0.39204401 0.         0.27894255 0.39204401 0.39204401 0.39204401
  0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: May s prize on My Question of the Day is a 50 Amazon gift card
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['may', 's', 'prize', 'on', 'my', 'question', 'of', 'the', 'day', 'is', 'a', 'amazon', 'gift', 'card']
cosine_similarity: 0.949104905128479
dev_input: [0.12536693798731732, 0.9491049], dev_label: 0
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: Win a 25 Amazon Gift Card with MeBookshelfandI
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.         0.
  0.6316672  0.        ]
 [0.4261596  0.         0.30321606 0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: Win a 25 Amazon Gift Card with MeBookshelfandI
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['win', 'a', 'amazon', 'gift', 'card', 'with']
cosine_similarity: 0.9486595392227173
dev_input: [0.1362763414390864, 0.94865954], dev_label: 0
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: ahhh but did you know you can get this sort of stuff on amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.6316672
  0.         0.        ]
 [0.         0.4261596  0.30321606 0.4261596  0.4261596  0.
  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: ahhh but did you know you can get this sort of stuff on amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['ahhh', 'but', 'did', 'you', 'know', 'you', 'can', 'get', 'this', 'sort', 'of', 'stuff', 'on', 'amazon']
cosine_similarity: 0.8785088062286377
dev_input: [0.1362763414390864, 0.8785088], dev_label: 0
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: check out the Yeti on Amazon it s not too pricey
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672  0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: check out the Yeti on Amazon it s not too pricey
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['check', 'out', 'the', 'yeti', 'on', 'amazon', 'it', 's', 'not', 'too', 'pricey']
cosine_similarity: 0.9318085312843323
dev_input: [0.17077611319011649, 0.93180853], dev_label: 0
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: just bought doom 3 for xbox off amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.6316672
  0.        ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: just bought doom 3 for xbox off amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['just', 'bought', 'doom', 'for', 'xbox', 'off', 'amazon']
cosine_similarity: 0.9379237294197083
dev_input: [0.15064018498706508, 0.9379237], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Do not Amber Alert me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678]
 [0.70710678 0.70710678]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 0.9999999999999998
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Do not Amber Alert me
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['do', 'not', 'amber', 'alert', 'me']
cosine_similarity: 0.9999999403953552
dev_input: [0.9999999999999998, 0.99999994], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Everyone s Amber Alerts going off at Zarape
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.        ]
 [0.         0.53404633 0.37997836 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Everyone s Amber Alerts going off at Zarape
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['everyone', 's', 'amber', 'alerts', 'going', 'off', 'at']
cosine_similarity: 0.9097906947135925
dev_input: [0.22028815056182965, 0.9097907], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I get scared when I get the amber alert messages
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.        ]
 [0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I get scared when I get the amber alert messages
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['i', 'get', 'scared', 'when', 'i', 'get', 'the', 'amber', 'alert', 'messages']
cosine_similarity: 0.9385988712310791
dev_input: [0.5797386715376657, 0.9385989], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I wonder if the kidnapper gets amber alerts as well
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.         0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I wonder if the kidnapper gets amber alerts as well
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['i', 'wonder', 'if', 'the', 'kidnapper', 'gets', 'amber', 'alerts', 'as', 'well']
cosine_similarity: 0.9371524453163147
dev_input: [0.19431434016858146, 0.93715245], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I would have put Amber in the top 2
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867]
 [0.         1.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I would have put Amber in the top 2
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['i', 'would', 'have', 'put', 'amber', 'in', 'the', 'top']
cosine_similarity: 0.9355815649032593
dev_input: [0.5797386715376657, 0.93558156], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Just found out Amber was eliminated
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.        ]
 [0.         0.44943642 0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Just found out Amber was eliminated
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['just', 'found', 'out', 'amber', 'was', 'eliminated']
cosine_similarity: 0.8901304006576538
dev_input: [0.2605556710562624, 0.8901304], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Look Im tired if these damn Amber Alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.         0.
  0.        ]
 [0.         0.4261596  0.30321606 0.4261596  0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Look Im tired if these damn Amber Alerts
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['look', 'im', 'tired', 'if', 'these', 'damn', 'amber', 'alerts']
cosine_similarity: 0.9126509428024292
dev_input: [0.17578607839334617, 0.91265094], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Plus Amber has a huge career
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Plus Amber has a huge career
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['plus', 'amber', 'has', 'a', 'huge', 'career']
cosine_similarity: 0.8870217800140381
dev_input: [0.22028815056182965, 0.8870218], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: im going to miss Amber on idol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.         0.        ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: im going to miss Amber on idol
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['im', 'going', 'to', 'miss', 'amber', 'on', 'idol']
cosine_similarity: 0.9308733940124512
dev_input: [0.19431434016858146, 0.9308734], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber alert gave me a damn heart attack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.57615236 0.        ]
 [0.31779954 0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber alert gave me a damn heart attack
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['amber', 'alert', 'gave', 'me', 'a', 'damn', 'heart', 'attack']
cosine_similarity: 0.9151096343994141
dev_input: [0.2605556710562624, 0.91510963], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber shouldnt have gone home
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.
  0.        ]
 [0.         0.37997836 0.         0.         0.53404633 0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber shouldnt have gone home
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['amber', 'shouldnt', 'have', 'gone', 'home']
cosine_similarity: 0.9532579779624939
dev_input: [0.1443835552773867, 0.953258], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Everyone keeps talking about amber alerts on their phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.         0.47107781
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Everyone keeps talking about amber alerts on their phone
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['everyone', 'keeps', 'talking', 'about', 'amber', 'alerts', 'on', 'their', 'phone']
cosine_similarity: 0.945505678653717
dev_input: [0.1273595297947935, 0.9455057], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: I get to see my Amber
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633]
 [0.         1.         0.         0.        ]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: I get to see my Amber
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['i', 'get', 'to', 'see', 'my', 'amber']
cosine_similarity: 0.9339374899864197
dev_input: [0.37997836159100784, 0.9339375], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: I thought amber alerts would only sound on androids
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.         0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: I thought amber alerts would only sound on androids
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['i', 'thought', 'amber', 'alerts', 'would', 'only', 'sound', 'on', 'androids']
cosine_similarity: 0.9612979888916016
dev_input: [0.1273595297947935, 0.961298], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Ive gotten the same amber alert 3 times
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Ive gotten the same amber alert 3 times
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['ive', 'gotten', 'the', 'same', 'amber', 'alert', 'times']
cosine_similarity: 0.9734466075897217
dev_input: [0.29121941856368966, 0.9734466], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Ok this is the fifth amber alert Ive received
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236 0.
  0.         0.        ]
 [0.31779954 0.31779954 0.         0.44665616 0.         0.44665616
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Ok this is the fifth amber alert Ive received
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['ok', 'this', 'is', 'the', 'fifth', 'amber', 'alert', 'ive', 'received']
cosine_similarity: 0.956674337387085
dev_input: [0.2605556710562624, 0.95667434], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Wats up with this amber alerts going off in church
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Wats up with this amber alerts going off in church
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['wats', 'up', 'with', 'this', 'amber', 'alerts', 'going', 'off', 'in', 'church']
cosine_similarity: 0.9436097741127014
dev_input: [0.1273595297947935, 0.9436098], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Why do I get amber alerts tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.        ]
 [0.         0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Why do I get amber alerts tho
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['why', 'do', 'i', 'get', 'amber', 'alerts', 'tho']
cosine_similarity: 0.9413134455680847
dev_input: [0.17077611319011649, 0.94131345], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Am I a bad person if I disabled the amber alert texts from my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.         0.57615236 0.        ]
 [0.29017021 0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Am I a bad person if I disabled the amber alert texts from my phone
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['am', 'i', 'a', 'bad', 'person', 'if', 'i', 'disabled', 'the', 'amber', 'alert', 'texts', 'from', 'my', 'phone']
cosine_similarity: 0.9815788269042969
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Amber is the cutest person ever
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]
 [0.         0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Amber is the cutest person ever
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['amber', 'is', 'the', 'cutest', 'person', 'ever']
cosine_similarity: 0.9629343152046204
dev_input: [0.17077611319011649, 0.9629343], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Everytime I get an amber alert on my phone I get freaked out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Everytime I get an amber alert on my phone I get freaked out
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['everytime', 'i', 'get', 'an', 'amber', 'alert', 'on', 'my', 'phone', 'i', 'get', 'freaked', 'out']
cosine_similarity: 0.9761330485343933
dev_input: [0.29121941856368966, 0.97613305], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: I wish these amber alerts would stop coming to my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.53404633 0.         0.        ]
 [0.         0.4261596  0.30321606 0.4261596  0.         0.4261596
  0.         0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: I wish these amber alerts would stop coming to my phone
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['i', 'wish', 'these', 'amber', 'alerts', 'would', 'stop', 'coming', 'to', 'my', 'phone']
cosine_similarity: 0.9723052978515625
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: If these amber alerts send me one more thing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.        ]
 [0.         0.53404633 0.37997836 0.         0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: If these amber alerts send me one more thing
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['if', 'these', 'amber', 'alerts', 'send', 'me', 'one', 'more', 'thing']
cosine_similarity: 0.9730462431907654
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Pray for the Amber Alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236]
 [0.50154891 0.50154891 0.         0.70490949 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Pray for the Amber Alert
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['pray', 'for', 'the', 'amber', 'alert']
cosine_similarity: 0.9498448371887207
dev_input: [0.4112070550676187, 0.94984484], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: These amber alerts are so annoying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.53404633]
 [0.         0.6316672  0.44943642 0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: These amber alerts are so annoying
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['these', 'amber', 'alerts', 'are', 'so', 'annoying']
cosine_similarity: 0.9492233395576477
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: This amber alert has popped up on my phone 3 times like DAMNNN
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.         0.57615236 0.        ]
 [0.29017021 0.29017021 0.         0.4078241  0.4078241  0.4078241
  0.4078241  0.         0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: This amber alert has popped up on my phone 3 times like DAMNNN
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['this', 'amber', 'alert', 'has', 'popped', 'up', 'on', 'my', 'phone', 'times', 'like']
cosine_similarity: 0.9882194399833679
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Why the fuck am I getting amber alerts on my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.
  0.         0.53404633]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Why the fuck am I getting amber alerts on my phone
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['why', 'the', 'fuck', 'am', 'i', 'getting', 'amber', 'alerts', 'on', 'my', 'phone']
cosine_similarity: 0.985689640045166
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Am I the only one who dont get Amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633]
 [0.6316672  0.         0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Am I the only one who dont get Amber alert
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['am', 'i', 'the', 'only', 'one', 'who', 'dont', 'get', 'amber', 'alert']
cosine_similarity: 0.9675059914588928
dev_input: [0.17077611319011649, 0.967506], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: How long has Amber been missing because I keep getting alerts about her
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: How long has Amber been missing because I keep getting alerts about her
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['how', 'long', 'has', 'amber', 'been', 'missing', 'because', 'i', 'keep', 'getting', 'alerts', 'about', 'her']
cosine_similarity: 0.9650647640228271
dev_input: [0.29121941856368966, 0.96506476], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I aint got no amber alerts yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.         0.57615236]
 [0.57615236 0.40993715 0.40993715 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I aint got no amber alerts yet
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['i', 'aint', 'got', 'no', 'amber', 'alerts', 'yet']
cosine_similarity: 0.9662990570068359
dev_input: [0.3360969272762575, 0.96629906], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I hate the amber alert sound
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I hate the amber alert sound
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['i', 'hate', 'the', 'amber', 'alert', 'sound']
cosine_similarity: 0.9742957353591919
dev_input: [0.1443835552773867, 0.97429574], dev_label: 1
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Just got the same amber alert three times today smh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.53404633 0.         0.         0.        ]
 [0.39204401 0.         0.27894255 0.         0.39204401 0.39204401
  0.         0.39204401 0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Just got the same amber alert three times today smh
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['just', 'got', 'the', 'same', 'amber', 'alert', 'three', 'times', 'today', 'smh']
cosine_similarity: 0.9669142365455627
dev_input: [0.1059921313509325, 0.96691424], dev_label: 1
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: That amber alert was getting annoying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.40993715 0.         0.57615236]
 [0.57615236 0.         0.40993715 0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: That amber alert was getting annoying
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['that', 'amber', 'alert', 'was', 'getting', 'annoying']
cosine_similarity: 0.9576845765113831
dev_input: [0.3360969272762575, 0.9576846], dev_label: 1
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: These amber alert things scare me everytime they go off on my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.57615236 0.         0.40993715
  0.         0.        ]
 [0.44665616 0.         0.31779954 0.         0.44665616 0.31779954
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: These amber alert things scare me everytime they go off on my phone
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['these', 'amber', 'alert', 'things', 'scare', 'me', 'everytime', 'they', 'go', 'off', 'on', 'my', 'phone']
cosine_similarity: 0.9887235760688782
dev_input: [0.2605556710562624, 0.9887236], dev_label: 1
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: These amber alerts on my phone always freak me out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.44832087]
 [0.44832087 0.44832087 0.         0.63009934 0.44832087]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: These amber alerts on my phone always freak me out
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['these', 'amber', 'alerts', 'on', 'my', 'phone', 'always', 'freak', 'me', 'out']
cosine_similarity: 0.9939929842948914
dev_input: [0.6029748160380572, 0.993993], dev_label: 1
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Turns out it s just another amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Turns out it s just another amber alert
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['turns', 'out', 'it', 's', 'just', 'another', 'amber', 'alert']
cosine_similarity: 0.9735161662101746
dev_input: [0.1443835552773867, 0.97351617], dev_label: 1
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: Does anybody else keep gettin this Amber Alert notification
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.         0.         0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.44943642]
 [0.44943642 1.        ]]
cosine_similarity: 0.4494364165239821
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: Does anybody else keep gettin this Amber Alert notification
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['does', 'anybody', 'else', 'keep', 'gettin', 'this', 'amber', 'alert', 'notification']
cosine_similarity: 0.9353646039962769
dev_input: [0.4494364165239821, 0.9353646], dev_label: 1
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: Husband s phone just had an Amber Alert Warning
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.         0.         0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.44943642]
 [0.44943642 1.        ]]
cosine_similarity: 0.4494364165239821
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: Husband s phone just had an Amber Alert Warning
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['husband', 's', 'phone', 'just', 'had', 'an', 'amber', 'alert', 'warning']
cosine_similarity: 0.980297327041626
dev_input: [0.4494364165239821, 0.9802973], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: I Know You Are Very Upset That Amber Went Home
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.         0.        ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: I Know You Are Very Upset That Amber Went Home
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['i', 'know', 'you', 'are', 'very', 'upset', 'that', 'amber', 'went', 'home']
cosine_similarity: 0.9401889443397522
dev_input: [0.19431434016858146, 0.94018894], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: If these amber alerts send me one more thing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.        ]
 [0.         0.53404633 0.37997836 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: If these amber alerts send me one more thing
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['if', 'these', 'amber', 'alerts', 'send', 'me', 'one', 'more', 'thing']
cosine_similarity: 0.9208953380584717
dev_input: [0.22028815056182965, 0.92089534], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: OMG AMBER I FREAKING HATE YOU
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: OMG AMBER I FREAKING HATE YOU
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['omg', 'amber', 'i', 'freaking', 'hate', 'you']
cosine_similarity: 0.9099948406219482
dev_input: [0.22028815056182965, 0.90999484], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: Oh Now I Know What The Amber Alert is O
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.        ]
 [0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: Oh Now I Know What The Amber Alert is O
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['oh', 'now', 'i', 'know', 'what', 'the', 'amber', 'alert', 'is', 'o']
cosine_similarity: 0.964421272277832
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: Pray for the Amber Alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.        ]
 [0.50154891 0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062738
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: Pray for the Amber Alert
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['pray', 'for', 'the', 'amber', 'alert']
cosine_similarity: 0.9476858377456665
dev_input: [0.7092972666062738, 0.94768584], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: These Amber Alerts are really annoying me right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.         0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: These Amber Alerts are really annoying me right now
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['these', 'amber', 'alerts', 'are', 'really', 'annoying', 'me', 'right', 'now']
cosine_similarity: 0.9225622415542603
dev_input: [0.19431434016858146, 0.92256224], dev_label: 0
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: Why I keep getting these Amber alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.        ]
 [0.         0.6316672  0.44943642 0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: Why I keep getting these Amber alerts
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['why', 'i', 'keep', 'getting', 'these', 'amber', 'alerts']
cosine_similarity: 0.9293582439422607
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: All these amber alerts leave people s kids alone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.57615236
  0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: All these amber alerts leave people s kids alone
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['all', 'these', 'amber', 'alerts', 'leave', 'people', 's', 'kids', 'alone']
cosine_similarity: 0.9790778756141663
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: I got this amber alert twice o
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: I got this amber alert twice o
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['i', 'got', 'this', 'amber', 'alert', 'twice', 'o']
cosine_similarity: 0.9552806615829468
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: I just got a third Amber Alert and Im crying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.         0.         0.53404633]
 [0.4261596  0.         0.30321606 0.         0.4261596  0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: I just got a third Amber Alert and Im crying
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['i', 'just', 'got', 'a', 'third', 'amber', 'alert', 'and', 'im', 'crying']
cosine_similarity: 0.9576783180236816
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: Ive been getting amber alerts all day like what the fuck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.         0.         0.57615236]
 [0.29017021 0.29017021 0.         0.4078241  0.4078241  0.4078241
  0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: Ive been getting amber alerts all day like what the fuck
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['ive', 'been', 'getting', 'amber', 'alerts', 'all', 'day', 'like', 'what', 'the', 'fuck']
cosine_similarity: 0.9717450737953186
dev_input: [0.23790309463326234, 0.9717451], dev_label: 1
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: My phone NEVER sends me Amber Alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: My phone NEVER sends me Amber Alerts
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['my', 'phone', 'never', 'sends', 'me', 'amber', 'alerts']
cosine_similarity: 0.9481172561645508
dev_input: [0.3360969272762575, 0.94811726], dev_label: 0
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: That s 5 amber alerts today Ive got
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.57615236
  0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: That s 5 amber alerts today Ive got
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['that', 's', 'amber', 'alerts', 'today', 'ive', 'got']
cosine_similarity: 0.9690792560577393
dev_input: [0.29121941856368966, 0.96907926], dev_label: 1
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: That s the third amber alert I got today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: That s the third amber alert I got today
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['that', 's', 'the', 'third', 'amber', 'alert', 'i', 'got', 'today']
cosine_similarity: 0.9587340354919434
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: These amber alert messages are scary as fudge man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.         0.53404633 0.        ]
 [0.4261596  0.         0.30321606 0.         0.4261596  0.4261596
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: These amber alert messages are scary as fudge man
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['these', 'amber', 'alert', 'messages', 'are', 'scary', 'as', 'fudge', 'man']
cosine_similarity: 0.9556382298469543
dev_input: [0.11521554337793122, 0.95563823], dev_label: 0
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: Well that Amber Alert alert just scared tf outta me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.         0.         0.        ]
 [0.68574252 0.         0.24395573 0.         0.34287126 0.
  0.34287126 0.34287126 0.34287126]]
pairwise_similarity: [[1.        0.0926979]
 [0.0926979 1.       ]]
cosine_similarity: 0.09269789668627057
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: Well that Amber Alert alert just scared tf outta me
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['well', 'that', 'amber', 'alert', 'alert', 'just', 'scared', 'tf', 'outta', 'me']
cosine_similarity: 0.9578795433044434
dev_input: [0.09269789668627057, 0.95787954], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: All these amber alerts leave people s kids alone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: All these amber alerts leave people s kids alone
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['all', 'these', 'amber', 'alerts', 'leave', 'people', 's', 'kids', 'alone']
cosine_similarity: 0.972187876701355
dev_input: [0.29121941856368966, 0.9721879], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Amber alerts freak me out when I get them on my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.44832087]
 [0.44832087 0.44832087 0.         0.63009934 0.44832087]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Amber alerts freak me out when I get them on my phone
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['amber', 'alerts', 'freak', 'me', 'out', 'when', 'i', 'get', 'them', 'on', 'my', 'phone']
cosine_similarity: 0.9905191659927368
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I got 4 AMBER ALERTS today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236 0.        ]
 [0.40993715 0.40993715 0.         0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I got 4 AMBER ALERTS today
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['i', 'got', 'amber', 'alerts', 'today']
cosine_similarity: 0.9631671905517578
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I hate these damn amber alerts coming to my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.         0.
  0.44832087]
 [0.33471228 0.33471228 0.         0.47042643 0.47042643 0.47042643
  0.33471228]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I hate these damn amber alerts coming to my phone
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['i', 'hate', 'these', 'damn', 'amber', 'alerts', 'coming', 'to', 'my', 'phone']
cosine_similarity: 0.9864579439163208
dev_input: [0.4501755023269898, 0.98645794], dev_label: 1
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I just got like my 5th amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.53404633 0.37997836 0.53404633 0.
  0.         0.         0.53404633]
 [0.4261596  0.4261596  0.         0.30321606 0.         0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I just got like my 5th amber alert
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['i', 'just', 'got', 'like', 'my', 'amber', 'alert']
cosine_similarity: 0.9774119853973389
dev_input: [0.11521554337793122, 0.977412], dev_label: 1
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Omfg Who The FUCK Invented This Fucking Amber Alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.         0.         0.53404633]
 [0.4261596  0.         0.30321606 0.         0.4261596  0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Omfg Who The FUCK Invented This Fucking Amber Alert
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['omfg', 'who', 'the', 'fuck', 'invented', 'this', 'fucking', 'amber', 'alert']
cosine_similarity: 0.9713367819786072
dev_input: [0.11521554337793122, 0.9713368], dev_label: 1
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: We all will miss our AMBER
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.53404633]
 [0.         0.57973867 0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: We all will miss our AMBER
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['we', 'all', 'will', 'miss', 'our', 'amber']
cosine_similarity: 0.9450591802597046
dev_input: [0.22028815056182965, 0.9450592], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: cx whats an amber alert thooo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.         0.        ]
 [0.47107781 0.         0.33517574 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: cx whats an amber alert thooo
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['cx', 'whats', 'an', 'amber', 'alert']
cosine_similarity: 0.9563042521476746
dev_input: [0.1273595297947935, 0.95630425], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: what is this amber alert thing on my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.57615236 0.40993715 0.        ]
 [0.57615236 0.         0.40993715 0.         0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: what is this amber alert thing on my phone
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['what', 'is', 'this', 'amber', 'alert', 'thing', 'on', 'my', 'phone']
cosine_similarity: 0.98467618227005
dev_input: [0.3360969272762575, 0.9846762], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber gone be a modelsinger
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]
 [0.         0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber gone be a modelsinger
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['amber', 'gone', 'be', 'a']
cosine_similarity: 0.9147270917892456
dev_input: [0.17077611319011649, 0.9147271], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Can we chill with the amber alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633]
 [0.         0.6316672  0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Can we chill with the amber alerts
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['can', 'we', 'chill', 'with', 'the', 'amber', 'alerts']
cosine_similarity: 0.9491514563560486
dev_input: [0.17077611319011649, 0.94915146], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Dam amber alert i got to my phone scared the hell out of me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236 0.
  0.         0.         0.        ]
 [0.29017021 0.29017021 0.         0.4078241  0.         0.4078241
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Dam amber alert i got to my phone scared the hell out of me
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['dam', 'amber', 'alert', 'i', 'got', 'to', 'my', 'phone', 'scared', 'the', 'hell', 'out', 'of', 'me']
cosine_similarity: 0.9593026638031006
dev_input: [0.23790309463326234, 0.95930266], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Everyone s Amber Alerts going off at Zarape
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.        ]
 [0.         0.53404633 0.37997836 0.         0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Everyone s Amber Alerts going off at Zarape
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['everyone', 's', 'amber', 'alerts', 'going', 'off', 'at']
cosine_similarity: 0.9165230393409729
dev_input: [0.1443835552773867, 0.91652304], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: How come I never get amber alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633]
 [0.         0.6316672  0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: How come I never get amber alerts
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['how', 'come', 'i', 'never', 'get', 'amber', 'alerts']
cosine_similarity: 0.9481540322303772
dev_input: [0.17077611319011649, 0.94815403], dev_label: 0
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: OH MY GOD FUCK THESE AMBER ALERTS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: OH MY GOD FUCK THESE AMBER ALERTS
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['oh', 'my', 'god', 'fuck', 'these', 'amber', 'alerts']
cosine_similarity: 0.9454683065414429
dev_input: [0.1273595297947935, 0.9454683], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: That s 5 amber alerts today Ive got
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.         0.47107781
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: That s 5 amber alerts today Ive got
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['that', 's', 'amber', 'alerts', 'today', 'ive', 'got']
cosine_similarity: 0.9632984399795532
dev_input: [0.1273595297947935, 0.96329844], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: These amber alerts on my phone been going off ALL day
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: These amber alerts on my phone been going off ALL day
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['these', 'amber', 'alerts', 'on', 'my', 'phone', 'been', 'going', 'off', 'all', 'day']
cosine_similarity: 0.9495298266410828
dev_input: [0.1273595297947935, 0.9495298], dev_label: 1
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: how would you not know what an amber alert is
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.        ]
 [0.50154891 0.50154891 0.         0.         0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: how would you not know what an amber alert is
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['how', 'would', 'you', 'not', 'know', 'what', 'an', 'amber', 'alert', 'is']
cosine_similarity: 0.9684567451477051
dev_input: [0.4112070550676187, 0.96845675], dev_label: 0
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: I need a amber alert to wake tf up in da mornings Ha
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.6316672  0.
  0.         0.         0.         0.        ]
 [0.36499647 0.         0.25969799 0.36499647 0.         0.36499647
  0.36499647 0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: I need a amber alert to wake tf up in da mornings Ha
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['i', 'need', 'a', 'amber', 'alert', 'to', 'wake', 'tf', 'up', 'in', 'da', 'mornings', 'ha']
cosine_similarity: 0.9622078537940979
dev_input: [0.11671773546032795, 0.96220785], dev_label: 0
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: I woke up to that Amber Alert I got
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.6316672  0.         0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: I woke up to that Amber Alert I got
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['i', 'woke', 'up', 'to', 'that', 'amber', 'alert', 'i', 'got']
cosine_similarity: 0.9836907386779785
dev_input: [0.17077611319011649, 0.98369074], dev_label: 0
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: Ive had like 3 Amber Alerts today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: Ive had like 3 Amber Alerts today
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['ive', 'had', 'like', 'amber', 'alerts', 'today']
cosine_similarity: 0.9768241047859192
dev_input: [0.3563004293331381, 0.9768241], dev_label: 0
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: My phone keeps sending me amber alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: My phone keeps sending me amber alerts
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['my', 'phone', 'keeps', 'sending', 'me', 'amber', 'alerts']
cosine_similarity: 0.9347301721572876
dev_input: [0.3563004293331381, 0.9347302], dev_label: 0
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: My phone started ringing and vibrated in class today for a amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.6316672  0.
  0.         0.         0.         0.        ]
 [0.36499647 0.         0.25969799 0.36499647 0.         0.36499647
  0.36499647 0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: My phone started ringing and vibrated in class today for a amber alert
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['my', 'phone', 'started', 'ringing', 'and', 'vibrated', 'in', 'class', 'today', 'for', 'a', 'amber', 'alert']
cosine_similarity: 0.9710473418235779
dev_input: [0.11671773546032795, 0.97104734], dev_label: 0
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: That amber alert on my phone be scaring the shit out of me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.6316672  0.         0.
  0.        ]
 [0.47107781 0.         0.33517574 0.         0.47107781 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: That amber alert on my phone be scaring the shit out of me
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['that', 'amber', 'alert', 'on', 'my', 'phone', 'be', 'scaring', 'the', 'shit', 'out', 'of', 'me']
cosine_similarity: 0.9799400568008423
dev_input: [0.15064018498706508, 0.97994006], dev_label: 0
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: This Amber alert keep cuttin my songs off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.6316672  0.        ]
 [0.53404633 0.         0.37997836 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: This Amber alert keep cuttin my songs off
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['this', 'amber', 'alert', 'keep', 'cuttin', 'my', 'songs', 'off']
cosine_similarity: 0.9648913145065308
dev_input: [0.17077611319011649, 0.9648913], dev_label: 0
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: Who is Amber and why does she keep sending me these alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: Who is Amber and why does she keep sending me these alerts
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['who', 'is', 'amber', 'and', 'why', 'does', 'she', 'keep', 'sending', 'me', 'these', 'alerts']
cosine_similarity: 0.9808192849159241
dev_input: [0.4112070550676187, 0.9808193], dev_label: 0
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: there has been alot of amber alerts today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.70490949 0.        ]
 [0.40993715 0.57615236 0.40993715 0.         0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: there has been alot of amber alerts today
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['there', 'has', 'been', 'alot', 'of', 'amber', 'alerts', 'today']
cosine_similarity: 0.9841583967208862
dev_input: [0.4112070550676187, 0.9841584], dev_label: 0
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: Amber Alerts to my iPhone scare me EVERYTIME
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.6316672  0.         0.
  0.        ]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: Amber Alerts to my iPhone scare me EVERYTIME
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['amber', 'alerts', 'to', 'my', 'iphone', 'scare', 'me', 'everytime']
cosine_similarity: 0.9605346918106079
dev_input: [0.15064018498706508, 0.9605347], dev_label: 1
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I aint got no amber alerts yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.         0.44943642 0.6316672  0.        ]
 [0.53404633 0.         0.53404633 0.37997836 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I aint got no amber alerts yet
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['i', 'aint', 'got', 'no', 'amber', 'alerts', 'yet']
cosine_similarity: 0.9754526615142822
dev_input: [0.17077611319011649, 0.97545266], dev_label: 0
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I have gotten 3 amber alert notices today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I have gotten 3 amber alert notices today
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['i', 'have', 'gotten', 'amber', 'alert', 'notices', 'today']
cosine_similarity: 0.9678113460540771
dev_input: [0.3563004293331381, 0.96781135], dev_label: 1
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I loved Amber the whole season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.6316672  0.         0.        ]
 [0.         0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I loved Amber the whole season
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['i', 'loved', 'amber', 'the', 'whole', 'season']
cosine_similarity: 0.9678996801376343
dev_input: [0.20199309249791833, 0.9678997], dev_label: 0
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: If I get one more of these loud ass amber alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.6316672  0.        ]
 [0.         0.53404633 0.37997836 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: If I get one more of these loud ass amber alerts
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['if', 'i', 'get', 'one', 'more', 'of', 'these', 'loud', 'ass', 'amber', 'alerts']
cosine_similarity: 0.9897758960723877
dev_input: [0.17077611319011649, 0.9897759], dev_label: 1
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: One more Amber Alert today Im throwing a bitch fit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.
  0.         0.        ]
 [0.29017021 0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: One more Amber Alert today Im throwing a bitch fit
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['one', 'more', 'amber', 'alert', 'today', 'im', 'throwing', 'a', 'bitch', 'fit']
cosine_similarity: 0.9876754283905029
dev_input: [0.2910691023819054, 0.9876754], dev_label: 1
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: That amber alert just scared the beep outta me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.
  0.        ]
 [0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: That amber alert just scared the beep outta me
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['that', 'amber', 'alert', 'just', 'scared', 'the', 'beep', 'outta', 'me']
cosine_similarity: 0.975342333316803
dev_input: [0.31878402175377923, 0.97534233], dev_label: 1
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: That amber girl got lost again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.6316672  0.         0.         0.        ]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: That amber girl got lost again
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['that', 'amber', 'girl', 'got', 'lost', 'again']
cosine_similarity: 0.9863369464874268
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: This is the 4th time Ive gotten that amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.50154891 0.70490949 0.         0.
  0.        ]
 [0.44665616 0.31779954 0.31779954 0.         0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: This is the 4th time Ive gotten that amber alert
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['this', 'is', 'the', 'time', 'ive', 'gotten', 'that', 'amber', 'alert']
cosine_similarity: 0.9812307953834534
dev_input: [0.31878402175377923, 0.9812308], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: ANDRE MILLER IS APART OF MY TOP 10
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.31779954 0.44665616]
 [0.57615236 0.40993715 0.57615236 0.         0.         0.
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: ANDRE MILLER IS APART OF MY TOP 10
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'is', 'apart', 'of', 'my', 'top']
cosine_similarity: 0.962755560874939
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller is a liability on defense
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.         0.44665616
  0.31779954 0.44665616]
 [0.40993715 0.         0.57615236 0.         0.57615236 0.
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller is a liability on defense
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'is', 'a', 'liability', 'on', 'defense']
cosine_similarity: 0.9412304162979126
dev_input: [0.2605556710562624, 0.9412304], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller is even slower in person
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.31779954 0.
  0.44665616 0.        ]
 [0.40993715 0.         0.         0.         0.40993715 0.57615236
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller is even slower in person
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'is', 'even', 'slower', 'in', 'person']
cosine_similarity: 0.9401905536651611
dev_input: [0.2605556710562624, 0.94019055], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller wit dat old man game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.33471228 0.47042643 0.
  0.33471228 0.         0.47042643 0.        ]
 [0.30287281 0.         0.42567716 0.30287281 0.         0.42567716
  0.30287281 0.42567716 0.         0.42567716]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller wit dat old man game
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'wit', 'dat', 'old', 'man', 'game']
cosine_similarity: 0.9437281489372253
dev_input: [0.30412574187549346, 0.94372815], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller you a bitch shut up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.44665616 0.31779954
  0.44665616 0.        ]
 [0.40993715 0.         0.57615236 0.         0.         0.40993715
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller you a bitch shut up
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'you', 'a', 'bitch', 'shut', 'up']
cosine_similarity: 0.9045161604881287
dev_input: [0.2605556710562624, 0.90451616], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: How does Andre miller still move this fast
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.         0.44665616 0.44665616
  0.31779954 0.44665616]
 [0.40993715 0.         0.57615236 0.57615236 0.         0.
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: How does Andre miller still move this fast
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['how', 'does', 'andre', 'miller', 'still', 'move', 'this', 'fast']
cosine_similarity: 0.9428938627243042
dev_input: [0.2605556710562624, 0.94289386], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: The fact that Andre miller is consistent really pisses me off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.         0.44665616 0.44665616
  0.31779954 0.44665616 0.         0.        ]
 [0.31779954 0.         0.44665616 0.44665616 0.         0.
  0.31779954 0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: The fact that Andre miller is consistent really pisses me off
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['the', 'fact', 'that', 'andre', 'miller', 'is', 'consistent', 'really', 'pisses', 'me', 'off']
cosine_similarity: 0.9284635782241821
dev_input: [0.20199309249791833, 0.9284636], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Warriors need a vet like Andre Miller on they team
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.         0.44665616 0.31779954
  0.         0.44665616 0.         0.         0.        ]
 [0.29017021 0.         0.         0.4078241  0.         0.29017021
  0.4078241  0.         0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Warriors need a vet like Andre Miller on they team
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['warriors', 'need', 'a', 'vet', 'like', 'andre', 'miller', 'on', 'they', 'team']
cosine_similarity: 0.9564310908317566
dev_input: [0.18443191662261305, 0.9564311], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Why is Andre Miller taking 3 s with his broke jumper
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.         0.44665616
  0.31779954 0.44665616 0.        ]
 [0.35520009 0.         0.49922133 0.         0.49922133 0.
  0.35520009 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Why is Andre Miller taking 3 s with his broke jumper
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['why', 'is', 'andre', 'miller', 'taking', 's', 'with', 'his', 'broke', 'jumper']
cosine_similarity: 0.9555771946907043
dev_input: [0.22576484600261604, 0.9555772], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: andre Miller aint never had no jumper
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.31779954 0.44665616]
 [0.57615236 0.40993715 0.         0.         0.57615236 0.
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: andre Miller aint never had no jumper
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'aint', 'never', 'had', 'no', 'jumper']
cosine_similarity: 0.915012776851654
dev_input: [0.2605556710562624, 0.9150128], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut about to die on the court
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut about to die on the court
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'about', 'to', 'die', 'on', 'the', 'court']
cosine_similarity: 0.9724290370941162
dev_input: [0.29121941856368966, 0.97242904], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut bitch ass goin off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.35520009 0.49922133 0.49922133
  0.         0.49922133]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.         0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut bitch ass goin off
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'bitch', 'ass', 'goin', 'off']
cosine_similarity: 0.883537769317627
dev_input: [0.2523342014336961, 0.88353777], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut is playing like ah 1 Overall Draft Pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.         0.49922133 0.49922133
  0.         0.         0.         0.49922133 0.        ]
 [0.37762778 0.26868528 0.26868528 0.37762778 0.         0.
  0.37762778 0.37762778 0.37762778 0.         0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut is playing like ah 1 Overall Draft Pick
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'is', 'playing', 'like', 'ah', 'overall', 'draft', 'pick']
cosine_similarity: 0.9759834408760071
dev_input: [0.1908740661302035, 0.97598344], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut just went behind his back
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.        ]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut just went behind his back
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'just', 'went', 'behind', 'his', 'back']
cosine_similarity: 0.9591800570487976
dev_input: [0.29121941856368966, 0.95918006], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut keeping Golden State in the game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.37930349 0.         0.
  0.53309782 0.        ]
 [0.33471228 0.33471228 0.         0.33471228 0.47042643 0.47042643
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut keeping Golden State in the game
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'keeping', 'golden', 'state', 'in', 'the', 'game']
cosine_similarity: 0.9735677242279053
dev_input: [0.38087260847594373, 0.9735677], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut looking like he did at the University of Utah
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.         0.49922133 0.         0.        ]
 [0.29017021 0.29017021 0.4078241  0.         0.         0.4078241
  0.4078241  0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut looking like he did at the University of Utah
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'looking', 'like', 'he', 'did', 'at', 'the', 'university', 'of', 'utah']
cosine_similarity: 0.9767675399780273
dev_input: [0.20613696606828605, 0.97676754], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Is Andrew Bogut rocking the stealth mullet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Is Andrew Bogut rocking the stealth mullet
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['is', 'andrew', 'bogut', 'rocking', 'the', 'stealth', 'mullet']
cosine_similarity: 0.923993706703186
dev_input: [0.2523342014336961, 0.9239937], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Literally nobody on Earth likes Andrew Bogut
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.         0.49922133]
 [0.35520009 0.35520009 0.49922133 0.         0.         0.49922133
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Literally nobody on Earth likes Andrew Bogut
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['literally', 'nobody', 'on', 'earth', 'likes', 'andrew', 'bogut']
cosine_similarity: 0.9579567909240723
dev_input: [0.2523342014336961, 0.9579568], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: So is Andrew Bogut juicing or
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133]
 [0.50154891 0.50154891 0.         0.         0.70490949 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: So is Andrew Bogut juicing or
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['so', 'is', 'andrew', 'bogut', 'juicing', 'or']
cosine_similarity: 0.9476170539855957
dev_input: [0.3563004293331381, 0.94761705], dev_label: 0
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Steph curry looks sick but Andrew Bogut is playing like an all star quietly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.         0.49922133 0.         0.         0.         0.
  0.        ]
 [0.23700504 0.23700504 0.33310232 0.         0.         0.33310232
  0.33310232 0.         0.33310232 0.33310232 0.33310232 0.33310232
  0.33310232]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Steph curry looks sick but Andrew Bogut is playing like an all star quietly
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['steph', 'curry', 'looks', 'sick', 'but', 'andrew', 'bogut', 'is', 'playing', 'like', 'an', 'all', 'star', 'quietly']
cosine_similarity: 0.9799492955207825
dev_input: [0.16836842163679844, 0.9799493], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Asik showed heart going to the line and knocking some of the FT s down
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.         0.
  0.57615236 0.40993715 0.57615236]
 [0.29017021 0.4078241  0.4078241  0.4078241  0.4078241  0.4078241
  0.         0.29017021 0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Asik showed heart going to the line and knocking some of the FT s down
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['asik', 'showed', 'heart', 'going', 'to', 'the', 'line', 'and', 'knocking', 'some', 'of', 'the', 'ft', 's', 'down']
cosine_similarity: 0.911510705947876
dev_input: [0.23790309463326234, 0.9115107], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Asik stepped up knocking down then FTs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.
  0.53404633]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Asik stepped up knocking down then FTs
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['asik', 'stepped', 'up', 'knocking', 'down', 'then', 'fts']
cosine_similarity: 0.8920496106147766
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: But hacking asik andforcing awful shots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.         0.53404633 0.
  0.53404633 0.53404633]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.         0.47107781
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: But hacking asik andforcing awful shots
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['but', 'hacking', 'asik', 'awful', 'shots']
cosine_similarity: 0.8808949589729309
dev_input: [0.1273595297947935, 0.88089496], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Omer Asik came through big tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.53404633 0.        ]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.         0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Omer Asik came through big tonight
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['omer', 'asik', 'came', 'through', 'big', 'tonight']
cosine_similarity: 0.9354583621025085
dev_input: [0.1273595297947935, 0.93545836], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Scott Brooks was worse with his hackAsik
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5 0.  0.  0.5 0.  0.5 0.5 0. ]
 [0.  0.5 0.5 0.  0.5 0.  0.  0.5]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Scott Brooks was worse with his hackAsik
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['scott', 'brooks', 'was', 'worse', 'with', 'his']
cosine_similarity: 0.8952993154525757
dev_input: [0.0, 0.8952993], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: So much for HackaAsik
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5 0.  0.5 0.5 0.5]
 [0.  1.  0.  0.  0. ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: So much for HackaAsik
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['so', 'much', 'for']
cosine_similarity: 0.8215305209159851
dev_input: [0.0, 0.8215305], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: That hack of Asik shit didnt work out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.53404633 0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: That hack of Asik shit didnt work out
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['that', 'hack', 'of', 'asik', 'shit', 'didnt', 'work', 'out']
cosine_similarity: 0.8893646001815796
dev_input: [0.1273595297947935, 0.8893646], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: especially when Asik is out of the game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.53404633]
 [0.44943642 0.6316672  0.6316672  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: especially when Asik is out of the game
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['especially', 'when', 'asik', 'is', 'out', 'of', 'the', 'game']
cosine_similarity: 0.9254744648933411
dev_input: [0.17077611319011649, 0.92547446], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: they have been fouling asik since about the 6th min mark
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.         0.         0.53404633
  0.53404633 0.53404633]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.47107781 0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: they have been fouling asik since about the 6th min mark
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['they', 'have', 'been', 'fouling', 'asik', 'since', 'about', 'the', 'min', 'mark']
cosine_similarity: 0.9286020994186401
dev_input: [0.1273595297947935, 0.9286021], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: what to do with Asik then
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633]
 [1.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: what to do with Asik then
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['what', 'to', 'do', 'with', 'asik', 'then']
cosine_similarity: 0.8563092350959778
dev_input: [0.37997836159100784, 0.85630924], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom goes down in warmup and the wild turn to josh Harding
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.33471228 0.47042643 0.
  0.47042643 0.         0.33471228 0.        ]
 [0.30287281 0.         0.42567716 0.30287281 0.         0.42567716
  0.         0.42567716 0.30287281 0.42567716]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom goes down in warmup and the wild turn to josh Harding
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'goes', 'down', 'in', 'warmup', 'and', 'the', 'wild', 'turn', 'to', 'josh', 'harding']
cosine_similarity: 0.9633684754371643
dev_input: [0.30412574187549346, 0.9633685], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom is out already good omen for the Hawks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.         0.4261596  0.         0.4261596
  0.         0.4261596  0.4261596 ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom is out already good omen for the Hawks
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'is', 'out', 'already', 'good', 'omen', 'for', 'the', 'hawks']
cosine_similarity: 0.9522525668144226
dev_input: [0.11521554337793122, 0.95225257], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom leaves ice during warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.         0.4261596  0.
  0.4261596  0.         0.         0.4261596 ]
 [0.33517574 0.         0.         0.47107781 0.         0.47107781
  0.         0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom leaves ice during warm ups
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'leaves', 'ice', 'during', 'warm', 'ups']
cosine_similarity: 0.9254976511001587
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom would get hurt the day we start playoffs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.44665616 0.         0.44665616
  0.         0.31779954 0.44665616]
 [0.35520009 0.49922133 0.         0.         0.49922133 0.
  0.49922133 0.35520009 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom would get hurt the day we start playoffs
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'would', 'get', 'hurt', 'the', 'day', 'we', 'start', 'playoffs']
cosine_similarity: 0.927696943283081
dev_input: [0.22576484600261604, 0.92769694], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Lol so Backstrom got hurt in warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.         0.4261596  0.         0.4261596
  0.         0.4261596  0.         0.         0.4261596 ]
 [0.30321606 0.         0.4261596  0.         0.4261596  0.
  0.4261596  0.         0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.09193998]
 [0.09193998 1.        ]]
cosine_similarity: 0.09193998174078082
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Lol so Backstrom got hurt in warm ups
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['lol', 'so', 'backstrom', 'got', 'hurt', 'in', 'warm', 'ups']
cosine_similarity: 0.8936111330986023
dev_input: [0.09193998174078082, 0.89361113], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wild dings up Backstrom in warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.         0.4261596  0.        ]
 [0.33517574 0.47107781 0.         0.         0.         0.
  0.47107781 0.47107781 0.         0.47107781]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wild dings up Backstrom in warm ups
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['wild', 'dings', 'up', 'backstrom', 'in', 'warm', 'ups']
cosine_similarity: 0.9265075325965881
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wishing Backstrom a speedy recovery
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.4261596  0.         0.
  0.4261596  0.4261596  0.        ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wishing Backstrom a speedy recovery
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['wishing', 'backstrom', 'a', 'speedy', 'recovery']
cosine_similarity: 0.8788540959358215
dev_input: [0.11521554337793122, 0.8788541], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wow backstrom would get hurt in warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.         0.4261596  0.4261596
  0.4261596  0.         0.        ]
 [0.37997836 0.         0.         0.53404633 0.         0.
  0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wow backstrom would get hurt in warmups
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['wow', 'backstrom', 'would', 'get', 'hurt', 'in', 'warmups']
cosine_similarity: 0.9549112915992737
dev_input: [0.11521554337793122, 0.9549113], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: backstrom just got hurt in the warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.         0.44665616
  0.         0.44665616 0.31779954]
 [0.35520009 0.         0.49922133 0.         0.49922133 0.
  0.49922133 0.         0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: backstrom just got hurt in the warmup
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup']
cosine_similarity: 0.9698944091796875
dev_input: [0.22576484600261604, 0.9698944], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: lose Backstrom in the warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.4261596  0.         0.4261596
  0.         0.         0.4261596 ]
 [0.37997836 0.         0.         0.         0.53404633 0.
  0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: lose Backstrom in the warm ups
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['lose', 'backstrom', 'in', 'the', 'warm', 'ups']
cosine_similarity: 0.9232298731803894
dev_input: [0.11521554337793122, 0.9232299], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom Heatley and Pominville out for Minny
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.         0.47107781 0.47107781 0.
  0.         0.47107781]
 [0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom Heatley and Pominville out for Minny
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['backstrom', 'heatley', 'and', 'pominville', 'out', 'for', 'minny']
cosine_similarity: 0.8747900724411011
dev_input: [0.1273595297947935, 0.8747901], dev_label: 0
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom apparently suffered an injury in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.35520009]
 [0.49922133 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.35520009]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom apparently suffered an injury in warmup
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['backstrom', 'apparently', 'suffered', 'an', 'injury', 'in', 'warmup']
cosine_similarity: 0.9212742447853088
dev_input: [0.2523342014336961, 0.92127424], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom injured in pregame for Minnesota
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.         0.47107781]
 [0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom injured in pregame for Minnesota
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['backstrom', 'injured', 'in', 'pregame', 'for', 'minnesota']
cosine_similarity: 0.9191176891326904
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom out in warm ups for the wild
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.47107781 0.         0.
  0.47107781 0.        ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom out in warm ups for the wild
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['backstrom', 'out', 'in', 'warm', 'ups', 'for', 'the', 'wild']
cosine_similarity: 0.9703841209411621
dev_input: [0.1273595297947935, 0.9703841], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Great Backstrom just got hurt and Harding most likely start
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.         0.         0.4090901  0.4090901
  0.         0.         0.57496187]
 [0.28986934 0.28986934 0.40740124 0.40740124 0.28986934 0.28986934
  0.40740124 0.40740124 0.        ]]
pairwise_similarity: [[1.         0.47433071]
 [0.47433071 1.        ]]
cosine_similarity: 0.4743307064971939
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Great Backstrom just got hurt and Harding most likely start
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['great', 'backstrom', 'just', 'got', 'hurt', 'and', 'harding', 'most', 'likely', 'start']
cosine_similarity: 0.9736016988754272
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: If Backstrom is out can I pick the Blackhawks to win in two
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.47107781 0.47107781 0.
  0.47107781 0.        ]
 [0.37997836 0.53404633 0.         0.         0.         0.53404633
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: If Backstrom is out can I pick the Blackhawks to win in two
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['if', 'backstrom', 'is', 'out', 'can', 'i', 'pick', 'the', 'blackhawks', 'to', 'win', 'in', 'two']
cosine_similarity: 0.9747431874275208
dev_input: [0.1273595297947935, 0.9747432], dev_label: 0
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Looks like Backstrom hurt himself in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.37930349 0.53309782 0.         0.
  0.37930349]
 [0.37930349 0.         0.37930349 0.         0.53309782 0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Looks like Backstrom hurt himself in warmup
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['looks', 'like', 'backstrom', 'hurt', 'himself', 'in', 'warmup']
cosine_similarity: 0.9841655492782593
dev_input: [0.43161341897075145, 0.98416555], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Niklas Backstrom potentially got hurt during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.37930349 0.53309782 0.         0.
  0.53309782 0.        ]
 [0.33471228 0.33471228 0.33471228 0.         0.47042643 0.47042643
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Niklas Backstrom potentially got hurt during warmups
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['niklas', 'backstrom', 'potentially', 'got', 'hurt', 'during', 'warmups']
cosine_similarity: 0.9139617681503296
dev_input: [0.38087260847594373, 0.91396177], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: and now Backstrom is injured in the warmups timmywhitehead
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.47107781 0.        ]
 [0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: and now Backstrom is injured in the warmups timmywhitehead
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['and', 'now', 'backstrom', 'is', 'injured', 'in', 'the', 'warmups']
cosine_similarity: 0.9795131683349609
dev_input: [0.1273595297947935, 0.97951317], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: backstrom was just injured in warmups for the
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.35520009 0.49922133
  0.        ]
 [0.40993715 0.         0.         0.57615236 0.40993715 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: backstrom was just injured in warmups for the
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['backstrom', 'was', 'just', 'injured', 'in', 'warmups', 'for', 'the']
cosine_similarity: 0.9828523397445679
dev_input: [0.29121941856368966, 0.98285234], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom gets hurt in warm ups for the wild
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.57615236 0.         0.
  0.57615236 0.        ]
 [0.31779954 0.44665616 0.31779954 0.         0.44665616 0.44665616
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom gets hurt in warm ups for the wild
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['backstrom', 'gets', 'hurt', 'in', 'warm', 'ups', 'for', 'the', 'wild']
cosine_similarity: 0.9307821393013
dev_input: [0.2605556710562624, 0.93078214], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom hurt and helped to the back during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.44832087 0.63009934 0.44832087]
 [0.44832087 0.63009934 0.44832087 0.         0.44832087]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom hurt and helped to the back during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['backstrom', 'hurt', 'and', 'helped', 'to', 'the', 'back', 'during', 'warmups']
cosine_similarity: 0.9595140218734741
dev_input: [0.6029748160380572, 0.959514], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom s out for MIN
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.53404633]
 [0.57973867 0.         0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom s out for MIN
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['backstrom', 's', 'out', 'for', 'min']
cosine_similarity: 0.8041871786117554
dev_input: [0.22028815056182965, 0.8041872], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom wussing out of the game before it even starts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.         0.53404633
  0.        ]
 [0.37997836 0.53404633 0.         0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom wussing out of the game before it even starts
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['backstrom', 'out', 'of', 'the', 'game', 'before', 'it', 'even', 'starts']
cosine_similarity: 0.9215506911277771
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Harding in net for Backstrom
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.         0.53404633]
 [0.44943642 0.6316672  0.         0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Harding in net for Backstrom
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['harding', 'in', 'net', 'for', 'backstrom']
cosine_similarity: 0.8532418012619019
dev_input: [0.17077611319011649, 0.8532418], dev_label: 0
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Is it a good thing backstrom left the ice
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.57615236 0.         0.40993715 0.
  0.57615236]
 [0.35520009 0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Is it a good thing backstrom left the ice
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['is', 'it', 'a', 'good', 'thing', 'backstrom', 'left', 'the', 'ice']
cosine_similarity: 0.8891968727111816
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Lmao they had to help Backstrom off the ice
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.53404633]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Lmao they had to help Backstrom off the ice
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['lmao', 'they', 'had', 'to', 'help', 'backstrom', 'off', 'the', 'ice']
cosine_similarity: 0.9083124399185181
dev_input: [0.1443835552773867, 0.90831244], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: WHAT THE HECK BACKSTROM GOT HURT IN WARMUPS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.44832087 0.63009934 0.44832087]
 [0.37930349 0.53309782 0.53309782 0.37930349 0.         0.37930349]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: WHAT THE HECK BACKSTROM GOT HURT IN WARMUPS
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['what', 'the', 'heck', 'backstrom', 'got', 'hurt', 'in', 'warmups']
cosine_similarity: 0.959818959236145
dev_input: [0.5101490193104813, 0.95981896], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Yikes Backstrom might be injured
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.53404633 0.        ]
 [0.44943642 0.         0.6316672  0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Yikes Backstrom might be injured
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['yikes', 'backstrom', 'might', 'be', 'injured']
cosine_similarity: 0.9508375525474548
dev_input: [0.17077611319011649, 0.95083755], dev_label: 1
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: you see backstrom is out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633]
 [1.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: you see backstrom is out
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['you', 'see', 'backstrom', 'is', 'out']
cosine_similarity: 0.9137299656867981
dev_input: [0.37997836159100784, 0.91372997], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Also hate that Backstrom went down in the warm up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Also hate that Backstrom went down in the warm up
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['also', 'hate', 'that', 'backstrom', 'went', 'down', 'in', 'the', 'warm', 'up']
cosine_similarity: 0.8365132212638855
dev_input: [0.17077611319011649, 0.8365132], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Backstrom is out we lose
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672 ]
 [0.57973867 0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Backstrom is out we lose
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['backstrom', 'is', 'out', 'we', 'lose']
cosine_similarity: 0.8366538286209106
dev_input: [0.2605556710562624, 0.8366538], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Backstrom just went down during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.50154891 0.        ]
 [0.40993715 0.         0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Backstrom just went down during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['backstrom', 'just', 'went', 'down', 'during', 'warmups']
cosine_similarity: 0.9343372583389282
dev_input: [0.4112070550676187, 0.93433726], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Hopefully Backstrom is back soon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672 ]
 [0.44943642 0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Hopefully Backstrom is back soon
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['hopefully', 'backstrom', 'is', 'back', 'soon']
cosine_similarity: 0.8274986147880554
dev_input: [0.20199309249791833, 0.8274986], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Man if Backstrom is out that s huuuuge
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Man if Backstrom is out that s huuuuge
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['man', 'if', 'backstrom', 'is', 'out', 'that', 's', 'huuuuge']
cosine_similarity: 0.8125864863395691
dev_input: [0.20199309249791833, 0.8125865], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Neve mind Niklas Backstrom s hurt
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.         0.         0.70490949]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Neve mind Niklas Backstrom s hurt
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['neve', 'mind', 'niklas', 'backstrom', 's', 'hurt']
cosine_similarity: 0.8112177848815918
dev_input: [0.3563004293331381, 0.8112178], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Omg backstrom injured in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.6316672 ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Omg backstrom injured in warmup
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['omg', 'backstrom', 'injured', 'in', 'warmup']
cosine_similarity: 0.9101243019104004
dev_input: [0.17077611319011649, 0.9101243], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wild Backstrom injured during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.50154891 0.        ]
 [0.40993715 0.         0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wild Backstrom injured during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['wild', 'backstrom', 'injured', 'during', 'warmups']
cosine_similarity: 0.9425503015518188
dev_input: [0.4112070550676187, 0.9425503], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wild in playoffs Backstrom injured in pregame warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.         0.         0.50154891
  0.        ]
 [0.31779954 0.         0.44665616 0.44665616 0.44665616 0.31779954
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wild in playoffs Backstrom injured in pregame warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['wild', 'in', 'playoffs', 'backstrom', 'injured', 'in', 'pregame', 'warmups']
cosine_similarity: 0.909921407699585
dev_input: [0.31878402175377923, 0.9099214], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wow no Backstrom for the Wild tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wow no Backstrom for the Wild tonight
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['wow', 'no', 'backstrom', 'for', 'the', 'wild', 'tonight']
cosine_similarity: 0.7810243964195251
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom hurt in warn ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom hurt in warn ups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['backstrom', 'hurt', 'in', 'warn', 'ups']
cosine_similarity: 0.9361975193023682
dev_input: [0.4112070550676187, 0.9361975], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom injured himself in the warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.50154891]
 [0.50154891 0.         0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom injured himself in the warmups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['backstrom', 'injured', 'himself', 'in', 'the', 'warmups']
cosine_similarity: 0.9359128475189209
dev_input: [0.5031026124151314, 0.93591285], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom was injured in warm up for Minnesota
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.6316672 ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom was injured in warm up for Minnesota
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['backstrom', 'was', 'injured', 'in', 'warm', 'up', 'for', 'minnesota']
cosine_similarity: 0.9448740482330322
dev_input: [0.17077611319011649, 0.94487405], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom was injured warming up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom was injured warming up
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['backstrom', 'was', 'injured', 'warming', 'up']
cosine_similarity: 0.9198800325393677
dev_input: [0.20199309249791833, 0.91988003], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Hurt himself during warm ups tough break for backstrom
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.         0.
  0.70490949]
 [0.31779954 0.44665616 0.31779954 0.44665616 0.44665616 0.44665616
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Hurt himself during warm ups tough break for backstrom
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['hurt', 'himself', 'during', 'warm', 'ups', 'tough', 'break', 'for', 'backstrom']
cosine_similarity: 0.9321033954620361
dev_input: [0.31878402175377923, 0.9321034], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: If you didnt know Backstrom inexplicably hurt himself in warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.         0.57735027 0.         0.         0.57735027]
 [0.33471228 0.47042643 0.33471228 0.47042643 0.47042643 0.33471228]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376658
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: If you didnt know Backstrom inexplicably hurt himself in warmups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['if', 'you', 'didnt', 'know', 'backstrom', 'inexplicably', 'hurt', 'himself', 'in', 'warmups']
cosine_similarity: 0.9043194055557251
dev_input: [0.5797386715376658, 0.9043194], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Minnesota Wild goalie Backstrom just got injured in warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.
  0.         0.         0.         0.6316672  0.        ]
 [0.24395573 0.34287126 0.34287126 0.         0.34287126 0.34287126
  0.34287126 0.34287126 0.34287126 0.         0.34287126]]
pairwise_similarity: [[1.         0.10964259]
 [0.10964259 1.        ]]
cosine_similarity: 0.10964258683453854
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Minnesota Wild goalie Backstrom just got injured in warm ups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['minnesota', 'wild', 'goalie', 'backstrom', 'just', 'got', 'injured', 'in', 'warm', 'ups']
cosine_similarity: 0.933113694190979
dev_input: [0.10964258683453854, 0.9331137], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: OMG Backstrom is already injured
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: OMG Backstrom is already injured
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['omg', 'backstrom', 'is', 'already', 'injured']
cosine_similarity: 0.9113175868988037
dev_input: [0.20199309249791833, 0.9113176], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: That sucks Nicklas Backstrom from the wild got hurt in warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.         0.
  0.         0.70490949 0.        ]
 [0.26868528 0.37762778 0.26868528 0.37762778 0.37762778 0.37762778
  0.37762778 0.         0.37762778]]
pairwise_similarity: [[1.         0.26951761]
 [0.26951761 1.        ]]
cosine_similarity: 0.26951761324603224
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: That sucks Nicklas Backstrom from the wild got hurt in warm ups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['that', 'sucks', 'nicklas', 'backstrom', 'from', 'the', 'wild', 'got', 'hurt', 'in', 'warm', 'ups']
cosine_similarity: 0.9379292130470276
dev_input: [0.26951761324603224, 0.9379292], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: When you hear Nicolas backstrom is injured
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.6316672 ]
 [0.37997836 0.53404633 0.         0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: When you hear Nicolas backstrom is injured
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['when', 'you', 'hear', 'nicolas', 'backstrom', 'is', 'injured']
cosine_similarity: 0.8923681974411011
dev_input: [0.17077611319011649, 0.8923682], dev_label: 0
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Backstrom for Min is hurt in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.31779954 0.         0.44665616
  0.         0.44665616]
 [0.40993715 0.         0.         0.40993715 0.57615236 0.
  0.57615236 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Backstrom for Min is hurt in warmup
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'for', 'min', 'is', 'hurt', 'in', 'warmup']
cosine_similarity: 0.9696056246757507
dev_input: [0.2605556710562624, 0.9696056], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Backstrom slowly off the ice for the MNWild
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.4261596  0.         0.
  0.         0.4261596  0.4261596 ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Backstrom slowly off the ice for the MNWild
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'slowly', 'off', 'the', 'ice', 'for', 'the', 'mnwild']
cosine_similarity: 0.959186851978302
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Everybody needs to step up without Backstrom in
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.4261596  0.4261596  0.4261596  0.
  0.4261596  0.         0.4261596 ]
 [0.37997836 0.53404633 0.         0.         0.         0.53404633
  0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Everybody needs to step up without Backstrom in
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['everybody', 'needs', 'to', 'step', 'up', 'without', 'backstrom', 'in']
cosine_similarity: 0.9564038515090942
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Looks like Backstrom injured in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.4261596  0.         0.
  0.         0.4261596  0.         0.4261596 ]
 [0.33517574 0.         0.         0.         0.47107781 0.47107781
  0.47107781 0.         0.47107781 0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Looks like Backstrom injured in warmup
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['looks', 'like', 'backstrom', 'injured', 'in', 'warmup']
cosine_similarity: 0.9800604581832886
dev_input: [0.10163066979112656, 0.98006046], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Maybe with backstrom injured the D will step up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.4261596  0.         0.
  0.4261596  0.         0.4261596 ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Maybe with backstrom injured the D will step up
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['maybe', 'with', 'backstrom', 'injured', 'the', 'd', 'will', 'step', 'up']
cosine_similarity: 0.9462360739707947
dev_input: [0.11521554337793122, 0.9462361], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Minnesota goalie Backstrom hurt in warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.31779954 0.
  0.44665616 0.         0.         0.44665616]
 [0.31779954 0.         0.44665616 0.         0.31779954 0.44665616
  0.         0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Minnesota goalie Backstrom hurt in warm ups
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['minnesota', 'goalie', 'backstrom', 'hurt', 'in', 'warm', 'ups']
cosine_similarity: 0.9571081399917603
dev_input: [0.20199309249791833, 0.95710814], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Nikalas backstrom hurt during warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.31779954 0.         0.44665616
  0.         0.         0.44665616]
 [0.35520009 0.         0.         0.35520009 0.49922133 0.
  0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Nikalas backstrom hurt during warm ups
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'hurt', 'during', 'warm', 'ups']
cosine_similarity: 0.9535923600196838
dev_input: [0.22576484600261604, 0.95359236], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Of course Backstrom would get injured during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.44665616 0.44665616 0.
  0.44665616 0.31779954]
 [0.40993715 0.57615236 0.         0.         0.         0.57615236
  0.         0.40993715]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Of course Backstrom would get injured during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['of', 'course', 'backstrom', 'would', 'get', 'injured', 'during', 'warmups']
cosine_similarity: 0.9818362593650818
dev_input: [0.2605556710562624, 0.98183626], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Tell flynnkatie Backstrom got injured during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.44665616 0.44665616
  0.         0.44665616 0.         0.31779954]
 [0.31779954 0.44665616 0.         0.44665616 0.         0.
  0.44665616 0.         0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Tell flynnkatie Backstrom got injured during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['tell', 'backstrom', 'got', 'injured', 'during', 'warmups']
cosine_similarity: 0.969366192817688
dev_input: [0.20199309249791833, 0.9693662], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: well first injury of the playoffs goes to backstrom in warmups of the first game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.44665616 0.44665616
  0.         0.         0.44665616 0.31779954]
 [0.31779954 0.44665616 0.         0.44665616 0.         0.
  0.44665616 0.44665616 0.         0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: well first injury of the playoffs goes to backstrom in warmups of the first game
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['well', 'first', 'injury', 'of', 'the', 'playoffs', 'goes', 'to', 'backstrom', 'in', 'warmups', 'of', 'the', 'first', 'game']
cosine_similarity: 0.9620372653007507
dev_input: [0.20199309249791833, 0.96203727], dev_label: 1
TF_IDF_cosine_similarity: sentence1: Just lost Backstrom in warmup, sentence2: Backstrom gets hurt in warm ups for the wild
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.
  0.         0.53404633 0.        ]
 [0.30321606 0.4261596  0.4261596  0.         0.         0.4261596
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Just lost Backstrom in warmup, sentence2: Backstrom gets hurt in warm ups for the wild
After tokenization, sentence1: ['just', 'lost', 'backstrom', 'in', 'warmup'], sentence2: ['backstrom', 'gets', 'hurt', 'in', 'warm', 'ups', 'for', 'the', 'wild']
cosine_similarity: 0.9632977843284607
dev_input: [0.11521554337793122, 0.9632978], dev_label: 1
TF_IDF_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: 8 mile is on thats my movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.53404633 0.37997836 0.         0.         0.53404633]
 [0.         0.         0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: 8 mile is on thats my movie
After tokenization, sentence1: ['all', 'the', 'home', 'alones', 'watching', 'mile'], sentence2: ['mile', 'is', 'on', 'thats', 'my', 'movie']
cosine_similarity: 0.9501103758811951
TF_IDF_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: The last rap battle in 8 Mile nevr gets old ahah
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.         0.         0.53404633 0.37997836
  0.         0.         0.         0.53404633]
 [0.39204401 0.         0.39204401 0.39204401 0.         0.27894255
  0.39204401 0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: The last rap battle in 8 Mile nevr gets old ahah
After tokenization, sentence1: ['all', 'the', 'home', 'alones', 'watching', 'mile'], sentence2: ['the', 'last', 'rap', 'battle', 'in', 'mile', 'nevr', 'gets', 'old', 'ahah']
cosine_similarity: 0.9554800987243652
test_input: [0.1059921313509325, 0.9554801], test_label: 0
TF_IDF_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: The rap battle at the end of 8 mile gets me so hype
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.         0.         0.53404633 0.
  0.37997836 0.         0.53404633]
 [0.         0.4261596  0.4261596  0.4261596  0.         0.4261596
  0.30321606 0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: The rap battle at the end of 8 mile gets me so hype
After tokenization, sentence1: ['all', 'the', 'home', 'alones', 'watching', 'mile'], sentence2: ['the', 'rap', 'battle', 'at', 'the', 'end', 'of', 'mile', 'gets', 'me', 'so', 'hype']
cosine_similarity: 0.9636808037757874
test_input: [0.11521554337793122, 0.9636808], test_label: 0
TF_IDF_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: Rabbit on 8 mile out of place but determined to make it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.53404633 0.         0.37997836 0.53404633
  0.         0.        ]
 [0.47107781 0.         0.         0.47107781 0.33517574 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: Rabbit on 8 mile out of place but determined to make it
After tokenization, sentence1: ['the', 'ending', 'to', 'mile', 'is', 'my', 'fav', 'part', 'of', 'the', 'whole', 'movie'], sentence2: ['rabbit', 'on', 'mile', 'out', 'of', 'place', 'but', 'determined', 'to', 'make', 'it']
cosine_similarity: 0.9746663570404053
test_input: [0.1273595297947935, 0.97466636], test_label: 0
TF_IDF_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: See 8 Mile is always on but it s the tv version so it s gay
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.53404633 0.         0.37997836 0.53404633 0.
  0.        ]
 [0.         0.         0.53404633 0.37997836 0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: See 8 Mile is always on but it s the tv version so it s gay
After tokenization, sentence1: ['the', 'ending', 'to', 'mile', 'is', 'my', 'fav', 'part', 'of', 'the', 'whole', 'movie'], sentence2: ['see', 'mile', 'is', 'always', 'on', 'but', 'it', 's', 'the', 'tv', 'version', 'so', 'it', 's', 'gay']
cosine_similarity: 0.9744922518730164
test_input: [0.1443835552773867, 0.97449225], test_label: 0
TF_IDF_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: Those last 3 battles in 8 Mile are THE shit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.53404633 0.37997836 0.53404633 0.        ]
 [0.6316672  0.         0.         0.44943642 0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: Those last 3 battles in 8 Mile are THE shit
After tokenization, sentence1: ['the', 'ending', 'to', 'mile', 'is', 'my', 'fav', 'part', 'of', 'the', 'whole', 'movie'], sentence2: ['those', 'last', 'battles', 'in', 'mile', 'are', 'the', 'shit']
cosine_similarity: 0.9784131646156311
test_input: [0.17077611319011649, 0.97841316], test_label: 1
TF_IDF_cosine_similarity: sentence1: 0 and 8 mile on at the same time, sentence2: It s just rap lyrics from the movie 8 mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57973867 0.         0.         0.81480247]
 [0.47107781 0.47107781 0.33517574 0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: 0 and 8 mile on at the same time, sentence2: It s just rap lyrics from the movie 8 mile
After tokenization, sentence1: ['and', 'mile', 'on', 'at', 'the', 'same', 'time'], sentence2: ['it', 's', 'just', 'rap', 'lyrics', 'from', 'the', 'movie', 'mile']
cosine_similarity: 0.9730812907218933
test_input: [0.19431434016858146, 0.9730813], test_label: 0
TF_IDF_cosine_similarity: sentence1: 0 and 8 mile on at the same time, sentence2: I will never get tired of 8 Mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.        ]
 [0.57973867 0.         0.81480247]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
word_to_vector_cosine_similarity: sentence1: 0 and 8 mile on at the same time, sentence2: I will never get tired of 8 Mile
After tokenization, sentence1: ['and', 'mile', 'on', 'at', 'the', 'same', 'time'], sentence2: ['i', 'will', 'never', 'get', 'tired', 'of', 'mile']
cosine_similarity: 0.9570022225379944
test_input: [0.3360969272762574, 0.9570022], test_label: 0
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Watching 8 Mile for the 76283936154th time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         1.         0.         0.        ]
 [0.53404633 0.37997836 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Watching 8 Mile for the 76283936154th time
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['watching', 'mile', 'for', 'the', 'time']
cosine_similarity: 0.9725162982940674
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: My favorite movie ever is 8 mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         1.         0.        ]
 [0.6316672  0.44943642 0.6316672 ]]
pairwise_similarity: [[1.         0.44943642]
 [0.44943642 1.        ]]
cosine_similarity: 0.4494364165239821
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: My favorite movie ever is 8 mile
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['my', 'favorite', 'movie', 'ever', 'is', 'mile']
cosine_similarity: 0.9535690546035767
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: In 8 Mile in a scene the background music is Sweet Home Alabama
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         1.         0.         0.
  0.        ]
 [0.39204401 0.39204401 0.39204401 0.27894255 0.39204401 0.39204401
  0.39204401]]
pairwise_similarity: [[1.         0.27894255]
 [0.27894255 1.        ]]
cosine_similarity: 0.2789425453258252
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: In 8 Mile in a scene the background music is Sweet Home Alabama
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['in', 'mile', 'in', 'a', 'scene', 'the', 'background', 'music', 'is', 'sweet', 'home', 'alabama']
cosine_similarity: 0.9774959087371826
test_input: [0.2789425453258252, 0.9774959], test_label: 0
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: 8 mile is on havent seen this movie in the longest
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         1.         0.         0.        ]
 [0.47107781 0.47107781 0.33517574 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.33517574]
 [0.33517574 1.        ]]
cosine_similarity: 0.33517574332792605
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: 8 mile is on havent seen this movie in the longest
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['mile', 'is', 'on', 'havent', 'seen', 'this', 'movie', 'in', 'the', 'longest']
cosine_similarity: 0.9806771278381348
test_input: [0.33517574332792605, 0.9806771], test_label: 1
TF_IDF_cosine_similarity: sentence1: The last rap battle in 8 mile though, sentence2: But why were people watching the heat play when 8 mile is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.6316672
  0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: The last rap battle in 8 mile though, sentence2: But why were people watching the heat play when 8 mile is on
After tokenization, sentence1: ['the', 'last', 'rap', 'battle', 'in', 'mile', 'though'], sentence2: ['but', 'why', 'were', 'people', 'watching', 'the', 'heat', 'play', 'when', 'mile', 'is', 'on']
cosine_similarity: 0.9733127355575562
test_input: [0.15064018498706508, 0.97331274], test_label: 0
TF_IDF_cosine_similarity: sentence1: The last rap battle in 8 mile though, sentence2: I think everyone is watching 8 mile Rn
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.6316672  0.         0.         0.        ]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: The last rap battle in 8 mile though, sentence2: I think everyone is watching 8 mile Rn
After tokenization, sentence1: ['the', 'last', 'rap', 'battle', 'in', 'mile', 'though'], sentence2: ['i', 'think', 'everyone', 'is', 'watching', 'mile', 'rn']
cosine_similarity: 0.9507029056549072
test_input: [0.17077611319011649, 0.9507029], test_label: 0
TF_IDF_cosine_similarity: sentence1: 8 mile is just a classic, sentence2: the ending of 8 mile is prolly the best part of the whole movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.         0.6316672  0.44943642 0.
  0.        ]
 [0.47107781 0.         0.47107781 0.         0.33517574 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: 8 mile is just a classic, sentence2: the ending of 8 mile is prolly the best part of the whole movie
After tokenization, sentence1: ['mile', 'is', 'just', 'a', 'classic'], sentence2: ['the', 'ending', 'of', 'mile', 'is', 'prolly', 'the', 'best', 'part', 'of', 'the', 'whole', 'movie']
cosine_similarity: 0.9651812314987183
TF_IDF_cosine_similarity: sentence1: 8 mile is just a classic, sentence2: After watching 8 mile I feel like such a thug
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.6316672  0.         0.44943642 0.
  0.        ]
 [0.         0.47107781 0.         0.47107781 0.33517574 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: 8 mile is just a classic, sentence2: After watching 8 mile I feel like such a thug
After tokenization, sentence1: ['mile', 'is', 'just', 'a', 'classic'], sentence2: ['after', 'watching', 'mile', 'i', 'feel', 'like', 'such', 'a', 'thug']
cosine_similarity: 0.9611015915870667
TF_IDF_cosine_similarity: sentence1: 8 mile is just a classic, sentence2: 8 mile that movie I love eminem in this movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.6316672  0.         0.44943642 0.        ]
 [0.         0.39204401 0.         0.39204401 0.27894255 0.78408803]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: 8 mile is just a classic, sentence2: 8 mile that movie I love eminem in this movie
After tokenization, sentence1: ['mile', 'is', 'just', 'a', 'classic'], sentence2: ['mile', 'that', 'movie', 'i', 'love', 'eminem', 'in', 'this', 'movie']
cosine_similarity: 0.9681278467178345
test_input: [0.12536693798731732, 0.96812785], test_label: 1
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Listened to the final rap battle from 8 mile and now Im watching the whole movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.         1.         0.
  0.         0.        ]
 [0.36499647 0.36499647 0.36499647 0.36499647 0.25969799 0.36499647
  0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.25969799]
 [0.25969799 1.        ]]
cosine_similarity: 0.25969799324016246
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Listened to the final rap battle from 8 mile and now Im watching the whole movie
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['listened', 'to', 'the', 'final', 'rap', 'battle', 'from', 'mile', 'and', 'now', 'im', 'watching', 'the', 'whole', 'movie']
cosine_similarity: 0.9706138968467712
test_input: [0.25969799324016246, 0.9706139], test_label: 0
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: 8 mile has been that movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[1.         0.        ]
 [0.57973867 0.81480247]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: 8 mile has been that movie
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['mile', 'has', 'been', 'that', 'movie']
cosine_similarity: 0.9649457931518555
test_input: [0.5797386715376657, 0.9649458], test_label: 0
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: I really did miss my part on 8 Mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         1.         0.         0.        ]
 [0.53404633 0.37997836 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: I really did miss my part on 8 Mile
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['i', 'really', 'did', 'miss', 'my', 'part', 'on', 'mile']
cosine_similarity: 0.9787110686302185
test_input: [0.37997836159100784, 0.97871107], test_label: 0
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Lose Yourself is the perfect song to end 8 mile on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         1.         0.         0.        ]
 [0.47107781 0.47107781 0.33517574 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.33517574]
 [0.33517574 1.        ]]
cosine_similarity: 0.33517574332792605
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Lose Yourself is the perfect song to end 8 mile on
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['lose', 'yourself', 'is', 'the', 'perfect', 'song', 'to', 'end', 'mile', 'on']
cosine_similarity: 0.9753764867782593
test_input: [0.33517574332792605, 0.9753765], test_label: 0
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: This is my favorite scene of 8 mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         1.         0.        ]
 [0.6316672  0.44943642 0.6316672 ]]
pairwise_similarity: [[1.         0.44943642]
 [0.44943642 1.        ]]
cosine_similarity: 0.4494364165239821
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: This is my favorite scene of 8 mile
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['this', 'is', 'my', 'favorite', 'scene', 'of', 'mile']
cosine_similarity: 0.969473123550415
TF_IDF_cosine_similarity: sentence1: 8 mile is on friday made, sentence2: I missed the best part of 8 mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.81480247 0.57973867 0.        ]
 [0.6316672  0.         0.44943642 0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: 8 mile is on friday made, sentence2: I missed the best part of 8 mile
After tokenization, sentence1: ['mile', 'is', 'on', 'friday', 'made'], sentence2: ['i', 'missed', 'the', 'best', 'part', 'of', 'mile']
cosine_similarity: 0.9859362840652466
test_input: [0.2605556710562624, 0.9859363], test_label: 0
TF_IDF_cosine_similarity: sentence1: 8 mile is on friday made, sentence2: 8 mile is such an awesome movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.81480247 0.57973867 0.        ]
 [0.6316672  0.         0.44943642 0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: 8 mile is on friday made, sentence2: 8 mile is such an awesome movie
After tokenization, sentence1: ['mile', 'is', 'on', 'friday', 'made'], sentence2: ['mile', 'is', 'such', 'an', 'awesome', 'movie']
cosine_similarity: 0.9594147801399231
test_input: [0.2605556710562624, 0.9594148], test_label: 0
TF_IDF_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: Eminem s rap in the final battle of 8 Mile gets me pumped every time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.         0.37997836 0.53404633
  0.53404633 0.         0.         0.53404633 0.        ]
 [0.36499647 0.36499647 0.36499647 0.36499647 0.25969799 0.
  0.         0.36499647 0.36499647 0.         0.36499647]]
pairwise_similarity: [[1.         0.09867962]
 [0.09867962 1.        ]]
cosine_similarity: 0.09867961797986956
word_to_vector_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: Eminem s rap in the final battle of 8 Mile gets me pumped every time
After tokenization, sentence1: ['oh', 'shit', 'niggy', 'mile', 'is', 'on'], sentence2: ['eminem', 's', 'rap', 'in', 'the', 'final', 'battle', 'of', 'mile', 'gets', 'me', 'pumped', 'every', 'time']
cosine_similarity: 0.9236377477645874
test_input: [0.09867961797986956, 0.92363775], test_label: 0
TF_IDF_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: My favorite part of 8 mile is about to come up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37997836 0.53404633 0.53404633 0.53404633]
 [0.6316672  0.6316672  0.44943642 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: My favorite part of 8 mile is about to come up
After tokenization, sentence1: ['oh', 'shit', 'niggy', 'mile', 'is', 'on'], sentence2: ['my', 'favorite', 'part', 'of', 'mile', 'is', 'about', 'to', 'come', 'up']
cosine_similarity: 0.9467772841453552
TF_IDF_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: While yall argue about the game 8 mile is on MTV
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37997836 0.         0.53404633 0.53404633
  0.53404633 0.        ]
 [0.47107781 0.47107781 0.33517574 0.47107781 0.         0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: While yall argue about the game 8 mile is on MTV
After tokenization, sentence1: ['oh', 'shit', 'niggy', 'mile', 'is', 'on'], sentence2: ['while', 'yall', 'argue', 'about', 'the', 'game', 'mile', 'is', 'on', 'mtv']
cosine_similarity: 0.9547627568244934
test_input: [0.1273595297947935, 0.95476276], test_label: 1
TF_IDF_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: The end of 8 Mile makes me so happy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.40993715 0.57615236]
 [0.40993715 0.         0.57615236 0.57615236 0.40993715 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: The end of 8 Mile makes me so happy
After tokenization, sentence1: ['ok', 'good', 'the', 'end', 'of', 'mile', 'is', 'on'], sentence2: ['the', 'end', 'of', 'mile', 'makes', 'me', 'so', 'happy']
cosine_similarity: 0.984442412853241
test_input: [0.3360969272762575, 0.9844424], test_label: 1
TF_IDF_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: The last 3 rap battles in 8 mile always get me hyped af
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.53404633 0.53404633 0.         0.37997836
  0.53404633 0.        ]
 [0.47107781 0.47107781 0.         0.         0.47107781 0.33517574
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: The last 3 rap battles in 8 mile always get me hyped af
After tokenization, sentence1: ['ok', 'good', 'the', 'end', 'of', 'mile', 'is', 'on'], sentence2: ['the', 'last', 'rap', 'battles', 'in', 'mile', 'always', 'get', 'me', 'hyped', 'af']
cosine_similarity: 0.964591920375824
test_input: [0.1273595297947935, 0.9645919], test_label: 1
TF_IDF_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: I always get the movies 8 mile and green mile mixed up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.53404633 0.         0.37997836 0.         0.
  0.53404633]
 [0.         0.         0.44610081 0.6348088  0.44610081 0.44610081
  0.        ]]
pairwise_similarity: [[1.         0.24121361]
 [0.24121361 1.        ]]
cosine_similarity: 0.24121360667506986
word_to_vector_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: I always get the movies 8 mile and green mile mixed up
After tokenization, sentence1: ['ok', 'good', 'the', 'end', 'of', 'mile', 'is', 'on'], sentence2: ['i', 'always', 'get', 'the', 'movies', 'mile', 'and', 'green', 'mile', 'mixed', 'up']
cosine_similarity: 0.9647327065467834
test_input: [0.24121360667506986, 0.9647327], test_label: 0
TF_IDF_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: All the home alones watching 8 mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.53404633 0.         0.37997836 0.53404633
  0.        ]
 [0.53404633 0.         0.         0.53404633 0.37997836 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: All the home alones watching 8 mile
After tokenization, sentence1: ['ok', 'good', 'the', 'end', 'of', 'mile', 'is', 'on'], sentence2: ['all', 'the', 'home', 'alones', 'watching', 'mile']
cosine_similarity: 0.9428839683532715
TF_IDF_cosine_similarity: sentence1: Let s go see after earth, sentence2: Did After Earth already come out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57973867 0.81480247]
 [0.6316672  0.6316672  0.44943642 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Let s go see after earth, sentence2: Did After Earth already come out
After tokenization, sentence1: ['let', 's', 'go', 'see', 'after', 'earth'], sentence2: ['did', 'after', 'earth', 'already', 'come', 'out']
cosine_similarity: 0.9674420356750488
test_input: [0.2605556710562624, 0.96744204], test_label: 0
TF_IDF_cosine_similarity: sentence1: Let s go see after earth, sentence2: I need to go see After Earth The purge ans Man Of Steel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.81480247 0.         0.         0.
  0.        ]
 [0.4261596  0.30321606 0.         0.4261596  0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: Let s go see after earth, sentence2: I need to go see After Earth The purge ans Man Of Steel
After tokenization, sentence1: ['let', 's', 'go', 'see', 'after', 'earth'], sentence2: ['i', 'need', 'to', 'go', 'see', 'after', 'earth', 'the', 'purge', 'ans', 'man', 'of', 'steel']
cosine_similarity: 0.9804036021232605
TF_IDF_cosine_similarity: sentence1: Let s go see after earth, sentence2: idk I think we might go see after earth later if we do you wana go
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.81480247 0.         0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: Let s go see after earth, sentence2: idk I think we might go see after earth later if we do you wana go
After tokenization, sentence1: ['let', 's', 'go', 'see', 'after', 'earth'], sentence2: ['idk', 'i', 'think', 'we', 'might', 'go', 'see', 'after', 'earth', 'later', 'if', 'we', 'do', 'you', 'wana', 'go']
cosine_similarity: 0.976414680480957
test_input: [0.19431434016858146, 0.9764147], test_label: 1
TF_IDF_cosine_similarity: sentence1: Anyone trying to see After Earth sometime soon, sentence2: Me and my son went to see After Earth last night
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672  0.        ]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Anyone trying to see After Earth sometime soon, sentence2: Me and my son went to see After Earth last night
After tokenization, sentence1: ['anyone', 'trying', 'to', 'see', 'after', 'earth', 'sometime', 'soon'], sentence2: ['me', 'and', 'my', 'son', 'went', 'to', 'see', 'after', 'earth', 'last', 'night']
cosine_similarity: 0.9673699140548706
test_input: [0.17077611319011649, 0.9673699], test_label: 0
TF_IDF_cosine_similarity: sentence1: Anyone trying to see After Earth sometime soon, sentence2: I wanna go see After Earth officialjaden
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.44943642 0.6316672  0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Anyone trying to see After Earth sometime soon, sentence2: I wanna go see After Earth officialjaden
After tokenization, sentence1: ['anyone', 'trying', 'to', 'see', 'after', 'earth', 'sometime', 'soon'], sentence2: ['i', 'wanna', 'go', 'see', 'after', 'earth']
cosine_similarity: 0.9838350415229797
TF_IDF_cosine_similarity: sentence1: Anyone trying to see After Earth sometime soon, sentence2: I wanna come watch after Earth with you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.         0.        ]
 [0.53404633 0.37997836 0.         0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Anyone trying to see After Earth sometime soon, sentence2: I wanna come watch after Earth with you
After tokenization, sentence1: ['anyone', 'trying', 'to', 'see', 'after', 'earth', 'sometime', 'soon'], sentence2: ['i', 'wanna', 'come', 'watch', 'after', 'earth', 'with', 'you']
cosine_similarity: 0.9835254549980164
TF_IDF_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I could of told Will Smith that After Earth was going 2 crash
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.
  0.6316672 ]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.         0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I could of told Will Smith that After Earth was going 2 crash
After tokenization, sentence1: ['who', 'wants', 'to', 'see', 'after', 'earth', 'with', 'me'], sentence2: ['i', 'could', 'of', 'told', 'will', 'smith', 'that', 'after', 'earth', 'was', 'going', 'crash']
cosine_similarity: 0.971786379814148
test_input: [0.15064018498706508, 0.9717864], test_label: 0
TF_IDF_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I knew After Earth would tank
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I knew After Earth would tank
After tokenization, sentence1: ['who', 'wants', 'to', 'see', 'after', 'earth', 'with', 'me'], sentence2: ['i', 'knew', 'after', 'earth', 'would', 'tank']
cosine_similarity: 0.9723041653633118
test_input: [0.20199309249791833, 0.97230417], test_label: 0
TF_IDF_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I wanna go see After Earth or Epic
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672 ]
 [0.44943642 0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I wanna go see After Earth or Epic
After tokenization, sentence1: ['who', 'wants', 'to', 'see', 'after', 'earth', 'with', 'me'], sentence2: ['i', 'wanna', 'go', 'see', 'after', 'earth', 'or', 'epic']
cosine_similarity: 0.9784883260726929
TF_IDF_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: After Earth finishes with 27 million
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: After Earth finishes with 27 million
After tokenization, sentence1: ['who', 'wants', 'to', 'see', 'after', 'earth', 'with', 'me'], sentence2: ['after', 'earth', 'finishes', 'with', 'million']
cosine_similarity: 0.922920823097229
test_input: [0.17077611319011649, 0.9229208], test_label: 0
TF_IDF_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: even if After Earth is a good movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: even if After Earth is a good movie
After tokenization, sentence1: ['who', 'wants', 'to', 'see', 'after', 'earth', 'with', 'me'], sentence2: ['even', 'if', 'after', 'earth', 'is', 'a', 'good', 'movie']
cosine_similarity: 0.9761160016059875
test_input: [0.20199309249791833, 0.976116], test_label: 0
TF_IDF_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: Ether to see after earth or fast 6
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: Ether to see after earth or fast 6
After tokenization, sentence1: ['i', 'see', 'the', 'after', 'earth', 'reviews', 'are', 'not', 'positive'], sentence2: ['ether', 'to', 'see', 'after', 'earth', 'or', 'fast']
cosine_similarity: 0.987482488155365
test_input: [0.20199309249791833, 0.9874825], test_label: 0
TF_IDF_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: Wow AFTER EARTH really bombed huh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.         0.6316672
  0.        ]
 [0.47107781 0.33517574 0.47107781 0.         0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: Wow AFTER EARTH really bombed huh
After tokenization, sentence1: ['i', 'see', 'the', 'after', 'earth', 'reviews', 'are', 'not', 'positive'], sentence2: ['wow', 'after', 'earth', 'really', 'bombed', 'huh']
cosine_similarity: 0.9413618445396423
TF_IDF_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: I need to watch after earth tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.6316672  0.         0.        ]
 [0.37997836 0.53404633 0.         0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: I need to watch after earth tho
After tokenization, sentence1: ['i', 'see', 'the', 'after', 'earth', 'reviews', 'are', 'not', 'positive'], sentence2: ['i', 'need', 'to', 'watch', 'after', 'earth', 'tho']
cosine_similarity: 0.976889431476593
test_input: [0.17077611319011649, 0.97688943], test_label: 0
TF_IDF_cosine_similarity: sentence1: I just want to go see after earth, sentence2: Im taking my little brother to go see After Earth today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.         0.
  0.         0.6316672 ]
 [0.4261596  0.30321606 0.4261596  0.         0.4261596  0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: I just want to go see after earth, sentence2: Im taking my little brother to go see After Earth today
After tokenization, sentence1: ['i', 'just', 'want', 'to', 'go', 'see', 'after', 'earth'], sentence2: ['im', 'taking', 'my', 'little', 'brother', 'to', 'go', 'see', 'after', 'earth', 'today']
cosine_similarity: 0.9875696897506714
TF_IDF_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: At the theater with the little about do watch After Earth
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.57615236 0.         0.40993715]
 [0.         0.40993715 0.57615236 0.         0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: At the theater with the little about do watch After Earth
After tokenization, sentence1: ['i', 'need', 'to', 'watch', 'after', 'earth', 'asap'], sentence2: ['at', 'the', 'theater', 'with', 'the', 'little', 'about', 'do', 'watch', 'after', 'earth']
cosine_similarity: 0.9769679307937622
test_input: [0.3360969272762575, 0.97696793], test_label: 0
TF_IDF_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: Someone needs to come see after earth with me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633]
 [0.         0.6316672  0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: Someone needs to come see after earth with me
After tokenization, sentence1: ['i', 'need', 'to', 'watch', 'after', 'earth', 'asap'], sentence2: ['someone', 'needs', 'to', 'come', 'see', 'after', 'earth', 'with', 'me']
cosine_similarity: 0.979856014251709
test_input: [0.17077611319011649, 0.979856], test_label: 1
TF_IDF_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: someone come watch after earth will me June 7th
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.         0.40993715 0.         0.57615236
  0.40993715]
 [0.49922133 0.         0.49922133 0.35520009 0.49922133 0.
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: someone come watch after earth will me June 7th
After tokenization, sentence1: ['i', 'need', 'to', 'watch', 'after', 'earth', 'asap'], sentence2: ['someone', 'come', 'watch', 'after', 'earth', 'will', 'me', 'june']
cosine_similarity: 0.9826463460922241
TF_IDF_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: After Earth is a great ass movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.         0.53404633
  0.53404633]
 [0.         0.53404633 0.37997836 0.53404633 0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: After Earth is a great ass movie
After tokenization, sentence1: ['i', 'need', 'to', 'watch', 'after', 'earth', 'asap'], sentence2: ['after', 'earth', 'is', 'a', 'great', 'ass', 'movie']
cosine_similarity: 0.9571712017059326
test_input: [0.1443835552773867, 0.9571712], test_label: 0
TF_IDF_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: Goin to see after earth with the fam
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672 ]
 [0.         0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: Goin to see after earth with the fam
After tokenization, sentence1: ['heading', 'out', 'to', 'see', 'after', 'earth', 'in', 'a', 'bit'], sentence2: ['goin', 'to', 'see', 'after', 'earth', 'with', 'the', 'fam']
cosine_similarity: 0.9843347072601318
test_input: [0.20199309249791833, 0.9843347], test_label: 1
TF_IDF_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: will smith s speech in after earth is so relevant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.6316672  0.         0.         0.        ]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: will smith s speech in after earth is so relevant
After tokenization, sentence1: ['heading', 'out', 'to', 'see', 'after', 'earth', 'in', 'a', 'bit'], sentence2: ['will', 'smith', 's', 'speech', 'in', 'after', 'earth', 'is', 'so', 'relevant']
cosine_similarity: 0.9647049903869629
test_input: [0.17077611319011649, 0.964705], test_label: 0
TF_IDF_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: Just got done eating chinese with the fam now ganna go see after earth
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.
  0.         0.6316672  0.        ]
 [0.         0.39204401 0.27894255 0.39204401 0.39204401 0.39204401
  0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: Just got done eating chinese with the fam now ganna go see after earth
After tokenization, sentence1: ['heading', 'out', 'to', 'see', 'after', 'earth', 'in', 'a', 'bit'], sentence2: ['just', 'got', 'done', 'eating', 'chinese', 'with', 'the', 'fam', 'now', 'ganna', 'go', 'see', 'after', 'earth']
cosine_similarity: 0.9673506617546082
test_input: [0.12536693798731732, 0.96735066], test_label: 1
TF_IDF_cosine_similarity: sentence1: Going to see after earth but, sentence2: After earth is out and I havent seen it yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.        ]
 [0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Going to see after earth but, sentence2: After earth is out and I havent seen it yet
After tokenization, sentence1: ['going', 'to', 'see', 'after', 'earth', 'but'], sentence2: ['after', 'earth', 'is', 'out', 'and', 'i', 'havent', 'seen', 'it', 'yet']
cosine_similarity: 0.9810016751289368
test_input: [0.2605556710562624, 0.9810017], test_label: 0
TF_IDF_cosine_similarity: sentence1: Going to see after earth but, sentence2: wanted to watch After Earth today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.        ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Going to see after earth but, sentence2: wanted to watch After Earth today
After tokenization, sentence1: ['going', 'to', 'see', 'after', 'earth', 'but'], sentence2: ['wanted', 'to', 'watch', 'after', 'earth', 'today']
cosine_similarity: 0.9930177330970764
TF_IDF_cosine_similarity: sentence1: Who wants to take me to see After Earth, sentence2: Finally in the theaters to see after earth
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.81480247]
 [0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Who wants to take me to see After Earth, sentence2: Finally in the theaters to see after earth
After tokenization, sentence1: ['who', 'wants', 'to', 'take', 'me', 'to', 'see', 'after', 'earth'], sentence2: ['finally', 'in', 'the', 'theaters', 'to', 'see', 'after', 'earth']
cosine_similarity: 0.9574757814407349
TF_IDF_cosine_similarity: sentence1: Who wants to take me to see After Earth, sentence2: the hangover 3 and after earth are both really good
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Who wants to take me to see After Earth, sentence2: the hangover 3 and after earth are both really good
After tokenization, sentence1: ['who', 'wants', 'to', 'take', 'me', 'to', 'see', 'after', 'earth'], sentence2: ['the', 'hangover', 'and', 'after', 'earth', 'are', 'both', 'really', 'good']
cosine_similarity: 0.9573397636413574
test_input: [0.22028815056182965, 0.95733976], test_label: 0
TF_IDF_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: I kinda wanna see After Earth as well
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.70490949 0.50154891]
 [0.50154891 0.70490949 0.         0.50154891]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
word_to_vector_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: I kinda wanna see After Earth as well
After tokenization, sentence1: ['i', 'wanna', 'see', 'the', 'movie', 'after', 'earth'], sentence2: ['i', 'kinda', 'wanna', 'see', 'after', 'earth', 'as', 'well']
cosine_similarity: 0.9859218001365662
test_input: [0.5031026124151314, 0.9859218], test_label: 1
TF_IDF_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: NOW YOU SEE ME and AFTER EARTH Cant Outpace FAST FURIOUS 6
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: NOW YOU SEE ME and AFTER EARTH Cant Outpace FAST FURIOUS 6
After tokenization, sentence1: ['i', 'wanna', 'see', 'the', 'movie', 'after', 'earth'], sentence2: ['now', 'you', 'see', 'me', 'and', 'after', 'earth', 'cant', 'outpace', 'fast', 'furious']
cosine_similarity: 0.9832678437232971
test_input: [0.17077611319011649, 0.98326784], test_label: 0
TF_IDF_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: After Earth 039 trumped by 039 Now You See Me 039 as 039 Fast
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.         0.6316672 ]
 [0.92982421 0.1653944  0.23245605 0.         0.23245605 0.        ]]
pairwise_similarity: [[1.         0.07433426]
 [0.07433426 1.        ]]
cosine_similarity: 0.07433426458775617
word_to_vector_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: After Earth 039 trumped by 039 Now You See Me 039 as 039 Fast
After tokenization, sentence1: ['i', 'wanna', 'see', 'the', 'movie', 'after', 'earth'], sentence2: ['after', 'earth', 'trumped', 'by', 'now', 'you', 'see', 'me', 'as', 'fast']
cosine_similarity: 0.9750257134437561
test_input: [0.07433426458775617, 0.9750257], test_label: 0
TF_IDF_cosine_similarity: sentence1: the US and belgium are tied at half, sentence2: Come on Romelu get some goals for Belgium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: the US and belgium are tied at half, sentence2: Come on Romelu get some goals for Belgium
After tokenization, sentence1: ['the', 'us', 'and', 'belgium', 'are', 'tied', 'at', 'half'], sentence2: ['come', 'on', 'romelu', 'get', 'some', 'goals', 'for', 'belgium']
cosine_similarity: 0.9718703031539917
test_input: [0.17077611319011649, 0.9718703], test_label: 0
TF_IDF_cosine_similarity: sentence1: the US and belgium are tied at half, sentence2: Mirallas with a soft and cool finish off the rebound to put Belgium up 10
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.
  0.         0.         0.6316672 ]
 [0.39204401 0.27894255 0.39204401 0.39204401 0.         0.39204401
  0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: the US and belgium are tied at half, sentence2: Mirallas with a soft and cool finish off the rebound to put Belgium up 10
After tokenization, sentence1: ['the', 'us', 'and', 'belgium', 'are', 'tied', 'at', 'half'], sentence2: ['mirallas', 'with', 'a', 'soft', 'and', 'cool', 'finish', 'off', 'the', 'rebound', 'to', 'put', 'belgium', 'up']
cosine_similarity: 0.9653958678245544
test_input: [0.12536693798731732, 0.96539587], test_label: 0
TF_IDF_cosine_similarity: sentence1: Dude Belgium is freakin staked, sentence2: US vs Belgium or the wings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633 0.         0.        ]
 [0.44943642 0.         0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Dude Belgium is freakin staked, sentence2: US vs Belgium or the wings
After tokenization, sentence1: ['dude', 'belgium', 'is', 'freakin', 'staked'], sentence2: ['us', 'vs', 'belgium', 'or', 'the', 'wings']
cosine_similarity: 0.8548768758773804
test_input: [0.17077611319011649, 0.8548769], test_label: 0
TF_IDF_cosine_similarity: sentence1: Dude Belgium is freakin staked, sentence2: Belgium almost take the lead in the 27th min
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.53404633 0.         0.
  0.53404633]
 [0.53404633 0.37997836 0.         0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Dude Belgium is freakin staked, sentence2: Belgium almost take the lead in the 27th min
After tokenization, sentence1: ['dude', 'belgium', 'is', 'freakin', 'staked'], sentence2: ['belgium', 'almost', 'take', 'the', 'lead', 'in', 'the', 'min']
cosine_similarity: 0.8784986138343811
test_input: [0.1443835552773867, 0.8784986], test_label: 0
TF_IDF_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: What kind of formation Belgium playing there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.53404633]
 [0.37997836 0.53404633 0.         0.53404633 0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: What kind of formation Belgium playing there
After tokenization, sentence1: ['belgium', 's', 'gonna', 'rape', 'by', 'the', 'usa'], sentence2: ['what', 'kind', 'of', 'formation', 'belgium', 'playing', 'there']
cosine_similarity: 0.9525660276412964
test_input: [0.1443835552773867, 0.952566], test_label: 0
TF_IDF_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: At half US 1 Belgium 1 Indians 5
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.53404633 0.53404633]
 [0.44943642 0.         0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: At half US 1 Belgium 1 Indians 5
After tokenization, sentence1: ['belgium', 's', 'gonna', 'rape', 'by', 'the', 'usa'], sentence2: ['at', 'half', 'us', 'belgium', 'indians']
cosine_similarity: 0.9350979328155518
test_input: [0.17077611319011649, 0.93509793], test_label: 0
TF_IDF_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: Who s in goal for Belgium for the USMNT friendly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.53404633
  0.        ]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: Who s in goal for Belgium for the USMNT friendly
After tokenization, sentence1: ['belgium', 's', 'gonna', 'rape', 'by', 'the', 'usa'], sentence2: ['who', 's', 'in', 'goal', 'for', 'belgium', 'for', 'the', 'usmnt', 'friendly']
cosine_similarity: 0.9755635261535645
test_input: [0.1443835552773867, 0.9755635], test_label: 0
TF_IDF_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Vernaelen always gets injured for Belgium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.53404633]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Vernaelen always gets injured for Belgium
After tokenization, sentence1: ['belgium', 'vs', 'usa', 'you', 'watching'], sentence2: ['always', 'gets', 'injured', 'for', 'belgium']
cosine_similarity: 0.9102368354797363
test_input: [0.1443835552773867, 0.91023684], test_label: 0
TF_IDF_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Im not even watching the game but i see Belgium scored already
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.57615236 0.57615236
  0.40993715]
 [0.35520009 0.49922133 0.49922133 0.49922133 0.         0.
  0.35520009]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Im not even watching the game but i see Belgium scored already
After tokenization, sentence1: ['belgium', 'vs', 'usa', 'you', 'watching'], sentence2: ['im', 'not', 'even', 'watching', 'the', 'game', 'but', 'i', 'see', 'belgium', 'scored', 'already']
cosine_similarity: 0.9224916696548462
TF_IDF_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: 6th minute Belgium with the score
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.         0.53404633 0.53404633
  0.53404633]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: 6th minute Belgium with the score
After tokenization, sentence1: ['belgium', 'vs', 'usa', 'you', 'watching'], sentence2: ['minute', 'belgium', 'with', 'the', 'score']
cosine_similarity: 0.9402766227722168
test_input: [0.1443835552773867, 0.9402766], test_label: 0
TF_IDF_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Belgium in a friendly instead
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.53404633]
 [0.44943642 0.6316672  0.6316672  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Belgium in a friendly instead
After tokenization, sentence1: ['belgium', 'vs', 'usa', 'you', 'watching'], sentence2: ['belgium', 'in', 'a', 'friendly', 'instead']
cosine_similarity: 0.9446312785148621
test_input: [0.17077611319011649, 0.9446313], test_label: 0
TF_IDF_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: The Belgium GK wasnt trying to concede another goal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.         0.         0.4261596  0.4261596
  0.         0.4261596  0.4261596  0.         0.4261596 ]
 [0.30321606 0.4261596  0.4261596  0.4261596  0.         0.
  0.4261596  0.         0.         0.4261596  0.        ]]
pairwise_similarity: [[1.         0.09193998]
 [0.09193998 1.        ]]
cosine_similarity: 0.09193998174078082
word_to_vector_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: The Belgium GK wasnt trying to concede another goal
After tokenization, sentence1: ['watching', 'the', 'usmnt', 'vs', 'the', 'talented', 'belgium', 'team'], sentence2: ['the', 'belgium', 'gk', 'wasnt', 'trying', 'to', 'concede', 'another', 'goal']
cosine_similarity: 0.9051010608673096
test_input: [0.09193998174078082, 0.90510106], test_label: 0
TF_IDF_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: that being said US 21 Belgium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30321606 0.         0.4261596  0.4261596  0.4261596
  0.4261596  0.4261596 ]
 [0.6316672  0.44943642 0.6316672  0.         0.         0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: that being said US 21 Belgium
After tokenization, sentence1: ['watching', 'the', 'usmnt', 'vs', 'the', 'talented', 'belgium', 'team'], sentence2: ['that', 'being', 'said', 'us', 'belgium']
cosine_similarity: 0.9022647142410278
test_input: [0.1362763414390864, 0.9022647], test_label: 0
TF_IDF_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: At half US 1 Belgium 1 Indians 5
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.         0.4261596  0.4261596  0.4261596
  0.4261596  0.4261596 ]
 [0.44943642 0.6316672  0.6316672  0.         0.         0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: At half US 1 Belgium 1 Indians 5
After tokenization, sentence1: ['watching', 'the', 'usmnt', 'vs', 'the', 'talented', 'belgium', 'team'], sentence2: ['at', 'half', 'us', 'belgium', 'indians']
cosine_similarity: 0.931226372718811
TF_IDF_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: what s up with the nonHD main camera at the USBelgium soccer game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.         0.5        0.         0.5        0.
  0.         0.         0.         0.5       ]
 [0.         0.40824829 0.         0.40824829 0.         0.40824829
  0.40824829 0.40824829 0.40824829 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: what s up with the nonHD main camera at the USBelgium soccer game
After tokenization, sentence1: ['yeah', 'belgium', 'is', 'definitely', 'good'], sentence2: ['what', 's', 'up', 'with', 'the', 'main', 'camera', 'at', 'the', 'soccer', 'game']
cosine_similarity: 0.9108554124832153
test_input: [0.0, 0.9108554], test_label: 0
TF_IDF_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: Belgium vs USA you watching
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633]
 [0.37997836 0.         0.         0.53404633 0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: Belgium vs USA you watching
After tokenization, sentence1: ['yeah', 'belgium', 'is', 'definitely', 'good'], sentence2: ['belgium', 'vs', 'usa', 'you', 'watching']
cosine_similarity: 0.9155781865119934
test_input: [0.1443835552773867, 0.9155782], test_label: 0
TF_IDF_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: USA not looking good early in their friendly with Belgium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.40993715 0.
  0.         0.57615236]
 [0.31779954 0.         0.44665616 0.44665616 0.31779954 0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: USA not looking good early in their friendly with Belgium
After tokenization, sentence1: ['yeah', 'belgium', 'is', 'definitely', 'good'], sentence2: ['usa', 'not', 'looking', 'good', 'early', 'in', 'their', 'friendly', 'with', 'belgium']
cosine_similarity: 0.9443671107292175
TF_IDF_cosine_similarity: sentence1: Belgium has a great team, sentence2: Predicting a 31 for Belgium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.6316672 ]
 [0.6316672  0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Belgium has a great team, sentence2: Predicting a 31 for Belgium
After tokenization, sentence1: ['belgium', 'has', 'a', 'great', 'team'], sentence2: ['predicting', 'a', 'for', 'belgium']
cosine_similarity: 0.9491974115371704
TF_IDF_cosine_similarity: sentence1: Belgium has a great team, sentence2: a little late to the USABelgium game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.         0.57735027 0.         0.         0.57735027
  0.        ]
 [0.         0.5        0.         0.5        0.5        0.
  0.5       ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Belgium has a great team, sentence2: a little late to the USABelgium game
After tokenization, sentence1: ['belgium', 'has', 'a', 'great', 'team'], sentence2: ['a', 'little', 'late', 'to', 'the', 'game']
cosine_similarity: 0.9370109438896179
test_input: [0.0, 0.93701094], test_label: 0
TF_IDF_cosine_similarity: sentence1: Belgium has a great team, sentence2: USA is taking a tough blow from the Belgium national team right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.70490949 0.         0.         0.
  0.50154891 0.         0.        ]
 [0.26868528 0.37762778 0.         0.37762778 0.37762778 0.37762778
  0.26868528 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.26951761]
 [0.26951761 1.        ]]
cosine_similarity: 0.26951761324603224
word_to_vector_cosine_similarity: sentence1: Belgium has a great team, sentence2: USA is taking a tough blow from the Belgium national team right now
After tokenization, sentence1: ['belgium', 'has', 'a', 'great', 'team'], sentence2: ['usa', 'is', 'taking', 'a', 'tough', 'blow', 'from', 'the', 'belgium', 'national', 'team', 'right', 'now']
cosine_similarity: 0.9620335102081299
TF_IDF_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: Belgium is playing 4 centerbacks and fellani and they concede on a set piece
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.
  0.         0.         0.6316672 ]
 [0.27894255 0.39204401 0.39204401 0.39204401 0.         0.39204401
  0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: Belgium is playing 4 centerbacks and fellani and they concede on a set piece
After tokenization, sentence1: ['i', 'love', 'everyone', 'on', 'the', 'belgium', 'squad'], sentence2: ['belgium', 'is', 'playing', 'and', 'fellani', 'and', 'they', 'concede', 'on', 'a', 'set', 'piece']
cosine_similarity: 0.9611261487007141
test_input: [0.12536693798731732, 0.96112615], test_label: 0
TF_IDF_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: In other news this Belgium squad taking on USMNT is STACKED
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.50154891 0.         0.
  0.        ]
 [0.31779954 0.         0.44665616 0.31779954 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: In other news this Belgium squad taking on USMNT is STACKED
After tokenization, sentence1: ['i', 'love', 'everyone', 'on', 'the', 'belgium', 'squad'], sentence2: ['in', 'other', 'news', 'this', 'belgium', 'squad', 'taking', 'on', 'usmnt', 'is', 'stacked']
cosine_similarity: 0.9652331471443176
test_input: [0.31878402175377923, 0.96523315], test_label: 1
TF_IDF_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: It s Belgium we got this
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.6316672 ]
 [0.57973867 0.81480247 0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: It s Belgium we got this
After tokenization, sentence1: ['i', 'love', 'everyone', 'on', 'the', 'belgium', 'squad'], sentence2: ['it', 's', 'belgium', 'we', 'got', 'this']
cosine_similarity: 0.9824178814888
TF_IDF_cosine_similarity: sentence1: Rafa Benitez is still a massive prick, sentence2: Mourinho to city Benitez to stay at Chelsea
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.53404633 0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Rafa Benitez is still a massive prick, sentence2: Mourinho to city Benitez to stay at Chelsea
After tokenization, sentence1: ['rafa', 'benitez', 'is', 'still', 'a', 'massive', 'prick'], sentence2: ['mourinho', 'to', 'city', 'benitez', 'to', 'stay', 'at', 'chelsea']
cosine_similarity: 0.9191781282424927
test_input: [0.1273595297947935, 0.9191781], test_label: 0
TF_IDF_cosine_similarity: sentence1: Rafa Benitez is still a massive prick, sentence2: Chelsea FC wouldnt get rid of Benitez now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.53404633
  0.         0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.         0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Rafa Benitez is still a massive prick, sentence2: Chelsea FC wouldnt get rid of Benitez now
After tokenization, sentence1: ['rafa', 'benitez', 'is', 'still', 'a', 'massive', 'prick'], sentence2: ['chelsea', 'fc', 'wouldnt', 'get', 'rid', 'of', 'benitez', 'now']
cosine_similarity: 0.9513035416603088
test_input: [0.1273595297947935, 0.95130354], test_label: 0
TF_IDF_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: I hope Chelsea fans are thoroughly embarrassed now with the way they treated Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.
  0.6316672  0.         0.         0.        ]
 [0.25969799 0.36499647 0.36499647 0.36499647 0.         0.36499647
  0.         0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
word_to_vector_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: I hope Chelsea fans are thoroughly embarrassed now with the way they treated Benitez
After tokenization, sentence1: ['rafa', 'benitez', 'he', 's', 'too', 'good', 'for', 'you'], sentence2: ['i', 'hope', 'chelsea', 'fans', 'are', 'thoroughly', 'embarrassed', 'now', 'with', 'the', 'way', 'they', 'treated', 'benitez']
cosine_similarity: 0.949142336845398
test_input: [0.11671773546032795, 0.94914234], test_label: 0
TF_IDF_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: Thank you for your tactics Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.         0.        ]
 [0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: Thank you for your tactics Benitez
After tokenization, sentence1: ['rafa', 'benitez', 'he', 's', 'too', 'good', 'for', 'you'], sentence2: ['thank', 'you', 'for', 'your', 'tactics', 'benitez']
cosine_similarity: 0.957180380821228
test_input: [0.20199309249791833, 0.9571804], test_label: 0
TF_IDF_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: Got alot of time for rafa Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.70490949 0.         0.50154891 0.        ]
 [0.49922133 0.35520009 0.         0.49922133 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: Got alot of time for rafa Benitez
After tokenization, sentence1: ['rafa', 'benitez', 'he', 's', 'too', 'good', 'for', 'you'], sentence2: ['got', 'alot', 'of', 'time', 'for', 'rafa', 'benitez']
cosine_similarity: 0.9788239598274231
test_input: [0.3563004293331381, 0.97882396], test_label: 0
TF_IDF_cosine_similarity: sentence1: Benitez is a sick manager, sentence2: Rafa Benitez EuropeanGenius and they didnt want him
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672
  0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Benitez is a sick manager, sentence2: Rafa Benitez EuropeanGenius and they didnt want him
After tokenization, sentence1: ['benitez', 'is', 'a', 'sick', 'manager'], sentence2: ['rafa', 'benitez', 'and', 'they', 'didnt', 'want', 'him']
cosine_similarity: 0.930245041847229
TF_IDF_cosine_similarity: sentence1: Benitez is a sick manager, sentence2: Well done to Rafa Benitez a dignified man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Benitez is a sick manager, sentence2: Well done to Rafa Benitez a dignified man
After tokenization, sentence1: ['benitez', 'is', 'a', 'sick', 'manager'], sentence2: ['well', 'done', 'to', 'rafa', 'benitez', 'a', 'dignified', 'man']
cosine_similarity: 0.9568064212799072
test_input: [0.17077611319011649, 0.9568064], test_label: 0
TF_IDF_cosine_similarity: sentence1: congratulations to petr ech and rafa benitez, sentence2: Rafa Benitez a free agent
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.35520009]
 [0.57615236 0.40993715 0.         0.         0.57615236 0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: congratulations to petr ech and rafa benitez, sentence2: Rafa Benitez a free agent
After tokenization, sentence1: ['congratulations', 'to', 'petr', 'ech', 'and', 'rafa', 'benitez'], sentence2: ['rafa', 'benitez', 'a', 'free', 'agent']
cosine_similarity: 0.908642590045929
test_input: [0.29121941856368966, 0.9086426], test_label: 0
TF_IDF_cosine_similarity: sentence1: congratulations to petr ech and rafa benitez, sentence2: Credit where credits due to Rafa Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.49922133 0.49922133
  0.35520009]
 [0.40993715 0.         0.57615236 0.57615236 0.         0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: congratulations to petr ech and rafa benitez, sentence2: Credit where credits due to Rafa Benitez
After tokenization, sentence1: ['congratulations', 'to', 'petr', 'ech', 'and', 'rafa', 'benitez'], sentence2: ['credit', 'where', 'credits', 'due', 'to', 'rafa', 'benitez']
cosine_similarity: 0.9204893112182617
test_input: [0.29121941856368966, 0.9204893], test_label: 1
TF_IDF_cosine_similarity: sentence1: God forbid lyknx Rafa Benitez, sentence2: How can chelsea fans still hate benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.         0.47107781 0.47107781 0.
  0.47107781 0.47107781]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: God forbid lyknx Rafa Benitez, sentence2: How can chelsea fans still hate benitez
After tokenization, sentence1: ['god', 'forbid', 'rafa', 'benitez'], sentence2: ['how', 'can', 'chelsea', 'fans', 'still', 'hate', 'benitez']
cosine_similarity: 0.868623673915863
test_input: [0.1273595297947935, 0.8686237], test_label: 0
TF_IDF_cosine_similarity: sentence1: Thank you very much Rafa Benitez, sentence2: Congratulation chelsea for winning the europa league most especially Rafa Benitez and Torres
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.         0.         0.         0.
  0.50154891 0.70490949 0.         0.        ]
 [0.25136004 0.35327777 0.35327777 0.35327777 0.35327777 0.35327777
  0.25136004 0.         0.35327777 0.35327777]]
pairwise_similarity: [[1.         0.25213871]
 [0.25213871 1.        ]]
cosine_similarity: 0.2521387069452626
word_to_vector_cosine_similarity: sentence1: Thank you very much Rafa Benitez, sentence2: Congratulation chelsea for winning the europa league most especially Rafa Benitez and Torres
After tokenization, sentence1: ['thank', 'you', 'very', 'much', 'rafa', 'benitez'], sentence2: ['congratulation', 'chelsea', 'for', 'winning', 'the', 'europa', 'league', 'most', 'especially', 'rafa', 'benitez', 'and', 'torres']
cosine_similarity: 0.9088006615638733
TF_IDF_cosine_similarity: sentence1: Thank you very much Rafa Benitez, sentence2: Why do liverpool fans love benitez so much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Thank you very much Rafa Benitez, sentence2: Why do liverpool fans love benitez so much
After tokenization, sentence1: ['thank', 'you', 'very', 'much', 'rafa', 'benitez'], sentence2: ['why', 'do', 'liverpool', 'fans', 'love', 'benitez', 'so', 'much']
cosine_similarity: 0.9646732807159424
test_input: [0.17077611319011649, 0.9646733], test_label: 0
TF_IDF_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: I am so happy for RAFA BENITEZ VictorMoses and Mikel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.         0.50154891 0.70490949 0.        ]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.         0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: I am so happy for RAFA BENITEZ VictorMoses and Mikel
After tokenization, sentence1: ['rafa', 'benitez', 'i', 'must', 'thank', 'you'], sentence2: ['i', 'am', 'so', 'happy', 'for', 'rafa', 'benitez', 'and', 'mikel']
cosine_similarity: 0.9848617315292358
test_input: [0.3563004293331381, 0.98486173], test_label: 0
TF_IDF_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: Pleased for Benitez hasnt deserved the stick he s got
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.
  0.6316672 ]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.         0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: Pleased for Benitez hasnt deserved the stick he s got
After tokenization, sentence1: ['rafa', 'benitez', 'i', 'must', 'thank', 'you'], sentence2: ['pleased', 'for', 'benitez', 'hasnt', 'deserved', 'the', 'stick', 'he', 's', 'got']
cosine_similarity: 0.9612414240837097
test_input: [0.15064018498706508, 0.9612414], test_label: 0
TF_IDF_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: Once a red always a blue rafa Benitez we want you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.70490949 0.        ]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: Once a red always a blue rafa Benitez we want you
After tokenization, sentence1: ['rafa', 'benitez', 'i', 'must', 'thank', 'you'], sentence2: ['once', 'a', 'red', 'always', 'a', 'blue', 'rafa', 'benitez', 'we', 'want', 'you']
cosine_similarity: 0.9512678384780884
test_input: [0.3563004293331381, 0.95126784], test_label: 0
TF_IDF_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Benitez is alright tho man fuck chelsea fans they suck asshole
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44943642 0.         0.         0.
  0.         0.6316672  0.         0.6316672  0.        ]
 [0.34287126 0.34287126 0.24395573 0.34287126 0.34287126 0.34287126
  0.34287126 0.         0.34287126 0.         0.34287126]]
pairwise_similarity: [[1.         0.10964259]
 [0.10964259 1.        ]]
cosine_similarity: 0.10964258683453854
word_to_vector_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Benitez is alright tho man fuck chelsea fans they suck asshole
After tokenization, sentence1: ['thank', 'you', 'so', 'much', 'rafa', 'benitez'], sentence2: ['benitez', 'is', 'alright', 'tho', 'man', 'fuck', 'chelsea', 'fans', 'they', 'suck', 'asshole']
cosine_similarity: 0.9270437359809875
test_input: [0.10964258683453854, 0.92704374], test_label: 0
TF_IDF_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Credit where credits due to Rafa Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.         0.50154891 0.70490949]
 [0.40993715 0.57615236 0.57615236 0.40993715 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
word_to_vector_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Credit where credits due to Rafa Benitez
After tokenization, sentence1: ['thank', 'you', 'so', 'much', 'rafa', 'benitez'], sentence2: ['credit', 'where', 'credits', 'due', 'to', 'rafa', 'benitez']
cosine_similarity: 0.9057286381721497
test_input: [0.4112070550676187, 0.90572864], test_label: 1
TF_IDF_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Cant see Chelsea players lifting Benitez high
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.         0.6316672
  0.6316672 ]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.47107781 0.
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Cant see Chelsea players lifting Benitez high
After tokenization, sentence1: ['thank', 'you', 'so', 'much', 'rafa', 'benitez'], sentence2: ['cant', 'see', 'chelsea', 'players', 'lifting', 'benitez', 'high']
cosine_similarity: 0.9014921188354492
TF_IDF_cosine_similarity: sentence1: Rafa Benitez deserves a hell of a thank you, sentence2: Any praise for Benitez from my Chelsea followers lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.         0.47107781 0.
  0.         0.47107781 0.47107781]
 [0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.47107781 0.         0.        ]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542778
word_to_vector_cosine_similarity: sentence1: Rafa Benitez deserves a hell of a thank you, sentence2: Any praise for Benitez from my Chelsea followers lol
After tokenization, sentence1: ['rafa', 'benitez', 'deserves', 'a', 'hell', 'of', 'a', 'thank', 'you'], sentence2: ['any', 'praise', 'for', 'benitez', 'from', 'my', 'chelsea', 'followers', 'lol']
cosine_similarity: 0.9636006951332092
test_input: [0.11234277891542778, 0.9636007], test_label: 1
TF_IDF_cosine_similarity: sentence1: I dont understand the hatred for Rafa Benitez, sentence2: Top 4 and a trophy and still they dont give any respect for Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: I dont understand the hatred for Rafa Benitez, sentence2: Top 4 and a trophy and still they dont give any respect for Benitez
After tokenization, sentence1: ['i', 'dont', 'understand', 'the', 'hatred', 'for', 'rafa', 'benitez'], sentence2: ['top', 'and', 'a', 'trophy', 'and', 'still', 'they', 'dont', 'give', 'any', 'respect', 'for', 'benitez']
cosine_similarity: 0.9840744137763977
test_input: [0.29121941856368966, 0.9840744], test_label: 1
TF_IDF_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Bill Self to the Big 12
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.47107781 0.47107781 0.33517574 0.47107781
  0.        ]
 [0.6316672  0.         0.         0.         0.44943642 0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Bill Self to the Big 12
After tokenization, sentence1: ['big', 'announce', 'basketball', 'challenge'], sentence2: ['bill', 'self', 'to', 'the', 'big']
cosine_similarity: 0.9071119427680969
test_input: [0.15064018498706508, 0.90711194], test_label: 0
TF_IDF_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: The Big 12 just got a whole lot more interesting
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.47107781 0.47107781 0.33517574 0.47107781
  0.         0.         0.         0.        ]
 [0.4261596  0.         0.         0.         0.30321606 0.
  0.4261596  0.4261596  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: The Big 12 just got a whole lot more interesting
After tokenization, sentence1: ['big', 'announce', 'basketball', 'challenge'], sentence2: ['the', 'big', 'just', 'got', 'a', 'whole', 'lot', 'more', 'interesting']
cosine_similarity: 0.907336950302124
test_input: [0.10163066979112656, 0.90733695], test_label: 0
TF_IDF_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Just when you thought Bill Self wasnt going to own the Big 12 for another year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.47107781 0.47107781 0.33517574 0.47107781
  0.         0.         0.         0.         0.         0.        ]
 [0.36499647 0.         0.         0.         0.25969799 0.
  0.36499647 0.36499647 0.36499647 0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.08704447]
 [0.08704447 1.        ]]
cosine_similarity: 0.08704446792504217
word_to_vector_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Just when you thought Bill Self wasnt going to own the Big 12 for another year
After tokenization, sentence1: ['big', 'announce', 'basketball', 'challenge'], sentence2: ['just', 'when', 'you', 'thought', 'bill', 'self', 'wasnt', 'going', 'to', 'own', 'the', 'big', 'for', 'another', 'year']
cosine_similarity: 0.9065315127372742
test_input: [0.08704446792504217, 0.9065315], test_label: 0
TF_IDF_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Oklahoma in Houston also among the 10 SECBig 12 matchups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.4472136  0.4472136  0.4472136  0.4472136
  0.4472136  0.         0.         0.         0.        ]
 [0.40824829 0.40824829 0.         0.         0.         0.
  0.         0.40824829 0.40824829 0.40824829 0.40824829]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Oklahoma in Houston also among the 10 SECBig 12 matchups
After tokenization, sentence1: ['big', 'announce', 'basketball', 'challenge'], sentence2: ['oklahoma', 'in', 'houston', 'also', 'among', 'the', 'matchups']
cosine_similarity: 0.9247687458992004
test_input: [0.0, 0.92476875], test_label: 0
TF_IDF_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: Did kU win the Big 12 Quidditch Championship
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.33471228 0.         0.47042643 0.47042643
  0.         0.         0.         0.47042643]
 [0.30287281 0.30287281 0.30287281 0.42567716 0.         0.
  0.42567716 0.42567716 0.42567716 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
word_to_vector_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: Did kU win the Big 12 Quidditch Championship
After tokenization, sentence1: ['kansas', 'just', 'won', 'the', 'big', 'championship', 'again'], sentence2: ['did', 'ku', 'win', 'the', 'big', 'quidditch', 'championship']
cosine_similarity: 0.9759225249290466
test_input: [0.30412574187549346, 0.9759225], test_label: 1
TF_IDF_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: He can fuck up the Big 12 all he wants
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.         0.44665616]
 [0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: He can fuck up the Big 12 all he wants
After tokenization, sentence1: ['kansas', 'just', 'won', 'the', 'big', 'championship', 'again'], sentence2: ['he', 'can', 'fuck', 'up', 'the', 'big', 'all', 'he', 'wants']
cosine_similarity: 0.9395322203636169
test_input: [0.2605556710562624, 0.9395322], test_label: 0
TF_IDF_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: So what if the Big 12 had 14 teams
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.44665616]
 [0.40993715 0.57615236 0.40993715 0.         0.         0.
  0.57615236 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: So what if the Big 12 had 14 teams
After tokenization, sentence1: ['kansas', 'just', 'won', 'the', 'big', 'championship', 'again'], sentence2: ['so', 'what', 'if', 'the', 'big', 'had', 'teams']
cosine_similarity: 0.9659425020217896
test_input: [0.2605556710562624, 0.9659425], test_label: 0
TF_IDF_cosine_similarity: sentence1: Sorry to the rest of the big 12, sentence2: And Kansas once again will win the Big 12
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.57615236 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Sorry to the rest of the big 12, sentence2: And Kansas once again will win the Big 12
After tokenization, sentence1: ['sorry', 'to', 'the', 'rest', 'of', 'the', 'big'], sentence2: ['and', 'kansas', 'once', 'again', 'will', 'win', 'the', 'big']
cosine_similarity: 0.9794686436653137
test_input: [0.3360969272762575, 0.97946864], test_label: 0
TF_IDF_cosine_similarity: sentence1: The big 12 is about to be so stacked next year, sentence2: How many of those who were handing the Big 12 to Okla
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: The big 12 is about to be so stacked next year, sentence2: How many of those who were handing the Big 12 to Okla
After tokenization, sentence1: ['the', 'big', 'is', 'about', 'to', 'be', 'so', 'stacked', 'next', 'year'], sentence2: ['how', 'many', 'of', 'those', 'who', 'were', 'handing', 'the', 'big', 'to', 'okla']
cosine_similarity: 0.9783647060394287
test_input: [0.3360969272762575, 0.9783647], test_label: 0
TF_IDF_cosine_similarity: sentence1: The big 12 is about to be so stacked next year, sentence2: well I think Kansas will not win the Big 12 next season in mens basketball
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.         0.         0.
  0.57615236 0.         0.         0.57615236]
 [0.26868528 0.37762778 0.26868528 0.37762778 0.37762778 0.37762778
  0.         0.37762778 0.37762778 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: The big 12 is about to be so stacked next year, sentence2: well I think Kansas will not win the Big 12 next season in mens basketball
After tokenization, sentence1: ['the', 'big', 'is', 'about', 'to', 'be', 'so', 'stacked', 'next', 'year'], sentence2: ['well', 'i', 'think', 'kansas', 'will', 'not', 'win', 'the', 'big', 'next', 'season', 'in', 'mens', 'basketball']
cosine_similarity: 0.9884328246116638
TF_IDF_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: UKBig 12 challenge officially announced today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.         0.         0.53404633]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: UKBig 12 challenge officially announced today
After tokenization, sentence1: ['will', 'prolly', 'win', 'the', 'big'], sentence2: ['challenge', 'officially', 'announced', 'today']
cosine_similarity: 0.9019830822944641
test_input: [0.11521554337793122, 0.9019831], test_label: 0
TF_IDF_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: 21 in Houston as part of Big 12SEC Challenge
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.         0.37997836 0.         0.
  0.53404633 0.53404633]
 [0.         0.47107781 0.47107781 0.33517574 0.47107781 0.47107781
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: 21 in Houston as part of Big 12SEC Challenge
After tokenization, sentence1: ['will', 'prolly', 'win', 'the', 'big'], sentence2: ['in', 'houston', 'as', 'part', 'of', 'big', 'challenge']
cosine_similarity: 0.9534963965415955
test_input: [0.1273595297947935, 0.9534964], test_label: 0
TF_IDF_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: No reason Kansas should lose a game in the big 12
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.         0.57615236]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: No reason Kansas should lose a game in the big 12
After tokenization, sentence1: ['will', 'prolly', 'win', 'the', 'big'], sentence2: ['no', 'reason', 'kansas', 'should', 'lose', 'a', 'game', 'in', 'the', 'big']
cosine_similarity: 0.9620205760002136
test_input: [0.2605556710562624, 0.9620206], test_label: 0
TF_IDF_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: The whole Big 12 but Okiestate in particular just lost their minds
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.
  0.         0.57615236 0.57615236]
 [0.29017021 0.29017021 0.4078241  0.4078241  0.4078241  0.4078241
  0.4078241  0.         0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
word_to_vector_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: The whole Big 12 but Okiestate in particular just lost their minds
After tokenization, sentence1: ['will', 'prolly', 'win', 'the', 'big'], sentence2: ['the', 'whole', 'big', 'but', 'in', 'particular', 'just', 'lost', 'their', 'minds']
cosine_similarity: 0.9629221558570862
test_input: [0.23790309463326234, 0.96292216], test_label: 0
TF_IDF_cosine_similarity: sentence1: Sorry to the rest of the big 12, sentence2: Texas Tech will play at Alabama in the SECBig 12 Basketball Challenge on Nov
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.         0.
  0.         0.53404633 0.         0.53404633 0.         0.        ]
 [0.24395573 0.34287126 0.34287126 0.         0.34287126 0.34287126
  0.34287126 0.         0.34287126 0.         0.34287126 0.34287126]]
pairwise_similarity: [[1.        0.0926979]
 [0.0926979 1.       ]]
cosine_similarity: 0.09269789668627057
word_to_vector_cosine_similarity: sentence1: Sorry to the rest of the big 12, sentence2: Texas Tech will play at Alabama in the SECBig 12 Basketball Challenge on Nov
After tokenization, sentence1: ['sorry', 'to', 'the', 'rest', 'of', 'the', 'big'], sentence2: ['texas', 'tech', 'will', 'play', 'at', 'alabama', 'in', 'the', 'basketball', 'challenge', 'on', 'nov']
cosine_similarity: 0.9469119906425476
test_input: [0.09269789668627057, 0.946912], test_label: 0
TF_IDF_cosine_similarity: sentence1: UK part of the Big 12SEC Challenge, sentence2: There is NOOOO competition in Big 12 basketball
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.         0.37997836 0.53404633 0.
  0.         0.53404633]
 [0.47107781 0.         0.47107781 0.33517574 0.         0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: UK part of the Big 12SEC Challenge, sentence2: There is NOOOO competition in Big 12 basketball
After tokenization, sentence1: ['uk', 'part', 'of', 'the', 'big', 'challenge'], sentence2: ['there', 'is', 'noooo', 'competition', 'in', 'big', 'basketball']
cosine_similarity: 0.9584144949913025
test_input: [0.1273595297947935, 0.9584145], test_label: 0
TF_IDF_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: to win the big 12 in all three major sports in the same year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.         0.        ]
 [0.31779954 0.31779954 0.         0.         0.         0.44665616
  0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
word_to_vector_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: to win the big 12 in all three major sports in the same year
After tokenization, sentence1: ['well', 'the', 'big', 'just', 'got', 'decided'], sentence2: ['to', 'win', 'the', 'big', 'in', 'all', 'three', 'major', 'sports', 'in', 'the', 'same', 'year']
cosine_similarity: 0.9616364240646362
test_input: [0.22576484600261604, 0.9616364], test_label: 0
TF_IDF_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: SECBig 12 Challenge in hoops has been announced to begin in 201314
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.         0.         0.47107781 0.
  0.47107781 0.47107781 0.         0.47107781 0.        ]
 [0.27894255 0.39204401 0.39204401 0.39204401 0.         0.39204401
  0.         0.         0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536718
word_to_vector_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: SECBig 12 Challenge in hoops has been announced to begin in 201314
After tokenization, sentence1: ['well', 'the', 'big', 'just', 'got', 'decided'], sentence2: ['challenge', 'in', 'hoops', 'has', 'been', 'announced', 'to', 'begin', 'in']
cosine_similarity: 0.9308721423149109
test_input: [0.09349477497536718, 0.93087214], test_label: 0
TF_IDF_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: So Wiggins Is Settling For Playing In The Garbage Ass Big 12
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.49922133 0.         0.49922133
  0.49922133 0.         0.         0.        ]
 [0.29017021 0.4078241  0.29017021 0.         0.4078241  0.
  0.         0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: So Wiggins Is Settling For Playing In The Garbage Ass Big 12
After tokenization, sentence1: ['well', 'the', 'big', 'just', 'got', 'decided'], sentence2: ['so', 'wiggins', 'is', 'settling', 'for', 'playing', 'in', 'the', 'garbage', 'ass', 'big']
cosine_similarity: 0.9753780961036682
test_input: [0.20613696606828605, 0.9753781], test_label: 0
TF_IDF_cosine_similarity: sentence1: Just wish he wasnt in the Big 12, sentence2: Big 12 is gonna be exciting
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Just wish he wasnt in the Big 12, sentence2: Big 12 is gonna be exciting
After tokenization, sentence1: ['just', 'wish', 'he', 'wasnt', 'in', 'the', 'big'], sentence2: ['big', 'is', 'gonna', 'be', 'exciting']
cosine_similarity: 0.9783661365509033
test_input: [0.29121941856368966, 0.97836614], test_label: 0
TF_IDF_cosine_similarity: sentence1: Just wish he wasnt in the Big 12, sentence2: the BIG 12 goes through LAWRENCE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.         0.49922133
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Just wish he wasnt in the Big 12, sentence2: the BIG 12 goes through LAWRENCE
After tokenization, sentence1: ['just', 'wish', 'he', 'wasnt', 'in', 'the', 'big'], sentence2: ['the', 'big', 'goes', 'through', 'lawrence']
cosine_similarity: 0.9580591917037964
test_input: [0.29121941856368966, 0.9580592], test_label: 0
TF_IDF_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: I just found out Marilyn Monroe has a full bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.70710678 0.         0.         0.         0.70710678]
 [0.5        0.         0.5        0.5        0.5        0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: I just found out Marilyn Monroe has a full bush
After tokenization, sentence1: ['were', 'in', 'reigate'], sentence2: ['i', 'just', 'found', 'out', 'marilyn', 'monroe', 'has', 'a', 'full', 'bush']
cosine_similarity: 0.8578868508338928
test_input: [0.0, 0.85788685], test_label: 0
TF_IDF_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: Classic redhead with a natural bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.70710678 0.         0.         0.70710678]
 [0.5        0.5        0.         0.5        0.5        0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: Classic redhead with a natural bush
After tokenization, sentence1: ['were', 'in', 'reigate'], sentence2: ['classic', 'redhead', 'with', 'a', 'natural', 'bush']
cosine_similarity: 0.7725995182991028
test_input: [0.0, 0.7725995], test_label: 0
TF_IDF_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: Hahah I heard your dumbass woke up in a bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.70710678 0.         0.70710678
  0.        ]
 [0.4472136  0.4472136  0.4472136  0.         0.4472136  0.
  0.4472136 ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: Hahah I heard your dumbass woke up in a bush
After tokenization, sentence1: ['were', 'in', 'reigate'], sentence2: ['hahah', 'i', 'heard', 'your', 'dumbass', 'woke', 'up', 'in', 'a', 'bush']
cosine_similarity: 0.8522698283195496
test_input: [0.0, 0.8522698], test_label: 0
TF_IDF_cosine_similarity: sentence1: I dont have time for beating around the bush, sentence2: Honey has a brush with her non existent bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.
  0.         0.53404633]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: I dont have time for beating around the bush, sentence2: Honey has a brush with her non existent bush
After tokenization, sentence1: ['i', 'dont', 'have', 'time', 'for', 'beating', 'around', 'the', 'bush'], sentence2: ['honey', 'has', 'a', 'brush', 'with', 'her', 'non', 'existent', 'bush']
cosine_similarity: 0.9344807863235474
test_input: [0.1273595297947935, 0.9344808], test_label: 0
TF_IDF_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: How the hell could Obama kill more than Bush did in Iraq
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.
  0.6316672  0.        ]
 [0.30321606 0.         0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: How the hell could Obama kill more than Bush did in Iraq
After tokenization, sentence1: ['ctfu', 'the', 'man', 'in', 'the', 'bush'], sentence2: ['how', 'the', 'hell', 'could', 'obama', 'kill', 'more', 'than', 'bush', 'did', 'in', 'iraq']
cosine_similarity: 0.9758034348487854
test_input: [0.1362763414390864, 0.97580343], test_label: 0
TF_IDF_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: george bush is never a truther
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.        ]
 [0.44943642 0.         0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: george bush is never a truther
After tokenization, sentence1: ['ctfu', 'the', 'man', 'in', 'the', 'bush'], sentence2: ['george', 'bush', 'is', 'never', 'a', 'truther']
cosine_similarity: 0.938325822353363
test_input: [0.20199309249791833, 0.9383258], test_label: 0
TF_IDF_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: Im a fan of Clintons Pretty much despise Bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.
  0.6316672  0.        ]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: Im a fan of Clintons Pretty much despise Bush
After tokenization, sentence1: ['ctfu', 'the', 'man', 'in', 'the', 'bush'], sentence2: ['im', 'a', 'fan', 'of', 'clintons', 'pretty', 'much', 'despise', 'bush']
cosine_similarity: 0.9562652707099915
test_input: [0.1362763414390864, 0.9562653], test_label: 0
TF_IDF_cosine_similarity: sentence1: He is worse than Bush, sentence2: It was under Bush it is now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247]
 [1.         0.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
word_to_vector_cosine_similarity: sentence1: He is worse than Bush, sentence2: It was under Bush it is now
After tokenization, sentence1: ['he', 'is', 'worse', 'than', 'bush'], sentence2: ['it', 'was', 'under', 'bush', 'it', 'is', 'now']
cosine_similarity: 0.9696451425552368
test_input: [0.5797386715376657, 0.96964514], test_label: 0
TF_IDF_cosine_similarity: sentence1: He is worse than Bush, sentence2: Did Obama and the Dems trust the Bush Government
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.
  0.81480247]
 [0.30321606 0.4261596  0.4261596  0.4261596  0.4261596  0.4261596
  0.        ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: He is worse than Bush, sentence2: Did Obama and the Dems trust the Bush Government
After tokenization, sentence1: ['he', 'is', 'worse', 'than', 'bush'], sentence2: ['did', 'obama', 'and', 'the', 'dems', 'trust', 'the', 'bush', 'government']
cosine_similarity: 0.9384075403213501
test_input: [0.17578607839334617, 0.93840754], test_label: 0
TF_IDF_cosine_similarity: sentence1: He is worse than Bush, sentence2: that the Libs are going to say its Bush s fault and Im a racist
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.
  0.         0.81480247]
 [0.27894255 0.39204401 0.39204401 0.39204401 0.39204401 0.39204401
  0.39204401 0.        ]]
pairwise_similarity: [[1.         0.16171378]
 [0.16171378 1.        ]]
cosine_similarity: 0.16171378066252898
word_to_vector_cosine_similarity: sentence1: He is worse than Bush, sentence2: that the Libs are going to say its Bush s fault and Im a racist
After tokenization, sentence1: ['he', 'is', 'worse', 'than', 'bush'], sentence2: ['that', 'the', 'libs', 'are', 'going', 'to', 'say', 'its', 'bush', 's', 'fault', 'and', 'im', 'a', 'racist']
cosine_similarity: 0.9586978554725647
test_input: [0.16171378066252898, 0.95869786], test_label: 0
TF_IDF_cosine_similarity: sentence1: Then time to trim the rose bush, sentence2: I dint like it under Bush either Obama has radically expanded it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.         0.         0.
  0.53404633 0.53404633 0.53404633]
 [0.30321606 0.4261596  0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Then time to trim the rose bush, sentence2: I dint like it under Bush either Obama has radically expanded it
After tokenization, sentence1: ['then', 'time', 'to', 'trim', 'the', 'rose', 'bush'], sentence2: ['i', 'dint', 'like', 'it', 'under', 'bush', 'either', 'obama', 'has', 'radically', 'expanded', 'it']
cosine_similarity: 0.9424854516983032
test_input: [0.11521554337793122, 0.94248545], test_label: 0
TF_IDF_cosine_similarity: sentence1: I want the Bush days back, sentence2: Dont beat around the bush just say it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.
  0.6316672 ]
 [0.47107781 0.33517574 0.         0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: I want the Bush days back, sentence2: Dont beat around the bush just say it
After tokenization, sentence1: ['i', 'want', 'the', 'bush', 'days', 'back'], sentence2: ['dont', 'beat', 'around', 'the', 'bush', 'just', 'say', 'it']
cosine_similarity: 0.984318733215332
test_input: [0.15064018498706508, 0.98431873], test_label: 0
TF_IDF_cosine_similarity: sentence1: I want the Bush days back, sentence2: fell in a spiky bush and I have a prickly thing in my finger
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.
  0.         0.6316672 ]
 [0.30321606 0.         0.4261596  0.4261596  0.4261596  0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: I want the Bush days back, sentence2: fell in a spiky bush and I have a prickly thing in my finger
After tokenization, sentence1: ['i', 'want', 'the', 'bush', 'days', 'back'], sentence2: ['fell', 'in', 'a', 'spiky', 'bush', 'and', 'i', 'have', 'a', 'prickly', 'thing', 'in', 'my', 'finger']
cosine_similarity: 0.9718008041381836
test_input: [0.1362763414390864, 0.9718008], test_label: 0
TF_IDF_cosine_similarity: sentence1: I want the Bush days back, sentence2: i once walked into a bush outside school and literally apologised to it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.
  0.         0.6316672 ]
 [0.4261596  0.30321606 0.         0.4261596  0.4261596  0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: I want the Bush days back, sentence2: i once walked into a bush outside school and literally apologised to it
After tokenization, sentence1: ['i', 'want', 'the', 'bush', 'days', 'back'], sentence2: ['i', 'once', 'walked', 'into', 'a', 'bush', 'outside', 'school', 'and', 'literally', 'apologised', 'to', 'it']
cosine_similarity: 0.9737642407417297
test_input: [0.1362763414390864, 0.97376424], test_label: 0
TF_IDF_cosine_similarity: sentence1: I want the Bush days back, sentence2: Bush wiretapped without warrants Obama had them
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I want the Bush days back, sentence2: Bush wiretapped without warrants Obama had them
After tokenization, sentence1: ['i', 'want', 'the', 'bush', 'days', 'back'], sentence2: ['bush', 'without', 'warrants', 'obama', 'had', 'them']
cosine_similarity: 0.9149393439292908
test_input: [0.17077611319011649, 0.91493934], test_label: 0
TF_IDF_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: started under bush and Im sure you were cool with it then
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.
  0.        ]
 [0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: started under bush and Im sure you were cool with it then
After tokenization, sentence1: ['footbridge', 'over', 'the', 'river', 'bush'], sentence2: ['started', 'under', 'bush', 'and', 'im', 'sure', 'you', 'were', 'cool', 'with', 'it', 'then']
cosine_similarity: 0.8623417615890503
test_input: [0.15064018498706508, 0.86234176], test_label: 0
TF_IDF_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: The new Bush tour merchandise is now available
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672
  0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: The new Bush tour merchandise is now available
After tokenization, sentence1: ['footbridge', 'over', 'the', 'river', 'bush'], sentence2: ['the', 'new', 'bush', 'tour', 'merchandise', 'is', 'now', 'available']
cosine_similarity: 0.8923953771591187
test_input: [0.15064018498706508, 0.8923954], test_label: 0
TF_IDF_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: we live near a bush reserve
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.6316672 ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: we live near a bush reserve
After tokenization, sentence1: ['footbridge', 'over', 'the', 'river', 'bush'], sentence2: ['we', 'live', 'near', 'a', 'bush', 'reserve']
cosine_similarity: 0.9388440251350403
test_input: [0.17077611319011649, 0.938844], test_label: 0
TF_IDF_cosine_similarity: sentence1: I did for Bush as well, sentence2: in my actual bush in a bush in Bushey
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.81480247]
 [0.49844628 0.70929727 0.49844628 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
word_to_vector_cosine_similarity: sentence1: I did for Bush as well, sentence2: in my actual bush in a bush in Bushey
After tokenization, sentence1: ['i', 'did', 'for', 'bush', 'as', 'well'], sentence2: ['in', 'my', 'actual', 'bush', 'in', 'a', 'bush', 'in', 'bushey']
cosine_similarity: 0.9389052987098694
test_input: [0.41120705506761857, 0.9389053], test_label: 0
TF_IDF_cosine_similarity: sentence1: I did for Bush as well, sentence2: it started way before Bush Jr
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.        ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: I did for Bush as well, sentence2: it started way before Bush Jr
After tokenization, sentence1: ['i', 'did', 'for', 'bush', 'as', 'well'], sentence2: ['it', 'started', 'way', 'before', 'bush', 'jr']
cosine_similarity: 0.9732149243354797
test_input: [0.22028815056182965, 0.9732149], test_label: 0
TF_IDF_cosine_similarity: sentence1: Darling stop beating around the bush, sentence2: They called Bush hitler too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.         0.53404633]
 [0.         0.44943642 0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Darling stop beating around the bush, sentence2: They called Bush hitler too
After tokenization, sentence1: ['darling', 'stop', 'beating', 'around', 'the', 'bush'], sentence2: ['they', 'called', 'bush', 'hitler', 'too']
cosine_similarity: 0.9547279477119446
test_input: [0.17077611319011649, 0.95472795], test_label: 0
TF_IDF_cosine_similarity: sentence1: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR, sentence2: Why cant everyone just tell it how it is instead of beating around the bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.2130798  0.         0.30321606 0.8523192  0.2130798  0.
  0.         0.2130798  0.2130798  0.        ]
 [0.         0.47107781 0.33517574 0.         0.         0.47107781
  0.47107781 0.         0.         0.47107781]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR, sentence2: Why cant everyone just tell it how it is instead of beating around the bush
After tokenization, sentence1: ['fuck', 'the', 'government', 'fuck', 'the', 'obama', 'administration', 'fuck', 'bush', 'fuck', 'bush', 'sr'], sentence2: ['why', 'cant', 'everyone', 'just', 'tell', 'it', 'how', 'it', 'is', 'instead', 'of', 'beating', 'around', 'the', 'bush']
cosine_similarity: 0.9465409517288208
test_input: [0.10163066979112656, 0.94654095], test_label: 0
TF_IDF_cosine_similarity: sentence1: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR, sentence2: And his legs can have hair but no bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.2130798  0.30321606 0.8523192  0.2130798  0.         0.
  0.2130798  0.2130798 ]
 [0.         0.44943642 0.         0.         0.6316672  0.6316672
  0.         0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR, sentence2: And his legs can have hair but no bush
After tokenization, sentence1: ['fuck', 'the', 'government', 'fuck', 'the', 'obama', 'administration', 'fuck', 'bush', 'fuck', 'bush', 'sr'], sentence2: ['and', 'his', 'legs', 'can', 'have', 'hair', 'but', 'no', 'bush']
cosine_similarity: 0.9225195646286011
test_input: [0.1362763414390864, 0.92251956], test_label: 0
TF_IDF_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Allen Bosh Chalmers James at Wade
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44943642 0.6316672  0.         0.
  0.6316672 ]
 [0.47107781 0.47107781 0.33517574 0.         0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Allen Bosh Chalmers James at Wade
After tokenization, sentence1: ['yeah', 'chalmers', 'that', 's', 'a', 'foul'], sentence2: ['allen', 'bosh', 'chalmers', 'james', 'at', 'wade']
cosine_similarity: 0.8636842966079712
test_input: [0.15064018498706508, 0.8636843], test_label: 0
TF_IDF_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Chalmers makes me mad low key
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.
  0.6316672 ]
 [0.33517574 0.         0.47107781 0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Chalmers makes me mad low key
After tokenization, sentence1: ['yeah', 'chalmers', 'that', 's', 'a', 'foul'], sentence2: ['chalmers', 'makes', 'me', 'mad', 'low', 'key']
cosine_similarity: 0.9685675501823425
test_input: [0.15064018498706508, 0.96856755], test_label: 0
TF_IDF_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Chalmers steady chasing on defense
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Chalmers steady chasing on defense
After tokenization, sentence1: ['yeah', 'chalmers', 'that', 's', 'a', 'foul'], sentence2: ['chalmers', 'steady', 'chasing', 'on', 'defense']
cosine_similarity: 0.9011731147766113
test_input: [0.17077611319011649, 0.9011731], test_label: 0
TF_IDF_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: I like Cole s onball defense better than Chalmers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.
  0.         0.6316672 ]
 [0.4261596  0.30321606 0.4261596  0.4261596  0.         0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: I like Cole s onball defense better than Chalmers
After tokenization, sentence1: ['yeah', 'chalmers', 'that', 's', 'a', 'foul'], sentence2: ['i', 'like', 'cole', 's', 'defense', 'better', 'than', 'chalmers']
cosine_similarity: 0.9816040992736816
test_input: [0.1362763414390864, 0.9816041], test_label: 0
TF_IDF_cosine_similarity: sentence1: And now chalmers with the scoop layup, sentence2: Anybody see David West Elbow the hell out of Chalmers bad shoulder
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44943642 0.         0.         0.
  0.6316672  0.6316672  0.         0.        ]
 [0.36499647 0.36499647 0.25969799 0.36499647 0.36499647 0.36499647
  0.         0.         0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
word_to_vector_cosine_similarity: sentence1: And now chalmers with the scoop layup, sentence2: Anybody see David West Elbow the hell out of Chalmers bad shoulder
After tokenization, sentence1: ['and', 'now', 'chalmers', 'with', 'the', 'scoop', 'layup'], sentence2: ['anybody', 'see', 'david', 'west', 'elbow', 'the', 'hell', 'out', 'of', 'chalmers', 'bad', 'shoulder']
cosine_similarity: 0.9774250984191895
test_input: [0.11671773546032795, 0.9774251], test_label: 0
TF_IDF_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: MARIO CHALMERS JUST THREW IT TO HIS COACH HAHAHA
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.         0.         0.50154891 0.70490949
  0.        ]
 [0.31779954 0.44665616 0.44665616 0.44665616 0.31779954 0.
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
word_to_vector_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: MARIO CHALMERS JUST THREW IT TO HIS COACH HAHAHA
After tokenization, sentence1: ['mario', 'chalmers', 'pissin', 'me', 'off'], sentence2: ['mario', 'chalmers', 'just', 'threw', 'it', 'to', 'his', 'coach', 'hahaha']
cosine_similarity: 0.9034698009490967
test_input: [0.31878402175377923, 0.9034698], test_label: 0
TF_IDF_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Spo aint in the game chalmers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Spo aint in the game chalmers
After tokenization, sentence1: ['mario', 'chalmers', 'pissin', 'me', 'off'], sentence2: ['spo', 'aint', 'in', 'the', 'game', 'chalmers']
cosine_similarity: 0.8749983906745911
test_input: [0.17077611319011649, 0.8749984], test_label: 0
TF_IDF_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Mario Chalmers needs to get punched in the face
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.70490949 0.        ]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Mario Chalmers needs to get punched in the face
After tokenization, sentence1: ['mario', 'chalmers', 'pissin', 'me', 'off'], sentence2: ['mario', 'chalmers', 'needs', 'to', 'get', 'punched', 'in', 'the', 'face']
cosine_similarity: 0.8946954011917114
test_input: [0.3563004293331381, 0.8946954], test_label: 1
TF_IDF_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Chalmers throws it out of bounds
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.        ]
 [0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Chalmers throws it out of bounds
After tokenization, sentence1: ['mario', 'chalmers', 'pissin', 'me', 'off'], sentence2: ['chalmers', 'throws', 'it', 'out', 'of', 'bounds']
cosine_similarity: 0.8635400533676147
test_input: [0.20199309249791833, 0.86354005], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chalmers just walked with the ball, sentence2: Uhm Chalmers didnt just travel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.40993715 0.         0.
  0.57615236]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chalmers just walked with the ball, sentence2: Uhm Chalmers didnt just travel
After tokenization, sentence1: ['chalmers', 'just', 'walked', 'with', 'the', 'ball'], sentence2: ['uhm', 'chalmers', 'didnt', 'just', 'travel']
cosine_similarity: 0.9450207948684692
test_input: [0.29121941856368966, 0.9450208], test_label: 1
TF_IDF_cosine_similarity: sentence1: Mario Chalmers looks like Mario, sentence2: Then Chalmers fucks up again lmao
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27894255 0.         0.39204401 0.         0.39204401 0.78408803]
 [0.44943642 0.6316672  0.         0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Mario Chalmers looks like Mario, sentence2: Then Chalmers fucks up again lmao
After tokenization, sentence1: ['mario', 'chalmers', 'looks', 'like', 'mario'], sentence2: ['then', 'chalmers', 'fucks', 'up', 'again', 'lmao']
cosine_similarity: 0.8611149191856384
test_input: [0.12536693798731732, 0.8611149], test_label: 0
TF_IDF_cosine_similarity: sentence1: Mario Chalmers looks like Mario, sentence2: Lucky ass shxt by Chalmers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.27894255 0.39204401 0.39204401 0.         0.78408803
  0.        ]
 [0.53404633 0.37997836 0.         0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Mario Chalmers looks like Mario, sentence2: Lucky ass shxt by Chalmers
After tokenization, sentence1: ['mario', 'chalmers', 'looks', 'like', 'mario'], sentence2: ['lucky', 'ass', 'shxt', 'by', 'chalmers']
cosine_similarity: 0.860129177570343
test_input: [0.1059921313509325, 0.8601292], test_label: 0
TF_IDF_cosine_similarity: sentence1: I hate Mario Chalmersdont know why, sentence2: idc idc chalmers be making me mad
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.5        0.5        0.         0.5        0.
  0.         0.5       ]
 [0.37796447 0.         0.         0.75592895 0.         0.37796447
  0.37796447 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: I hate Mario Chalmersdont know why, sentence2: idc idc chalmers be making me mad
After tokenization, sentence1: ['i', 'hate', 'mario', 'know', 'why'], sentence2: ['idc', 'idc', 'chalmers', 'be', 'making', 'me', 'mad']
cosine_similarity: 0.9532608389854431
test_input: [0.0, 0.95326084], test_label: 1
TF_IDF_cosine_similarity: sentence1: I hate Mario Chalmersdont know why, sentence2: Why is Mario Chalmers starting
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.53404633 0.53404633 0.37997836 0.        ]
 [0.6316672  0.         0.         0.         0.44943642 0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: I hate Mario Chalmersdont know why, sentence2: Why is Mario Chalmers starting
After tokenization, sentence1: ['i', 'hate', 'mario', 'know', 'why'], sentence2: ['why', 'is', 'mario', 'chalmers', 'starting']
cosine_similarity: 0.9151886701583862
test_input: [0.17077611319011649, 0.91518867], test_label: 0
TF_IDF_cosine_similarity: sentence1: CHALMERS IS THE TURNOVER KING, sentence2: Didnt everyone love Mario Chalmers last year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.6316672
  0.        ]
 [0.33517574 0.47107781 0.         0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: CHALMERS IS THE TURNOVER KING, sentence2: Didnt everyone love Mario Chalmers last year
After tokenization, sentence1: ['chalmers', 'is', 'the', 'turnover', 'king'], sentence2: ['didnt', 'everyone', 'love', 'mario', 'chalmers', 'last', 'year']
cosine_similarity: 0.9002931714057922
test_input: [0.15064018498706508, 0.9002932], test_label: 0
TF_IDF_cosine_similarity: sentence1: CHALMERS IS THE TURNOVER KING, sentence2: Mario chalmers might as well go to the locker room and put on a pacer jersey
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.
  0.         0.6316672 ]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: CHALMERS IS THE TURNOVER KING, sentence2: Mario chalmers might as well go to the locker room and put on a pacer jersey
After tokenization, sentence1: ['chalmers', 'is', 'the', 'turnover', 'king'], sentence2: ['mario', 'chalmers', 'might', 'as', 'well', 'go', 'to', 'the', 'locker', 'room', 'and', 'put', 'on', 'a', 'pacer', 'jersey']
cosine_similarity: 0.9410765171051025
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: LMFAOOO who tf you throwing to Chalmers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: LMFAOOO who tf you throwing to Chalmers
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['lmfaooo', 'who', 'tf', 'you', 'throwing', 'to', 'chalmers']
cosine_similarity: 0.964851975440979
test_input: [0.17077611319011649, 0.964852], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: James with the block and then wade to James James to chalmers for the finish
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672
  0.        ]
 [0.2827721  0.20119468 0.         0.2827721  0.84831629 0.
  0.2827721 ]]
pairwise_similarity: [[1.         0.09042421]
 [0.09042421 1.        ]]
cosine_similarity: 0.09042421401858963
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: James with the block and then wade to James James to chalmers for the finish
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['james', 'with', 'the', 'block', 'and', 'then', 'wade', 'to', 'james', 'james', 'to', 'chalmers', 'for', 'the', 'finish']
cosine_similarity: 0.9590420126914978
test_input: [0.09042421401858963, 0.959042], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: This is like the 27th time chalmers fucked up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672
  0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: This is like the 27th time chalmers fucked up
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['this', 'is', 'like', 'the', 'time', 'chalmers', 'fucked', 'up']
cosine_similarity: 0.9723256826400757
test_input: [0.15064018498706508, 0.9723257], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: Chalmers would lock you down bubs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.6316672 ]
 [0.6316672  0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: Chalmers would lock you down bubs
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['chalmers', 'would', 'lock', 'you', 'down', 'bubs']
cosine_similarity: 0.9606724381446838
test_input: [0.20199309249791833, 0.96067244], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: Mario Chalmers is easily my worst favorite player in the NBA
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.
  0.         0.6316672  0.        ]
 [0.27894255 0.         0.39204401 0.39204401 0.39204401 0.39204401
  0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: Mario Chalmers is easily my worst favorite player in the NBA
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['mario', 'chalmers', 'is', 'easily', 'my', 'worst', 'favorite', 'player', 'in', 'the', 'nba']
cosine_similarity: 0.9614307284355164
test_input: [0.12536693798731732, 0.9614307], test_label: 0
TF_IDF_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: The fuck Chalmers is doing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.57735027 0.57735027]
 [0.57735027 0.57735027 0.57735027]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 1.0000000000000002
word_to_vector_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: The fuck Chalmers is doing
After tokenization, sentence1: ['the', 'fuck', 'chalmers', 'is', 'doing'], sentence2: ['the', 'fuck', 'chalmers', 'is', 'doing']
cosine_similarity: 1.0
test_input: [1.0000000000000002, 1.0], test_label: 1
TF_IDF_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: It s so hard to root for Chalmers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.         0.        ]
 [0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: It s so hard to root for Chalmers
After tokenization, sentence1: ['the', 'fuck', 'chalmers', 'is', 'doing'], sentence2: ['it', 's', 'so', 'hard', 'to', 'root', 'for', 'chalmers']
cosine_similarity: 0.9573018550872803
TF_IDF_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: Do I watch the game or Chalmers face
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: Do I watch the game or Chalmers face
After tokenization, sentence1: ['the', 'fuck', 'chalmers', 'is', 'doing'], sentence2: ['do', 'i', 'watch', 'the', 'game', 'or', 'chalmers', 'face']
cosine_similarity: 0.9621689915657043
test_input: [0.17077611319011649, 0.962169], test_label: 0
TF_IDF_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: Uhm Chalmers didnt just travel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.6316672  0.         0.
  0.        ]
 [0.33517574 0.47107781 0.         0.         0.47107781 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: Uhm Chalmers didnt just travel
After tokenization, sentence1: ['the', 'fuck', 'chalmers', 'is', 'doing'], sentence2: ['uhm', 'chalmers', 'didnt', 'just', 'travel']
cosine_similarity: 0.9516076445579529
test_input: [0.15064018498706508, 0.95160764], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chara get in that box, sentence2: Chara is dirtier than a Fresno adult film star
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.81480247 0.57973867 0.         0.         0.
  0.        ]
 [0.4261596  0.         0.30321606 0.4261596  0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: Chara get in that box, sentence2: Chara is dirtier than a Fresno adult film star
After tokenization, sentence1: ['chara', 'get', 'in', 'that', 'box'], sentence2: ['chara', 'is', 'dirtier', 'than', 'a', 'fresno', 'adult', 'film', 'star']
cosine_similarity: 0.9363540410995483
test_input: [0.17578607839334617, 0.93635404], test_label: 1
TF_IDF_cosine_similarity: sentence1: Chara is just a big goon, sentence2: wheres the call on chara
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.        ]
 [0.         0.57973867 0.         0.         0.81480247]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: Chara is just a big goon, sentence2: wheres the call on chara
After tokenization, sentence1: ['chara', 'is', 'just', 'a', 'big', 'goon'], sentence2: ['wheres', 'the', 'call', 'on', 'chara']
cosine_similarity: 0.953905463218689
TF_IDF_cosine_similarity: sentence1: Orr should beat the shit out of chara, sentence2: What s better than seeing Chara getting dropped by Orr
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.         0.40993715 0.         0.         0.40993715
  0.         0.57615236]
 [0.         0.44665616 0.31779954 0.44665616 0.44665616 0.31779954
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Orr should beat the shit out of chara, sentence2: What s better than seeing Chara getting dropped by Orr
After tokenization, sentence1: ['orr', 'should', 'beat', 'the', 'shit', 'out', 'of', 'chara'], sentence2: ['what', 's', 'better', 'than', 'seeing', 'chara', 'getting', 'dropped', 'by', 'orr']
cosine_similarity: 0.9870923161506653
test_input: [0.2605556710562624, 0.9870923], test_label: 1
TF_IDF_cosine_similarity: sentence1: watching chara go down is the best feeling, sentence2: Chara s playing with hate in heart
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.
  0.53404633]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: watching chara go down is the best feeling, sentence2: Chara s playing with hate in heart
After tokenization, sentence1: ['watching', 'chara', 'go', 'down', 'is', 'the', 'best', 'feeling'], sentence2: ['chara', 's', 'playing', 'with', 'hate', 'in', 'heart']
cosine_similarity: 0.9745883941650391
test_input: [0.1443835552773867, 0.9745884], test_label: 0
TF_IDF_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara is 7 feet tall basically a Titan
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.23076793 0.         0.32433627 0.32433627 0.32433627
  0.32433627 0.         0.         0.64867255 0.32433627]
 [0.47107781 0.33517574 0.47107781 0.         0.         0.
  0.         0.47107781 0.47107781 0.         0.        ]]
pairwise_similarity: [[1.         0.07734781]
 [0.07734781 1.        ]]
cosine_similarity: 0.07734781234369076
word_to_vector_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara is 7 feet tall basically a Titan
After tokenization, sentence1: ['orr', 'vs', 'chara', 'is', 'like', 'a', 'jack', 'russell', 'vs', 'a', 'wolfhound'], sentence2: ['chara', 'is', 'feet', 'tall', 'basically', 'a', 'titan']
cosine_similarity: 0.9124714136123657
TF_IDF_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara is the most overrated player in the league
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.32433627 0.         0.32433627 0.32433627 0.
  0.         0.32433627 0.64867255 0.32433627]
 [0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.53404633 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.08768682]
 [0.08768682 1.        ]]
cosine_similarity: 0.08768681980142445
word_to_vector_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara is the most overrated player in the league
After tokenization, sentence1: ['orr', 'vs', 'chara', 'is', 'like', 'a', 'jack', 'russell', 'vs', 'a', 'wolfhound'], sentence2: ['chara', 'is', 'the', 'most', 'overrated', 'player', 'in', 'the', 'league']
cosine_similarity: 0.9133561253547668
test_input: [0.08768681980142445, 0.9133561], test_label: 0
TF_IDF_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara move your feet along the boards
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.23076793 0.         0.32433627 0.32433627 0.32433627
  0.32433627 0.64867255 0.32433627]
 [0.6316672  0.44943642 0.6316672  0.         0.         0.
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.10371551]
 [0.10371551 1.        ]]
cosine_similarity: 0.10371551133313005
word_to_vector_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara move your feet along the boards
After tokenization, sentence1: ['orr', 'vs', 'chara', 'is', 'like', 'a', 'jack', 'russell', 'vs', 'a', 'wolfhound'], sentence2: ['chara', 'move', 'your', 'feet', 'along', 'the', 'boards']
cosine_similarity: 0.8382033705711365
test_input: [0.10371551133313005, 0.8382034], test_label: 0
TF_IDF_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: FINALLY CHARA IS GIVEN A PENALTY
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.         0.         0.32433627 0.32433627 0.32433627
  0.         0.32433627 0.64867255 0.32433627]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.08768682]
 [0.08768682 1.        ]]
cosine_similarity: 0.08768681980142445
word_to_vector_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: FINALLY CHARA IS GIVEN A PENALTY
After tokenization, sentence1: ['orr', 'vs', 'chara', 'is', 'like', 'a', 'jack', 'russell', 'vs', 'a', 'wolfhound'], sentence2: ['finally', 'chara', 'is', 'given', 'a', 'penalty']
cosine_similarity: 0.9263771176338196
test_input: [0.08768681980142445, 0.9263771], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Orr wants a piece of chara I swear
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.53404633
  0.         0.        ]
 [0.         0.33517574 0.         0.47107781 0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Orr wants a piece of chara I swear
After tokenization, sentence1: ['chara', 'is', 'holy', 'shit'], sentence2: ['orr', 'wants', 'a', 'piece', 'of', 'chara', 'i', 'swear']
cosine_similarity: 0.9525408148765564
test_input: [0.1273595297947935, 0.9525408], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Okay who else saw that beauty hit to chara by orr
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.         0.         0.53404633]
 [0.         0.4261596  0.30321606 0.4261596  0.         0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Okay who else saw that beauty hit to chara by orr
After tokenization, sentence1: ['chara', 'is', 'holy', 'shit'], sentence2: ['okay', 'who', 'else', 'saw', 'that', 'beauty', 'hit', 'to', 'chara', 'by', 'orr']
cosine_similarity: 0.9455086588859558
test_input: [0.11521554337793122, 0.94550866], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: When anyone on the leafs knock down Chara I laugh so hard
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.         0.
  0.         0.53404633]
 [0.         0.33517574 0.47107781 0.         0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: When anyone on the leafs knock down Chara I laugh so hard
After tokenization, sentence1: ['chara', 'is', 'holy', 'shit'], sentence2: ['when', 'anyone', 'on', 'the', 'leafs', 'knock', 'down', 'chara', 'i', 'laugh', 'so', 'hard']
cosine_similarity: 0.9463491439819336
test_input: [0.1273595297947935, 0.94634914], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Amazing how bad Chara looks when the speed of the game picks up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.         0.37997836 0.         0.53404633
  0.         0.         0.53404633 0.        ]
 [0.         0.39204401 0.39204401 0.27894255 0.39204401 0.
  0.39204401 0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Amazing how bad Chara looks when the speed of the game picks up
After tokenization, sentence1: ['chara', 'is', 'holy', 'shit'], sentence2: ['amazing', 'how', 'bad', 'chara', 'looks', 'when', 'the', 'speed', 'of', 'the', 'game', 'picks', 'up']
cosine_similarity: 0.9517599940299988
test_input: [0.1059921313509325, 0.95176], test_label: 0
TF_IDF_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: Why is chara playing like a bitch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.33517574 0.47107781 0.         0.47107781
  0.47107781 0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: Why is chara playing like a bitch
After tokenization, sentence1: ['colton', 'orr', 'lowers', 'the', 'boom', 'on', 'chara'], sentence2: ['why', 'is', 'chara', 'playing', 'like', 'a', 'bitch']
cosine_similarity: 0.9095085859298706
test_input: [0.1273595297947935, 0.9095086], test_label: 0
TF_IDF_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: BOOOOOMM down went chara again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.47107781 0.47107781 0.47107781
  0.        ]
 [0.         0.6316672  0.44943642 0.         0.         0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: BOOOOOMM down went chara again
After tokenization, sentence1: ['colton', 'orr', 'lowers', 'the', 'boom', 'on', 'chara'], sentence2: ['down', 'went', 'chara', 'again']
cosine_similarity: 0.9100167751312256
test_input: [0.15064018498706508, 0.9100168], test_label: 0
TF_IDF_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: Zideno Chara is a brick wall on skates
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.47107781 0.47107781 0.47107781
  0.         0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.         0.
  0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
word_to_vector_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: Zideno Chara is a brick wall on skates
After tokenization, sentence1: ['colton', 'orr', 'lowers', 'the', 'boom', 'on', 'chara'], sentence2: ['chara', 'is', 'a', 'brick', 'wall', 'on', 'skates']
cosine_similarity: 0.9233399033546448
test_input: [0.11234277891542777, 0.9233399], test_label: 0
TF_IDF_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: Chara penalty on the play
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]
 [0.         0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: Chara penalty on the play
After tokenization, sentence1: ['orr', 'with', 'a', 'big', 'hit', 'on', 'chara'], sentence2: ['chara', 'penalty', 'on', 'the', 'play']
cosine_similarity: 0.9615659117698669
test_input: [0.17077611319011649, 0.9615659], test_label: 0
TF_IDF_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: Chara called for a high stick on the play too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.         0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: Chara called for a high stick on the play too
After tokenization, sentence1: ['orr', 'with', 'a', 'big', 'hit', 'on', 'chara'], sentence2: ['chara', 'called', 'for', 'a', 'high', 'stick', 'on', 'the', 'play', 'too']
cosine_similarity: 0.9856194853782654
test_input: [0.1273595297947935, 0.9856195], test_label: 0
TF_IDF_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: I keep waiting for the chara vs orr fight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.57615236 0.40993715 0.
  0.        ]
 [0.         0.35520009 0.49922133 0.         0.35520009 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: I keep waiting for the chara vs orr fight
After tokenization, sentence1: ['orr', 'with', 'a', 'big', 'hit', 'on', 'chara'], sentence2: ['i', 'keep', 'waiting', 'for', 'the', 'chara', 'vs', 'orr', 'fight']
cosine_similarity: 0.9659247398376465
test_input: [0.29121941856368966, 0.96592474], test_label: 1
TF_IDF_cosine_similarity: sentence1: Chara is a disgrace to the NHL, sentence2: Chara is a crash dummy in this game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.6316672 ]
 [0.37997836 0.53404633 0.         0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Chara is a disgrace to the NHL, sentence2: Chara is a crash dummy in this game
After tokenization, sentence1: ['chara', 'is', 'a', 'disgrace', 'to', 'the', 'nhl'], sentence2: ['chara', 'is', 'a', 'crash', 'dummy', 'in', 'this', 'game']
cosine_similarity: 0.9684118628501892
TF_IDF_cosine_similarity: sentence1: Chara is a disgrace to the NHL, sentence2: Chara is just a big goon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.53404633 0.37997836 0.         0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Chara is a disgrace to the NHL, sentence2: Chara is just a big goon
After tokenization, sentence1: ['chara', 'is', 'a', 'disgrace', 'to', 'the', 'nhl'], sentence2: ['chara', 'is', 'just', 'a', 'big', 'goon']
cosine_similarity: 0.960723340511322
test_input: [0.17077611319011649, 0.96072334], test_label: 1
TF_IDF_cosine_similarity: sentence1: Chara is a disgrace to the NHL, sentence2: When chara gets checked and hits the ground I just get happy idk why
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.
  0.         0.         0.         0.6316672 ]
 [0.25969799 0.36499647 0.         0.36499647 0.36499647 0.36499647
  0.36499647 0.36499647 0.36499647 0.        ]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
word_to_vector_cosine_similarity: sentence1: Chara is a disgrace to the NHL, sentence2: When chara gets checked and hits the ground I just get happy idk why
After tokenization, sentence1: ['chara', 'is', 'a', 'disgrace', 'to', 'the', 'nhl'], sentence2: ['when', 'chara', 'gets', 'checked', 'and', 'hits', 'the', 'ground', 'i', 'just', 'get', 'happy', 'idk', 'why']
cosine_similarity: 0.936051607131958
TF_IDF_cosine_similarity: sentence1: but orr keeps pushing chara, sentence2: Imma blame Chara for Lupul missing that one
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.         0.
  0.53404633 0.53404633]
 [0.47107781 0.33517574 0.47107781 0.         0.47107781 0.47107781
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: but orr keeps pushing chara, sentence2: Imma blame Chara for Lupul missing that one
After tokenization, sentence1: ['but', 'orr', 'keeps', 'pushing', 'chara'], sentence2: ['imma', 'blame', 'chara', 'for', 'lupul', 'missing', 'that', 'one']
cosine_similarity: 0.9350329041481018
test_input: [0.1273595297947935, 0.9350329], test_label: 0
TF_IDF_cosine_similarity: sentence1: but orr keeps pushing chara, sentence2: Why is Chara allowed to take down people without the puck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.53404633 0.         0.
  0.53404633]
 [0.53404633 0.37997836 0.         0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: but orr keeps pushing chara, sentence2: Why is Chara allowed to take down people without the puck
After tokenization, sentence1: ['but', 'orr', 'keeps', 'pushing', 'chara'], sentence2: ['why', 'is', 'chara', 'allowed', 'to', 'take', 'down', 'people', 'without', 'the', 'puck']
cosine_similarity: 0.9288696646690369
test_input: [0.1443835552773867, 0.92886966], test_label: 0
TF_IDF_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: When do you plan to be at 600 W Chicago next
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.        ]
 [0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: When do you plan to be at 600 W Chicago next
After tokenization, sentence1: ['except', 'im', 'in', 'chicago', 'at', 'the', 'moment'], sentence2: ['when', 'do', 'you', 'plan', 'to', 'be', 'at', 'w', 'chicago', 'next']
cosine_similarity: 0.9618741869926453
test_input: [0.20199309249791833, 0.9618742], test_label: 0
TF_IDF_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: Big game 7 here in Chicago tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: Big game 7 here in Chicago tonight
After tokenization, sentence1: ['except', 'im', 'in', 'chicago', 'at', 'the', 'moment'], sentence2: ['big', 'game', 'here', 'in', 'chicago', 'tonight']
cosine_similarity: 0.9665164351463318
test_input: [0.17077611319011649, 0.96651644], test_label: 0
TF_IDF_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: I checked in at Chicago Park District
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: I checked in at Chicago Park District
After tokenization, sentence1: ['except', 'im', 'in', 'chicago', 'at', 'the', 'moment'], sentence2: ['i', 'checked', 'in', 'at', 'chicago', 'park', 'district']
cosine_similarity: 0.964242696762085
test_input: [0.17077611319011649, 0.9642427], test_label: 0
TF_IDF_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: Nice new Guideshop in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.47107781 0.
  0.         0.47107781]
 [0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
word_to_vector_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: Nice new Guideshop in Chicago
After tokenization, sentence1: ['anyone', 'wanting', 'to', 'go', 'to', 'the', 'jt', 'concert', 'in', 'chicago'], sentence2: ['nice', 'new', 'in', 'chicago']
cosine_similarity: 0.9602895379066467
test_input: [0.1273595297947935, 0.96028954], test_label: 0
TF_IDF_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: The North side of Chicago is happy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.47107781 0.
  0.47107781]
 [0.         0.44943642 0.         0.6316672  0.         0.6316672
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: The North side of Chicago is happy
After tokenization, sentence1: ['anyone', 'wanting', 'to', 'go', 'to', 'the', 'jt', 'concert', 'in', 'chicago'], sentence2: ['the', 'north', 'side', 'of', 'chicago', 'is', 'happy']
cosine_similarity: 0.9605454206466675
test_input: [0.15064018498706508, 0.9605454], test_label: 0
TF_IDF_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: just watched season finale of chicago fire and cried
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.         0.47107781
  0.         0.         0.47107781 0.        ]
 [0.         0.30321606 0.         0.4261596  0.4261596  0.
  0.4261596  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
word_to_vector_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: just watched season finale of chicago fire and cried
After tokenization, sentence1: ['anyone', 'wanting', 'to', 'go', 'to', 'the', 'jt', 'concert', 'in', 'chicago'], sentence2: ['just', 'watched', 'season', 'finale', 'of', 'chicago', 'fire', 'and', 'cried']
cosine_similarity: 0.9426076412200928
test_input: [0.10163066979112656, 0.94260764], test_label: 0
TF_IDF_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: I really wanna do the color run in Chicago lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.81480247 0.         0.         0.
  0.        ]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
word_to_vector_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: I really wanna do the color run in Chicago lol
After tokenization, sentence1: ['see', 'you', 'in', 'chicago', 'dierks'], sentence2: ['i', 'really', 'wanna', 'do', 'the', 'color', 'run', 'in', 'chicago', 'lol']
cosine_similarity: 0.9568437933921814
test_input: [0.17578607839334617, 0.9568438], test_label: 0
TF_IDF_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: The ungeekedeliteschicago Daily is out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.         0.70710678 0.        ]
 [0.         0.70710678 0.         0.70710678]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
word_to_vector_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: The ungeekedeliteschicago Daily is out
After tokenization, sentence1: ['see', 'you', 'in', 'chicago', 'dierks'], sentence2: ['the', 'daily', 'is', 'out']
cosine_similarity: 0.9502812623977661
test_input: [0.0, 0.95028126], test_label: 0
TF_IDF_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: I am watching Chicago Fire A Hell of a Ride
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.        ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: I am watching Chicago Fire A Hell of a Ride
After tokenization, sentence1: ['see', 'you', 'in', 'chicago', 'dierks'], sentence2: ['i', 'am', 'watching', 'chicago', 'fire', 'a', 'hell', 'of', 'a', 'ride']
cosine_similarity: 0.9527386426925659
test_input: [0.22028815056182965, 0.95273864], test_label: 0
TF_IDF_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: panoramic shot of noKXL and stop deportations rallies at Obama fundraiser in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.81480247 0.         0.         0.
  0.         0.         0.         0.        ]
 [0.24395573 0.34287126 0.         0.34287126 0.34287126 0.34287126
  0.34287126 0.34287126 0.34287126 0.34287126]]
pairwise_similarity: [[1.         0.14143057]
 [0.14143057 1.        ]]
cosine_similarity: 0.14143056792554487
word_to_vector_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: panoramic shot of noKXL and stop deportations rallies at Obama fundraiser in Chicago
After tokenization, sentence1: ['see', 'you', 'in', 'chicago', 'dierks'], sentence2: ['panoramic', 'shot', 'of', 'nokxl', 'and', 'stop', 'deportations', 'rallies', 'at', 'obama', 'fundraiser', 'in', 'chicago']
cosine_similarity: 0.9118006825447083
test_input: [0.14143056792554487, 0.9118007], test_label: 0
TF_IDF_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: blogher is in chicago this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781
  0.        ]
 [0.6316672  0.44943642 0.         0.         0.         0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: blogher is in chicago this year
After tokenization, sentence1: ['im', 'just', 'saying', 'game', 'in', 'chicago'], sentence2: ['blogher', 'is', 'in', 'chicago', 'this', 'year']
cosine_similarity: 0.9521575570106506
test_input: [0.15064018498706508, 0.95215756], test_label: 0
TF_IDF_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: Wings in a must win in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.47107781 0.47107781 0.
  0.        ]
 [0.44943642 0.         0.         0.         0.         0.6316672
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
word_to_vector_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: Wings in a must win in Chicago
After tokenization, sentence1: ['im', 'just', 'saying', 'game', 'in', 'chicago'], sentence2: ['wings', 'in', 'a', 'must', 'win', 'in', 'chicago']
cosine_similarity: 0.945781409740448
test_input: [0.15064018498706508, 0.9457814], test_label: 0
TF_IDF_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: Id definitely be happy to help but I wont be back in Chicago until Sunday
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.         0.         0.
  0.47107781 0.47107781 0.47107781 0.         0.        ]
 [0.27894255 0.39204401 0.         0.39204401 0.39204401 0.39204401
  0.         0.         0.         0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536716
word_to_vector_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: Id definitely be happy to help but I wont be back in Chicago until Sunday
After tokenization, sentence1: ['im', 'just', 'saying', 'game', 'in', 'chicago'], sentence2: ['id', 'definitely', 'be', 'happy', 'to', 'help', 'but', 'i', 'wont', 'be', 'back', 'in', 'chicago', 'until', 'sunday']
cosine_similarity: 0.9689367413520813
test_input: [0.09349477497536716, 0.96893674], test_label: 0
TF_IDF_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: By Micheline Maynard Contributor Chicago has
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.6316672 ]
 [0.37997836 0.53404633 0.         0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: By Micheline Maynard Contributor Chicago has
After tokenization, sentence1: ['game', 'in', 'chicago', 'tonight'], sentence2: ['by', 'micheline', 'maynard', 'contributor', 'chicago', 'has']
cosine_similarity: 0.8050628304481506
test_input: [0.17077611319011649, 0.80506283], test_label: 0
TF_IDF_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: bro you in chicago widdit yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.        ]
 [0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: bro you in chicago widdit yet
After tokenization, sentence1: ['game', 'in', 'chicago', 'tonight'], sentence2: ['bro', 'you', 'in', 'chicago', 'widdit', 'yet']
cosine_similarity: 0.92421954870224
test_input: [0.20199309249791833, 0.92421955], test_label: 0
TF_IDF_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: is L still looking for a copy of Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672 ]
 [0.44943642 0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: is L still looking for a copy of Chicago
After tokenization, sentence1: ['game', 'in', 'chicago', 'tonight'], sentence2: ['is', 'l', 'still', 'looking', 'for', 'a', 'copy', 'of', 'chicago']
cosine_similarity: 0.9348651766777039
test_input: [0.20199309249791833, 0.9348652], test_label: 0
TF_IDF_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: hope you and Hunter made it to Chicago this time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.
  0.53404633]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
word_to_vector_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: hope you and Hunter made it to Chicago this time
After tokenization, sentence1: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl'], sentence2: ['hope', 'you', 'and', 'hunter', 'made', 'it', 'to', 'chicago', 'this', 'time']
cosine_similarity: 0.980858325958252
test_input: [0.1443835552773867, 0.9808583], test_label: 0
TF_IDF_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: Flying to chicago on the 14th
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]
 [0.6316672  0.44943642 0.6316672  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: Flying to chicago on the 14th
After tokenization, sentence1: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl'], sentence2: ['flying', 'to', 'chicago', 'on', 'the']
cosine_similarity: 0.9652056694030762
test_input: [0.17077611319011649, 0.96520567], test_label: 0
TF_IDF_cosine_similarity: sentence1: Good week to be a Northsiderrr in Chicago, sentence2: go to north coast in chicago its way cheaper and the same weekend
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.         0.53404633 0.        ]
 [0.4261596  0.30321606 0.4261596  0.         0.4261596  0.
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Good week to be a Northsiderrr in Chicago, sentence2: go to north coast in chicago its way cheaper and the same weekend
After tokenization, sentence1: ['good', 'week', 'to', 'be', 'a', 'in', 'chicago'], sentence2: ['go', 'to', 'north', 'coast', 'in', 'chicago', 'its', 'way', 'cheaper', 'and', 'the', 'same', 'weekend']
cosine_similarity: 0.9771391749382019
test_input: [0.11521554337793122, 0.9771392], test_label: 0
TF_IDF_cosine_similarity: sentence1: Good week to be a Northsiderrr in Chicago, sentence2: Leaving early tomorrow morning to go to the Hospital in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.         0.
  0.53404633 0.         0.53404633]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.4261596
  0.         0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: Good week to be a Northsiderrr in Chicago, sentence2: Leaving early tomorrow morning to go to the Hospital in Chicago
After tokenization, sentence1: ['good', 'week', 'to', 'be', 'a', 'in', 'chicago'], sentence2: ['leaving', 'early', 'tomorrow', 'morning', 'to', 'go', 'to', 'the', 'hospital', 'in', 'chicago']
cosine_similarity: 0.9800469279289246
test_input: [0.11521554337793122, 0.9800469], test_label: 0
TF_IDF_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: I live in South Ontario but have great friends in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.6316672
  0.         0.        ]
 [0.         0.30321606 0.4261596  0.4261596  0.4261596  0.
  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: I live in South Ontario but have great friends in Chicago
After tokenization, sentence1: ['ahh', 'obama', 's', 'in', 'chicago'], sentence2: ['i', 'live', 'in', 'south', 'ontario', 'but', 'have', 'great', 'friends', 'in', 'chicago']
cosine_similarity: 0.9408639073371887
test_input: [0.1362763414390864, 0.9408639], test_label: 0
TF_IDF_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: did u really quit your job to go to edc chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.6316672
  0.         0.        ]
 [0.         0.30321606 0.4261596  0.4261596  0.4261596  0.
  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
word_to_vector_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: did u really quit your job to go to edc chicago
After tokenization, sentence1: ['ahh', 'obama', 's', 'in', 'chicago'], sentence2: ['did', 'u', 'really', 'quit', 'your', 'job', 'to', 'go', 'to', 'edc', 'chicago']
cosine_similarity: 0.9221509695053101
test_input: [0.1362763414390864, 0.92215097], test_label: 0
TF_IDF_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: youre in Chicago and were here to say nokxl
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.6316672  0.         0.        ]
 [0.         0.37997836 0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
word_to_vector_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: youre in Chicago and were here to say nokxl
After tokenization, sentence1: ['ahh', 'obama', 's', 'in', 'chicago'], sentence2: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl']
cosine_similarity: 0.9216969609260559
test_input: [0.17077611319011649, 0.92169696], test_label: 0
TF_IDF_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: Night in Chicago with my ladies
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672 ]
 [0.         0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
word_to_vector_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: Night in Chicago with my ladies
After tokenization, sentence1: ['ahh', 'obama', 's', 'in', 'chicago'], sentence2: ['night', 'in', 'chicago', 'with', 'my', 'ladies']
cosine_similarity: 0.9075974225997925
test_input: [0.20199309249791833, 0.9075974], test_label: 0
TF_IDF_cosine_similarity: sentence1: I cannot WAIT to go to Chicago, sentence2: Chicago is saying this todayEven nonhockey fans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.81480247]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
word_to_vector_cosine_similarity: sentence1: I cannot WAIT to go to Chicago, sentence2: Chicago is saying this todayEven nonhockey fans
After tokenization, sentence1: ['i', 'cannot', 'wait', 'to', 'go', 'to', 'chicago'], sentence2: ['chicago', 'is', 'saying', 'this', 'fans']
cosine_similarity: 0.9341691136360168
test_input: [0.19431434016858146, 0.9341691], test_label: 0
TF_IDF_cosine_similarity: sentence1: I cannot WAIT to go to Chicago, sentence2: It was a day of impulse buys in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.         0.81480247]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
word_to_vector_cosine_similarity: sentence1: I cannot WAIT to go to Chicago, sentence2: It was a day of impulse buys in Chicago
After tokenization, sentence1: ['i', 'cannot', 'wait', 'to', 'go', 'to', 'chicago'], sentence2: ['it', 'was', 'a', 'day', 'of', 'impulse', 'buys', 'in', 'chicago']
cosine_similarity: 0.9383672475814819
test_input: [0.22028815056182965, 0.93836725], test_label: 0
TF_IDF_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: Just bought a ticket to Chicago for 2350 round trip
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37997836 0.         0.53404633 0.
  0.53404633 0.         0.         0.53404633]
 [0.39204401 0.39204401 0.27894255 0.39204401 0.         0.39204401
  0.         0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
word_to_vector_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: Just bought a ticket to Chicago for 2350 round trip
After tokenization, sentence1: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl'], sentence2: ['just', 'bought', 'a', 'ticket', 'to', 'chicago', 'for', 'round', 'trip']
cosine_similarity: 0.9569721221923828
test_input: [0.1059921313509325, 0.9569721], test_label: 0
TF_IDF_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: If Chicago does I can officially turn off the TV for the season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.         0.         0.53404633]
 [0.30321606 0.4261596  0.         0.4261596  0.         0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
word_to_vector_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: If Chicago does I can officially turn off the TV for the season
After tokenization, sentence1: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl'], sentence2: ['if', 'chicago', 'does', 'i', 'can', 'officially', 'turn', 'off', 'the', 'tv', 'for', 'the', 'season']
cosine_similarity: 0.9840550422668457
test_input: [0.11521554337793122, 0.98405504], test_label: 0
TF_IDF_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: Would you want to watch Chris Davis in the home run derby
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.         0.         0.
  0.         0.         0.49922133 0.49922133]
 [0.         0.29017021 0.29017021 0.4078241  0.4078241  0.4078241
  0.4078241  0.4078241  0.         0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: Would you want to watch Chris Davis in the home run derby
After tokenization, sentence1: ['yes', 'yo', 'chris', 'davis', 'is', 'bats'], sentence2: ['would', 'you', 'want', 'to', 'watch', 'chris', 'davis', 'in', 'the', 'home', 'run', 'derby']
cosine_similarity: 0.9361323714256287
TF_IDF_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: And Chris Davis homer s again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.         0.49922133 0.49922133]
 [0.         0.50154891 0.50154891 0.70490949 0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
word_to_vector_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: And Chris Davis homer s again
After tokenization, sentence1: ['yes', 'yo', 'chris', 'davis', 'is', 'bats'], sentence2: ['and', 'chris', 'davis', 'homer', 's', 'again']
cosine_similarity: 0.9646919965744019
TF_IDF_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: Is that Chris Davis out there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.49922133 0.49922133]
 [0.         0.70710678 0.70710678 0.         0.        ]]
pairwise_similarity: [[1.         0.50232878]
 [0.50232878 1.        ]]
cosine_similarity: 0.5023287782256717
word_to_vector_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: Is that Chris Davis out there
After tokenization, sentence1: ['yes', 'yo', 'chris', 'davis', 'is', 'bats'], sentence2: ['is', 'that', 'chris', 'davis', 'out', 'there']
cosine_similarity: 0.960939884185791
test_input: [0.5023287782256717, 0.9609399], test_label: 0
TF_IDF_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: people actually keep pitching to Chris Davis
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.35520009 0.35520009 0.         0.
  0.49922133 0.49922133]
 [0.49922133 0.         0.35520009 0.35520009 0.49922133 0.49922133
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: people actually keep pitching to Chris Davis
After tokenization, sentence1: ['yes', 'yo', 'chris', 'davis', 'is', 'bats'], sentence2: ['people', 'actually', 'keep', 'pitching', 'to', 'chris', 'davis']
cosine_similarity: 0.9247652292251587
TF_IDF_cosine_similarity: sentence1: Go Os Chris Davis is a hunk, sentence2: Chris Davis launches a tworun homer to add a few insurance runs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.
  0.         0.57615236 0.         0.        ]
 [0.37762778 0.26868528 0.26868528 0.37762778 0.         0.37762778
  0.37762778 0.         0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Go Os Chris Davis is a hunk, sentence2: Chris Davis launches a tworun homer to add a few insurance runs
After tokenization, sentence1: ['go', 'os', 'chris', 'davis', 'is', 'a', 'hunk'], sentence2: ['chris', 'davis', 'launches', 'a', 'homer', 'to', 'add', 'a', 'few', 'insurance', 'runs']
cosine_similarity: 0.9422498941421509
TF_IDF_cosine_similarity: sentence1: Go Os Chris Davis is a hunk, sentence2: Chris Davis is on pace for 58 HR and 153 RBI this season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.40993715 0.40993715 0.         0.57615236
  0.57615236 0.         0.         0.        ]
 [0.37762778 0.37762778 0.26868528 0.26868528 0.37762778 0.
  0.         0.37762778 0.37762778 0.37762778]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.2202881505618297
word_to_vector_cosine_similarity: sentence1: Go Os Chris Davis is a hunk, sentence2: Chris Davis is on pace for 58 HR and 153 RBI this season
After tokenization, sentence1: ['go', 'os', 'chris', 'davis', 'is', 'a', 'hunk'], sentence2: ['chris', 'davis', 'is', 'on', 'pace', 'for', 'hr', 'and', 'rbi', 'this', 'season']
cosine_similarity: 0.9154907464981079
TF_IDF_cosine_similarity: sentence1: Wow Chris Davis is only 27, sentence2: Chris Davis actually reminds me of Hamilton at the plate
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.         0.40993715 0.40993715 0.         0.
  0.         0.57615236]
 [0.         0.44665616 0.31779954 0.31779954 0.44665616 0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Wow Chris Davis is only 27, sentence2: Chris Davis actually reminds me of Hamilton at the plate
After tokenization, sentence1: ['wow', 'chris', 'davis', 'is', 'only'], sentence2: ['chris', 'davis', 'actually', 'reminds', 'me', 'of', 'hamilton', 'at', 'the', 'plate']
cosine_similarity: 0.966641902923584
test_input: [0.2605556710562624, 0.9666419], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: So um Chris Davis you guys
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.         0.         0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: So um Chris Davis you guys
After tokenization, sentence1: ['chris', 'davis', 'rules', 'the', 'yard', 'tonight'], sentence2: ['so', 'um', 'chris', 'davis', 'you', 'guys']
cosine_similarity: 0.8951800465583801
test_input: [0.29121941856368966, 0.89518005], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: So happy Chris Davis is on my fantasy team
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.49922133]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: So happy Chris Davis is on my fantasy team
After tokenization, sentence1: ['chris', 'davis', 'rules', 'the', 'yard', 'tonight'], sentence2: ['so', 'happy', 'chris', 'davis', 'is', 'on', 'my', 'fantasy', 'team']
cosine_similarity: 0.9518905282020569
test_input: [0.2523342014336961, 0.9518905], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: No other words but shut the front door Chris Davis is my not so secret crush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.49922133 0.
  0.         0.49922133 0.         0.49922133]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.         0.4078241
  0.4078241  0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
word_to_vector_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: No other words but shut the front door Chris Davis is my not so secret crush
After tokenization, sentence1: ['chris', 'davis', 'rules', 'the', 'yard', 'tonight'], sentence2: ['no', 'other', 'words', 'but', 'shut', 'the', 'front', 'door', 'chris', 'davis', 'is', 'my', 'not', 'so', 'secret', 'crush']
cosine_similarity: 0.9239528775215149
test_input: [0.20613696606828605, 0.9239529], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: Is Chris Davis a top 5 hitter in baseball right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.
  0.57615236]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.         0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
word_to_vector_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: Is Chris Davis a top 5 hitter in baseball right now
After tokenization, sentence1: ['chris', 'davis', 'is', 'way', 'to', 'nice'], sentence2: ['is', 'chris', 'davis', 'a', 'top', 'hitter', 'in', 'baseball', 'right', 'now']
cosine_similarity: 0.9775925874710083
test_input: [0.29121941856368966, 0.9775926], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: I cant lie Chris Davis is a babe
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.57615236]
 [0.57615236 0.40993715 0.40993715 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
word_to_vector_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: I cant lie Chris Davis is a babe
After tokenization, sentence1: ['chris', 'davis', 'is', 'way', 'to', 'nice'], sentence2: ['i', 'cant', 'lie', 'chris', 'davis', 'is', 'a', 'babe']
cosine_similarity: 0.9774053692817688
TF_IDF_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: WHAT THE HELL DOES CHRIS DAVIS EAT FOR BREAKFAST
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.
  0.57615236 0.57615236]
 [0.44665616 0.31779954 0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
word_to_vector_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: WHAT THE HELL DOES CHRIS DAVIS EAT FOR BREAKFAST
After tokenization, sentence1: ['chris', 'davis', 'is', 'way', 'to', 'nice'], sentence2: ['what', 'the', 'hell', 'does', 'chris', 'davis', 'eat', 'for', 'breakfast']
cosine_similarity: 0.9768643379211426
test_input: [0.2605556710562624, 0.97686434], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: Ayo smh Chris Davis TEACH ME
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.35520009 0.35520009 0.49922133 0.
  0.         0.49922133]
 [0.49922133 0.         0.35520009 0.35520009 0.         0.49922133
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: Ayo smh Chris Davis TEACH ME
After tokenization, sentence1: ['chris', 'davis', 'is', 'on', 'the', 'roids', 'big', 'time'], sentence2: ['ayo', 'smh', 'chris', 'davis', 'teach', 'me']
cosine_similarity: 0.9262897372245789
test_input: [0.2523342014336961, 0.92628974], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: When is the Chris Davis ped suspension coming
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.35520009 0.         0.49922133
  0.         0.49922133]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: When is the Chris Davis ped suspension coming
After tokenization, sentence1: ['chris', 'davis', 'is', 'on', 'the', 'roids', 'big', 'time'], sentence2: ['when', 'is', 'the', 'chris', 'davis', 'ped', 'suspension', 'coming']
cosine_similarity: 0.9863466024398804
test_input: [0.2523342014336961, 0.9863466], test_label: 0
TF_IDF_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: Chris Davis is on pace for about 56 HRs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.35520009 0.35520009 0.         0.
  0.49922133 0.49922133]
 [0.49922133 0.         0.35520009 0.35520009 0.49922133 0.49922133
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: Chris Davis is on pace for about 56 HRs
After tokenization, sentence1: ['chris', 'davis', 'is', 'on', 'the', 'roids', 'big', 'time'], sentence2: ['chris', 'davis', 'is', 'on', 'pace', 'for', 'about', 'hrs']
cosine_similarity: 0.982689380645752
test_input: [0.2523342014336961, 0.9826894], test_label: 0
test_sentence_pair_index_dict: {0: ['All the home alones watching 8 mile', 'The last rap battle in 8 Mile nevr gets old ahah'], 1: ['All the home alones watching 8 mile', 'The rap battle at the end of 8 mile gets me so hype'], 2: ['The Ending to 8 Mile is my fav part of the whole movie', 'Rabbit on 8 mile out of place but determined to make it'], 3: ['The Ending to 8 Mile is my fav part of the whole movie', 'See 8 Mile is always on but it s the tv version so it s gay'], 4: ['The Ending to 8 Mile is my fav part of the whole movie', 'Those last 3 battles in 8 Mile are THE shit'], 5: ['0 and 8 mile on at the same time', 'It s just rap lyrics from the movie 8 mile'], 6: ['0 and 8 mile on at the same time', 'I will never get tired of 8 Mile'], 7: ['Well at least 8 Mile is on', 'In 8 Mile in a scene the background music is Sweet Home Alabama'], 8: ['Well at least 8 Mile is on', '8 mile is on havent seen this movie in the longest'], 9: ['The last rap battle in 8 mile though', 'But why were people watching the heat play when 8 mile is on'], 10: ['The last rap battle in 8 mile though', 'I think everyone is watching 8 mile Rn'], 11: ['8 mile is just a classic', '8 mile that movie I love eminem in this movie'], 12: ['Well at least 8 Mile is on', 'Listened to the final rap battle from 8 mile and now Im watching the whole movie'], 13: ['Well at least 8 Mile is on', '8 mile has been that movie'], 14: ['Well at least 8 Mile is on', 'I really did miss my part on 8 Mile'], 15: ['Well at least 8 Mile is on', 'Lose Yourself is the perfect song to end 8 mile on'], 16: ['8 mile is on friday made', 'I missed the best part of 8 mile'], 17: ['8 mile is on friday made', '8 mile is such an awesome movie'], 18: ['Oh shit niggy 8 mile is on', 'Eminem s rap in the final battle of 8 Mile gets me pumped every time'], 19: ['Oh shit niggy 8 mile is on', 'While yall argue about the game 8 mile is on MTV'], 20: ['Ok good the end of 8 Mile is on', 'The end of 8 Mile makes me so happy'], 21: ['Ok good the end of 8 Mile is on', 'The last 3 rap battles in 8 mile always get me hyped af'], 22: ['Ok good the end of 8 Mile is on', 'I always get the movies 8 mile and green mile mixed up'], 23: ['Let s go see after earth', 'Did After Earth already come out'], 24: ['Let s go see after earth', 'idk I think we might go see after earth later if we do you wana go'], 25: ['Anyone trying to see After Earth sometime soon', 'Me and my son went to see After Earth last night'], 26: ['who wants to see after earth with me todayyyy', 'I could of told Will Smith that After Earth was going 2 crash'], 27: ['who wants to see after earth with me todayyyy', 'I knew After Earth would tank'], 28: ['who wants to see after earth with me todayyyy', 'After Earth finishes with 27 million'], 29: ['who wants to see after earth with me todayyyy', 'even if After Earth is a good movie'], 30: ['I see the After Earth reviews are not positive', 'Ether to see after earth or fast 6'], 31: ['I see the After Earth reviews are not positive', 'I need to watch after earth tho'], 32: ['i need to watch after earth asap', 'At the theater with the little about do watch After Earth'], 33: ['i need to watch after earth asap', 'Someone needs to come see after earth with me'], 34: ['i need to watch after earth asap', 'After Earth is a great ass movie'], 35: ['Heading out to see After Earth in a bit', 'Goin to see after earth with the fam'], 36: ['Heading out to see After Earth in a bit', 'will smith s speech in after earth is so relevant'], 37: ['Heading out to see After Earth in a bit', 'Just got done eating chinese with the fam now ganna go see after earth'], 38: ['Going to see after earth but', 'After earth is out and I havent seen it yet'], 39: ['Who wants to take me to see After Earth', 'the hangover 3 and after earth are both really good'], 40: ['I wanna see the movie after earth', 'I kinda wanna see After Earth as well'], 41: ['I wanna see the movie after earth', 'NOW YOU SEE ME and AFTER EARTH Cant Outpace FAST FURIOUS 6'], 42: ['I wanna see the movie after earth', 'After Earth 039 trumped by 039 Now You See Me 039 as 039 Fast'], 43: ['the US and belgium are tied at half', 'Come on Romelu get some goals for Belgium'], 44: ['the US and belgium are tied at half', 'Mirallas with a soft and cool finish off the rebound to put Belgium up 10'], 45: ['Dude Belgium is freakin staked', 'US vs Belgium or the wings'], 46: ['Dude Belgium is freakin staked', 'Belgium almost take the lead in the 27th min'], 47: ['Belgium s gonna rape by the USA', 'What kind of formation Belgium playing there'], 48: ['Belgium s gonna rape by the USA', 'At half US 1 Belgium 1 Indians 5'], 49: ['Belgium s gonna rape by the USA', 'Who s in goal for Belgium for the USMNT friendly'], 50: ['Belgium vs USA you watching', 'Vernaelen always gets injured for Belgium'], 51: ['Belgium vs USA you watching', '6th minute Belgium with the score'], 52: ['Belgium vs USA you watching', 'Belgium in a friendly instead'], 53: ['Watching the USMNT vs the talented Belgium team', 'The Belgium GK wasnt trying to concede another goal'], 54: ['Watching the USMNT vs the talented Belgium team', 'that being said US 21 Belgium'], 55: ['yeah Belgium is definitely good', 'what s up with the nonHD main camera at the USBelgium soccer game'], 56: ['yeah Belgium is definitely good', 'Belgium vs USA you watching'], 57: ['Belgium has a great team', 'a little late to the USABelgium game'], 58: ['I love everyone on the Belgium squad', 'Belgium is playing 4 centerbacks and fellani and they concede on a set piece'], 59: ['I love everyone on the Belgium squad', 'In other news this Belgium squad taking on USMNT is STACKED'], 60: ['Rafa Benitez is still a massive prick', 'Mourinho to city Benitez to stay at Chelsea'], 61: ['Rafa Benitez is still a massive prick', 'Chelsea FC wouldnt get rid of Benitez now'], 62: ['Rafa Benitez he s too good for you', 'I hope Chelsea fans are thoroughly embarrassed now with the way they treated Benitez'], 63: ['Rafa Benitez he s too good for you', 'Thank you for your tactics Benitez'], 64: ['Rafa Benitez he s too good for you', 'Got alot of time for rafa Benitez'], 65: ['Benitez is a sick manager', 'Well done to Rafa Benitez a dignified man'], 66: ['congratulations to petr ech and rafa benitez', 'Rafa Benitez a free agent'], 67: ['congratulations to petr ech and rafa benitez', 'Credit where credits due to Rafa Benitez'], 68: ['God forbid lyknx Rafa Benitez', 'How can chelsea fans still hate benitez'], 69: ['Thank you very much Rafa Benitez', 'Why do liverpool fans love benitez so much'], 70: ['Rafa Benitez I must thank you', 'I am so happy for RAFA BENITEZ VictorMoses and Mikel'], 71: ['Rafa Benitez I must thank you', 'Pleased for Benitez hasnt deserved the stick he s got'], 72: ['Rafa Benitez I must thank you', 'Once a red always a blue rafa Benitez we want you'], 73: ['THANK YOU SO MUCH RAFA BENITEZ', 'Benitez is alright tho man fuck chelsea fans they suck asshole'], 74: ['THANK YOU SO MUCH RAFA BENITEZ', 'Credit where credits due to Rafa Benitez'], 75: ['Rafa Benitez deserves a hell of a thank you', 'Any praise for Benitez from my Chelsea followers lol'], 76: ['I dont understand the hatred for Rafa Benitez', 'Top 4 and a trophy and still they dont give any respect for Benitez'], 77: ['Big 12SEC announce basketball challenge', 'Bill Self to the Big 12'], 78: ['Big 12SEC announce basketball challenge', 'The Big 12 just got a whole lot more interesting'], 79: ['Big 12SEC announce basketball challenge', 'Just when you thought Bill Self wasnt going to own the Big 12 for another year'], 80: ['Big 12SEC announce basketball challenge', 'Oklahoma in Houston also among the 10 SECBig 12 matchups'], 81: ['Kansas just won the Big 12 championship again', 'Did kU win the Big 12 Quidditch Championship'], 82: ['Kansas just won the Big 12 championship again', 'He can fuck up the Big 12 all he wants'], 83: ['Kansas just won the Big 12 championship again', 'So what if the Big 12 had 14 teams'], 84: ['Sorry to the rest of the big 12', 'And Kansas once again will win the Big 12'], 85: ['The big 12 is about to be so stacked next year', 'How many of those who were handing the Big 12 to Okla'], 86: ['will prolly win the Big 12', 'UKBig 12 challenge officially announced today'], 87: ['will prolly win the Big 12', '21 in Houston as part of Big 12SEC Challenge'], 88: ['will prolly win the Big 12', 'No reason Kansas should lose a game in the big 12'], 89: ['will prolly win the Big 12', 'The whole Big 12 but Okiestate in particular just lost their minds'], 90: ['Sorry to the rest of the big 12', 'Texas Tech will play at Alabama in the SECBig 12 Basketball Challenge on Nov'], 91: ['UK part of the Big 12SEC Challenge', 'There is NOOOO competition in Big 12 basketball'], 92: ['well the big 12 just got decided', 'to win the big 12 in all three major sports in the same year'], 93: ['well the big 12 just got decided', 'SECBig 12 Challenge in hoops has been announced to begin in 201314'], 94: ['well the big 12 just got decided', 'So Wiggins Is Settling For Playing In The Garbage Ass Big 12'], 95: ['Just wish he wasnt in the Big 12', 'Big 12 is gonna be exciting'], 96: ['Just wish he wasnt in the Big 12', 'the BIG 12 goes through LAWRENCE'], 97: ['hannahbush were in Reigate', 'I just found out Marilyn Monroe has a full bush'], 98: ['hannahbush were in Reigate', 'Classic redhead with a natural bush'], 99: ['hannahbush were in Reigate', 'Hahah I heard your dumbass woke up in a bush'], 100: ['I dont have time for beating around the bush', 'Honey has a brush with her non existent bush'], 101: ['Ctfu the man in the bush', 'How the hell could Obama kill more than Bush did in Iraq'], 102: ['Ctfu the man in the bush', 'george bush is never a truther'], 103: ['Ctfu the man in the bush', 'Im a fan of Clintons Pretty much despise Bush'], 104: ['He is worse than Bush', 'It was under Bush it is now'], 105: ['He is worse than Bush', 'Did Obama and the Dems trust the Bush Government'], 106: ['He is worse than Bush', 'that the Libs are going to say its Bush s fault and Im a racist'], 107: ['Then time to trim the rose bush', 'I dint like it under Bush either Obama has radically expanded it'], 108: ['I want the Bush days back', 'Dont beat around the bush just say it'], 109: ['I want the Bush days back', 'fell in a spiky bush and I have a prickly thing in my finger'], 110: ['I want the Bush days back', 'i once walked into a bush outside school and literally apologised to it'], 111: ['I want the Bush days back', 'Bush wiretapped without warrants Obama had them'], 112: ['Footbridge over the River Bush', 'started under bush and Im sure you were cool with it then'], 113: ['Footbridge over the River Bush', 'The new Bush tour merchandise is now available'], 114: ['Footbridge over the River Bush', 'we live near a bush reserve'], 115: ['I did for Bush as well', 'in my actual bush in a bush in Bushey'], 116: ['I did for Bush as well', 'it started way before Bush Jr'], 117: ['Darling stop beating around the bush', 'They called Bush hitler too'], 118: ['FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR', 'Why cant everyone just tell it how it is instead of beating around the bush'], 119: ['FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR', 'And his legs can have hair but no bush'], 120: ['Yeah CHALMERS that s a foul', 'Allen Bosh Chalmers James at Wade'], 121: ['Yeah CHALMERS that s a foul', 'Chalmers makes me mad low key'], 122: ['Yeah CHALMERS that s a foul', 'Chalmers steady chasing on defense'], 123: ['Yeah CHALMERS that s a foul', 'I like Cole s onball defense better than Chalmers'], 124: ['And now chalmers with the scoop layup', 'Anybody see David West Elbow the hell out of Chalmers bad shoulder'], 125: ['Mario Chalmers pissin me off', 'MARIO CHALMERS JUST THREW IT TO HIS COACH HAHAHA'], 126: ['Mario Chalmers pissin me off', 'Spo aint in the game chalmers'], 127: ['Mario Chalmers pissin me off', 'Mario Chalmers needs to get punched in the face'], 128: ['Mario Chalmers pissin me off', 'Chalmers throws it out of bounds'], 129: ['Chalmers just walked with the ball', 'Uhm Chalmers didnt just travel'], 130: ['Mario Chalmers looks like Mario', 'Then Chalmers fucks up again lmao'], 131: ['Mario Chalmers looks like Mario', 'Lucky ass shxt by Chalmers'], 132: ['I hate Mario Chalmersdont know why', 'idc idc chalmers be making me mad'], 133: ['I hate Mario Chalmersdont know why', 'Why is Mario Chalmers starting'], 134: ['CHALMERS IS THE TURNOVER KING', 'Didnt everyone love Mario Chalmers last year'], 135: ['Chalmers is always complaining to the refs', 'LMFAOOO who tf you throwing to Chalmers'], 136: ['Chalmers is always complaining to the refs', 'James with the block and then wade to James James to chalmers for the finish'], 137: ['Chalmers is always complaining to the refs', 'This is like the 27th time chalmers fucked up'], 138: ['Chalmers is always complaining to the refs', 'Chalmers would lock you down bubs'], 139: ['Chalmers is always complaining to the refs', 'Mario Chalmers is easily my worst favorite player in the NBA'], 140: ['The fuck Chalmers is doing', 'The fuck Chalmers is doing'], 141: ['The fuck Chalmers is doing', 'Do I watch the game or Chalmers face'], 142: ['The fuck Chalmers is doing', 'Uhm Chalmers didnt just travel'], 143: ['Chara get in that box', 'Chara is dirtier than a Fresno adult film star'], 144: ['Orr should beat the shit out of chara', 'What s better than seeing Chara getting dropped by Orr'], 145: ['watching chara go down is the best feeling', 'Chara s playing with hate in heart'], 146: ['Orr vs Chara is like a Jack Russell vs a Wolfhound', 'Chara is the most overrated player in the league'], 147: ['Orr vs Chara is like a Jack Russell vs a Wolfhound', 'Chara move your feet along the boards'], 148: ['Orr vs Chara is like a Jack Russell vs a Wolfhound', 'FINALLY CHARA IS GIVEN A PENALTY'], 149: ['Chara is 69 Holy shit', 'Orr wants a piece of chara I swear'], 150: ['Chara is 69 Holy shit', 'Okay who else saw that beauty hit to chara by orr'], 151: ['Chara is 69 Holy shit', 'When anyone on the leafs knock down Chara I laugh so hard'], 152: ['Chara is 69 Holy shit', 'Amazing how bad Chara looks when the speed of the game picks up'], 153: ['Colton Orr lowers the boom on Chara', 'Why is chara playing like a bitch'], 154: ['Colton Orr lowers the boom on Chara', 'BOOOOOMM down went chara again'], 155: ['Colton Orr lowers the boom on Chara', 'Zideno Chara is a brick wall on skates'], 156: ['Orr with a big hit on Chara', 'Chara penalty on the play'], 157: ['Orr with a big hit on Chara', 'Chara called for a high stick on the play too'], 158: ['Orr with a big hit on Chara', 'I keep waiting for the chara vs orr fight'], 159: ['Chara is a disgrace to the NHL', 'Chara is just a big goon'], 160: ['but orr keeps pushing chara', 'Imma blame Chara for Lupul missing that one'], 161: ['but orr keeps pushing chara', 'Why is Chara allowed to take down people without the puck'], 162: ['except Im in Chicago at the moment', 'When do you plan to be at 600 W Chicago next'], 163: ['except Im in Chicago at the moment', 'Big game 7 here in Chicago tonight'], 164: ['except Im in Chicago at the moment', 'I checked in at Chicago Park District'], 165: ['Anyone wanting to go to the JT concert 722 in Chicago', 'Nice new Guideshop in Chicago'], 166: ['Anyone wanting to go to the JT concert 722 in Chicago', 'The North side of Chicago is happy'], 167: ['Anyone wanting to go to the JT concert 722 in Chicago', 'just watched season finale of chicago fire and cried'], 168: ['See you in Chicago Dierks', 'I really wanna do the color run in Chicago lol'], 169: ['See you in Chicago Dierks', 'The ungeekedeliteschicago Daily is out'], 170: ['See you in Chicago Dierks', 'I am watching Chicago Fire A Hell of a Ride'], 171: ['See you in Chicago Dierks', 'panoramic shot of noKXL and stop deportations rallies at Obama fundraiser in Chicago'], 172: ['Im just saying game 7 in Chicago', 'blogher is in chicago this year'], 173: ['Im just saying game 7 in Chicago', 'Wings in a must win in Chicago'], 174: ['Im just saying game 7 in Chicago', 'Id definitely be happy to help but I wont be back in Chicago until Sunday'], 175: ['Game 7 in Chicago tonight', 'By Micheline Maynard Contributor Chicago has'], 176: ['Game 7 in Chicago tonight', 'bro you in chicago widdit yet'], 177: ['Game 7 in Chicago tonight', 'is L still looking for a copy of Chicago'], 178: ['youre in Chicago and were here to say nokxl', 'hope you and Hunter made it to Chicago this time'], 179: ['youre in Chicago and were here to say nokxl', 'Flying to chicago on the 14th'], 180: ['Good week to be a Northsiderrr in Chicago', 'go to north coast in chicago its way cheaper and the same weekend'], 181: ['Good week to be a Northsiderrr in Chicago', 'Leaving early tomorrow morning to go to the Hospital in Chicago'], 182: ['Ahh Obama s in Chicago', 'I live in South Ontario but have great friends in Chicago'], 183: ['Ahh Obama s in Chicago', 'did u really quit your job to go to edc chicago'], 184: ['Ahh Obama s in Chicago', 'youre in Chicago and were here to say nokxl'], 185: ['Ahh Obama s in Chicago', 'Night in Chicago with my ladies'], 186: ['I cannot WAIT to go to Chicago', 'Chicago is saying this todayEven nonhockey fans'], 187: ['I cannot WAIT to go to Chicago', 'It was a day of impulse buys in Chicago'], 188: ['youre in Chicago and were here to say nokxl', 'Just bought a ticket to Chicago for 2350 round trip'], 189: ['youre in Chicago and were here to say nokxl', 'If Chicago does I can officially turn off the TV for the season'], 190: ['Yes yo CHRIS DAVIS IS BATS', 'Is that Chris Davis out there'], 191: ['Wow Chris Davis is only 27', 'Chris Davis actually reminds me of Hamilton at the plate'], 192: ['Chris Davis rules the Yard tonight', 'So um Chris Davis you guys'], 193: ['Chris Davis rules the Yard tonight', 'So happy Chris Davis is on my fantasy team'], 194: ['Chris Davis rules the Yard tonight', 'No other words but shut the front door Chris Davis is my not so secret crush'], 195: ['Chris Davis is way to nice', 'Is Chris Davis a top 5 hitter in baseball right now'], 196: ['Chris Davis is way to nice', 'WHAT THE HELL DOES CHRIS DAVIS EAT FOR BREAKFAST'], 197: ['Chris Davis is on the roids BIG TIME', 'Ayo smh Chris Davis TEACH ME'], 198: ['Chris Davis is on the roids BIG TIME', 'When is the Chris Davis ped suspension coming'], 199: ['Chris Davis is on the roids BIG TIME', 'Chris Davis is on pace for about 56 HRs']}
baseline_algo: short_sentence: A Walk to Remember is the definition of true love, long_sentence: A Walk to Remember is on and Im in town and Im upset
match_number: 23, short_sentence_length: 49, match_percentage: 0.46938775510204084
baseline_algo: short_sentence: A Walk to Remember is the cutest thing, long_sentence: A Walk to Remember is the definition of true love
match_number: 26, short_sentence_length: 38, match_percentage: 0.6842105263157895
baseline_algo: short_sentence: A Walk to Remember is the definition of true love, long_sentence: A walk to remember is on ABC family youre welcome
match_number: 24, short_sentence_length: 49, match_percentage: 0.4897959183673469
baseline_algo: short_sentence: A walk to remember is so amazing and inspiring, long_sentence: A Walk to Remember is the definition of true love
match_number: 23, short_sentence_length: 46, match_percentage: 0.5
baseline_algo: short_sentence: A Walk to Remember is the definition of true love, long_sentence: BUT GUYS ITS ON MY FAVE PART OF A WALK TO REMEMBER
match_number: 1, short_sentence_length: 49, match_percentage: 0.02040816326530612
baseline_algo: short_sentence: Day is made A Walk to Remember is on, long_sentence: A Walk to Remember is the definition of true love
match_number: 6, short_sentence_length: 36, match_percentage: 0.16666666666666666
baseline_algo: short_sentence: A Walk to Remember is the definition of true love, long_sentence: The only Nicholas Sparks movie I genuinely like is A Walk To Remember
match_number: 1, short_sentence_length: 49, match_percentage: 0.02040816326530612
baseline_algo: short_sentence: A Walk to Remember is the definition of true love, long_sentence: Watching A Walk To Remember for the millionth time and for the millionth time I will cry
match_number: 3, short_sentence_length: 49, match_percentage: 0.061224489795918366
baseline_algo: short_sentence: A Walk to Remember is on tv right now, long_sentence: On the real I like the movie A Walk to Remember
match_number: 2, short_sentence_length: 37, match_percentage: 0.05405405405405406
baseline_algo: short_sentence: I never even seen a walk to remember, long_sentence: On the real I like the movie A Walk to Remember
match_number: 2, short_sentence_length: 36, match_percentage: 0.05555555555555555
baseline_algo: short_sentence: On the real I like the movie A Walk to Remember, long_sentence: I want a love like Jamie and Landon on A Walk To Remember
match_number: 10, short_sentence_length: 47, match_percentage: 0.2127659574468085
baseline_algo: short_sentence: On the real I like the movie A Walk to Remember, long_sentence: THE GUY IN A WALK TO REMEMBER IS SO CUTE IM PISSING
match_number: 1, short_sentence_length: 47, match_percentage: 0.02127659574468085
baseline_algo: short_sentence: On the real I like the movie A Walk to Remember, long_sentence: The part on A Walk To Remember when they are looking at the stars
match_number: 6, short_sentence_length: 47, match_percentage: 0.1276595744680851
baseline_algo: short_sentence: Turned on the tv and A Walk to Remember is on, long_sentence: On the real I like the movie A Walk to Remember
match_number: 4, short_sentence_length: 45, match_percentage: 0.08888888888888889
baseline_algo: short_sentence: On the real I like the movie A Walk to Remember, long_sentence: Watching A Walk To Remember is seriously making me ball right now it s so perfect
match_number: 5, short_sentence_length: 47, match_percentage: 0.10638297872340426
baseline_algo: short_sentence: When a walk to remember is on tv, long_sentence: On the real I like the movie A Walk to Remember
match_number: 3, short_sentence_length: 32, match_percentage: 0.09375
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: AAP in that Adidas Commercial lol
match_number: 9, short_sentence_length: 31, match_percentage: 0.2903225806451613
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: AAP was nice on that Adidas commercial
match_number: 7, short_sentence_length: 31, match_percentage: 0.22580645161290322
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: Dammnnnnnn that ASAP commercial for Adidas is sweet
match_number: 12, short_sentence_length: 31, match_percentage: 0.3870967741935484
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: Rocky and john wall in that new quickaintfair Adidas commercial
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: Rocky killed that Adidas commercial too
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: That AAP Rocky Adidas commerical is hard af
match_number: 2, short_sentence_length: 31, match_percentage: 0.06451612903225806
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: That Adidas ASAP Rocky commercial dope af
match_number: 2, short_sentence_length: 31, match_percentage: 0.06451612903225806
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: That Adidas commercial with ASAP Rocky goes hard
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: The new Asap rocky Adidas commercial is fresh as shit
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: AAP is in the Adidas commercial, long_sentence: and Adidas commercial that was legit
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: What the fuck did Amanda Bynes do to her face, long_sentence: About to get amanda bynes nudes tatted on my forearm
match_number: 2, short_sentence_length: 45, match_percentage: 0.044444444444444446
baseline_algo: short_sentence: What the fuck did Amanda Bynes do to her face, long_sentence: Can someone explain what the fuck happened to Amanda Bynes face
match_number: 4, short_sentence_length: 45, match_percentage: 0.08888888888888889
baseline_algo: short_sentence: Dude Amanda bynes is like cracked out, long_sentence: What the fuck did Amanda Bynes do to her face
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: What the fuck did Amanda Bynes do to her face, long_sentence: For the love of God someone please 5150 Amanda Bynes
match_number: 1, short_sentence_length: 45, match_percentage: 0.022222222222222223
baseline_algo: short_sentence: What the fuck did Amanda Bynes do to her face, long_sentence: Hoes confused as to why everyone is infatuated with Amanda Bynes
match_number: 4, short_sentence_length: 45, match_percentage: 0.08888888888888889
baseline_algo: short_sentence: What the fuck did Amanda Bynes do to her face, long_sentence: If Amanda Bynes can make it through the year I think we all can
match_number: 4, short_sentence_length: 45, match_percentage: 0.08888888888888889
baseline_algo: short_sentence: Im going to call the police on Amanda Bynes, long_sentence: What the fuck did Amanda Bynes do to her face
match_number: 2, short_sentence_length: 43, match_percentage: 0.046511627906976744
baseline_algo: short_sentence: Is amanda bynes on crack or something, long_sentence: What the fuck did Amanda Bynes do to her face
match_number: 1, short_sentence_length: 37, match_percentage: 0.02702702702702703
baseline_algo: short_sentence: What the fuck did Amanda Bynes do to her face, long_sentence: wow what the hell has happened to amanda bynes
match_number: 10, short_sentence_length: 45, match_percentage: 0.2222222222222222
baseline_algo: short_sentence: According to Amazon s review, long_sentence: Colorado Mandala is NOW AVAILABLE for purchase on Amazon
match_number: 4, short_sentence_length: 28, match_percentage: 0.14285714285714285
baseline_algo: short_sentence: According to Amazon s review, long_sentence: I am going to deliver the camera tomorrow when I purchased it in the Amazon
match_number: 1, short_sentence_length: 28, match_percentage: 0.03571428571428571
baseline_algo: short_sentence: I got it from amazon, long_sentence: According to Amazon s review
match_number: 3, short_sentence_length: 20, match_percentage: 0.15
baseline_algo: short_sentence: According to Amazon s review, long_sentence: I hope to win a 50 Amazon Gift Code
match_number: 2, short_sentence_length: 28, match_percentage: 0.07142857142857142
baseline_algo: short_sentence: According to Amazon s review, long_sentence: Learn to Publish Your Hot Selling eBooks to Amazon Kindle
match_number: 1, short_sentence_length: 28, match_percentage: 0.03571428571428571
baseline_algo: short_sentence: According to Amazon s review, long_sentence: May s prize on My Question of the Day is a 50 Amazon gift card
match_number: 0, short_sentence_length: 28, match_percentage: 0.0
baseline_algo: short_sentence: According to Amazon s review, long_sentence: Win a 25 Amazon Gift Card with MeBookshelfandI
match_number: 0, short_sentence_length: 28, match_percentage: 0.0
baseline_algo: short_sentence: According to Amazon s review, long_sentence: ahhh but did you know you can get this sort of stuff on amazon
match_number: 3, short_sentence_length: 28, match_percentage: 0.10714285714285714
baseline_algo: short_sentence: According to Amazon s review, long_sentence: check out the Yeti on Amazon it s not too pricey
match_number: 3, short_sentence_length: 28, match_percentage: 0.10714285714285714
baseline_algo: short_sentence: According to Amazon s review, long_sentence: just bought doom 3 for xbox off amazon
match_number: 2, short_sentence_length: 28, match_percentage: 0.07142857142857142
baseline_algo: short_sentence: Do not Amber Alert me, long_sentence: Do not Amber Alert me
match_number: 21, short_sentence_length: 21, match_percentage: 1.0
baseline_algo: short_sentence: Do not Amber Alert me, long_sentence: Everyone s Amber Alerts going off at Zarape
match_number: 0, short_sentence_length: 21, match_percentage: 0.0
baseline_algo: short_sentence: Do not Amber Alert me, long_sentence: I get scared when I get the amber alert messages
match_number: 3, short_sentence_length: 21, match_percentage: 0.14285714285714285
baseline_algo: short_sentence: Do not Amber Alert me, long_sentence: I wonder if the kidnapper gets amber alerts as well
match_number: 0, short_sentence_length: 21, match_percentage: 0.0
baseline_algo: short_sentence: Do not Amber Alert me, long_sentence: I would have put Amber in the top 2
match_number: 2, short_sentence_length: 21, match_percentage: 0.09523809523809523
baseline_algo: short_sentence: Do not Amber Alert me, long_sentence: Just found out Amber was eliminated
match_number: 0, short_sentence_length: 21, match_percentage: 0.0
baseline_algo: short_sentence: Do not Amber Alert me, long_sentence: Look Im tired if these damn Amber Alerts
match_number: 2, short_sentence_length: 21, match_percentage: 0.09523809523809523
baseline_algo: short_sentence: Do not Amber Alert me, long_sentence: Plus Amber has a huge career
match_number: 1, short_sentence_length: 21, match_percentage: 0.047619047619047616
baseline_algo: short_sentence: Do not Amber Alert me, long_sentence: im going to miss Amber on idol
match_number: 3, short_sentence_length: 21, match_percentage: 0.14285714285714285
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: Amber alert gave me a damn heart attack
match_number: 4, short_sentence_length: 37, match_percentage: 0.10810810810810811
baseline_algo: short_sentence: Amber shouldnt have gone home, long_sentence: That amber alert was getting annoying
match_number: 0, short_sentence_length: 29, match_percentage: 0.0
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: Everyone keeps talking about amber alerts on their phone
match_number: 4, short_sentence_length: 37, match_percentage: 0.10810810810810811
baseline_algo: short_sentence: I get to see my Amber, long_sentence: That amber alert was getting annoying
match_number: 0, short_sentence_length: 21, match_percentage: 0.0
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: I thought amber alerts would only sound on androids
match_number: 4, short_sentence_length: 37, match_percentage: 0.10810810810810811
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: Ive gotten the same amber alert 3 times
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: Ok this is the fifth amber alert Ive received
match_number: 4, short_sentence_length: 37, match_percentage: 0.10810810810810811
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: Wats up with this amber alerts going off in church
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: Why do I get amber alerts tho, long_sentence: That amber alert was getting annoying
match_number: 2, short_sentence_length: 29, match_percentage: 0.06896551724137931
baseline_algo: short_sentence: Amber is the cutest person ever, long_sentence: That Amber alert scared the crap out of me
match_number: 3, short_sentence_length: 31, match_percentage: 0.0967741935483871
baseline_algo: short_sentence: That Amber alert scared the crap out of me, long_sentence: Everytime I get an amber alert on my phone I get freaked out
match_number: 5, short_sentence_length: 42, match_percentage: 0.11904761904761904
baseline_algo: short_sentence: Pray for the Amber Alert, long_sentence: That Amber alert scared the crap out of me
match_number: 3, short_sentence_length: 24, match_percentage: 0.125
baseline_algo: short_sentence: Am I the only one who dont get Amber alert, long_sentence: My phone is annoying me with these amber alerts
match_number: 4, short_sentence_length: 42, match_percentage: 0.09523809523809523
baseline_algo: short_sentence: My phone is annoying me with these amber alerts, long_sentence: How long has Amber been missing because I keep getting alerts about her
match_number: 9, short_sentence_length: 47, match_percentage: 0.19148936170212766
baseline_algo: short_sentence: I aint got no amber alerts yet, long_sentence: My phone is annoying me with these amber alerts
match_number: 2, short_sentence_length: 30, match_percentage: 0.06666666666666667
baseline_algo: short_sentence: I hate the amber alert sound, long_sentence: My phone is annoying me with these amber alerts
match_number: 0, short_sentence_length: 28, match_percentage: 0.0
baseline_algo: short_sentence: My phone is annoying me with these amber alerts, long_sentence: Just got the same amber alert three times today smh
match_number: 3, short_sentence_length: 47, match_percentage: 0.06382978723404255
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: My phone is annoying me with these amber alerts
match_number: 4, short_sentence_length: 37, match_percentage: 0.10810810810810811
baseline_algo: short_sentence: My phone is annoying me with these amber alerts, long_sentence: These amber alert things scare me everytime they go off on my phone
match_number: 2, short_sentence_length: 47, match_percentage: 0.0425531914893617
baseline_algo: short_sentence: My phone is annoying me with these amber alerts, long_sentence: These amber alerts on my phone always freak me out
match_number: 4, short_sentence_length: 47, match_percentage: 0.0851063829787234
baseline_algo: short_sentence: Turns out it s just another amber alert, long_sentence: My phone is annoying me with these amber alerts
match_number: 1, short_sentence_length: 39, match_percentage: 0.02564102564102564
baseline_algo: short_sentence: What is an amber alert, long_sentence: Does anybody else keep gettin this Amber Alert notification
match_number: 2, short_sentence_length: 22, match_percentage: 0.09090909090909091
baseline_algo: short_sentence: What is an amber alert, long_sentence: Husband s phone just had an Amber Alert Warning
match_number: 2, short_sentence_length: 22, match_percentage: 0.09090909090909091
baseline_algo: short_sentence: What is an amber alert, long_sentence: I Know You Are Very Upset That Amber Went Home
match_number: 1, short_sentence_length: 22, match_percentage: 0.045454545454545456
baseline_algo: short_sentence: What is an amber alert, long_sentence: If these amber alerts send me one more thing
match_number: 2, short_sentence_length: 22, match_percentage: 0.09090909090909091
baseline_algo: short_sentence: What is an amber alert, long_sentence: OMG AMBER I FREAKING HATE YOU
match_number: 0, short_sentence_length: 22, match_percentage: 0.0
baseline_algo: short_sentence: What is an amber alert, long_sentence: Pray for the Amber Alert
match_number: 2, short_sentence_length: 22, match_percentage: 0.09090909090909091
baseline_algo: short_sentence: What is an amber alert, long_sentence: These Amber Alerts are really annoying me right now
match_number: 4, short_sentence_length: 22, match_percentage: 0.18181818181818182
baseline_algo: short_sentence: These Amber Alerts need to chill, long_sentence: Ive been getting amber alerts all day like what the fuck
match_number: 3, short_sentence_length: 32, match_percentage: 0.09375
baseline_algo: short_sentence: These Amber Alerts need to chill, long_sentence: My phone NEVER sends me Amber Alerts
match_number: 2, short_sentence_length: 32, match_percentage: 0.0625
baseline_algo: short_sentence: These Amber Alerts need to chill, long_sentence: That s 5 amber alerts today Ive got
match_number: 2, short_sentence_length: 32, match_percentage: 0.0625
baseline_algo: short_sentence: These Amber Alerts need to chill, long_sentence: These amber alert messages are scary as fudge man
match_number: 16, short_sentence_length: 32, match_percentage: 0.5
baseline_algo: short_sentence: These Amber Alerts need to chill, long_sentence: Well that Amber Alert alert just scared tf outta me
match_number: 0, short_sentence_length: 32, match_percentage: 0.0
baseline_algo: short_sentence: My phone is annoying me with these amber alerts, long_sentence: All these amber alerts leave people s kids alone
match_number: 1, short_sentence_length: 47, match_percentage: 0.02127659574468085
baseline_algo: short_sentence: My phone is annoying me with these amber alerts, long_sentence: I hate these damn amber alerts coming to my phone
match_number: 3, short_sentence_length: 47, match_percentage: 0.06382978723404255
baseline_algo: short_sentence: I just got like my 5th amber alert, long_sentence: My phone is annoying me with these amber alerts
match_number: 2, short_sentence_length: 34, match_percentage: 0.058823529411764705
baseline_algo: short_sentence: My phone is annoying me with these amber alerts, long_sentence: Omfg Who The FUCK Invented This Fucking Amber Alert
match_number: 3, short_sentence_length: 47, match_percentage: 0.06382978723404255
baseline_algo: short_sentence: We all will miss our AMBER, long_sentence: My phone is annoying me with these amber alerts
match_number: 3, short_sentence_length: 26, match_percentage: 0.11538461538461539
baseline_algo: short_sentence: cx whats an amber alert thooo, long_sentence: My phone is annoying me with these amber alerts
match_number: 6, short_sentence_length: 29, match_percentage: 0.20689655172413793
baseline_algo: short_sentence: what is this amber alert thing on my phone, long_sentence: My phone is annoying me with these amber alerts
match_number: 0, short_sentence_length: 42, match_percentage: 0.0
baseline_algo: short_sentence: Amber gone be a modelsinger, long_sentence: That amber alert was getting annoying
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: Can we chill with the amber alerts, long_sentence: That amber alert was getting annoying
match_number: 1, short_sentence_length: 34, match_percentage: 0.029411764705882353
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: Dam amber alert i got to my phone scared the hell out of me
match_number: 1, short_sentence_length: 37, match_percentage: 0.02702702702702703
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: Everyone s Amber Alerts going off at Zarape
match_number: 2, short_sentence_length: 37, match_percentage: 0.05405405405405406
baseline_algo: short_sentence: How come I never get amber alerts, long_sentence: That amber alert was getting annoying
match_number: 4, short_sentence_length: 33, match_percentage: 0.12121212121212122
baseline_algo: short_sentence: OH MY GOD FUCK THESE AMBER ALERTS, long_sentence: That amber alert was getting annoying
match_number: 1, short_sentence_length: 33, match_percentage: 0.030303030303030304
baseline_algo: short_sentence: That s 5 amber alerts today Ive got, long_sentence: That amber alert was getting annoying
match_number: 5, short_sentence_length: 35, match_percentage: 0.14285714285714285
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: These amber alerts on my phone been going off ALL day
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: That amber alert was getting annoying, long_sentence: how would you not know what an amber alert is
match_number: 0, short_sentence_length: 37, match_percentage: 0.0
baseline_algo: short_sentence: am i the only one not getting amber alerts, long_sentence: I need a amber alert to wake tf up in da mornings Ha
match_number: 1, short_sentence_length: 42, match_percentage: 0.023809523809523808
baseline_algo: short_sentence: I woke up to that Amber Alert I got, long_sentence: am i the only one not getting amber alerts
match_number: 2, short_sentence_length: 35, match_percentage: 0.05714285714285714
baseline_algo: short_sentence: Ive had like 3 Amber Alerts today, long_sentence: am i the only one not getting amber alerts
match_number: 2, short_sentence_length: 33, match_percentage: 0.06060606060606061
baseline_algo: short_sentence: My phone keeps sending me amber alerts, long_sentence: am i the only one not getting amber alerts
match_number: 4, short_sentence_length: 38, match_percentage: 0.10526315789473684
baseline_algo: short_sentence: am i the only one not getting amber alerts, long_sentence: My phone started ringing and vibrated in class today for a amber alert
match_number: 3, short_sentence_length: 42, match_percentage: 0.07142857142857142
baseline_algo: short_sentence: am i the only one not getting amber alerts, long_sentence: That amber alert on my phone be scaring the shit out of me
match_number: 3, short_sentence_length: 42, match_percentage: 0.07142857142857142
baseline_algo: short_sentence: This Amber alert keep cuttin my songs off, long_sentence: am i the only one not getting amber alerts
match_number: 6, short_sentence_length: 41, match_percentage: 0.14634146341463414
baseline_algo: short_sentence: am i the only one not getting amber alerts, long_sentence: Who is Amber and why does she keep sending me these alerts
match_number: 2, short_sentence_length: 42, match_percentage: 0.047619047619047616
baseline_algo: short_sentence: there has been alot of amber alerts today, long_sentence: am i the only one not getting amber alerts
match_number: 2, short_sentence_length: 41, match_percentage: 0.04878048780487805
baseline_algo: short_sentence: If I get one more damn amber alert, long_sentence: Amber Alerts to my iPhone scare me EVERYTIME
match_number: 2, short_sentence_length: 34, match_percentage: 0.058823529411764705
baseline_algo: short_sentence: I aint got no amber alerts yet, long_sentence: If I get one more damn amber alert
match_number: 1, short_sentence_length: 30, match_percentage: 0.03333333333333333
baseline_algo: short_sentence: If I get one more damn amber alert, long_sentence: I have gotten 3 amber alert notices today
match_number: 2, short_sentence_length: 34, match_percentage: 0.058823529411764705
baseline_algo: short_sentence: I loved Amber the whole season, long_sentence: If I get one more damn amber alert
match_number: 4, short_sentence_length: 30, match_percentage: 0.13333333333333333
baseline_algo: short_sentence: If I get one more damn amber alert, long_sentence: If I get one more of these loud ass amber alerts
match_number: 18, short_sentence_length: 34, match_percentage: 0.5294117647058824
baseline_algo: short_sentence: If I get one more damn amber alert, long_sentence: One more Amber Alert today Im throwing a bitch fit
match_number: 2, short_sentence_length: 34, match_percentage: 0.058823529411764705
baseline_algo: short_sentence: If I get one more damn amber alert, long_sentence: That amber alert just scared the beep outta me
match_number: 4, short_sentence_length: 34, match_percentage: 0.11764705882352941
baseline_algo: short_sentence: If I get one more damn amber alert, long_sentence: This is the 4th time Ive gotten that amber alert
match_number: 1, short_sentence_length: 34, match_percentage: 0.029411764705882353
baseline_algo: short_sentence: Andre miller is a liability on defense, long_sentence: Andre miller best lobbing pg in the game
match_number: 18, short_sentence_length: 38, match_percentage: 0.47368421052631576
baseline_algo: short_sentence: Andre miller is even slower in person, long_sentence: Andre miller best lobbing pg in the game
match_number: 13, short_sentence_length: 37, match_percentage: 0.35135135135135137
baseline_algo: short_sentence: Andre miller wit dat old man game, long_sentence: Andre miller best lobbing pg in the game
match_number: 14, short_sentence_length: 33, match_percentage: 0.42424242424242425
baseline_algo: short_sentence: Andre miller you a bitch shut up, long_sentence: Andre miller best lobbing pg in the game
match_number: 13, short_sentence_length: 32, match_percentage: 0.40625
baseline_algo: short_sentence: Andre miller best lobbing pg in the game, long_sentence: How does Andre miller still move this fast
match_number: 1, short_sentence_length: 40, match_percentage: 0.025
baseline_algo: short_sentence: Andre miller best lobbing pg in the game, long_sentence: The fact that Andre miller is consistent really pisses me off
match_number: 0, short_sentence_length: 40, match_percentage: 0.0
baseline_algo: short_sentence: Andre miller best lobbing pg in the game, long_sentence: Warriors need a vet like Andre Miller on they team
match_number: 2, short_sentence_length: 40, match_percentage: 0.05
baseline_algo: short_sentence: Andre miller best lobbing pg in the game, long_sentence: Why is Andre Miller taking 3 s with his broke jumper
match_number: 3, short_sentence_length: 40, match_percentage: 0.075
baseline_algo: short_sentence: andre Miller aint never had no jumper, long_sentence: Andre miller best lobbing pg in the game
match_number: 13, short_sentence_length: 37, match_percentage: 0.35135135135135137
baseline_algo: short_sentence: Andrew Bogut about to die on the court, long_sentence: My player of the game so far is Andrew Bogut
match_number: 3, short_sentence_length: 38, match_percentage: 0.07894736842105263
baseline_algo: short_sentence: Andrew Bogut bitch ass goin off, long_sentence: My player of the game so far is Andrew Bogut
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: My player of the game so far is Andrew Bogut, long_sentence: Andrew Bogut is playing like ah 1 Overall Draft Pick
match_number: 5, short_sentence_length: 44, match_percentage: 0.11363636363636363
baseline_algo: short_sentence: Andrew Bogut just went behind his back, long_sentence: My player of the game so far is Andrew Bogut
match_number: 1, short_sentence_length: 38, match_percentage: 0.02631578947368421
baseline_algo: short_sentence: My player of the game so far is Andrew Bogut, long_sentence: Andrew Bogut keeping Golden State in the game
match_number: 3, short_sentence_length: 44, match_percentage: 0.06818181818181818
baseline_algo: short_sentence: My player of the game so far is Andrew Bogut, long_sentence: Andrew Bogut looking like he did at the University of Utah
match_number: 2, short_sentence_length: 44, match_percentage: 0.045454545454545456
baseline_algo: short_sentence: Is Andrew Bogut rocking the stealth mullet, long_sentence: My player of the game so far is Andrew Bogut
match_number: 3, short_sentence_length: 42, match_percentage: 0.07142857142857142
baseline_algo: short_sentence: My player of the game so far is Andrew Bogut, long_sentence: Literally nobody on Earth likes Andrew Bogut
match_number: 17, short_sentence_length: 44, match_percentage: 0.38636363636363635
baseline_algo: short_sentence: So is Andrew Bogut juicing or, long_sentence: My player of the game so far is Andrew Bogut
match_number: 2, short_sentence_length: 29, match_percentage: 0.06896551724137931
baseline_algo: short_sentence: My player of the game so far is Andrew Bogut, long_sentence: Steph curry looks sick but Andrew Bogut is playing like an all star quietly
match_number: 2, short_sentence_length: 44, match_percentage: 0.045454545454545456
baseline_algo: short_sentence: Rockets Asik showed the Thunder, long_sentence: Asik showed heart going to the line and knocking some of the FT s down
match_number: 3, short_sentence_length: 31, match_percentage: 0.0967741935483871
baseline_algo: short_sentence: Rockets Asik showed the Thunder, long_sentence: But hacking asik andforcing awful shots
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: Rockets Asik showed the Thunder, long_sentence: Omer Asik came through big tonight
match_number: 2, short_sentence_length: 31, match_percentage: 0.06451612903225806
baseline_algo: short_sentence: Rockets Asik showed the Thunder, long_sentence: Scott Brooks was worse with his hackAsik
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: So much for HackaAsik, long_sentence: Rockets Asik showed the Thunder
match_number: 2, short_sentence_length: 21, match_percentage: 0.09523809523809523
baseline_algo: short_sentence: Rockets Asik showed the Thunder, long_sentence: That hack of Asik shit didnt work out
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: Rockets Asik showed the Thunder, long_sentence: especially when Asik is out of the game
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: Rockets Asik showed the Thunder, long_sentence: they have been fouling asik since about the 6th min mark
match_number: 0, short_sentence_length: 31, match_percentage: 0.0
baseline_algo: short_sentence: what to do with Asik then, long_sentence: Rockets Asik showed the Thunder
match_number: 3, short_sentence_length: 25, match_percentage: 0.12
baseline_algo: short_sentence: Backstrom injured in warmup Harding gets the start, long_sentence: Backstrom goes down in warmup and the wild turn to josh Harding
match_number: 10, short_sentence_length: 50, match_percentage: 0.2
baseline_algo: short_sentence: Backstrom is out already good omen for the Hawks, long_sentence: Backstrom injured in warmup Harding gets the start
match_number: 12, short_sentence_length: 48, match_percentage: 0.25
baseline_algo: short_sentence: Backstrom injured in warmup Harding gets the start, long_sentence: Backstrom would get hurt the day we start playoffs
match_number: 11, short_sentence_length: 50, match_percentage: 0.22
baseline_algo: short_sentence: Lol so Backstrom got hurt in warm ups, long_sentence: Backstrom injured in warmup Harding gets the start
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: Wishing Backstrom a speedy recovery, long_sentence: Backstrom injured in warmup Harding gets the start
match_number: 2, short_sentence_length: 35, match_percentage: 0.05714285714285714
baseline_algo: short_sentence: Wow backstrom would get hurt in warmups, long_sentence: Backstrom injured in warmup Harding gets the start
match_number: 1, short_sentence_length: 39, match_percentage: 0.02564102564102564
baseline_algo: short_sentence: backstrom just got hurt in the warmup, long_sentence: Backstrom injured in warmup Harding gets the start
match_number: 9, short_sentence_length: 37, match_percentage: 0.24324324324324326
baseline_algo: short_sentence: lose Backstrom in the warm ups, long_sentence: Backstrom injured in warmup Harding gets the start
match_number: 1, short_sentence_length: 30, match_percentage: 0.03333333333333333
baseline_algo: short_sentence: backstrom just got hurt in the warmup, long_sentence: Backstrom Heatley and Pominville out for Minny
match_number: 10, short_sentence_length: 37, match_percentage: 0.2702702702702703
baseline_algo: short_sentence: backstrom just got hurt in the warmup, long_sentence: Backstrom apparently suffered an injury in warmup
match_number: 10, short_sentence_length: 37, match_percentage: 0.2702702702702703
baseline_algo: short_sentence: backstrom just got hurt in the warmup, long_sentence: Backstrom out in warm ups for the wild
match_number: 10, short_sentence_length: 37, match_percentage: 0.2702702702702703
baseline_algo: short_sentence: backstrom just got hurt in the warmup, long_sentence: If Backstrom is out can I pick the Blackhawks to win in two
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: backstrom just got hurt in the warmup, long_sentence: Looks like Backstrom hurt himself in warmup
match_number: 2, short_sentence_length: 37, match_percentage: 0.05405405405405406
baseline_algo: short_sentence: backstrom just got hurt in the warmup, long_sentence: Niklas Backstrom potentially got hurt during warmups
match_number: 1, short_sentence_length: 37, match_percentage: 0.02702702702702703
baseline_algo: short_sentence: backstrom just got hurt in the warmup, long_sentence: and now Backstrom is injured in the warmups timmywhitehead
match_number: 2, short_sentence_length: 37, match_percentage: 0.05405405405405406
baseline_algo: short_sentence: backstrom just got hurt in the warmup, long_sentence: backstrom was just injured in warmups for the
match_number: 14, short_sentence_length: 37, match_percentage: 0.3783783783783784
baseline_algo: short_sentence: backstrom hurt and left warmups, long_sentence: Backstrom gets hurt in warm ups for the wild
match_number: 13, short_sentence_length: 31, match_percentage: 0.41935483870967744
baseline_algo: short_sentence: backstrom hurt and left warmups, long_sentence: Backstrom hurt and helped to the back during warmups
match_number: 19, short_sentence_length: 31, match_percentage: 0.6129032258064516
baseline_algo: short_sentence: Backstrom s out for MIN, long_sentence: backstrom hurt and left warmups
match_number: 9, short_sentence_length: 23, match_percentage: 0.391304347826087
baseline_algo: short_sentence: Harding in net for Backstrom, long_sentence: backstrom hurt and left warmups
match_number: 5, short_sentence_length: 28, match_percentage: 0.17857142857142858
baseline_algo: short_sentence: backstrom hurt and left warmups, long_sentence: Lmao they had to help Backstrom off the ice
match_number: 4, short_sentence_length: 31, match_percentage: 0.12903225806451613
baseline_algo: short_sentence: backstrom hurt and left warmups, long_sentence: WHAT THE HECK BACKSTROM GOT HURT IN WARMUPS
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: backstrom hurt and left warmups, long_sentence: Yikes Backstrom might be injured
match_number: 3, short_sentence_length: 31, match_percentage: 0.0967741935483871
baseline_algo: short_sentence: you see backstrom is out, long_sentence: backstrom hurt and left warmups
match_number: 2, short_sentence_length: 24, match_percentage: 0.08333333333333333
baseline_algo: short_sentence: Backstrom hurt himself during warmups, long_sentence: Also hate that Backstrom went down in the warm up
match_number: 5, short_sentence_length: 37, match_percentage: 0.13513513513513514
baseline_algo: short_sentence: Backstrom is out we lose, long_sentence: Backstrom hurt himself during warmups
match_number: 11, short_sentence_length: 24, match_percentage: 0.4583333333333333
baseline_algo: short_sentence: Backstrom hurt himself during warmups, long_sentence: Backstrom just went down during warmups
match_number: 13, short_sentence_length: 37, match_percentage: 0.35135135135135137
baseline_algo: short_sentence: Hopefully Backstrom is back soon, long_sentence: Backstrom hurt himself during warmups
match_number: 2, short_sentence_length: 32, match_percentage: 0.0625
baseline_algo: short_sentence: Backstrom hurt himself during warmups, long_sentence: Man if Backstrom is out that s huuuuge
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: Neve mind Niklas Backstrom s hurt, long_sentence: Backstrom hurt himself during warmups
match_number: 1, short_sentence_length: 33, match_percentage: 0.030303030303030304
baseline_algo: short_sentence: Omg backstrom injured in warmup, long_sentence: Backstrom hurt himself during warmups
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: Backstrom hurt himself during warmups, long_sentence: Wild Backstrom injured during warmups
match_number: 16, short_sentence_length: 37, match_percentage: 0.43243243243243246
baseline_algo: short_sentence: Backstrom hurt himself during warmups, long_sentence: Wild in playoffs Backstrom injured in pregame warmups
match_number: 0, short_sentence_length: 37, match_percentage: 0.0
baseline_algo: short_sentence: Backstrom hurt in warn ups, long_sentence: Backstrom s hurt in warmups
match_number: 10, short_sentence_length: 26, match_percentage: 0.38461538461538464
baseline_algo: short_sentence: Backstrom s hurt in warmups, long_sentence: Backstrom injured himself in the warmups
match_number: 12, short_sentence_length: 27, match_percentage: 0.4444444444444444
baseline_algo: short_sentence: Backstrom s hurt in warmups, long_sentence: Backstrom was injured in warm up for Minnesota
match_number: 10, short_sentence_length: 27, match_percentage: 0.37037037037037035
baseline_algo: short_sentence: Backstrom s hurt in warmups, long_sentence: Backstrom was injured warming up
match_number: 10, short_sentence_length: 27, match_percentage: 0.37037037037037035
baseline_algo: short_sentence: Backstrom s hurt in warmups, long_sentence: Hurt himself during warm ups tough break for backstrom
match_number: 5, short_sentence_length: 27, match_percentage: 0.18518518518518517
baseline_algo: short_sentence: Backstrom s hurt in warmups, long_sentence: If you didnt know Backstrom inexplicably hurt himself in warmups
match_number: 0, short_sentence_length: 27, match_percentage: 0.0
baseline_algo: short_sentence: Backstrom s hurt in warmups, long_sentence: Minnesota Wild goalie Backstrom just got injured in warm ups
match_number: 2, short_sentence_length: 27, match_percentage: 0.07407407407407407
baseline_algo: short_sentence: Backstrom s hurt in warmups, long_sentence: OMG Backstrom is already injured
match_number: 2, short_sentence_length: 27, match_percentage: 0.07407407407407407
baseline_algo: short_sentence: Backstrom s hurt in warmups, long_sentence: That sucks Nicklas Backstrom from the wild got hurt in warm ups
match_number: 0, short_sentence_length: 27, match_percentage: 0.0
baseline_algo: short_sentence: Backstrom s hurt in warmups, long_sentence: When you hear Nicolas backstrom is injured
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: Backstrom for Min is hurt in warmup, long_sentence: Backstrom Hurt in warmups Harding gets the start
match_number: 16, short_sentence_length: 35, match_percentage: 0.45714285714285713
baseline_algo: short_sentence: Looks like Backstrom injured in warmup, long_sentence: Backstrom Hurt in warmups Harding gets the start
match_number: 2, short_sentence_length: 38, match_percentage: 0.05263157894736842
baseline_algo: short_sentence: Maybe with backstrom injured the D will step up, long_sentence: Backstrom Hurt in warmups Harding gets the start
match_number: 1, short_sentence_length: 47, match_percentage: 0.02127659574468085
baseline_algo: short_sentence: Minnesota goalie Backstrom hurt in warm ups, long_sentence: Backstrom Hurt in warmups Harding gets the start
match_number: 1, short_sentence_length: 43, match_percentage: 0.023255813953488372
baseline_algo: short_sentence: Nikalas backstrom hurt during warm ups, long_sentence: Backstrom Hurt in warmups Harding gets the start
match_number: 4, short_sentence_length: 38, match_percentage: 0.10526315789473684
baseline_algo: short_sentence: Backstrom Hurt in warmups Harding gets the start, long_sentence: Of course Backstrom would get injured during warmups
match_number: 7, short_sentence_length: 48, match_percentage: 0.14583333333333334
baseline_algo: short_sentence: Backstrom Hurt in warmups Harding gets the start, long_sentence: Tell flynnkatie Backstrom got injured during warmups
match_number: 4, short_sentence_length: 48, match_percentage: 0.08333333333333333
baseline_algo: short_sentence: Backstrom Hurt in warmups Harding gets the start, long_sentence: well first injury of the playoffs goes to backstrom in warmups of the first game
match_number: 8, short_sentence_length: 48, match_percentage: 0.16666666666666666
baseline_algo: short_sentence: Just lost Backstrom in warmup, long_sentence: Backstrom gets hurt in warm ups for the wild
match_number: 9, short_sentence_length: 29, match_percentage: 0.3103448275862069
baseline_algo: short_sentence: All the home alones watching 8 mile, long_sentence: The last rap battle in 8 Mile nevr gets old ahah
match_number: 3, short_sentence_length: 35, match_percentage: 0.08571428571428572
baseline_algo: short_sentence: All the home alones watching 8 mile, long_sentence: The rap battle at the end of 8 mile gets me so hype
match_number: 9, short_sentence_length: 35, match_percentage: 0.2571428571428571
baseline_algo: short_sentence: The Ending to 8 Mile is my fav part of the whole movie, long_sentence: Rabbit on 8 mile out of place but determined to make it
match_number: 3, short_sentence_length: 54, match_percentage: 0.05555555555555555
baseline_algo: short_sentence: The Ending to 8 Mile is my fav part of the whole movie, long_sentence: See 8 Mile is always on but it s the tv version so it s gay
match_number: 10, short_sentence_length: 54, match_percentage: 0.18518518518518517
baseline_algo: short_sentence: Those last 3 battles in 8 Mile are THE shit, long_sentence: The Ending to 8 Mile is my fav part of the whole movie
match_number: 9, short_sentence_length: 43, match_percentage: 0.20930232558139536
baseline_algo: short_sentence: 0 and 8 mile on at the same time, long_sentence: It s just rap lyrics from the movie 8 mile
match_number: 1, short_sentence_length: 32, match_percentage: 0.03125
baseline_algo: short_sentence: 0 and 8 mile on at the same time, long_sentence: I will never get tired of 8 Mile
match_number: 7, short_sentence_length: 32, match_percentage: 0.21875
baseline_algo: short_sentence: Well at least 8 Mile is on, long_sentence: In 8 Mile in a scene the background music is Sweet Home Alabama
match_number: 3, short_sentence_length: 26, match_percentage: 0.11538461538461539
baseline_algo: short_sentence: Well at least 8 Mile is on, long_sentence: 8 mile is on havent seen this movie in the longest
match_number: 0, short_sentence_length: 26, match_percentage: 0.0
baseline_algo: short_sentence: The last rap battle in 8 mile though, long_sentence: But why were people watching the heat play when 8 mile is on
match_number: 5, short_sentence_length: 36, match_percentage: 0.1388888888888889
baseline_algo: short_sentence: The last rap battle in 8 mile though, long_sentence: I think everyone is watching 8 mile Rn
match_number: 1, short_sentence_length: 36, match_percentage: 0.027777777777777776
baseline_algo: short_sentence: 8 mile is just a classic, long_sentence: 8 mile that movie I love eminem in this movie
match_number: 7, short_sentence_length: 24, match_percentage: 0.2916666666666667
baseline_algo: short_sentence: Well at least 8 Mile is on, long_sentence: Listened to the final rap battle from 8 mile and now Im watching the whole movie
match_number: 3, short_sentence_length: 26, match_percentage: 0.11538461538461539
baseline_algo: short_sentence: Well at least 8 Mile is on, long_sentence: 8 mile has been that movie
match_number: 2, short_sentence_length: 26, match_percentage: 0.07692307692307693
baseline_algo: short_sentence: Well at least 8 Mile is on, long_sentence: I really did miss my part on 8 Mile
match_number: 1, short_sentence_length: 26, match_percentage: 0.038461538461538464
baseline_algo: short_sentence: Well at least 8 Mile is on, long_sentence: Lose Yourself is the perfect song to end 8 mile on
match_number: 4, short_sentence_length: 26, match_percentage: 0.15384615384615385
baseline_algo: short_sentence: 8 mile is on friday made, long_sentence: I missed the best part of 8 mile
match_number: 4, short_sentence_length: 24, match_percentage: 0.16666666666666666
baseline_algo: short_sentence: 8 mile is on friday made, long_sentence: 8 mile is such an awesome movie
match_number: 10, short_sentence_length: 24, match_percentage: 0.4166666666666667
baseline_algo: short_sentence: Oh shit niggy 8 mile is on, long_sentence: Eminem s rap in the final battle of 8 Mile gets me pumped every time
match_number: 2, short_sentence_length: 26, match_percentage: 0.07692307692307693
baseline_algo: short_sentence: Oh shit niggy 8 mile is on, long_sentence: While yall argue about the game 8 mile is on MTV
match_number: 1, short_sentence_length: 26, match_percentage: 0.038461538461538464
baseline_algo: short_sentence: Ok good the end of 8 Mile is on, long_sentence: The end of 8 Mile makes me so happy
match_number: 3, short_sentence_length: 31, match_percentage: 0.0967741935483871
baseline_algo: short_sentence: Ok good the end of 8 Mile is on, long_sentence: The last 3 rap battles in 8 mile always get me hyped af
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: Ok good the end of 8 Mile is on, long_sentence: I always get the movies 8 mile and green mile mixed up
match_number: 2, short_sentence_length: 31, match_percentage: 0.06451612903225806
baseline_algo: short_sentence: Let s go see after earth, long_sentence: Did After Earth already come out
match_number: 3, short_sentence_length: 24, match_percentage: 0.125
baseline_algo: short_sentence: Let s go see after earth, long_sentence: idk I think we might go see after earth later if we do you wana go
match_number: 2, short_sentence_length: 24, match_percentage: 0.08333333333333333
baseline_algo: short_sentence: Anyone trying to see After Earth sometime soon, long_sentence: Me and my son went to see After Earth last night
match_number: 4, short_sentence_length: 46, match_percentage: 0.08695652173913043
baseline_algo: short_sentence: who wants to see after earth with me todayyyy, long_sentence: I could of told Will Smith that After Earth was going 2 crash
match_number: 1, short_sentence_length: 45, match_percentage: 0.022222222222222223
baseline_algo: short_sentence: I knew After Earth would tank, long_sentence: who wants to see after earth with me todayyyy
match_number: 1, short_sentence_length: 29, match_percentage: 0.034482758620689655
baseline_algo: short_sentence: After Earth finishes with 27 million, long_sentence: who wants to see after earth with me todayyyy
match_number: 2, short_sentence_length: 36, match_percentage: 0.05555555555555555
baseline_algo: short_sentence: even if After Earth is a good movie, long_sentence: who wants to see after earth with me todayyyy
match_number: 2, short_sentence_length: 35, match_percentage: 0.05714285714285714
baseline_algo: short_sentence: Ether to see after earth or fast 6, long_sentence: I see the After Earth reviews are not positive
match_number: 3, short_sentence_length: 34, match_percentage: 0.08823529411764706
baseline_algo: short_sentence: I need to watch after earth tho, long_sentence: I see the After Earth reviews are not positive
match_number: 8, short_sentence_length: 31, match_percentage: 0.25806451612903225
baseline_algo: short_sentence: i need to watch after earth asap, long_sentence: At the theater with the little about do watch After Earth
match_number: 3, short_sentence_length: 32, match_percentage: 0.09375
baseline_algo: short_sentence: i need to watch after earth asap, long_sentence: Someone needs to come see after earth with me
match_number: 2, short_sentence_length: 32, match_percentage: 0.0625
baseline_algo: short_sentence: i need to watch after earth asap, long_sentence: After Earth is a great ass movie
match_number: 3, short_sentence_length: 32, match_percentage: 0.09375
baseline_algo: short_sentence: Goin to see after earth with the fam, long_sentence: Heading out to see After Earth in a bit
match_number: 4, short_sentence_length: 36, match_percentage: 0.1111111111111111
baseline_algo: short_sentence: Heading out to see After Earth in a bit, long_sentence: will smith s speech in after earth is so relevant
match_number: 2, short_sentence_length: 39, match_percentage: 0.05128205128205128
baseline_algo: short_sentence: Heading out to see After Earth in a bit, long_sentence: Just got done eating chinese with the fam now ganna go see after earth
match_number: 1, short_sentence_length: 39, match_percentage: 0.02564102564102564
baseline_algo: short_sentence: Going to see after earth but, long_sentence: After earth is out and I havent seen it yet
match_number: 3, short_sentence_length: 28, match_percentage: 0.10714285714285714
baseline_algo: short_sentence: Who wants to take me to see After Earth, long_sentence: the hangover 3 and after earth are both really good
match_number: 10, short_sentence_length: 39, match_percentage: 0.2564102564102564
baseline_algo: short_sentence: I wanna see the movie after earth, long_sentence: I kinda wanna see After Earth as well
match_number: 6, short_sentence_length: 33, match_percentage: 0.18181818181818182
baseline_algo: short_sentence: I wanna see the movie after earth, long_sentence: NOW YOU SEE ME and AFTER EARTH Cant Outpace FAST FURIOUS 6
match_number: 2, short_sentence_length: 33, match_percentage: 0.06060606060606061
baseline_algo: short_sentence: I wanna see the movie after earth, long_sentence: After Earth 039 trumped by 039 Now You See Me 039 as 039 Fast
match_number: 2, short_sentence_length: 33, match_percentage: 0.06060606060606061
baseline_algo: short_sentence: the US and belgium are tied at half, long_sentence: Come on Romelu get some goals for Belgium
match_number: 1, short_sentence_length: 35, match_percentage: 0.02857142857142857
baseline_algo: short_sentence: the US and belgium are tied at half, long_sentence: Mirallas with a soft and cool finish off the rebound to put Belgium up 10
match_number: 0, short_sentence_length: 35, match_percentage: 0.0
baseline_algo: short_sentence: US vs Belgium or the wings, long_sentence: Dude Belgium is freakin staked
match_number: 0, short_sentence_length: 26, match_percentage: 0.0
baseline_algo: short_sentence: Dude Belgium is freakin staked, long_sentence: Belgium almost take the lead in the 27th min
match_number: 3, short_sentence_length: 30, match_percentage: 0.1
baseline_algo: short_sentence: Belgium s gonna rape by the USA, long_sentence: What kind of formation Belgium playing there
match_number: 2, short_sentence_length: 31, match_percentage: 0.06451612903225806
baseline_algo: short_sentence: Belgium s gonna rape by the USA, long_sentence: At half US 1 Belgium 1 Indians 5
match_number: 2, short_sentence_length: 31, match_percentage: 0.06451612903225806
baseline_algo: short_sentence: Belgium s gonna rape by the USA, long_sentence: Who s in goal for Belgium for the USMNT friendly
match_number: 2, short_sentence_length: 31, match_percentage: 0.06451612903225806
baseline_algo: short_sentence: Belgium vs USA you watching, long_sentence: Vernaelen always gets injured for Belgium
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: Belgium vs USA you watching, long_sentence: 6th minute Belgium with the score
match_number: 4, short_sentence_length: 27, match_percentage: 0.14814814814814814
baseline_algo: short_sentence: Belgium vs USA you watching, long_sentence: Belgium in a friendly instead
match_number: 9, short_sentence_length: 27, match_percentage: 0.3333333333333333
baseline_algo: short_sentence: Watching the USMNT vs the talented Belgium team, long_sentence: The Belgium GK wasnt trying to concede another goal
match_number: 2, short_sentence_length: 47, match_percentage: 0.0425531914893617
baseline_algo: short_sentence: that being said US 21 Belgium, long_sentence: Watching the USMNT vs the talented Belgium team
match_number: 2, short_sentence_length: 29, match_percentage: 0.06896551724137931
baseline_algo: short_sentence: yeah Belgium is definitely good, long_sentence: what s up with the nonHD main camera at the USBelgium soccer game
match_number: 3, short_sentence_length: 31, match_percentage: 0.0967741935483871
baseline_algo: short_sentence: Belgium vs USA you watching, long_sentence: yeah Belgium is definitely good
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: Belgium has a great team, long_sentence: a little late to the USABelgium game
match_number: 2, short_sentence_length: 24, match_percentage: 0.08333333333333333
baseline_algo: short_sentence: I love everyone on the Belgium squad, long_sentence: Belgium is playing 4 centerbacks and fellani and they concede on a set piece
match_number: 4, short_sentence_length: 36, match_percentage: 0.1111111111111111
baseline_algo: short_sentence: I love everyone on the Belgium squad, long_sentence: In other news this Belgium squad taking on USMNT is STACKED
match_number: 4, short_sentence_length: 36, match_percentage: 0.1111111111111111
baseline_algo: short_sentence: Rafa Benitez is still a massive prick, long_sentence: Mourinho to city Benitez to stay at Chelsea
match_number: 2, short_sentence_length: 37, match_percentage: 0.05405405405405406
baseline_algo: short_sentence: Rafa Benitez is still a massive prick, long_sentence: Chelsea FC wouldnt get rid of Benitez now
match_number: 1, short_sentence_length: 37, match_percentage: 0.02702702702702703
baseline_algo: short_sentence: Rafa Benitez he s too good for you, long_sentence: I hope Chelsea fans are thoroughly embarrassed now with the way they treated Benitez
match_number: 1, short_sentence_length: 34, match_percentage: 0.029411764705882353
baseline_algo: short_sentence: Rafa Benitez he s too good for you, long_sentence: Thank you for your tactics Benitez
match_number: 1, short_sentence_length: 34, match_percentage: 0.029411764705882353
baseline_algo: short_sentence: Got alot of time for rafa Benitez, long_sentence: Rafa Benitez he s too good for you
match_number: 0, short_sentence_length: 33, match_percentage: 0.0
baseline_algo: short_sentence: Benitez is a sick manager, long_sentence: Well done to Rafa Benitez a dignified man
match_number: 5, short_sentence_length: 25, match_percentage: 0.2
baseline_algo: short_sentence: Rafa Benitez a free agent, long_sentence: congratulations to petr ech and rafa benitez
match_number: 0, short_sentence_length: 25, match_percentage: 0.0
baseline_algo: short_sentence: Credit where credits due to Rafa Benitez, long_sentence: congratulations to petr ech and rafa benitez
match_number: 2, short_sentence_length: 40, match_percentage: 0.05
baseline_algo: short_sentence: God forbid lyknx Rafa Benitez, long_sentence: How can chelsea fans still hate benitez
match_number: 3, short_sentence_length: 29, match_percentage: 0.10344827586206896
baseline_algo: short_sentence: Thank you very much Rafa Benitez, long_sentence: Why do liverpool fans love benitez so much
match_number: 1, short_sentence_length: 32, match_percentage: 0.03125
baseline_algo: short_sentence: Rafa Benitez I must thank you, long_sentence: I am so happy for RAFA BENITEZ VictorMoses and Mikel
match_number: 1, short_sentence_length: 29, match_percentage: 0.034482758620689655
baseline_algo: short_sentence: Rafa Benitez I must thank you, long_sentence: Pleased for Benitez hasnt deserved the stick he s got
match_number: 4, short_sentence_length: 29, match_percentage: 0.13793103448275862
baseline_algo: short_sentence: Rafa Benitez I must thank you, long_sentence: Once a red always a blue rafa Benitez we want you
match_number: 2, short_sentence_length: 29, match_percentage: 0.06896551724137931
baseline_algo: short_sentence: THANK YOU SO MUCH RAFA BENITEZ, long_sentence: Benitez is alright tho man fuck chelsea fans they suck asshole
match_number: 1, short_sentence_length: 30, match_percentage: 0.03333333333333333
baseline_algo: short_sentence: THANK YOU SO MUCH RAFA BENITEZ, long_sentence: Credit where credits due to Rafa Benitez
match_number: 1, short_sentence_length: 30, match_percentage: 0.03333333333333333
baseline_algo: short_sentence: Rafa Benitez deserves a hell of a thank you, long_sentence: Any praise for Benitez from my Chelsea followers lol
match_number: 1, short_sentence_length: 43, match_percentage: 0.023255813953488372
baseline_algo: short_sentence: I dont understand the hatred for Rafa Benitez, long_sentence: Top 4 and a trophy and still they dont give any respect for Benitez
match_number: 2, short_sentence_length: 45, match_percentage: 0.044444444444444446
baseline_algo: short_sentence: Bill Self to the Big 12, long_sentence: Big 12SEC announce basketball challenge
match_number: 3, short_sentence_length: 23, match_percentage: 0.13043478260869565
baseline_algo: short_sentence: Big 12SEC announce basketball challenge, long_sentence: The Big 12 just got a whole lot more interesting
match_number: 4, short_sentence_length: 39, match_percentage: 0.10256410256410256
baseline_algo: short_sentence: Big 12SEC announce basketball challenge, long_sentence: Just when you thought Bill Self wasnt going to own the Big 12 for another year
match_number: 1, short_sentence_length: 39, match_percentage: 0.02564102564102564
baseline_algo: short_sentence: Big 12SEC announce basketball challenge, long_sentence: Oklahoma in Houston also among the 10 SECBig 12 matchups
match_number: 3, short_sentence_length: 39, match_percentage: 0.07692307692307693
baseline_algo: short_sentence: Did kU win the Big 12 Quidditch Championship, long_sentence: Kansas just won the Big 12 championship again
match_number: 2, short_sentence_length: 44, match_percentage: 0.045454545454545456
baseline_algo: short_sentence: He can fuck up the Big 12 all he wants, long_sentence: Kansas just won the Big 12 championship again
match_number: 4, short_sentence_length: 38, match_percentage: 0.10526315789473684
baseline_algo: short_sentence: So what if the Big 12 had 14 teams, long_sentence: Kansas just won the Big 12 championship again
match_number: 0, short_sentence_length: 34, match_percentage: 0.0
baseline_algo: short_sentence: Sorry to the rest of the big 12, long_sentence: And Kansas once again will win the Big 12
match_number: 1, short_sentence_length: 31, match_percentage: 0.03225806451612903
baseline_algo: short_sentence: The big 12 is about to be so stacked next year, long_sentence: How many of those who were handing the Big 12 to Okla
match_number: 1, short_sentence_length: 46, match_percentage: 0.021739130434782608
baseline_algo: short_sentence: will prolly win the Big 12, long_sentence: UKBig 12 challenge officially announced today
match_number: 0, short_sentence_length: 26, match_percentage: 0.0
baseline_algo: short_sentence: will prolly win the Big 12, long_sentence: 21 in Houston as part of Big 12SEC Challenge
match_number: 1, short_sentence_length: 26, match_percentage: 0.038461538461538464
baseline_algo: short_sentence: will prolly win the Big 12, long_sentence: No reason Kansas should lose a game in the big 12
match_number: 2, short_sentence_length: 26, match_percentage: 0.07692307692307693
baseline_algo: short_sentence: will prolly win the Big 12, long_sentence: The whole Big 12 but Okiestate in particular just lost their minds
match_number: 0, short_sentence_length: 26, match_percentage: 0.0
baseline_algo: short_sentence: Sorry to the rest of the big 12, long_sentence: Texas Tech will play at Alabama in the SECBig 12 Basketball Challenge on Nov
match_number: 2, short_sentence_length: 31, match_percentage: 0.06451612903225806
baseline_algo: short_sentence: UK part of the Big 12SEC Challenge, long_sentence: There is NOOOO competition in Big 12 basketball
match_number: 2, short_sentence_length: 34, match_percentage: 0.058823529411764705
baseline_algo: short_sentence: well the big 12 just got decided, long_sentence: to win the big 12 in all three major sports in the same year
match_number: 2, short_sentence_length: 32, match_percentage: 0.0625
baseline_algo: short_sentence: well the big 12 just got decided, long_sentence: SECBig 12 Challenge in hoops has been announced to begin in 201314
match_number: 0, short_sentence_length: 32, match_percentage: 0.0
baseline_algo: short_sentence: well the big 12 just got decided, long_sentence: So Wiggins Is Settling For Playing In The Garbage Ass Big 12
match_number: 1, short_sentence_length: 32, match_percentage: 0.03125
baseline_algo: short_sentence: Big 12 is gonna be exciting, long_sentence: Just wish he wasnt in the Big 12
match_number: 3, short_sentence_length: 27, match_percentage: 0.1111111111111111
baseline_algo: short_sentence: Just wish he wasnt in the Big 12, long_sentence: the BIG 12 goes through LAWRENCE
match_number: 0, short_sentence_length: 32, match_percentage: 0.0
baseline_algo: short_sentence: hannahbush were in Reigate, long_sentence: I just found out Marilyn Monroe has a full bush
match_number: 0, short_sentence_length: 26, match_percentage: 0.0
baseline_algo: short_sentence: hannahbush were in Reigate, long_sentence: Classic redhead with a natural bush
match_number: 2, short_sentence_length: 26, match_percentage: 0.07692307692307693
baseline_algo: short_sentence: hannahbush were in Reigate, long_sentence: Hahah I heard your dumbass woke up in a bush
match_number: 3, short_sentence_length: 26, match_percentage: 0.11538461538461539
baseline_algo: short_sentence: I dont have time for beating around the bush, long_sentence: Honey has a brush with her non existent bush
match_number: 6, short_sentence_length: 44, match_percentage: 0.13636363636363635
baseline_algo: short_sentence: Ctfu the man in the bush, long_sentence: How the hell could Obama kill more than Bush did in Iraq
match_number: 2, short_sentence_length: 24, match_percentage: 0.08333333333333333
baseline_algo: short_sentence: Ctfu the man in the bush, long_sentence: george bush is never a truther
match_number: 1, short_sentence_length: 24, match_percentage: 0.041666666666666664
baseline_algo: short_sentence: Ctfu the man in the bush, long_sentence: Im a fan of Clintons Pretty much despise Bush
match_number: 3, short_sentence_length: 24, match_percentage: 0.125
baseline_algo: short_sentence: He is worse than Bush, long_sentence: It was under Bush it is now
match_number: 2, short_sentence_length: 21, match_percentage: 0.09523809523809523
baseline_algo: short_sentence: He is worse than Bush, long_sentence: Did Obama and the Dems trust the Bush Government
match_number: 0, short_sentence_length: 21, match_percentage: 0.0
baseline_algo: short_sentence: He is worse than Bush, long_sentence: that the Libs are going to say its Bush s fault and Im a racist
match_number: 1, short_sentence_length: 21, match_percentage: 0.047619047619047616
baseline_algo: short_sentence: Then time to trim the rose bush, long_sentence: I dint like it under Bush either Obama has radically expanded it
match_number: 2, short_sentence_length: 31, match_percentage: 0.06451612903225806
baseline_algo: short_sentence: I want the Bush days back, long_sentence: Dont beat around the bush just say it
match_number: 2, short_sentence_length: 25, match_percentage: 0.08
baseline_algo: short_sentence: I want the Bush days back, long_sentence: fell in a spiky bush and I have a prickly thing in my finger
match_number: 2, short_sentence_length: 25, match_percentage: 0.08
baseline_algo: short_sentence: I want the Bush days back, long_sentence: i once walked into a bush outside school and literally apologised to it
match_number: 4, short_sentence_length: 25, match_percentage: 0.16
baseline_algo: short_sentence: I want the Bush days back, long_sentence: Bush wiretapped without warrants Obama had them
match_number: 1, short_sentence_length: 25, match_percentage: 0.04
baseline_algo: short_sentence: Footbridge over the River Bush, long_sentence: started under bush and Im sure you were cool with it then
match_number: 3, short_sentence_length: 30, match_percentage: 0.1
baseline_algo: short_sentence: Footbridge over the River Bush, long_sentence: The new Bush tour merchandise is now available
match_number: 0, short_sentence_length: 30, match_percentage: 0.0
baseline_algo: short_sentence: we live near a bush reserve, long_sentence: Footbridge over the River Bush
match_number: 4, short_sentence_length: 27, match_percentage: 0.14814814814814814
baseline_algo: short_sentence: I did for Bush as well, long_sentence: in my actual bush in a bush in Bushey
match_number: 2, short_sentence_length: 22, match_percentage: 0.09090909090909091
baseline_algo: short_sentence: I did for Bush as well, long_sentence: it started way before Bush Jr
match_number: 1, short_sentence_length: 22, match_percentage: 0.045454545454545456
baseline_algo: short_sentence: They called Bush hitler too, long_sentence: Darling stop beating around the bush
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR, long_sentence: Why cant everyone just tell it how it is instead of beating around the bush
match_number: 3, short_sentence_length: 72, match_percentage: 0.041666666666666664
baseline_algo: short_sentence: And his legs can have hair but no bush, long_sentence: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR
match_number: 0, short_sentence_length: 38, match_percentage: 0.0
baseline_algo: short_sentence: Yeah CHALMERS that s a foul, long_sentence: Allen Bosh Chalmers James at Wade
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: Yeah CHALMERS that s a foul, long_sentence: Chalmers makes me mad low key
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: Yeah CHALMERS that s a foul, long_sentence: Chalmers steady chasing on defense
match_number: 3, short_sentence_length: 27, match_percentage: 0.1111111111111111
baseline_algo: short_sentence: Yeah CHALMERS that s a foul, long_sentence: I like Cole s onball defense better than Chalmers
match_number: 3, short_sentence_length: 27, match_percentage: 0.1111111111111111
baseline_algo: short_sentence: And now chalmers with the scoop layup, long_sentence: Anybody see David West Elbow the hell out of Chalmers bad shoulder
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: Mario Chalmers pissin me off, long_sentence: MARIO CHALMERS JUST THREW IT TO HIS COACH HAHAHA
match_number: 4, short_sentence_length: 28, match_percentage: 0.14285714285714285
baseline_algo: short_sentence: Mario Chalmers pissin me off, long_sentence: Spo aint in the game chalmers
match_number: 0, short_sentence_length: 28, match_percentage: 0.0
baseline_algo: short_sentence: Mario Chalmers pissin me off, long_sentence: Mario Chalmers needs to get punched in the face
match_number: 15, short_sentence_length: 28, match_percentage: 0.5357142857142857
baseline_algo: short_sentence: Mario Chalmers pissin me off, long_sentence: Chalmers throws it out of bounds
match_number: 1, short_sentence_length: 28, match_percentage: 0.03571428571428571
baseline_algo: short_sentence: Uhm Chalmers didnt just travel, long_sentence: Chalmers just walked with the ball
match_number: 3, short_sentence_length: 30, match_percentage: 0.1
baseline_algo: short_sentence: Mario Chalmers looks like Mario, long_sentence: Then Chalmers fucks up again lmao
match_number: 0, short_sentence_length: 31, match_percentage: 0.0
baseline_algo: short_sentence: Lucky ass shxt by Chalmers, long_sentence: Mario Chalmers looks like Mario
match_number: 3, short_sentence_length: 26, match_percentage: 0.11538461538461539
baseline_algo: short_sentence: idc idc chalmers be making me mad, long_sentence: I hate Mario Chalmersdont know why
match_number: 1, short_sentence_length: 33, match_percentage: 0.030303030303030304
baseline_algo: short_sentence: Why is Mario Chalmers starting, long_sentence: I hate Mario Chalmersdont know why
match_number: 15, short_sentence_length: 30, match_percentage: 0.5
baseline_algo: short_sentence: CHALMERS IS THE TURNOVER KING, long_sentence: Didnt everyone love Mario Chalmers last year
match_number: 0, short_sentence_length: 29, match_percentage: 0.0
baseline_algo: short_sentence: LMFAOOO who tf you throwing to Chalmers, long_sentence: Chalmers is always complaining to the refs
match_number: 4, short_sentence_length: 39, match_percentage: 0.10256410256410256
baseline_algo: short_sentence: Chalmers is always complaining to the refs, long_sentence: James with the block and then wade to James James to chalmers for the finish
match_number: 2, short_sentence_length: 42, match_percentage: 0.047619047619047616
baseline_algo: short_sentence: Chalmers is always complaining to the refs, long_sentence: This is like the 27th time chalmers fucked up
match_number: 2, short_sentence_length: 42, match_percentage: 0.047619047619047616
baseline_algo: short_sentence: Chalmers would lock you down bubs, long_sentence: Chalmers is always complaining to the refs
match_number: 9, short_sentence_length: 33, match_percentage: 0.2727272727272727
baseline_algo: short_sentence: Chalmers is always complaining to the refs, long_sentence: Mario Chalmers is easily my worst favorite player in the NBA
match_number: 2, short_sentence_length: 42, match_percentage: 0.047619047619047616
baseline_algo: short_sentence: The fuck Chalmers is doing, long_sentence: The fuck Chalmers is doing
match_number: 26, short_sentence_length: 26, match_percentage: 1.0
baseline_algo: short_sentence: The fuck Chalmers is doing, long_sentence: Do I watch the game or Chalmers face
match_number: 0, short_sentence_length: 26, match_percentage: 0.0
baseline_algo: short_sentence: The fuck Chalmers is doing, long_sentence: Uhm Chalmers didnt just travel
match_number: 2, short_sentence_length: 26, match_percentage: 0.07692307692307693
baseline_algo: short_sentence: Chara get in that box, long_sentence: Chara is dirtier than a Fresno adult film star
match_number: 7, short_sentence_length: 21, match_percentage: 0.3333333333333333
baseline_algo: short_sentence: Orr should beat the shit out of chara, long_sentence: What s better than seeing Chara getting dropped by Orr
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: Chara s playing with hate in heart, long_sentence: watching chara go down is the best feeling
match_number: 2, short_sentence_length: 34, match_percentage: 0.058823529411764705
baseline_algo: short_sentence: Chara is the most overrated player in the league, long_sentence: Orr vs Chara is like a Jack Russell vs a Wolfhound
match_number: 3, short_sentence_length: 48, match_percentage: 0.0625
baseline_algo: short_sentence: Chara move your feet along the boards, long_sentence: Orr vs Chara is like a Jack Russell vs a Wolfhound
match_number: 3, short_sentence_length: 37, match_percentage: 0.08108108108108109
baseline_algo: short_sentence: FINALLY CHARA IS GIVEN A PENALTY, long_sentence: Orr vs Chara is like a Jack Russell vs a Wolfhound
match_number: 1, short_sentence_length: 32, match_percentage: 0.03125
baseline_algo: short_sentence: Chara is 69 Holy shit, long_sentence: Orr wants a piece of chara I swear
match_number: 1, short_sentence_length: 21, match_percentage: 0.047619047619047616
baseline_algo: short_sentence: Chara is 69 Holy shit, long_sentence: Okay who else saw that beauty hit to chara by orr
match_number: 2, short_sentence_length: 21, match_percentage: 0.09523809523809523
baseline_algo: short_sentence: Chara is 69 Holy shit, long_sentence: When anyone on the leafs knock down Chara I laugh so hard
match_number: 2, short_sentence_length: 21, match_percentage: 0.09523809523809523
baseline_algo: short_sentence: Chara is 69 Holy shit, long_sentence: Amazing how bad Chara looks when the speed of the game picks up
match_number: 2, short_sentence_length: 21, match_percentage: 0.09523809523809523
baseline_algo: short_sentence: Why is chara playing like a bitch, long_sentence: Colton Orr lowers the boom on Chara
match_number: 1, short_sentence_length: 33, match_percentage: 0.030303030303030304
baseline_algo: short_sentence: BOOOOOMM down went chara again, long_sentence: Colton Orr lowers the boom on Chara
match_number: 0, short_sentence_length: 30, match_percentage: 0.0
baseline_algo: short_sentence: Colton Orr lowers the boom on Chara, long_sentence: Zideno Chara is a brick wall on skates
match_number: 3, short_sentence_length: 35, match_percentage: 0.08571428571428572
baseline_algo: short_sentence: Chara penalty on the play, long_sentence: Orr with a big hit on Chara
match_number: 2, short_sentence_length: 25, match_percentage: 0.08
baseline_algo: short_sentence: Orr with a big hit on Chara, long_sentence: Chara called for a high stick on the play too
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: Orr with a big hit on Chara, long_sentence: I keep waiting for the chara vs orr fight
match_number: 3, short_sentence_length: 27, match_percentage: 0.1111111111111111
baseline_algo: short_sentence: Chara is just a big goon, long_sentence: Chara is a disgrace to the NHL
match_number: 11, short_sentence_length: 24, match_percentage: 0.4583333333333333
baseline_algo: short_sentence: but orr keeps pushing chara, long_sentence: Imma blame Chara for Lupul missing that one
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: but orr keeps pushing chara, long_sentence: Why is Chara allowed to take down people without the puck
match_number: 1, short_sentence_length: 27, match_percentage: 0.037037037037037035
baseline_algo: short_sentence: except Im in Chicago at the moment, long_sentence: When do you plan to be at 600 W Chicago next
match_number: 1, short_sentence_length: 34, match_percentage: 0.029411764705882353
baseline_algo: short_sentence: except Im in Chicago at the moment, long_sentence: Big game 7 here in Chicago tonight
match_number: 1, short_sentence_length: 34, match_percentage: 0.029411764705882353
baseline_algo: short_sentence: except Im in Chicago at the moment, long_sentence: I checked in at Chicago Park District
match_number: 6, short_sentence_length: 34, match_percentage: 0.17647058823529413
baseline_algo: short_sentence: Nice new Guideshop in Chicago, long_sentence: Anyone wanting to go to the JT concert 722 in Chicago
match_number: 3, short_sentence_length: 29, match_percentage: 0.10344827586206896
baseline_algo: short_sentence: The North side of Chicago is happy, long_sentence: Anyone wanting to go to the JT concert 722 in Chicago
match_number: 3, short_sentence_length: 34, match_percentage: 0.08823529411764706
baseline_algo: short_sentence: just watched season finale of chicago fire and cried, long_sentence: Anyone wanting to go to the JT concert 722 in Chicago
match_number: 2, short_sentence_length: 52, match_percentage: 0.038461538461538464
baseline_algo: short_sentence: See you in Chicago Dierks, long_sentence: I really wanna do the color run in Chicago lol
match_number: 0, short_sentence_length: 25, match_percentage: 0.0
baseline_algo: short_sentence: See you in Chicago Dierks, long_sentence: The ungeekedeliteschicago Daily is out
match_number: 3, short_sentence_length: 25, match_percentage: 0.12
baseline_algo: short_sentence: See you in Chicago Dierks, long_sentence: I am watching Chicago Fire A Hell of a Ride
match_number: 0, short_sentence_length: 25, match_percentage: 0.0
baseline_algo: short_sentence: See you in Chicago Dierks, long_sentence: panoramic shot of noKXL and stop deportations rallies at Obama fundraiser in Chicago
match_number: 0, short_sentence_length: 25, match_percentage: 0.0
baseline_algo: short_sentence: blogher is in chicago this year, long_sentence: Im just saying game 7 in Chicago
match_number: 5, short_sentence_length: 31, match_percentage: 0.16129032258064516
baseline_algo: short_sentence: Wings in a must win in Chicago, long_sentence: Im just saying game 7 in Chicago
match_number: 2, short_sentence_length: 30, match_percentage: 0.06666666666666667
baseline_algo: short_sentence: Im just saying game 7 in Chicago, long_sentence: Id definitely be happy to help but I wont be back in Chicago until Sunday
match_number: 3, short_sentence_length: 32, match_percentage: 0.09375
baseline_algo: short_sentence: Game 7 in Chicago tonight, long_sentence: By Micheline Maynard Contributor Chicago has
match_number: 2, short_sentence_length: 25, match_percentage: 0.08
baseline_algo: short_sentence: Game 7 in Chicago tonight, long_sentence: bro you in chicago widdit yet
match_number: 1, short_sentence_length: 25, match_percentage: 0.04
baseline_algo: short_sentence: Game 7 in Chicago tonight, long_sentence: is L still looking for a copy of Chicago
match_number: 2, short_sentence_length: 25, match_percentage: 0.08
baseline_algo: short_sentence: youre in Chicago and were here to say nokxl, long_sentence: hope you and Hunter made it to Chicago this time
match_number: 4, short_sentence_length: 43, match_percentage: 0.09302325581395349
baseline_algo: short_sentence: Flying to chicago on the 14th, long_sentence: youre in Chicago and were here to say nokxl
match_number: 1, short_sentence_length: 29, match_percentage: 0.034482758620689655
baseline_algo: short_sentence: Good week to be a Northsiderrr in Chicago, long_sentence: go to north coast in chicago its way cheaper and the same weekend
match_number: 4, short_sentence_length: 41, match_percentage: 0.0975609756097561
baseline_algo: short_sentence: Good week to be a Northsiderrr in Chicago, long_sentence: Leaving early tomorrow morning to go to the Hospital in Chicago
match_number: 2, short_sentence_length: 41, match_percentage: 0.04878048780487805
baseline_algo: short_sentence: Ahh Obama s in Chicago, long_sentence: I live in South Ontario but have great friends in Chicago
match_number: 2, short_sentence_length: 22, match_percentage: 0.09090909090909091
baseline_algo: short_sentence: Ahh Obama s in Chicago, long_sentence: did u really quit your job to go to edc chicago
match_number: 2, short_sentence_length: 22, match_percentage: 0.09090909090909091
baseline_algo: short_sentence: Ahh Obama s in Chicago, long_sentence: youre in Chicago and were here to say nokxl
match_number: 0, short_sentence_length: 22, match_percentage: 0.0
baseline_algo: short_sentence: Ahh Obama s in Chicago, long_sentence: Night in Chicago with my ladies
match_number: 0, short_sentence_length: 22, match_percentage: 0.0
baseline_algo: short_sentence: I cannot WAIT to go to Chicago, long_sentence: Chicago is saying this todayEven nonhockey fans
match_number: 2, short_sentence_length: 30, match_percentage: 0.06666666666666667
baseline_algo: short_sentence: I cannot WAIT to go to Chicago, long_sentence: It was a day of impulse buys in Chicago
match_number: 2, short_sentence_length: 30, match_percentage: 0.06666666666666667
baseline_algo: short_sentence: youre in Chicago and were here to say nokxl, long_sentence: Just bought a ticket to Chicago for 2350 round trip
match_number: 1, short_sentence_length: 43, match_percentage: 0.023255813953488372
baseline_algo: short_sentence: youre in Chicago and were here to say nokxl, long_sentence: If Chicago does I can officially turn off the TV for the season
match_number: 1, short_sentence_length: 43, match_percentage: 0.023255813953488372
baseline_algo: short_sentence: Yes yo CHRIS DAVIS IS BATS, long_sentence: Is that Chris Davis out there
match_number: 0, short_sentence_length: 26, match_percentage: 0.0
baseline_algo: short_sentence: Wow Chris Davis is only 27, long_sentence: Chris Davis actually reminds me of Hamilton at the plate
match_number: 0, short_sentence_length: 26, match_percentage: 0.0
baseline_algo: short_sentence: So um Chris Davis you guys, long_sentence: Chris Davis rules the Yard tonight
match_number: 7, short_sentence_length: 26, match_percentage: 0.2692307692307692
baseline_algo: short_sentence: Chris Davis rules the Yard tonight, long_sentence: So happy Chris Davis is on my fantasy team
match_number: 2, short_sentence_length: 34, match_percentage: 0.058823529411764705
baseline_algo: short_sentence: Chris Davis rules the Yard tonight, long_sentence: No other words but shut the front door Chris Davis is my not so secret crush
match_number: 0, short_sentence_length: 34, match_percentage: 0.0
baseline_algo: short_sentence: Chris Davis is way to nice, long_sentence: Is Chris Davis a top 5 hitter in baseball right now
match_number: 3, short_sentence_length: 26, match_percentage: 0.11538461538461539
baseline_algo: short_sentence: Chris Davis is way to nice, long_sentence: WHAT THE HELL DOES CHRIS DAVIS EAT FOR BREAKFAST
match_number: 1, short_sentence_length: 26, match_percentage: 0.038461538461538464
baseline_algo: short_sentence: Ayo smh Chris Davis TEACH ME, long_sentence: Chris Davis is on the roids BIG TIME
match_number: 1, short_sentence_length: 28, match_percentage: 0.03571428571428571
baseline_algo: short_sentence: Chris Davis is on the roids BIG TIME, long_sentence: When is the Chris Davis ped suspension coming
match_number: 4, short_sentence_length: 36, match_percentage: 0.1111111111111111
baseline_algo: short_sentence: Chris Davis is on the roids BIG TIME, long_sentence: Chris Davis is on pace for about 56 HRs
match_number: 18, short_sentence_length: 36, match_percentage: 0.5
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A Walk to Remember is on and Im in town and Im upset
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.49922133 0.35520009 0.         0.49922133
  0.         0.35520009]
 [0.         0.75525556 0.         0.26868528 0.37762778 0.
  0.37762778 0.26868528]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A Walk to Remember is the cutest thing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.49922133 0.35520009 0.         0.49922133
  0.35520009]
 [0.57615236 0.         0.         0.40993715 0.57615236 0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A walk to remember is on ABC family youre welcome
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.35520009 0.         0.        ]
 [0.44665616 0.         0.44665616 0.         0.31779954 0.
  0.31779954 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A walk to remember is so amazing and inspiring
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.49922133 0.35520009 0.49922133
  0.35520009]
 [0.57615236 0.         0.57615236 0.         0.40993715 0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: BUT GUYS ITS ON MY FAVE PART OF A WALK TO REMEMBER
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.         0.49922133 0.35520009 0.49922133
  0.35520009]
 [0.         0.57615236 0.57615236 0.         0.40993715 0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Day is made A Walk to Remember is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.49922133 0.35520009 0.49922133 0.35520009]
 [0.70490949 0.         0.         0.50154891 0.         0.50154891]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: The only Nicholas Sparks movie I genuinely like is A Walk To Remember
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.         0.49922133 0.         0.
  0.35520009 0.         0.49922133 0.35520009]
 [0.         0.4078241  0.4078241  0.         0.4078241  0.4078241
  0.29017021 0.4078241  0.         0.29017021]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
TF_IDF_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Watching A Walk To Remember for the millionth time and for the millionth time I will cry
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.         0.35520009 0.         0.49922133
  0.35520009 0.        ]
 [0.         0.         0.632061   0.2248583  0.632061   0.
  0.2248583  0.3160305 ]]
pairwise_similarity: [[1.         0.15973938]
 [0.15973938 1.        ]]
cosine_similarity: 0.15973937686327605
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: A Walk to Remember is on tv right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.49922133 0.35520009 0.         0.
  0.35520009]
 [0.         0.         0.         0.40993715 0.57615236 0.57615236
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: I never even seen a walk to remember
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.49922133 0.35520009 0.         0.35520009]
 [0.         0.         0.         0.50154891 0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: I want a love like Jamie and Landon on A Walk To Remember
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37930349 0.         0.53309782 0.53309782
  0.37930349 0.37930349 0.        ]
 [0.42567716 0.42567716 0.30287281 0.42567716 0.         0.
  0.30287281 0.30287281 0.42567716]]
pairwise_similarity: [[1.         0.34464214]
 [0.34464214 1.        ]]
cosine_similarity: 0.34464214103805474
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: THE GUY IN A WALK TO REMEMBER IS SO CUTE IM PISSING
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.49922133 0.49922133 0.
  0.49922133 0.35520009 0.35520009]
 [0.44665616 0.44665616 0.44665616 0.         0.         0.44665616
  0.         0.31779954 0.31779954]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: The part on A Walk To Remember when they are looking at the stars
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.         0.49922133 0.49922133 0.35520009 0.
  0.35520009]
 [0.         0.57615236 0.         0.         0.40993715 0.57615236
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: Turned on the tv and A Walk to Remember is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.49922133 0.35520009 0.         0.
  0.35520009]
 [0.         0.         0.         0.40993715 0.57615236 0.57615236
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: Watching A Walk To Remember is seriously making me ball right now it s so perfect
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.         0.49922133 0.         0.49922133
  0.35520009 0.         0.         0.35520009 0.        ]
 [0.37762778 0.         0.37762778 0.         0.37762778 0.
  0.26868528 0.37762778 0.37762778 0.26868528 0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
TF_IDF_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: When a walk to remember is on tv
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.49922133 0.49922133 0.35520009 0.         0.35520009]
 [0.         0.         0.         0.50154891 0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: AAP in that Adidas Commercial lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.57735027 0.57735027 0.        ]
 [0.44832087 0.44832087 0.44832087 0.63009934]]
pairwise_similarity: [[1.         0.77651453]
 [0.77651453 1.        ]]
cosine_similarity: 0.7765145304745156
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: AAP was nice on that Adidas commercial
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.57735027 0.57735027 0.        ]
 [0.44832087 0.44832087 0.44832087 0.63009934]]
pairwise_similarity: [[1.         0.77651453]
 [0.77651453 1.        ]]
cosine_similarity: 0.7765145304745156
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Dammnnnnnn that ASAP commercial for Adidas is sweet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.        ]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Rocky and john wall in that new quickaintfair Adidas commercial
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.         0.
  0.         0.        ]
 [0.         0.29017021 0.29017021 0.4078241  0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Rocky killed that Adidas commercial too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.         0.        ]
 [0.         0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That AAP Rocky Adidas commerical is hard af
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.
  0.        ]
 [0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That Adidas ASAP Rocky commercial dope af
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.         0.50154891 0.
  0.        ]
 [0.         0.31779954 0.44665616 0.44665616 0.31779954 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That Adidas commercial with ASAP Rocky goes hard
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.        ]
 [0.         0.31779954 0.44665616 0.31779954 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: The new Asap rocky Adidas commercial is fresh as shit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.         0.50154891 0.         0.
  0.         0.        ]
 [0.         0.29017021 0.4078241  0.29017021 0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
TF_IDF_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: and Adidas commercial that was legit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70490949 0.50154891 0.50154891 0.        ]
 [0.         0.50154891 0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: About to get amanda bynes nudes tatted on my forearm
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Can someone explain what the fuck happened to Amanda Bynes face
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.4090901  0.4090901  0.57496187 0.         0.4090901  0.4090901
  0.        ]
 [0.35464863 0.35464863 0.         0.49844628 0.35464863 0.35464863
  0.49844628]]
pairwise_similarity: [[1.         0.58033298]
 [0.58033298 1.        ]]
cosine_similarity: 0.5803329846765685
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Dude Amanda bynes is like cracked out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.         0.49922133
  0.49922133 0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.49922133 0.
  0.         0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: For the love of God someone please 5150 Amanda Bynes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.49922133 0.49922133 0.49922133
  0.         0.        ]
 [0.49922133 0.35520009 0.35520009 0.         0.         0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Hoes confused as to why everyone is infatuated with Amanda Bynes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.49922133 0.         0.         0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: If Amanda Bynes can make it through the year I think we all can
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Im going to call the police on Amanda Bynes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Is amanda bynes on crack or something
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]
 [0.50154891 0.50154891 0.70490949 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: wow what the hell has happened to amanda bynes
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.         0.49922133
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: Colorado Mandala is NOW AVAILABLE for purchase on Amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.
  0.6316672 ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: I am going to deliver the camera tomorrow when I purchased it in the Amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.
  0.6316672  0.        ]
 [0.         0.30321606 0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: I got it from amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.6316672 ]
 [0.         0.57973867 0.81480247 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: I hope to win a 50 Amazon Gift Code
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.         0.
  0.6316672  0.        ]
 [0.4261596  0.         0.30321606 0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: Learn to Publish Your Hot Selling eBooks to Amazon Kindle
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.
  0.         0.6316672  0.        ]
 [0.         0.27894255 0.39204401 0.39204401 0.39204401 0.39204401
  0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: May s prize on My Question of the Day is a 50 Amazon gift card
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.         0.
  0.         0.         0.6316672 ]
 [0.39204401 0.         0.27894255 0.39204401 0.39204401 0.39204401
  0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: Win a 25 Amazon Gift Card with MeBookshelfandI
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.         0.
  0.6316672  0.        ]
 [0.4261596  0.         0.30321606 0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: ahhh but did you know you can get this sort of stuff on amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.6316672
  0.         0.        ]
 [0.         0.4261596  0.30321606 0.4261596  0.4261596  0.
  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: check out the Yeti on Amazon it s not too pricey
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672  0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: According to Amazon s review, sentence2: just bought doom 3 for xbox off amazon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.6316672
  0.        ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Do not Amber Alert me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678]
 [0.70710678 0.70710678]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 0.9999999999999998
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Everyone s Amber Alerts going off at Zarape
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.        ]
 [0.         0.53404633 0.37997836 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I get scared when I get the amber alert messages
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.        ]
 [0.40993715 0.40993715 0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I wonder if the kidnapper gets amber alerts as well
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.         0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I would have put Amber in the top 2
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867]
 [0.         1.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Just found out Amber was eliminated
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.        ]
 [0.         0.44943642 0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Look Im tired if these damn Amber Alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.         0.
  0.        ]
 [0.         0.4261596  0.30321606 0.4261596  0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Plus Amber has a huge career
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: im going to miss Amber on idol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.         0.        ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber alert gave me a damn heart attack
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.57615236 0.        ]
 [0.31779954 0.31779954 0.         0.44665616 0.44665616 0.44665616
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber shouldnt have gone home
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.
  0.        ]
 [0.         0.37997836 0.         0.         0.53404633 0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Everyone keeps talking about amber alerts on their phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.         0.47107781
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: I get to see my Amber
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633]
 [0.         1.         0.         0.        ]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: I thought amber alerts would only sound on androids
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.         0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Ive gotten the same amber alert 3 times
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Ok this is the fifth amber alert Ive received
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236 0.
  0.         0.        ]
 [0.31779954 0.31779954 0.         0.44665616 0.         0.44665616
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Wats up with this amber alerts going off in church
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Why do I get amber alerts tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.        ]
 [0.         0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Amber is the cutest person ever
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]
 [0.         0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Everytime I get an amber alert on my phone I get freaked out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Pray for the Amber Alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236]
 [0.50154891 0.50154891 0.         0.70490949 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Am I the only one who dont get Amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633]
 [0.6316672  0.         0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: How long has Amber been missing because I keep getting alerts about her
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I aint got no amber alerts yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.57615236 0.         0.57615236]
 [0.57615236 0.40993715 0.40993715 0.         0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I hate the amber alert sound
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Just got the same amber alert three times today smh
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.53404633 0.         0.         0.        ]
 [0.39204401 0.         0.27894255 0.         0.39204401 0.39204401
  0.         0.39204401 0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: That amber alert was getting annoying
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.40993715 0.         0.57615236]
 [0.57615236 0.         0.40993715 0.40993715 0.57615236 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: These amber alert things scare me everytime they go off on my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.57615236 0.         0.40993715
  0.         0.        ]
 [0.44665616 0.         0.31779954 0.         0.44665616 0.31779954
  0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: These amber alerts on my phone always freak me out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.44832087]
 [0.44832087 0.44832087 0.         0.63009934 0.44832087]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Turns out it s just another amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: Does anybody else keep gettin this Amber Alert notification
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.         0.         0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.44943642]
 [0.44943642 1.        ]]
cosine_similarity: 0.4494364165239821
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: Husband s phone just had an Amber Alert Warning
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.         0.         0.         0.        ]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.44943642]
 [0.44943642 1.        ]]
cosine_similarity: 0.4494364165239821
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: I Know You Are Very Upset That Amber Went Home
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.         0.        ]
 [0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: If these amber alerts send me one more thing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.        ]
 [0.         0.53404633 0.37997836 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: OMG AMBER I FREAKING HATE YOU
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.57973867 0.         0.         0.        ]
 [0.         0.37997836 0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: Pray for the Amber Alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.70710678 0.        ]
 [0.50154891 0.50154891 0.70490949]]
pairwise_similarity: [[1.         0.70929727]
 [0.70929727 1.        ]]
cosine_similarity: 0.7092972666062738
TF_IDF_cosine_similarity: sentence1: What is an amber alert, sentence2: These Amber Alerts are really annoying me right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.81480247 0.         0.57973867 0.         0.         0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: Ive been getting amber alerts all day like what the fuck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.         0.         0.57615236]
 [0.29017021 0.29017021 0.         0.4078241  0.4078241  0.4078241
  0.4078241  0.4078241  0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: My phone NEVER sends me Amber Alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: That s 5 amber alerts today Ive got
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.57615236
  0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: These amber alert messages are scary as fudge man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.         0.53404633 0.        ]
 [0.4261596  0.         0.30321606 0.         0.4261596  0.4261596
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: Well that Amber Alert alert just scared tf outta me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.         0.         0.        ]
 [0.68574252 0.         0.24395573 0.         0.34287126 0.
  0.34287126 0.34287126 0.34287126]]
pairwise_similarity: [[1.        0.0926979]
 [0.0926979 1.       ]]
cosine_similarity: 0.09269789668627057
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: All these amber alerts leave people s kids alone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.         0.
  0.57615236]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I hate these damn amber alerts coming to my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.44832087 0.63009934 0.         0.         0.
  0.44832087]
 [0.33471228 0.33471228 0.         0.47042643 0.47042643 0.47042643
  0.33471228]]
pairwise_similarity: [[1.        0.4501755]
 [0.4501755 1.       ]]
cosine_similarity: 0.4501755023269898
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I just got like my 5th amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.53404633 0.37997836 0.53404633 0.
  0.         0.         0.53404633]
 [0.4261596  0.4261596  0.         0.30321606 0.         0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Omfg Who The FUCK Invented This Fucking Amber Alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.
  0.         0.         0.53404633]
 [0.4261596  0.         0.30321606 0.         0.4261596  0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: We all will miss our AMBER
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.53404633]
 [0.         0.57973867 0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: cx whats an amber alert thooo
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.37997836 0.53404633 0.         0.53404633
  0.         0.        ]
 [0.47107781 0.         0.33517574 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: what is this amber alert thing on my phone
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57615236 0.40993715 0.57615236 0.40993715 0.        ]
 [0.57615236 0.         0.40993715 0.         0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber gone be a modelsinger
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]
 [0.         0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Can we chill with the amber alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633]
 [0.         0.6316672  0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Dam amber alert i got to my phone scared the hell out of me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.         0.57615236 0.
  0.         0.         0.        ]
 [0.29017021 0.29017021 0.         0.4078241  0.         0.4078241
  0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Everyone s Amber Alerts going off at Zarape
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.        ]
 [0.         0.53404633 0.37997836 0.         0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: How come I never get amber alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633]
 [0.         0.6316672  0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: OH MY GOD FUCK THESE AMBER ALERTS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: That s 5 amber alerts today Ive got
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.53404633 0.
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.         0.47107781
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: These amber alerts on my phone been going off ALL day
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: how would you not know what an amber alert is
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.57615236 0.57615236 0.        ]
 [0.50154891 0.50154891 0.         0.         0.70490949]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: I need a amber alert to wake tf up in da mornings Ha
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.6316672  0.
  0.         0.         0.         0.        ]
 [0.36499647 0.         0.25969799 0.36499647 0.         0.36499647
  0.36499647 0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: I woke up to that Amber Alert I got
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.6316672  0.         0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: Ive had like 3 Amber Alerts today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: My phone keeps sending me amber alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: My phone started ringing and vibrated in class today for a amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.6316672  0.
  0.         0.         0.         0.        ]
 [0.36499647 0.         0.25969799 0.36499647 0.         0.36499647
  0.36499647 0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: That amber alert on my phone be scaring the shit out of me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.6316672  0.         0.
  0.        ]
 [0.47107781 0.         0.33517574 0.         0.47107781 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: This Amber alert keep cuttin my songs off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.44943642 0.         0.6316672  0.        ]
 [0.53404633 0.         0.37997836 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: Who is Amber and why does she keep sending me these alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
TF_IDF_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: there has been alot of amber alerts today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.70490949 0.        ]
 [0.40993715 0.57615236 0.40993715 0.         0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: Amber Alerts to my iPhone scare me EVERYTIME
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.6316672  0.         0.
  0.        ]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I aint got no amber alerts yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.6316672  0.         0.44943642 0.6316672  0.        ]
 [0.53404633 0.         0.53404633 0.37997836 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I have gotten 3 amber alert notices today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.70490949 0.         0.         0.        ]
 [0.35520009 0.35520009 0.         0.49922133 0.49922133 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I loved Amber the whole season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.6316672  0.         0.        ]
 [0.         0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: If I get one more of these loud ass amber alerts
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.6316672  0.        ]
 [0.         0.53404633 0.37997836 0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: One more Amber Alert today Im throwing a bitch fit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.
  0.         0.        ]
 [0.29017021 0.29017021 0.4078241  0.         0.4078241  0.4078241
  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.        0.2910691]
 [0.2910691 1.       ]]
cosine_similarity: 0.2910691023819054
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: That amber alert just scared the beep outta me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.         0.
  0.        ]
 [0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
TF_IDF_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: This is the 4th time Ive gotten that amber alert
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.50154891 0.70490949 0.         0.
  0.        ]
 [0.44665616 0.31779954 0.31779954 0.         0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller is a liability on defense
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.         0.44665616
  0.31779954 0.44665616]
 [0.40993715 0.         0.57615236 0.         0.57615236 0.
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller is even slower in person
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.44665616 0.31779954 0.
  0.44665616 0.        ]
 [0.40993715 0.         0.         0.         0.40993715 0.57615236
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller wit dat old man game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.33471228 0.47042643 0.
  0.33471228 0.         0.47042643 0.        ]
 [0.30287281 0.         0.42567716 0.30287281 0.         0.42567716
  0.30287281 0.42567716 0.         0.42567716]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller you a bitch shut up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.44665616 0.31779954
  0.44665616 0.        ]
 [0.40993715 0.         0.57615236 0.         0.         0.40993715
  0.         0.57615236]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: How does Andre miller still move this fast
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.         0.44665616 0.44665616
  0.31779954 0.44665616]
 [0.40993715 0.         0.57615236 0.57615236 0.         0.
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: The fact that Andre miller is consistent really pisses me off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.         0.44665616 0.44665616
  0.31779954 0.44665616 0.         0.        ]
 [0.31779954 0.         0.44665616 0.44665616 0.         0.
  0.31779954 0.         0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Warriors need a vet like Andre Miller on they team
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.         0.44665616 0.31779954
  0.         0.44665616 0.         0.         0.        ]
 [0.29017021 0.         0.         0.4078241  0.         0.29017021
  0.4078241  0.         0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.18443192]
 [0.18443192 1.        ]]
cosine_similarity: 0.18443191662261305
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Why is Andre Miller taking 3 s with his broke jumper
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.         0.44665616
  0.31779954 0.44665616 0.        ]
 [0.35520009 0.         0.49922133 0.         0.49922133 0.
  0.35520009 0.         0.49922133]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
TF_IDF_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: andre Miller aint never had no jumper
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.31779954 0.44665616 0.44665616 0.         0.44665616
  0.31779954 0.44665616]
 [0.57615236 0.40993715 0.         0.         0.57615236 0.
  0.40993715 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut about to die on the court
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut bitch ass goin off
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.         0.35520009 0.49922133 0.49922133
  0.         0.49922133]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.         0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut is playing like ah 1 Overall Draft Pick
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.35520009 0.         0.49922133 0.49922133
  0.         0.         0.         0.49922133 0.        ]
 [0.37762778 0.26868528 0.26868528 0.37762778 0.         0.
  0.37762778 0.37762778 0.37762778 0.         0.37762778]]
pairwise_similarity: [[1.         0.19087407]
 [0.19087407 1.        ]]
cosine_similarity: 0.1908740661302035
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut just went behind his back
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.        ]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut keeping Golden State in the game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.53309782 0.37930349 0.         0.
  0.53309782 0.        ]
 [0.33471228 0.33471228 0.         0.33471228 0.47042643 0.47042643
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut looking like he did at the University of Utah
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.         0.49922133 0.         0.        ]
 [0.29017021 0.29017021 0.4078241  0.         0.         0.4078241
  0.4078241  0.         0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Is Andrew Bogut rocking the stealth mullet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.        ]
 [0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.49922133]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Literally nobody on Earth likes Andrew Bogut
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.         0.49922133]
 [0.35520009 0.35520009 0.49922133 0.         0.         0.49922133
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: So is Andrew Bogut juicing or
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133]
 [0.50154891 0.50154891 0.         0.         0.70490949 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Steph curry looks sick but Andrew Bogut is playing like an all star quietly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.         0.49922133 0.         0.         0.         0.
  0.        ]
 [0.23700504 0.23700504 0.33310232 0.         0.         0.33310232
  0.33310232 0.         0.33310232 0.33310232 0.33310232 0.33310232
  0.33310232]]
pairwise_similarity: [[1.         0.16836842]
 [0.16836842 1.        ]]
cosine_similarity: 0.16836842163679844
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Asik showed heart going to the line and knocking some of the FT s down
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.         0.         0.         0.
  0.57615236 0.40993715 0.57615236]
 [0.29017021 0.4078241  0.4078241  0.4078241  0.4078241  0.4078241
  0.         0.29017021 0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: But hacking asik andforcing awful shots
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.         0.53404633 0.
  0.53404633 0.53404633]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.         0.47107781
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Omer Asik came through big tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.53404633 0.        ]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.         0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Scott Brooks was worse with his hackAsik
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5 0.  0.  0.5 0.  0.5 0.5 0. ]
 [0.  0.5 0.5 0.  0.5 0.  0.  0.5]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: So much for HackaAsik
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5 0.  0.5 0.5 0.5]
 [0.  1.  0.  0.  0. ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: That hack of Asik shit didnt work out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.53404633 0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: especially when Asik is out of the game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.53404633]
 [0.44943642 0.6316672  0.6316672  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: they have been fouling asik since about the 6th min mark
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.         0.         0.53404633
  0.53404633 0.53404633]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.47107781 0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: what to do with Asik then
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633]
 [1.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom goes down in warmup and the wild turn to josh Harding
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.47042643 0.         0.33471228 0.47042643 0.
  0.47042643 0.         0.33471228 0.        ]
 [0.30287281 0.         0.42567716 0.30287281 0.         0.42567716
  0.         0.42567716 0.30287281 0.42567716]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom is out already good omen for the Hawks
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.         0.4261596  0.         0.4261596
  0.         0.4261596  0.4261596 ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom would get hurt the day we start playoffs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.44665616 0.         0.44665616
  0.         0.31779954 0.44665616]
 [0.35520009 0.49922133 0.         0.         0.49922133 0.
  0.49922133 0.35520009 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Lol so Backstrom got hurt in warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.         0.4261596  0.         0.4261596
  0.         0.4261596  0.         0.         0.4261596 ]
 [0.30321606 0.         0.4261596  0.         0.4261596  0.
  0.4261596  0.         0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.09193998]
 [0.09193998 1.        ]]
cosine_similarity: 0.09193998174078082
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wishing Backstrom a speedy recovery
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.4261596  0.         0.
  0.4261596  0.4261596  0.        ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wow backstrom would get hurt in warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.         0.4261596  0.4261596
  0.4261596  0.         0.        ]
 [0.37997836 0.         0.         0.53404633 0.         0.
  0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: backstrom just got hurt in the warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.         0.44665616
  0.         0.44665616 0.31779954]
 [0.35520009 0.         0.49922133 0.         0.49922133 0.
  0.49922133 0.         0.35520009]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
TF_IDF_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: lose Backstrom in the warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.4261596  0.         0.4261596
  0.         0.         0.4261596 ]
 [0.37997836 0.         0.         0.         0.53404633 0.
  0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom Heatley and Pominville out for Minny
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.         0.47107781 0.47107781 0.
  0.         0.47107781]
 [0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom apparently suffered an injury in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.35520009]
 [0.49922133 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.35520009]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom out in warm ups for the wild
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.47107781 0.         0.
  0.47107781 0.        ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: If Backstrom is out can I pick the Blackhawks to win in two
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.47107781 0.47107781 0.
  0.47107781 0.        ]
 [0.37997836 0.53404633 0.         0.         0.         0.53404633
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Looks like Backstrom hurt himself in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.53309782 0.37930349 0.53309782 0.         0.
  0.37930349]
 [0.37930349 0.         0.37930349 0.         0.53309782 0.53309782
  0.37930349]]
pairwise_similarity: [[1.         0.43161342]
 [0.43161342 1.        ]]
cosine_similarity: 0.43161341897075145
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Niklas Backstrom potentially got hurt during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37930349 0.37930349 0.37930349 0.53309782 0.         0.
  0.53309782 0.        ]
 [0.33471228 0.33471228 0.33471228 0.         0.47042643 0.47042643
  0.         0.47042643]]
pairwise_similarity: [[1.         0.38087261]
 [0.38087261 1.        ]]
cosine_similarity: 0.38087260847594373
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: and now Backstrom is injured in the warmups timmywhitehead
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.47107781 0.        ]
 [0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: backstrom was just injured in warmups for the
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.49922133 0.         0.35520009 0.49922133
  0.        ]
 [0.40993715 0.         0.         0.57615236 0.40993715 0.
  0.57615236]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom gets hurt in warm ups for the wild
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.         0.40993715 0.57615236 0.         0.
  0.57615236 0.        ]
 [0.31779954 0.44665616 0.31779954 0.         0.44665616 0.44665616
  0.         0.44665616]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom hurt and helped to the back during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.44832087 0.63009934 0.44832087]
 [0.44832087 0.63009934 0.44832087 0.         0.44832087]]
pairwise_similarity: [[1.         0.60297482]
 [0.60297482 1.        ]]
cosine_similarity: 0.6029748160380572
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom s out for MIN
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.53404633]
 [0.57973867 0.         0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Harding in net for Backstrom
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.53404633 0.         0.53404633]
 [0.44943642 0.6316672  0.         0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Lmao they had to help Backstrom off the ice
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.53404633]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: WHAT THE HECK BACKSTROM GOT HURT IN WARMUPS
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44832087 0.         0.         0.44832087 0.63009934 0.44832087]
 [0.37930349 0.53309782 0.53309782 0.37930349 0.         0.37930349]]
pairwise_similarity: [[1.         0.51014902]
 [0.51014902 1.        ]]
cosine_similarity: 0.5101490193104813
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Yikes Backstrom might be injured
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.53404633 0.53404633 0.        ]
 [0.44943642 0.         0.6316672  0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: you see backstrom is out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633]
 [1.         0.         0.         0.        ]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Also hate that Backstrom went down in the warm up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.        ]
 [0.37997836 0.53404633 0.         0.53404633 0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Backstrom is out we lose
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672 ]
 [0.57973867 0.         0.81480247 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Backstrom just went down during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.50154891 0.        ]
 [0.40993715 0.         0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Hopefully Backstrom is back soon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672 ]
 [0.44943642 0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Man if Backstrom is out that s huuuuge
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Neve mind Niklas Backstrom s hurt
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.         0.         0.70490949]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Omg backstrom injured in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.6316672 ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wild Backstrom injured during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.50154891 0.        ]
 [0.40993715 0.         0.57615236 0.40993715 0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
TF_IDF_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wild in playoffs Backstrom injured in pregame warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.         0.         0.50154891
  0.        ]
 [0.31779954 0.         0.44665616 0.44665616 0.44665616 0.31779954
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom hurt in warn ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.50154891 0.         0.70490949 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom injured himself in the warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.50154891]
 [0.50154891 0.         0.70490949 0.50154891]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom was injured in warm up for Minnesota
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.6316672 ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom was injured warming up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Hurt himself during warm ups tough break for backstrom
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.         0.
  0.70490949]
 [0.31779954 0.44665616 0.31779954 0.44665616 0.44665616 0.44665616
  0.        ]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: If you didnt know Backstrom inexplicably hurt himself in warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.         0.57735027 0.         0.         0.57735027]
 [0.33471228 0.47042643 0.33471228 0.47042643 0.47042643 0.33471228]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376658
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Minnesota Wild goalie Backstrom just got injured in warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.
  0.         0.         0.         0.6316672  0.        ]
 [0.24395573 0.34287126 0.34287126 0.         0.34287126 0.34287126
  0.34287126 0.34287126 0.34287126 0.         0.34287126]]
pairwise_similarity: [[1.         0.10964259]
 [0.10964259 1.        ]]
cosine_similarity: 0.10964258683453854
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: OMG Backstrom is already injured
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.44943642 0.         0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: That sucks Nicklas Backstrom from the wild got hurt in warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.         0.
  0.         0.70490949 0.        ]
 [0.26868528 0.37762778 0.26868528 0.37762778 0.37762778 0.37762778
  0.37762778 0.         0.37762778]]
pairwise_similarity: [[1.         0.26951761]
 [0.26951761 1.        ]]
cosine_similarity: 0.26951761324603224
TF_IDF_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: When you hear Nicolas backstrom is injured
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.6316672 ]
 [0.37997836 0.53404633 0.         0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Backstrom for Min is hurt in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.31779954 0.         0.44665616
  0.         0.44665616]
 [0.40993715 0.         0.         0.40993715 0.57615236 0.
  0.57615236 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Looks like Backstrom injured in warmup
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.4261596  0.         0.
  0.         0.4261596  0.         0.4261596 ]
 [0.33517574 0.         0.         0.         0.47107781 0.47107781
  0.47107781 0.         0.47107781 0.        ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Maybe with backstrom injured the D will step up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.4261596  0.4261596  0.4261596  0.         0.
  0.4261596  0.         0.4261596 ]
 [0.37997836 0.         0.         0.         0.53404633 0.53404633
  0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Minnesota goalie Backstrom hurt in warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.         0.44665616 0.31779954 0.
  0.44665616 0.         0.         0.44665616]
 [0.31779954 0.         0.44665616 0.         0.31779954 0.44665616
  0.         0.44665616 0.44665616 0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Nikalas backstrom hurt during warm ups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.44665616 0.44665616 0.31779954 0.         0.44665616
  0.         0.         0.44665616]
 [0.35520009 0.         0.         0.35520009 0.49922133 0.
  0.49922133 0.49922133 0.        ]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Of course Backstrom would get injured during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.44665616 0.44665616 0.
  0.44665616 0.31779954]
 [0.40993715 0.57615236 0.         0.         0.         0.57615236
  0.         0.40993715]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Tell flynnkatie Backstrom got injured during warmups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.44665616 0.44665616
  0.         0.44665616 0.         0.31779954]
 [0.31779954 0.44665616 0.         0.44665616 0.         0.
  0.44665616 0.         0.44665616 0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: well first injury of the playoffs goes to backstrom in warmups of the first game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.44665616 0.         0.44665616 0.44665616
  0.         0.         0.44665616 0.31779954]
 [0.31779954 0.44665616 0.         0.44665616 0.         0.
  0.44665616 0.44665616 0.         0.31779954]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Just lost Backstrom in warmup, sentence2: Backstrom gets hurt in warm ups for the wild
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.
  0.         0.53404633 0.        ]
 [0.30321606 0.4261596  0.4261596  0.         0.         0.4261596
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: The last rap battle in 8 Mile nevr gets old ahah
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.         0.         0.53404633 0.37997836
  0.         0.         0.         0.53404633]
 [0.39204401 0.         0.39204401 0.39204401 0.         0.27894255
  0.39204401 0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
TF_IDF_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: The rap battle at the end of 8 mile gets me so hype
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.         0.         0.53404633 0.
  0.37997836 0.         0.53404633]
 [0.         0.4261596  0.4261596  0.4261596  0.         0.4261596
  0.30321606 0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: Rabbit on 8 mile out of place but determined to make it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.53404633 0.         0.37997836 0.53404633
  0.         0.        ]
 [0.47107781 0.         0.         0.47107781 0.33517574 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: See 8 Mile is always on but it s the tv version so it s gay
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.53404633 0.         0.37997836 0.53404633 0.
  0.        ]
 [0.         0.         0.53404633 0.37997836 0.         0.53404633
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: Those last 3 battles in 8 Mile are THE shit
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.53404633 0.37997836 0.53404633 0.        ]
 [0.6316672  0.         0.         0.44943642 0.         0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: 0 and 8 mile on at the same time, sentence2: It s just rap lyrics from the movie 8 mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57973867 0.         0.         0.81480247]
 [0.47107781 0.47107781 0.33517574 0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
TF_IDF_cosine_similarity: sentence1: 0 and 8 mile on at the same time, sentence2: I will never get tired of 8 Mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.        ]
 [0.57973867 0.         0.81480247]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762574
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: In 8 Mile in a scene the background music is Sweet Home Alabama
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         1.         0.         0.
  0.        ]
 [0.39204401 0.39204401 0.39204401 0.27894255 0.39204401 0.39204401
  0.39204401]]
pairwise_similarity: [[1.         0.27894255]
 [0.27894255 1.        ]]
cosine_similarity: 0.2789425453258252
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: 8 mile is on havent seen this movie in the longest
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         1.         0.         0.        ]
 [0.47107781 0.47107781 0.33517574 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.33517574]
 [0.33517574 1.        ]]
cosine_similarity: 0.33517574332792605
TF_IDF_cosine_similarity: sentence1: The last rap battle in 8 mile though, sentence2: But why were people watching the heat play when 8 mile is on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.6316672
  0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: The last rap battle in 8 mile though, sentence2: I think everyone is watching 8 mile Rn
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.6316672  0.         0.         0.        ]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: 8 mile is just a classic, sentence2: 8 mile that movie I love eminem in this movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.6316672  0.         0.44943642 0.        ]
 [0.         0.39204401 0.         0.39204401 0.27894255 0.78408803]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Listened to the final rap battle from 8 mile and now Im watching the whole movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.         1.         0.
  0.         0.        ]
 [0.36499647 0.36499647 0.36499647 0.36499647 0.25969799 0.36499647
  0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.25969799]
 [0.25969799 1.        ]]
cosine_similarity: 0.25969799324016246
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: 8 mile has been that movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[1.         0.        ]
 [0.57973867 0.81480247]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: I really did miss my part on 8 Mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         1.         0.         0.        ]
 [0.53404633 0.37997836 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.37997836]
 [0.37997836 1.        ]]
cosine_similarity: 0.37997836159100784
TF_IDF_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Lose Yourself is the perfect song to end 8 mile on
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         1.         0.         0.        ]
 [0.47107781 0.47107781 0.33517574 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.33517574]
 [0.33517574 1.        ]]
cosine_similarity: 0.33517574332792605
TF_IDF_cosine_similarity: sentence1: 8 mile is on friday made, sentence2: I missed the best part of 8 mile
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.81480247 0.57973867 0.        ]
 [0.6316672  0.         0.44943642 0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: 8 mile is on friday made, sentence2: 8 mile is such an awesome movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.81480247 0.57973867 0.        ]
 [0.6316672  0.         0.44943642 0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: Eminem s rap in the final battle of 8 Mile gets me pumped every time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.         0.37997836 0.53404633
  0.53404633 0.         0.         0.53404633 0.        ]
 [0.36499647 0.36499647 0.36499647 0.36499647 0.25969799 0.
  0.         0.36499647 0.36499647 0.         0.36499647]]
pairwise_similarity: [[1.         0.09867962]
 [0.09867962 1.        ]]
cosine_similarity: 0.09867961797986956
TF_IDF_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: While yall argue about the game 8 mile is on MTV
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37997836 0.         0.53404633 0.53404633
  0.53404633 0.        ]
 [0.47107781 0.47107781 0.33517574 0.47107781 0.         0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: The end of 8 Mile makes me so happy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.57615236 0.         0.         0.40993715 0.57615236]
 [0.40993715 0.         0.57615236 0.57615236 0.40993715 0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
TF_IDF_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: The last 3 rap battles in 8 mile always get me hyped af
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.53404633 0.53404633 0.         0.37997836
  0.53404633 0.        ]
 [0.47107781 0.47107781 0.         0.         0.47107781 0.33517574
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: I always get the movies 8 mile and green mile mixed up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.53404633 0.         0.37997836 0.         0.
  0.53404633]
 [0.         0.         0.44610081 0.6348088  0.44610081 0.44610081
  0.        ]]
pairwise_similarity: [[1.         0.24121361]
 [0.24121361 1.        ]]
cosine_similarity: 0.24121360667506986
TF_IDF_cosine_similarity: sentence1: Let s go see after earth, sentence2: Did After Earth already come out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.57973867 0.81480247]
 [0.6316672  0.6316672  0.44943642 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Let s go see after earth, sentence2: idk I think we might go see after earth later if we do you wana go
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.81480247 0.         0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
TF_IDF_cosine_similarity: sentence1: Anyone trying to see After Earth sometime soon, sentence2: Me and my son went to see After Earth last night
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672  0.        ]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I could of told Will Smith that After Earth was going 2 crash
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.
  0.6316672 ]
 [0.47107781 0.33517574 0.47107781 0.47107781 0.         0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I knew After Earth would tank
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: After Earth finishes with 27 million
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: even if After Earth is a good movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: Ether to see after earth or fast 6
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.6316672 ]
 [0.44943642 0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: I need to watch after earth tho
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.6316672  0.         0.        ]
 [0.37997836 0.53404633 0.         0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: At the theater with the little about do watch After Earth
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.57615236 0.         0.40993715]
 [0.         0.40993715 0.57615236 0.         0.57615236 0.40993715]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
TF_IDF_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: Someone needs to come see after earth with me
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.53404633]
 [0.         0.6316672  0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: After Earth is a great ass movie
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.         0.53404633
  0.53404633]
 [0.         0.53404633 0.37997836 0.53404633 0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: Goin to see after earth with the fam
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672 ]
 [0.         0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: will smith s speech in after earth is so relevant
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.6316672  0.         0.         0.        ]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: Just got done eating chinese with the fam now ganna go see after earth
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.         0.44943642 0.         0.         0.
  0.         0.6316672  0.        ]
 [0.         0.39204401 0.27894255 0.39204401 0.39204401 0.39204401
  0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
TF_IDF_cosine_similarity: sentence1: Going to see after earth but, sentence2: After earth is out and I havent seen it yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.        ]
 [0.44943642 0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Who wants to take me to see After Earth, sentence2: the hangover 3 and after earth are both really good
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.81480247]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: I kinda wanna see After Earth as well
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.70490949 0.50154891]
 [0.50154891 0.70490949 0.         0.50154891]]
pairwise_similarity: [[1.         0.50310261]
 [0.50310261 1.        ]]
cosine_similarity: 0.5031026124151314
TF_IDF_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: NOW YOU SEE ME and AFTER EARTH Cant Outpace FAST FURIOUS 6
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: After Earth 039 trumped by 039 Now You See Me 039 as 039 Fast
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.         0.6316672 ]
 [0.92982421 0.1653944  0.23245605 0.         0.23245605 0.        ]]
pairwise_similarity: [[1.         0.07433426]
 [0.07433426 1.        ]]
cosine_similarity: 0.07433426458775617
TF_IDF_cosine_similarity: sentence1: the US and belgium are tied at half, sentence2: Come on Romelu get some goals for Belgium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: the US and belgium are tied at half, sentence2: Mirallas with a soft and cool finish off the rebound to put Belgium up 10
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.
  0.         0.         0.6316672 ]
 [0.39204401 0.27894255 0.39204401 0.39204401 0.         0.39204401
  0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
TF_IDF_cosine_similarity: sentence1: Dude Belgium is freakin staked, sentence2: US vs Belgium or the wings
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.53404633 0.         0.        ]
 [0.44943642 0.         0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Dude Belgium is freakin staked, sentence2: Belgium almost take the lead in the 27th min
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.53404633 0.         0.
  0.53404633]
 [0.53404633 0.37997836 0.         0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: What kind of formation Belgium playing there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.53404633]
 [0.37997836 0.53404633 0.         0.53404633 0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: At half US 1 Belgium 1 Indians 5
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.         0.         0.53404633 0.53404633]
 [0.44943642 0.         0.6316672  0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: Who s in goal for Belgium for the USMNT friendly
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.53404633
  0.        ]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Vernaelen always gets injured for Belgium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.53404633]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: 6th minute Belgium with the score
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.         0.53404633 0.53404633
  0.53404633]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Belgium in a friendly instead
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.53404633]
 [0.44943642 0.6316672  0.6316672  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: The Belgium GK wasnt trying to concede another goal
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.30321606 0.         0.         0.         0.4261596  0.4261596
  0.         0.4261596  0.4261596  0.         0.4261596 ]
 [0.30321606 0.4261596  0.4261596  0.4261596  0.         0.
  0.4261596  0.         0.         0.4261596  0.        ]]
pairwise_similarity: [[1.         0.09193998]
 [0.09193998 1.        ]]
cosine_similarity: 0.09193998174078082
TF_IDF_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: that being said US 21 Belgium
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.30321606 0.         0.4261596  0.4261596  0.4261596
  0.4261596  0.4261596 ]
 [0.6316672  0.44943642 0.6316672  0.         0.         0.
  0.         0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: what s up with the nonHD main camera at the USBelgium soccer game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.5        0.         0.5        0.         0.5        0.
  0.         0.         0.         0.5       ]
 [0.         0.40824829 0.         0.40824829 0.         0.40824829
  0.40824829 0.40824829 0.40824829 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: Belgium vs USA you watching
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633]
 [0.37997836 0.         0.         0.53404633 0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: Belgium has a great team, sentence2: a little late to the USABelgium game
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.         0.57735027 0.         0.         0.57735027
  0.        ]
 [0.         0.5        0.         0.5        0.5        0.
  0.5       ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: Belgium is playing 4 centerbacks and fellani and they concede on a set piece
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.
  0.         0.         0.6316672 ]
 [0.27894255 0.39204401 0.39204401 0.39204401 0.         0.39204401
  0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
TF_IDF_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: In other news this Belgium squad taking on USMNT is STACKED
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.70490949 0.         0.50154891 0.         0.
  0.        ]
 [0.31779954 0.         0.44665616 0.31779954 0.44665616 0.44665616
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
TF_IDF_cosine_similarity: sentence1: Rafa Benitez is still a massive prick, sentence2: Mourinho to city Benitez to stay at Chelsea
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.         0.53404633
  0.53404633 0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.47107781 0.
  0.         0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Rafa Benitez is still a massive prick, sentence2: Chelsea FC wouldnt get rid of Benitez now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.53404633
  0.         0.        ]
 [0.33517574 0.47107781 0.47107781 0.         0.         0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: I hope Chelsea fans are thoroughly embarrassed now with the way they treated Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.
  0.6316672  0.         0.         0.        ]
 [0.25969799 0.36499647 0.36499647 0.36499647 0.         0.36499647
  0.         0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
TF_IDF_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: Thank you for your tactics Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.6316672  0.         0.        ]
 [0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: Got alot of time for rafa Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.50154891 0.70490949 0.         0.50154891 0.        ]
 [0.49922133 0.35520009 0.         0.49922133 0.35520009 0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: Benitez is a sick manager, sentence2: Well done to Rafa Benitez a dignified man
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: congratulations to petr ech and rafa benitez, sentence2: Rafa Benitez a free agent
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.35520009]
 [0.57615236 0.40993715 0.         0.         0.57615236 0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: congratulations to petr ech and rafa benitez, sentence2: Credit where credits due to Rafa Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.49922133 0.         0.         0.49922133 0.49922133
  0.35520009]
 [0.40993715 0.         0.57615236 0.57615236 0.         0.
  0.40993715]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: God forbid lyknx Rafa Benitez, sentence2: How can chelsea fans still hate benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.         0.47107781 0.47107781 0.
  0.47107781 0.47107781]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Thank you very much Rafa Benitez, sentence2: Why do liverpool fans love benitez so much
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.53404633 0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: I am so happy for RAFA BENITEZ VictorMoses and Mikel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.         0.50154891 0.70490949 0.        ]
 [0.35520009 0.49922133 0.49922133 0.35520009 0.         0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: Pleased for Benitez hasnt deserved the stick he s got
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.         0.6316672  0.
  0.6316672 ]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.         0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: Once a red always a blue rafa Benitez we want you
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.70490949 0.        ]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Benitez is alright tho man fuck chelsea fans they suck asshole
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44943642 0.         0.         0.
  0.         0.6316672  0.         0.6316672  0.        ]
 [0.34287126 0.34287126 0.24395573 0.34287126 0.34287126 0.34287126
  0.34287126 0.         0.34287126 0.         0.34287126]]
pairwise_similarity: [[1.         0.10964259]
 [0.10964259 1.        ]]
cosine_similarity: 0.10964258683453854
TF_IDF_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Credit where credits due to Rafa Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.         0.50154891 0.70490949]
 [0.40993715 0.57615236 0.57615236 0.40993715 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.4112070550676187
TF_IDF_cosine_similarity: sentence1: Rafa Benitez deserves a hell of a thank you, sentence2: Any praise for Benitez from my Chelsea followers lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.         0.47107781 0.
  0.         0.47107781 0.47107781]
 [0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.47107781 0.         0.        ]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542778
TF_IDF_cosine_similarity: sentence1: I dont understand the hatred for Rafa Benitez, sentence2: Top 4 and a trophy and still they dont give any respect for Benitez
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.         0.
  0.49922133]
 [0.40993715 0.40993715 0.         0.         0.57615236 0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Bill Self to the Big 12
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.47107781 0.47107781 0.33517574 0.47107781
  0.        ]
 [0.6316672  0.         0.         0.         0.44943642 0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: The Big 12 just got a whole lot more interesting
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.47107781 0.47107781 0.33517574 0.47107781
  0.         0.         0.         0.        ]
 [0.4261596  0.         0.         0.         0.30321606 0.
  0.4261596  0.4261596  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
TF_IDF_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Just when you thought Bill Self wasnt going to own the Big 12 for another year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.47107781 0.47107781 0.33517574 0.47107781
  0.         0.         0.         0.         0.         0.        ]
 [0.36499647 0.         0.         0.         0.25969799 0.
  0.36499647 0.36499647 0.36499647 0.36499647 0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.08704447]
 [0.08704447 1.        ]]
cosine_similarity: 0.08704446792504217
TF_IDF_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Oklahoma in Houston also among the 10 SECBig 12 matchups
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.4472136  0.4472136  0.4472136  0.4472136
  0.4472136  0.         0.         0.         0.        ]
 [0.40824829 0.40824829 0.         0.         0.         0.
  0.         0.40824829 0.40824829 0.40824829 0.40824829]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: Did kU win the Big 12 Quidditch Championship
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33471228 0.33471228 0.33471228 0.         0.47042643 0.47042643
  0.         0.         0.         0.47042643]
 [0.30287281 0.30287281 0.30287281 0.42567716 0.         0.
  0.42567716 0.42567716 0.42567716 0.        ]]
pairwise_similarity: [[1.         0.30412574]
 [0.30412574 1.        ]]
cosine_similarity: 0.30412574187549346
TF_IDF_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: He can fuck up the Big 12 all he wants
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.31779954 0.44665616 0.         0.44665616 0.44665616
  0.         0.44665616]
 [0.40993715 0.40993715 0.         0.57615236 0.         0.
  0.57615236 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: So what if the Big 12 had 14 teams
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.31779954 0.         0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.44665616]
 [0.40993715 0.57615236 0.40993715 0.         0.         0.
  0.57615236 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Sorry to the rest of the big 12, sentence2: And Kansas once again will win the Big 12
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.57615236 0.57615236 0.        ]
 [0.40993715 0.40993715 0.57615236 0.         0.         0.57615236]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
TF_IDF_cosine_similarity: sentence1: The big 12 is about to be so stacked next year, sentence2: How many of those who were handing the Big 12 to Okla
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.57615236 0.57615236]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.        ]]
pairwise_similarity: [[1.         0.33609693]
 [0.33609693 1.        ]]
cosine_similarity: 0.3360969272762575
TF_IDF_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: UKBig 12 challenge officially announced today
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.         0.         0.53404633]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: 21 in Houston as part of Big 12SEC Challenge
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.         0.37997836 0.         0.
  0.53404633 0.53404633]
 [0.         0.47107781 0.47107781 0.33517574 0.47107781 0.47107781
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: No reason Kansas should lose a game in the big 12
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.57615236
  0.         0.57615236]
 [0.31779954 0.31779954 0.44665616 0.44665616 0.44665616 0.
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: The whole Big 12 but Okiestate in particular just lost their minds
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.40993715 0.40993715 0.         0.         0.         0.
  0.         0.57615236 0.57615236]
 [0.29017021 0.29017021 0.4078241  0.4078241  0.4078241  0.4078241
  0.4078241  0.         0.        ]]
pairwise_similarity: [[1.         0.23790309]
 [0.23790309 1.        ]]
cosine_similarity: 0.23790309463326234
TF_IDF_cosine_similarity: sentence1: Sorry to the rest of the big 12, sentence2: Texas Tech will play at Alabama in the SECBig 12 Basketball Challenge on Nov
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.         0.
  0.         0.53404633 0.         0.53404633 0.         0.        ]
 [0.24395573 0.34287126 0.34287126 0.         0.34287126 0.34287126
  0.34287126 0.         0.34287126 0.         0.34287126 0.34287126]]
pairwise_similarity: [[1.        0.0926979]
 [0.0926979 1.       ]]
cosine_similarity: 0.09269789668627057
TF_IDF_cosine_similarity: sentence1: UK part of the Big 12SEC Challenge, sentence2: There is NOOOO competition in Big 12 basketball
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.         0.37997836 0.53404633 0.
  0.         0.53404633]
 [0.47107781 0.         0.47107781 0.33517574 0.         0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: to win the big 12 in all three major sports in the same year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.49922133 0.49922133 0.49922133 0.
  0.         0.         0.        ]
 [0.31779954 0.31779954 0.         0.         0.         0.44665616
  0.44665616 0.44665616 0.44665616]]
pairwise_similarity: [[1.         0.22576485]
 [0.22576485 1.        ]]
cosine_similarity: 0.22576484600261604
TF_IDF_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: SECBig 12 Challenge in hoops has been announced to begin in 201314
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.         0.         0.47107781 0.
  0.47107781 0.47107781 0.         0.47107781 0.        ]
 [0.27894255 0.39204401 0.39204401 0.39204401 0.         0.39204401
  0.         0.         0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536718
TF_IDF_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: So Wiggins Is Settling For Playing In The Garbage Ass Big 12
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.49922133 0.         0.49922133
  0.49922133 0.         0.         0.        ]
 [0.29017021 0.4078241  0.29017021 0.         0.4078241  0.
  0.         0.4078241  0.4078241  0.4078241 ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
TF_IDF_cosine_similarity: sentence1: Just wish he wasnt in the Big 12, sentence2: Big 12 is gonna be exciting
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.         0.49922133 0.49922133
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.57615236 0.         0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: Just wish he wasnt in the Big 12, sentence2: the BIG 12 goes through LAWRENCE
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.         0.49922133
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.         0.57615236 0.
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: I just found out Marilyn Monroe has a full bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.70710678 0.         0.         0.         0.70710678]
 [0.5        0.         0.5        0.5        0.5        0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: Classic redhead with a natural bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.70710678 0.         0.         0.70710678]
 [0.5        0.5        0.         0.5        0.5        0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: Hahah I heard your dumbass woke up in a bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.         0.70710678 0.         0.70710678
  0.        ]
 [0.4472136  0.4472136  0.4472136  0.         0.4472136  0.
  0.4472136 ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: I dont have time for beating around the bush, sentence2: Honey has a brush with her non existent bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.53404633 0.         0.
  0.         0.53404633]
 [0.         0.47107781 0.33517574 0.         0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: How the hell could Obama kill more than Bush did in Iraq
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.
  0.6316672  0.        ]
 [0.30321606 0.         0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: george bush is never a truther
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.        ]
 [0.44943642 0.         0.6316672  0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: Im a fan of Clintons Pretty much despise Bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.
  0.6316672  0.        ]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.4261596
  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: He is worse than Bush, sentence2: It was under Bush it is now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247]
 [1.         0.        ]]
pairwise_similarity: [[1.         0.57973867]
 [0.57973867 1.        ]]
cosine_similarity: 0.5797386715376657
TF_IDF_cosine_similarity: sentence1: He is worse than Bush, sentence2: Did Obama and the Dems trust the Bush Government
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.
  0.81480247]
 [0.30321606 0.4261596  0.4261596  0.4261596  0.4261596  0.4261596
  0.        ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
TF_IDF_cosine_similarity: sentence1: He is worse than Bush, sentence2: that the Libs are going to say its Bush s fault and Im a racist
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.
  0.         0.81480247]
 [0.27894255 0.39204401 0.39204401 0.39204401 0.39204401 0.39204401
  0.39204401 0.        ]]
pairwise_similarity: [[1.         0.16171378]
 [0.16171378 1.        ]]
cosine_similarity: 0.16171378066252898
TF_IDF_cosine_similarity: sentence1: Then time to trim the rose bush, sentence2: I dint like it under Bush either Obama has radically expanded it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.         0.         0.
  0.53404633 0.53404633 0.53404633]
 [0.30321606 0.4261596  0.4261596  0.4261596  0.4261596  0.4261596
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: I want the Bush days back, sentence2: Dont beat around the bush just say it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.
  0.6316672 ]
 [0.47107781 0.33517574 0.         0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: I want the Bush days back, sentence2: fell in a spiky bush and I have a prickly thing in my finger
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.
  0.         0.6316672 ]
 [0.30321606 0.         0.4261596  0.4261596  0.4261596  0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: I want the Bush days back, sentence2: i once walked into a bush outside school and literally apologised to it
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.
  0.         0.6316672 ]
 [0.4261596  0.30321606 0.         0.4261596  0.4261596  0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: I want the Bush days back, sentence2: Bush wiretapped without warrants Obama had them
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: started under bush and Im sure you were cool with it then
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672  0.
  0.        ]
 [0.33517574 0.47107781 0.         0.47107781 0.         0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: The new Bush tour merchandise is now available
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672
  0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: we live near a bush reserve
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.6316672 ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: I did for Bush as well, sentence2: in my actual bush in a bush in Bushey
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.81480247]
 [0.49844628 0.70929727 0.49844628 0.        ]]
pairwise_similarity: [[1.         0.41120706]
 [0.41120706 1.        ]]
cosine_similarity: 0.41120705506761857
TF_IDF_cosine_similarity: sentence1: I did for Bush as well, sentence2: it started way before Bush Jr
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.        ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: Darling stop beating around the bush, sentence2: They called Bush hitler too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.         0.53404633]
 [0.         0.44943642 0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR, sentence2: Why cant everyone just tell it how it is instead of beating around the bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.2130798  0.         0.30321606 0.8523192  0.2130798  0.
  0.         0.2130798  0.2130798  0.        ]
 [0.         0.47107781 0.33517574 0.         0.         0.47107781
  0.47107781 0.         0.         0.47107781]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
TF_IDF_cosine_similarity: sentence1: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR, sentence2: And his legs can have hair but no bush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.2130798  0.30321606 0.8523192  0.2130798  0.         0.
  0.2130798  0.2130798 ]
 [0.         0.44943642 0.         0.         0.6316672  0.6316672
  0.         0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Allen Bosh Chalmers James at Wade
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44943642 0.6316672  0.         0.
  0.6316672 ]
 [0.47107781 0.47107781 0.33517574 0.         0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Chalmers makes me mad low key
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.
  0.6316672 ]
 [0.33517574 0.         0.47107781 0.47107781 0.47107781 0.47107781
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Chalmers steady chasing on defense
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.         0.6316672  0.         0.6316672 ]
 [0.37997836 0.53404633 0.53404633 0.         0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: I like Cole s onball defense better than Chalmers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.         0.6316672  0.
  0.         0.6316672 ]
 [0.4261596  0.30321606 0.4261596  0.4261596  0.         0.4261596
  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: And now chalmers with the scoop layup, sentence2: Anybody see David West Elbow the hell out of Chalmers bad shoulder
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.44943642 0.         0.         0.
  0.6316672  0.6316672  0.         0.        ]
 [0.36499647 0.36499647 0.25969799 0.36499647 0.36499647 0.36499647
  0.         0.         0.36499647 0.36499647]]
pairwise_similarity: [[1.         0.11671774]
 [0.11671774 1.        ]]
cosine_similarity: 0.11671773546032795
TF_IDF_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: MARIO CHALMERS JUST THREW IT TO HIS COACH HAHAHA
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.         0.         0.50154891 0.70490949
  0.        ]
 [0.31779954 0.44665616 0.44665616 0.44665616 0.31779954 0.
  0.44665616]]
pairwise_similarity: [[1.         0.31878402]
 [0.31878402 1.        ]]
cosine_similarity: 0.31878402175377923
TF_IDF_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Spo aint in the game chalmers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Mario Chalmers needs to get punched in the face
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.50154891 0.         0.50154891 0.         0.70490949 0.        ]
 [0.35520009 0.49922133 0.35520009 0.49922133 0.         0.49922133]]
pairwise_similarity: [[1.         0.35630043]
 [0.35630043 1.        ]]
cosine_similarity: 0.3563004293331381
TF_IDF_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Chalmers throws it out of bounds
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.        ]
 [0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Chalmers just walked with the ball, sentence2: Uhm Chalmers didnt just travel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.40993715 0.         0.
  0.57615236]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: Mario Chalmers looks like Mario, sentence2: Then Chalmers fucks up again lmao
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.27894255 0.         0.39204401 0.         0.39204401 0.78408803]
 [0.44943642 0.6316672  0.         0.6316672  0.         0.        ]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
TF_IDF_cosine_similarity: sentence1: Mario Chalmers looks like Mario, sentence2: Lucky ass shxt by Chalmers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.27894255 0.39204401 0.39204401 0.         0.78408803
  0.        ]
 [0.53404633 0.37997836 0.         0.         0.53404633 0.
  0.53404633]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
TF_IDF_cosine_similarity: sentence1: I hate Mario Chalmersdont know why, sentence2: idc idc chalmers be making me mad
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.5        0.5        0.         0.5        0.
  0.         0.5       ]
 [0.37796447 0.         0.         0.75592895 0.         0.37796447
  0.37796447 0.        ]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: I hate Mario Chalmersdont know why, sentence2: Why is Mario Chalmers starting
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.53404633 0.53404633 0.53404633 0.37997836 0.        ]
 [0.6316672  0.         0.         0.         0.44943642 0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: CHALMERS IS THE TURNOVER KING, sentence2: Didnt everyone love Mario Chalmers last year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.6316672
  0.        ]
 [0.33517574 0.47107781 0.         0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: LMFAOOO who tf you throwing to Chalmers
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: James with the block and then wade to James James to chalmers for the finish
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672
  0.        ]
 [0.2827721  0.20119468 0.         0.2827721  0.84831629 0.
  0.2827721 ]]
pairwise_similarity: [[1.         0.09042421]
 [0.09042421 1.        ]]
cosine_similarity: 0.09042421401858963
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: This is like the 27th time chalmers fucked up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672
  0.        ]
 [0.47107781 0.33517574 0.         0.47107781 0.47107781 0.
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: Chalmers would lock you down bubs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.6316672 ]
 [0.6316672  0.44943642 0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: Mario Chalmers is easily my worst favorite player in the NBA
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.         0.         0.
  0.         0.6316672  0.        ]
 [0.27894255 0.         0.39204401 0.39204401 0.39204401 0.39204401
  0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.12536694]
 [0.12536694 1.        ]]
cosine_similarity: 0.12536693798731732
TF_IDF_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: The fuck Chalmers is doing
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57735027 0.57735027 0.57735027]
 [0.57735027 0.57735027 0.57735027]]
pairwise_similarity: [[1. 1.]
 [1. 1.]]
cosine_similarity: 1.0000000000000002
TF_IDF_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: Do I watch the game or Chalmers face
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.6316672  0.         0.6316672  0.         0.        ]
 [0.37997836 0.         0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: Uhm Chalmers didnt just travel
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.6316672  0.         0.
  0.        ]
 [0.33517574 0.47107781 0.         0.         0.47107781 0.47107781
  0.47107781]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Chara get in that box, sentence2: Chara is dirtier than a Fresno adult film star
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.81480247 0.57973867 0.         0.         0.
  0.        ]
 [0.4261596  0.         0.30321606 0.4261596  0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
TF_IDF_cosine_similarity: sentence1: Orr should beat the shit out of chara, sentence2: What s better than seeing Chara getting dropped by Orr
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.         0.40993715 0.         0.         0.40993715
  0.         0.57615236]
 [0.         0.44665616 0.31779954 0.44665616 0.44665616 0.31779954
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: watching chara go down is the best feeling, sentence2: Chara s playing with hate in heart
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.
  0.53404633]
 [0.         0.37997836 0.         0.53404633 0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara is the most overrated player in the league
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.32433627 0.         0.32433627 0.32433627 0.
  0.         0.32433627 0.64867255 0.32433627]
 [0.37997836 0.         0.53404633 0.         0.         0.53404633
  0.53404633 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.08768682]
 [0.08768682 1.        ]]
cosine_similarity: 0.08768681980142445
TF_IDF_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara move your feet along the boards
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.23076793 0.         0.32433627 0.32433627 0.32433627
  0.32433627 0.64867255 0.32433627]
 [0.6316672  0.44943642 0.6316672  0.         0.         0.
  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.10371551]
 [0.10371551 1.        ]]
cosine_similarity: 0.10371551133313005
TF_IDF_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: FINALLY CHARA IS GIVEN A PENALTY
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.23076793 0.         0.         0.32433627 0.32433627 0.32433627
  0.         0.32433627 0.64867255 0.32433627]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.
  0.53404633 0.         0.         0.        ]]
pairwise_similarity: [[1.         0.08768682]
 [0.08768682 1.        ]]
cosine_similarity: 0.08768681980142445
TF_IDF_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Orr wants a piece of chara I swear
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.         0.         0.53404633
  0.         0.        ]
 [0.         0.33517574 0.         0.47107781 0.47107781 0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Okay who else saw that beauty hit to chara by orr
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.         0.         0.53404633]
 [0.         0.4261596  0.30321606 0.4261596  0.         0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: When anyone on the leafs knock down Chara I laugh so hard
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.         0.53404633 0.         0.
  0.         0.53404633]
 [0.         0.33517574 0.47107781 0.         0.47107781 0.47107781
  0.47107781 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Amazing how bad Chara looks when the speed of the game picks up
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.         0.37997836 0.         0.53404633
  0.         0.         0.53404633 0.        ]
 [0.         0.39204401 0.39204401 0.27894255 0.39204401 0.
  0.39204401 0.39204401 0.         0.39204401]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
TF_IDF_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: Why is chara playing like a bitch
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.47107781 0.33517574 0.47107781 0.         0.47107781
  0.47107781 0.        ]
 [0.53404633 0.         0.37997836 0.         0.53404633 0.
  0.         0.53404633]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: BOOOOOMM down went chara again
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.47107781 0.47107781 0.47107781
  0.        ]
 [0.         0.6316672  0.44943642 0.         0.         0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: Zideno Chara is a brick wall on skates
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.         0.33517574 0.47107781 0.47107781 0.47107781
  0.         0.         0.        ]
 [0.         0.47107781 0.33517574 0.         0.         0.
  0.47107781 0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.11234278]
 [0.11234278 1.        ]]
cosine_similarity: 0.11234277891542777
TF_IDF_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: Chara penalty on the play
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.37997836 0.53404633 0.53404633 0.         0.        ]
 [0.         0.44943642 0.         0.         0.6316672  0.6316672 ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: Chara called for a high stick on the play too
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.53404633 0.         0.37997836 0.         0.53404633 0.53404633
  0.         0.        ]
 [0.         0.47107781 0.33517574 0.47107781 0.         0.
  0.47107781 0.47107781]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: I keep waiting for the chara vs orr fight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.40993715 0.         0.57615236 0.40993715 0.
  0.        ]
 [0.         0.35520009 0.49922133 0.         0.35520009 0.49922133
  0.49922133]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: Chara is a disgrace to the NHL, sentence2: Chara is just a big goon
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.         0.         0.6316672 ]
 [0.53404633 0.37997836 0.         0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: but orr keeps pushing chara, sentence2: Imma blame Chara for Lupul missing that one
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.         0.
  0.53404633 0.53404633]
 [0.47107781 0.33517574 0.47107781 0.         0.47107781 0.47107781
  0.         0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: but orr keeps pushing chara, sentence2: Why is Chara allowed to take down people without the puck
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.53404633 0.53404633 0.         0.
  0.53404633]
 [0.53404633 0.37997836 0.         0.         0.53404633 0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: When do you plan to be at 600 W Chicago next
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.        ]
 [0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: Big game 7 here in Chicago tonight
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: I checked in at Chicago Park District
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.         0.6316672  0.6316672  0.        ]
 [0.53404633 0.37997836 0.53404633 0.         0.         0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: Nice new Guideshop in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.47107781 0.
  0.         0.47107781]
 [0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.53404633 0.        ]]
pairwise_similarity: [[1.         0.12735953]
 [0.12735953 1.        ]]
cosine_similarity: 0.1273595297947935
TF_IDF_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: The North side of Chicago is happy
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.47107781 0.
  0.47107781]
 [0.         0.44943642 0.         0.6316672  0.         0.6316672
  0.        ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: just watched season finale of chicago fire and cried
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.47107781 0.33517574 0.47107781 0.         0.         0.47107781
  0.         0.         0.47107781 0.        ]
 [0.         0.30321606 0.         0.4261596  0.4261596  0.
  0.4261596  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.10163067]
 [0.10163067 1.        ]]
cosine_similarity: 0.10163066979112656
TF_IDF_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: I really wanna do the color run in Chicago lol
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.81480247 0.         0.         0.
  0.        ]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.4261596
  0.4261596 ]]
pairwise_similarity: [[1.         0.17578608]
 [0.17578608 1.        ]]
cosine_similarity: 0.17578607839334617
TF_IDF_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: The ungeekedeliteschicago Daily is out
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.70710678 0.         0.70710678 0.        ]
 [0.         0.70710678 0.         0.70710678]]
pairwise_similarity: [[1. 0.]
 [0. 1.]]
cosine_similarity: 0.0
TF_IDF_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: I am watching Chicago Fire A Hell of a Ride
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.81480247 0.         0.         0.        ]
 [0.37997836 0.         0.53404633 0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: panoramic shot of noKXL and stop deportations rallies at Obama fundraiser in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.81480247 0.         0.         0.
  0.         0.         0.         0.        ]
 [0.24395573 0.34287126 0.         0.34287126 0.34287126 0.34287126
  0.34287126 0.34287126 0.34287126 0.34287126]]
pairwise_similarity: [[1.         0.14143057]
 [0.14143057 1.        ]]
cosine_similarity: 0.14143056792554487
TF_IDF_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: blogher is in chicago this year
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.33517574 0.47107781 0.47107781 0.47107781 0.47107781
  0.        ]
 [0.6316672  0.44943642 0.         0.         0.         0.
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: Wings in a must win in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.47107781 0.47107781 0.47107781 0.47107781 0.
  0.        ]
 [0.44943642 0.         0.         0.         0.         0.6316672
  0.6316672 ]]
pairwise_similarity: [[1.         0.15064018]
 [0.15064018 1.        ]]
cosine_similarity: 0.15064018498706508
TF_IDF_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: Id definitely be happy to help but I wont be back in Chicago until Sunday
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.33517574 0.         0.47107781 0.         0.         0.
  0.47107781 0.47107781 0.47107781 0.         0.        ]
 [0.27894255 0.39204401 0.         0.39204401 0.39204401 0.39204401
  0.         0.         0.         0.39204401 0.39204401]]
pairwise_similarity: [[1.         0.09349477]
 [0.09349477 1.        ]]
cosine_similarity: 0.09349477497536716
TF_IDF_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: By Micheline Maynard Contributor Chicago has
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.         0.6316672 ]
 [0.37997836 0.53404633 0.         0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: bro you in chicago widdit yet
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.44943642 0.6316672  0.6316672  0.        ]
 [0.6316672  0.44943642 0.         0.         0.6316672 ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: is L still looking for a copy of Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.44943642 0.         0.6316672  0.         0.6316672 ]
 [0.44943642 0.6316672  0.         0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: hope you and Hunter made it to Chicago this time
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.         0.53404633 0.53404633 0.
  0.53404633]
 [0.37997836 0.53404633 0.53404633 0.         0.         0.53404633
  0.        ]]
pairwise_similarity: [[1.         0.14438356]
 [0.14438356 1.        ]]
cosine_similarity: 0.1443835552773867
TF_IDF_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: Flying to chicago on the 14th
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.53404633 0.53404633]
 [0.6316672  0.44943642 0.6316672  0.         0.         0.        ]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Good week to be a Northsiderrr in Chicago, sentence2: go to north coast in chicago its way cheaper and the same weekend
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.37997836 0.         0.53404633 0.         0.53404633
  0.         0.53404633 0.        ]
 [0.4261596  0.30321606 0.4261596  0.         0.4261596  0.
  0.4261596  0.         0.4261596 ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: Good week to be a Northsiderrr in Chicago, sentence2: Leaving early tomorrow morning to go to the Hospital in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.         0.
  0.53404633 0.         0.53404633]
 [0.30321606 0.4261596  0.         0.4261596  0.4261596  0.4261596
  0.         0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: I live in South Ontario but have great friends in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.6316672
  0.         0.        ]
 [0.         0.30321606 0.4261596  0.4261596  0.4261596  0.
  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: did u really quit your job to go to edc chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.         0.6316672
  0.         0.        ]
 [0.         0.30321606 0.4261596  0.4261596  0.4261596  0.
  0.4261596  0.4261596 ]]
pairwise_similarity: [[1.         0.13627634]
 [0.13627634 1.        ]]
cosine_similarity: 0.1362763414390864
TF_IDF_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: youre in Chicago and were here to say nokxl
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.6316672  0.         0.        ]
 [0.         0.37997836 0.53404633 0.         0.53404633 0.53404633]]
pairwise_similarity: [[1.         0.17077611]
 [0.17077611 1.        ]]
cosine_similarity: 0.17077611319011649
TF_IDF_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: Night in Chicago with my ladies
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.6316672  0.44943642 0.         0.         0.6316672 ]
 [0.         0.44943642 0.6316672  0.6316672  0.        ]]
pairwise_similarity: [[1.         0.20199309]
 [0.20199309 1.        ]]
cosine_similarity: 0.20199309249791833
TF_IDF_cosine_similarity: sentence1: I cannot WAIT to go to Chicago, sentence2: Chicago is saying this todayEven nonhockey fans
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57973867 0.         0.         0.         0.         0.81480247]
 [0.33517574 0.47107781 0.47107781 0.47107781 0.47107781 0.        ]]
pairwise_similarity: [[1.         0.19431434]
 [0.19431434 1.        ]]
cosine_similarity: 0.19431434016858146
TF_IDF_cosine_similarity: sentence1: I cannot WAIT to go to Chicago, sentence2: It was a day of impulse buys in Chicago
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.57973867 0.         0.         0.81480247]
 [0.53404633 0.37997836 0.53404633 0.53404633 0.        ]]
pairwise_similarity: [[1.         0.22028815]
 [0.22028815 1.        ]]
cosine_similarity: 0.22028815056182965
TF_IDF_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: Just bought a ticket to Chicago for 2350 round trip
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.         0.37997836 0.         0.53404633 0.
  0.53404633 0.         0.         0.53404633]
 [0.39204401 0.39204401 0.27894255 0.39204401 0.         0.39204401
  0.         0.39204401 0.39204401 0.        ]]
pairwise_similarity: [[1.         0.10599213]
 [0.10599213 1.        ]]
cosine_similarity: 0.1059921313509325
TF_IDF_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: If Chicago does I can officially turn off the TV for the season
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.37997836 0.         0.53404633 0.         0.53404633 0.
  0.         0.         0.53404633]
 [0.30321606 0.4261596  0.         0.4261596  0.         0.4261596
  0.4261596  0.4261596  0.        ]]
pairwise_similarity: [[1.         0.11521554]
 [0.11521554 1.        ]]
cosine_similarity: 0.11521554337793122
TF_IDF_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: Is that Chris Davis out there
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.35520009 0.49922133 0.49922133]
 [0.         0.70710678 0.70710678 0.         0.        ]]
pairwise_similarity: [[1.         0.50232878]
 [0.50232878 1.        ]]
cosine_similarity: 0.5023287782256717
TF_IDF_cosine_similarity: sentence1: Wow Chris Davis is only 27, sentence2: Chris Davis actually reminds me of Hamilton at the plate
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.57615236 0.         0.40993715 0.40993715 0.         0.
  0.         0.57615236]
 [0.         0.44665616 0.31779954 0.31779954 0.44665616 0.44665616
  0.44665616 0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: So um Chris Davis you guys
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.49922133 0.49922133 0.
  0.49922133]
 [0.40993715 0.40993715 0.57615236 0.         0.         0.57615236
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: So happy Chris Davis is on my fantasy team
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.35520009 0.         0.         0.49922133 0.
  0.49922133 0.49922133]
 [0.35520009 0.35520009 0.49922133 0.49922133 0.         0.49922133
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: No other words but shut the front door Chris Davis is my not so secret crush
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.35520009 0.         0.35520009 0.         0.49922133 0.
  0.         0.49922133 0.         0.49922133]
 [0.29017021 0.4078241  0.29017021 0.4078241  0.         0.4078241
  0.4078241  0.         0.4078241  0.        ]]
pairwise_similarity: [[1.         0.20613697]
 [0.20613697 1.        ]]
cosine_similarity: 0.20613696606828605
TF_IDF_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: Is Chris Davis a top 5 hitter in baseball right now
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.57615236 0.
  0.57615236]
 [0.49922133 0.35520009 0.35520009 0.49922133 0.         0.49922133
  0.        ]]
pairwise_similarity: [[1.         0.29121942]
 [0.29121942 1.        ]]
cosine_similarity: 0.29121941856368966
TF_IDF_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: WHAT THE HELL DOES CHRIS DAVIS EAT FOR BREAKFAST
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.40993715 0.40993715 0.         0.         0.
  0.57615236 0.57615236]
 [0.44665616 0.31779954 0.31779954 0.44665616 0.44665616 0.44665616
  0.         0.        ]]
pairwise_similarity: [[1.         0.26055567]
 [0.26055567 1.        ]]
cosine_similarity: 0.2605556710562624
TF_IDF_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: Ayo smh Chris Davis TEACH ME
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.35520009 0.35520009 0.49922133 0.
  0.         0.49922133]
 [0.49922133 0.         0.35520009 0.35520009 0.         0.49922133
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: When is the Chris Davis ped suspension coming
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.49922133 0.35520009 0.         0.35520009 0.         0.49922133
  0.         0.49922133]
 [0.         0.35520009 0.49922133 0.35520009 0.49922133 0.
  0.49922133 0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
TF_IDF_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: Chris Davis is on pace for about 56 HRs
vect: TfidfVectorizer(stop_words='english'), tfidf: [[0.         0.49922133 0.35520009 0.35520009 0.         0.
  0.49922133 0.49922133]
 [0.49922133 0.         0.35520009 0.35520009 0.49922133 0.49922133
  0.         0.        ]]
pairwise_similarity: [[1.        0.2523342]
 [0.2523342 1.       ]]
cosine_similarity: 0.2523342014336961
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A Walk to Remember is on and Im in town and Im upset
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'on', 'and', 'im', 'in', 'town', 'and', 'im', 'upset']
cosine_similarity: 0.9772884845733643
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A Walk to Remember is the cutest thing
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'the', 'cutest', 'thing']
cosine_similarity: 0.9919882416725159
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A walk to remember is on ABC family youre welcome
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'on', 'abc', 'family', 'youre', 'welcome']
cosine_similarity: 0.9850156307220459
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: A walk to remember is so amazing and inspiring
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'so', 'amazing', 'and', 'inspiring']
cosine_similarity: 0.9902955293655396
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: BUT GUYS ITS ON MY FAVE PART OF A WALK TO REMEMBER
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['but', 'guys', 'its', 'on', 'my', 'fave', 'part', 'of', 'a', 'walk', 'to', 'remember']
cosine_similarity: 0.9889701008796692
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Day is made A Walk to Remember is on
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['day', 'is', 'made', 'a', 'walk', 'to', 'remember', 'is', 'on']
cosine_similarity: 0.9902157187461853
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: The only Nicholas Sparks movie I genuinely like is A Walk To Remember
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['the', 'only', 'nicholas', 'sparks', 'movie', 'i', 'genuinely', 'like', 'is', 'a', 'walk', 'to', 'remember']
cosine_similarity: 0.9909297227859497
word_to_vector_cosine_similarity: sentence1: A Walk to Remember is the definition of true love, sentence2: Watching A Walk To Remember for the millionth time and for the millionth time I will cry
After tokenization, sentence1: ['a', 'walk', 'to', 'remember', 'is', 'the', 'definition', 'of', 'true', 'love'], sentence2: ['watching', 'a', 'walk', 'to', 'remember', 'for', 'the', 'millionth', 'time', 'and', 'for', 'the', 'millionth', 'time', 'i', 'will', 'cry']
cosine_similarity: 0.9798288941383362
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: A Walk to Remember is on tv right now
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['a', 'walk', 'to', 'remember', 'is', 'on', 'tv', 'right', 'now']
cosine_similarity: 0.9940584897994995
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: I never even seen a walk to remember
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['i', 'never', 'even', 'seen', 'a', 'walk', 'to', 'remember']
cosine_similarity: 0.9818761944770813
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: I want a love like Jamie and Landon on A Walk To Remember
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['i', 'want', 'a', 'love', 'like', 'jamie', 'and', 'landon', 'on', 'a', 'walk', 'to', 'remember']
cosine_similarity: 0.9859643578529358
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: THE GUY IN A WALK TO REMEMBER IS SO CUTE IM PISSING
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['the', 'guy', 'in', 'a', 'walk', 'to', 'remember', 'is', 'so', 'cute', 'im', 'pissing']
cosine_similarity: 0.9813199639320374
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: The part on A Walk To Remember when they are looking at the stars
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['the', 'part', 'on', 'a', 'walk', 'to', 'remember', 'when', 'they', 'are', 'looking', 'at', 'the', 'stars']
cosine_similarity: 0.9928199648857117
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: Turned on the tv and A Walk to Remember is on
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['turned', 'on', 'the', 'tv', 'and', 'a', 'walk', 'to', 'remember', 'is', 'on']
cosine_similarity: 0.9940493106842041
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: Watching A Walk To Remember is seriously making me ball right now it s so perfect
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['watching', 'a', 'walk', 'to', 'remember', 'is', 'seriously', 'making', 'me', 'ball', 'right', 'now', 'it', 's', 'so', 'perfect']
cosine_similarity: 0.9878873229026794
word_to_vector_cosine_similarity: sentence1: On the real I like the movie A Walk to Remember, sentence2: When a walk to remember is on tv
After tokenization, sentence1: ['on', 'the', 'real', 'i', 'like', 'the', 'movie', 'a', 'walk', 'to', 'remember'], sentence2: ['when', 'a', 'walk', 'to', 'remember', 'is', 'on', 'tv']
cosine_similarity: 0.9957384467124939
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: AAP in that Adidas Commercial lol
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['aap', 'in', 'that', 'adidas', 'commercial', 'lol']
cosine_similarity: 0.9831011295318604
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: AAP was nice on that Adidas commercial
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['aap', 'was', 'nice', 'on', 'that', 'adidas', 'commercial']
cosine_similarity: 0.9807596206665039
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Dammnnnnnn that ASAP commercial for Adidas is sweet
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['that', 'asap', 'commercial', 'for', 'adidas', 'is', 'sweet']
cosine_similarity: 0.9391748905181885
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Rocky and john wall in that new quickaintfair Adidas commercial
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['rocky', 'and', 'john', 'wall', 'in', 'that', 'new', 'quickaintfair', 'adidas', 'commercial']
cosine_similarity: 0.9532425999641418
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: Rocky killed that Adidas commercial too
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['rocky', 'killed', 'that', 'adidas', 'commercial', 'too']
cosine_similarity: 0.9264121055603027
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That AAP Rocky Adidas commerical is hard af
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['that', 'aap', 'rocky', 'adidas', 'commerical', 'is', 'hard', 'af']
cosine_similarity: 0.9520277380943298
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That Adidas ASAP Rocky commercial dope af
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['that', 'adidas', 'asap', 'rocky', 'commercial', 'dope', 'af']
cosine_similarity: 0.8890411853790283
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: That Adidas commercial with ASAP Rocky goes hard
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['that', 'adidas', 'commercial', 'with', 'asap', 'rocky', 'goes', 'hard']
cosine_similarity: 0.9248361587524414
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: The new Asap rocky Adidas commercial is fresh as shit
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['the', 'new', 'asap', 'rocky', 'adidas', 'commercial', 'is', 'fresh', 'as', 'shit']
cosine_similarity: 0.9488048553466797
word_to_vector_cosine_similarity: sentence1: AAP is in the Adidas commercial, sentence2: and Adidas commercial that was legit
After tokenization, sentence1: ['aap', 'is', 'in', 'the', 'adidas', 'commercial'], sentence2: ['and', 'adidas', 'commercial', 'that', 'was', 'legit']
cosine_similarity: 0.9446026682853699
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: About to get amanda bynes nudes tatted on my forearm
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['about', 'to', 'get', 'amanda', 'bynes', 'nudes', 'tatted', 'on', 'my', 'forearm']
cosine_similarity: 0.9650211334228516
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Can someone explain what the fuck happened to Amanda Bynes face
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['can', 'someone', 'explain', 'what', 'the', 'fuck', 'happened', 'to', 'amanda', 'bynes', 'face']
cosine_similarity: 0.9859800934791565
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Dude Amanda bynes is like cracked out
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['dude', 'amanda', 'bynes', 'is', 'like', 'cracked', 'out']
cosine_similarity: 0.9614703059196472
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: For the love of God someone please 5150 Amanda Bynes
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['for', 'the', 'love', 'of', 'god', 'someone', 'please', 'amanda', 'bynes']
cosine_similarity: 0.9606016874313354
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Hoes confused as to why everyone is infatuated with Amanda Bynes
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['hoes', 'confused', 'as', 'to', 'why', 'everyone', 'is', 'infatuated', 'with', 'amanda', 'bynes']
cosine_similarity: 0.9732469916343689
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: If Amanda Bynes can make it through the year I think we all can
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['if', 'amanda', 'bynes', 'can', 'make', 'it', 'through', 'the', 'year', 'i', 'think', 'we', 'all', 'can']
cosine_similarity: 0.9652775526046753
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Im going to call the police on Amanda Bynes
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['im', 'going', 'to', 'call', 'the', 'police', 'on', 'amanda', 'bynes']
cosine_similarity: 0.9701753854751587
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: Is amanda bynes on crack or something
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['is', 'amanda', 'bynes', 'on', 'crack', 'or', 'something']
cosine_similarity: 0.9750398993492126
word_to_vector_cosine_similarity: sentence1: What the fuck did Amanda Bynes do to her face, sentence2: wow what the hell has happened to amanda bynes
After tokenization, sentence1: ['what', 'the', 'fuck', 'did', 'amanda', 'bynes', 'do', 'to', 'her', 'face'], sentence2: ['wow', 'what', 'the', 'hell', 'has', 'happened', 'to', 'amanda', 'bynes']
cosine_similarity: 0.9739439487457275
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: Colorado Mandala is NOW AVAILABLE for purchase on Amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['colorado', 'mandala', 'is', 'now', 'available', 'for', 'purchase', 'on', 'amazon']
cosine_similarity: 0.9594444036483765
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: I am going to deliver the camera tomorrow when I purchased it in the Amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['i', 'am', 'going', 'to', 'deliver', 'the', 'camera', 'tomorrow', 'when', 'i', 'purchased', 'it', 'in', 'the', 'amazon']
cosine_similarity: 0.9169626832008362
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: I got it from amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['i', 'got', 'it', 'from', 'amazon']
cosine_similarity: 0.9146691560745239
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: I hope to win a 50 Amazon Gift Code
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['i', 'hope', 'to', 'win', 'a', 'amazon', 'gift', 'code']
cosine_similarity: 0.9539032578468323
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: Learn to Publish Your Hot Selling eBooks to Amazon Kindle
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['learn', 'to', 'publish', 'your', 'hot', 'selling', 'ebooks', 'to', 'amazon', 'kindle']
cosine_similarity: 0.9446346759796143
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: May s prize on My Question of the Day is a 50 Amazon gift card
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['may', 's', 'prize', 'on', 'my', 'question', 'of', 'the', 'day', 'is', 'a', 'amazon', 'gift', 'card']
cosine_similarity: 0.949104905128479
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: Win a 25 Amazon Gift Card with MeBookshelfandI
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['win', 'a', 'amazon', 'gift', 'card', 'with']
cosine_similarity: 0.9486595392227173
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: ahhh but did you know you can get this sort of stuff on amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['ahhh', 'but', 'did', 'you', 'know', 'you', 'can', 'get', 'this', 'sort', 'of', 'stuff', 'on', 'amazon']
cosine_similarity: 0.8785088062286377
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: check out the Yeti on Amazon it s not too pricey
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['check', 'out', 'the', 'yeti', 'on', 'amazon', 'it', 's', 'not', 'too', 'pricey']
cosine_similarity: 0.9318085312843323
word_to_vector_cosine_similarity: sentence1: According to Amazon s review, sentence2: just bought doom 3 for xbox off amazon
After tokenization, sentence1: ['according', 'to', 'amazon', 's', 'review'], sentence2: ['just', 'bought', 'doom', 'for', 'xbox', 'off', 'amazon']
cosine_similarity: 0.9379237294197083
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Do not Amber Alert me
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['do', 'not', 'amber', 'alert', 'me']
cosine_similarity: 0.9999999403953552
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Everyone s Amber Alerts going off at Zarape
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['everyone', 's', 'amber', 'alerts', 'going', 'off', 'at']
cosine_similarity: 0.9097906947135925
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I get scared when I get the amber alert messages
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['i', 'get', 'scared', 'when', 'i', 'get', 'the', 'amber', 'alert', 'messages']
cosine_similarity: 0.9385988712310791
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I wonder if the kidnapper gets amber alerts as well
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['i', 'wonder', 'if', 'the', 'kidnapper', 'gets', 'amber', 'alerts', 'as', 'well']
cosine_similarity: 0.9371524453163147
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: I would have put Amber in the top 2
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['i', 'would', 'have', 'put', 'amber', 'in', 'the', 'top']
cosine_similarity: 0.9355815649032593
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Just found out Amber was eliminated
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['just', 'found', 'out', 'amber', 'was', 'eliminated']
cosine_similarity: 0.8901304006576538
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Look Im tired if these damn Amber Alerts
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['look', 'im', 'tired', 'if', 'these', 'damn', 'amber', 'alerts']
cosine_similarity: 0.9126509428024292
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: Plus Amber has a huge career
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['plus', 'amber', 'has', 'a', 'huge', 'career']
cosine_similarity: 0.8870217800140381
word_to_vector_cosine_similarity: sentence1: Do not Amber Alert me, sentence2: im going to miss Amber on idol
After tokenization, sentence1: ['do', 'not', 'amber', 'alert', 'me'], sentence2: ['im', 'going', 'to', 'miss', 'amber', 'on', 'idol']
cosine_similarity: 0.9308733940124512
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber alert gave me a damn heart attack
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['amber', 'alert', 'gave', 'me', 'a', 'damn', 'heart', 'attack']
cosine_similarity: 0.9151096343994141
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber shouldnt have gone home
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['amber', 'shouldnt', 'have', 'gone', 'home']
cosine_similarity: 0.9532579779624939
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Everyone keeps talking about amber alerts on their phone
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['everyone', 'keeps', 'talking', 'about', 'amber', 'alerts', 'on', 'their', 'phone']
cosine_similarity: 0.945505678653717
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: I get to see my Amber
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['i', 'get', 'to', 'see', 'my', 'amber']
cosine_similarity: 0.9339374899864197
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: I thought amber alerts would only sound on androids
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['i', 'thought', 'amber', 'alerts', 'would', 'only', 'sound', 'on', 'androids']
cosine_similarity: 0.9612979888916016
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Ive gotten the same amber alert 3 times
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['ive', 'gotten', 'the', 'same', 'amber', 'alert', 'times']
cosine_similarity: 0.9734466075897217
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Ok this is the fifth amber alert Ive received
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['ok', 'this', 'is', 'the', 'fifth', 'amber', 'alert', 'ive', 'received']
cosine_similarity: 0.956674337387085
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Wats up with this amber alerts going off in church
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['wats', 'up', 'with', 'this', 'amber', 'alerts', 'going', 'off', 'in', 'church']
cosine_similarity: 0.9436097741127014
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Why do I get amber alerts tho
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['why', 'do', 'i', 'get', 'amber', 'alerts', 'tho']
cosine_similarity: 0.9413134455680847
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Amber is the cutest person ever
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['amber', 'is', 'the', 'cutest', 'person', 'ever']
cosine_similarity: 0.9629343152046204
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Everytime I get an amber alert on my phone I get freaked out
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['everytime', 'i', 'get', 'an', 'amber', 'alert', 'on', 'my', 'phone', 'i', 'get', 'freaked', 'out']
cosine_similarity: 0.9761330485343933
word_to_vector_cosine_similarity: sentence1: That Amber alert scared the crap out of me, sentence2: Pray for the Amber Alert
After tokenization, sentence1: ['that', 'amber', 'alert', 'scared', 'the', 'crap', 'out', 'of', 'me'], sentence2: ['pray', 'for', 'the', 'amber', 'alert']
cosine_similarity: 0.9498448371887207
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Am I the only one who dont get Amber alert
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['am', 'i', 'the', 'only', 'one', 'who', 'dont', 'get', 'amber', 'alert']
cosine_similarity: 0.9675059914588928
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: How long has Amber been missing because I keep getting alerts about her
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['how', 'long', 'has', 'amber', 'been', 'missing', 'because', 'i', 'keep', 'getting', 'alerts', 'about', 'her']
cosine_similarity: 0.9650647640228271
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I aint got no amber alerts yet
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['i', 'aint', 'got', 'no', 'amber', 'alerts', 'yet']
cosine_similarity: 0.9662990570068359
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I hate the amber alert sound
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['i', 'hate', 'the', 'amber', 'alert', 'sound']
cosine_similarity: 0.9742957353591919
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Just got the same amber alert three times today smh
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['just', 'got', 'the', 'same', 'amber', 'alert', 'three', 'times', 'today', 'smh']
cosine_similarity: 0.9669142365455627
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: That amber alert was getting annoying
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['that', 'amber', 'alert', 'was', 'getting', 'annoying']
cosine_similarity: 0.9576845765113831
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: These amber alert things scare me everytime they go off on my phone
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['these', 'amber', 'alert', 'things', 'scare', 'me', 'everytime', 'they', 'go', 'off', 'on', 'my', 'phone']
cosine_similarity: 0.9887235760688782
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: These amber alerts on my phone always freak me out
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['these', 'amber', 'alerts', 'on', 'my', 'phone', 'always', 'freak', 'me', 'out']
cosine_similarity: 0.9939929842948914
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Turns out it s just another amber alert
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['turns', 'out', 'it', 's', 'just', 'another', 'amber', 'alert']
cosine_similarity: 0.9735161662101746
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: Does anybody else keep gettin this Amber Alert notification
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['does', 'anybody', 'else', 'keep', 'gettin', 'this', 'amber', 'alert', 'notification']
cosine_similarity: 0.9353646039962769
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: Husband s phone just had an Amber Alert Warning
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['husband', 's', 'phone', 'just', 'had', 'an', 'amber', 'alert', 'warning']
cosine_similarity: 0.980297327041626
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: I Know You Are Very Upset That Amber Went Home
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['i', 'know', 'you', 'are', 'very', 'upset', 'that', 'amber', 'went', 'home']
cosine_similarity: 0.9401889443397522
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: If these amber alerts send me one more thing
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['if', 'these', 'amber', 'alerts', 'send', 'me', 'one', 'more', 'thing']
cosine_similarity: 0.9208953380584717
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: OMG AMBER I FREAKING HATE YOU
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['omg', 'amber', 'i', 'freaking', 'hate', 'you']
cosine_similarity: 0.9099948406219482
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: Pray for the Amber Alert
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['pray', 'for', 'the', 'amber', 'alert']
cosine_similarity: 0.9476858377456665
word_to_vector_cosine_similarity: sentence1: What is an amber alert, sentence2: These Amber Alerts are really annoying me right now
After tokenization, sentence1: ['what', 'is', 'an', 'amber', 'alert'], sentence2: ['these', 'amber', 'alerts', 'are', 'really', 'annoying', 'me', 'right', 'now']
cosine_similarity: 0.9225622415542603
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: Ive been getting amber alerts all day like what the fuck
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['ive', 'been', 'getting', 'amber', 'alerts', 'all', 'day', 'like', 'what', 'the', 'fuck']
cosine_similarity: 0.9717450737953186
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: My phone NEVER sends me Amber Alerts
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['my', 'phone', 'never', 'sends', 'me', 'amber', 'alerts']
cosine_similarity: 0.9481172561645508
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: That s 5 amber alerts today Ive got
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['that', 's', 'amber', 'alerts', 'today', 'ive', 'got']
cosine_similarity: 0.9690792560577393
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: These amber alert messages are scary as fudge man
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['these', 'amber', 'alert', 'messages', 'are', 'scary', 'as', 'fudge', 'man']
cosine_similarity: 0.9556382298469543
word_to_vector_cosine_similarity: sentence1: These Amber Alerts need to chill, sentence2: Well that Amber Alert alert just scared tf outta me
After tokenization, sentence1: ['these', 'amber', 'alerts', 'need', 'to', 'chill'], sentence2: ['well', 'that', 'amber', 'alert', 'alert', 'just', 'scared', 'tf', 'outta', 'me']
cosine_similarity: 0.9578795433044434
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: All these amber alerts leave people s kids alone
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['all', 'these', 'amber', 'alerts', 'leave', 'people', 's', 'kids', 'alone']
cosine_similarity: 0.972187876701355
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I hate these damn amber alerts coming to my phone
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['i', 'hate', 'these', 'damn', 'amber', 'alerts', 'coming', 'to', 'my', 'phone']
cosine_similarity: 0.9864579439163208
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: I just got like my 5th amber alert
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['i', 'just', 'got', 'like', 'my', 'amber', 'alert']
cosine_similarity: 0.9774119853973389
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: Omfg Who The FUCK Invented This Fucking Amber Alert
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['omfg', 'who', 'the', 'fuck', 'invented', 'this', 'fucking', 'amber', 'alert']
cosine_similarity: 0.9713367819786072
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: We all will miss our AMBER
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['we', 'all', 'will', 'miss', 'our', 'amber']
cosine_similarity: 0.9450591802597046
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: cx whats an amber alert thooo
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['cx', 'whats', 'an', 'amber', 'alert']
cosine_similarity: 0.9563042521476746
word_to_vector_cosine_similarity: sentence1: My phone is annoying me with these amber alerts, sentence2: what is this amber alert thing on my phone
After tokenization, sentence1: ['my', 'phone', 'is', 'annoying', 'me', 'with', 'these', 'amber', 'alerts'], sentence2: ['what', 'is', 'this', 'amber', 'alert', 'thing', 'on', 'my', 'phone']
cosine_similarity: 0.98467618227005
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Amber gone be a modelsinger
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['amber', 'gone', 'be', 'a']
cosine_similarity: 0.9147270917892456
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Can we chill with the amber alerts
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['can', 'we', 'chill', 'with', 'the', 'amber', 'alerts']
cosine_similarity: 0.9491514563560486
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Dam amber alert i got to my phone scared the hell out of me
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['dam', 'amber', 'alert', 'i', 'got', 'to', 'my', 'phone', 'scared', 'the', 'hell', 'out', 'of', 'me']
cosine_similarity: 0.9593026638031006
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: Everyone s Amber Alerts going off at Zarape
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['everyone', 's', 'amber', 'alerts', 'going', 'off', 'at']
cosine_similarity: 0.9165230393409729
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: How come I never get amber alerts
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['how', 'come', 'i', 'never', 'get', 'amber', 'alerts']
cosine_similarity: 0.9481540322303772
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: OH MY GOD FUCK THESE AMBER ALERTS
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['oh', 'my', 'god', 'fuck', 'these', 'amber', 'alerts']
cosine_similarity: 0.9454683065414429
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: That s 5 amber alerts today Ive got
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['that', 's', 'amber', 'alerts', 'today', 'ive', 'got']
cosine_similarity: 0.9632984399795532
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: These amber alerts on my phone been going off ALL day
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['these', 'amber', 'alerts', 'on', 'my', 'phone', 'been', 'going', 'off', 'all', 'day']
cosine_similarity: 0.9495298266410828
word_to_vector_cosine_similarity: sentence1: That amber alert was getting annoying, sentence2: how would you not know what an amber alert is
After tokenization, sentence1: ['that', 'amber', 'alert', 'was', 'getting', 'annoying'], sentence2: ['how', 'would', 'you', 'not', 'know', 'what', 'an', 'amber', 'alert', 'is']
cosine_similarity: 0.9684567451477051
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: I need a amber alert to wake tf up in da mornings Ha
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['i', 'need', 'a', 'amber', 'alert', 'to', 'wake', 'tf', 'up', 'in', 'da', 'mornings', 'ha']
cosine_similarity: 0.9622078537940979
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: I woke up to that Amber Alert I got
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['i', 'woke', 'up', 'to', 'that', 'amber', 'alert', 'i', 'got']
cosine_similarity: 0.9836907386779785
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: Ive had like 3 Amber Alerts today
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['ive', 'had', 'like', 'amber', 'alerts', 'today']
cosine_similarity: 0.9768241047859192
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: My phone keeps sending me amber alerts
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['my', 'phone', 'keeps', 'sending', 'me', 'amber', 'alerts']
cosine_similarity: 0.9347301721572876
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: My phone started ringing and vibrated in class today for a amber alert
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['my', 'phone', 'started', 'ringing', 'and', 'vibrated', 'in', 'class', 'today', 'for', 'a', 'amber', 'alert']
cosine_similarity: 0.9710473418235779
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: That amber alert on my phone be scaring the shit out of me
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['that', 'amber', 'alert', 'on', 'my', 'phone', 'be', 'scaring', 'the', 'shit', 'out', 'of', 'me']
cosine_similarity: 0.9799400568008423
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: This Amber alert keep cuttin my songs off
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['this', 'amber', 'alert', 'keep', 'cuttin', 'my', 'songs', 'off']
cosine_similarity: 0.9648913145065308
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: Who is Amber and why does she keep sending me these alerts
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['who', 'is', 'amber', 'and', 'why', 'does', 'she', 'keep', 'sending', 'me', 'these', 'alerts']
cosine_similarity: 0.9808192849159241
word_to_vector_cosine_similarity: sentence1: am i the only one not getting amber alerts, sentence2: there has been alot of amber alerts today
After tokenization, sentence1: ['am', 'i', 'the', 'only', 'one', 'not', 'getting', 'amber', 'alerts'], sentence2: ['there', 'has', 'been', 'alot', 'of', 'amber', 'alerts', 'today']
cosine_similarity: 0.9841583967208862
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: Amber Alerts to my iPhone scare me EVERYTIME
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['amber', 'alerts', 'to', 'my', 'iphone', 'scare', 'me', 'everytime']
cosine_similarity: 0.9605346918106079
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I aint got no amber alerts yet
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['i', 'aint', 'got', 'no', 'amber', 'alerts', 'yet']
cosine_similarity: 0.9754526615142822
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I have gotten 3 amber alert notices today
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['i', 'have', 'gotten', 'amber', 'alert', 'notices', 'today']
cosine_similarity: 0.9678113460540771
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: I loved Amber the whole season
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['i', 'loved', 'amber', 'the', 'whole', 'season']
cosine_similarity: 0.9678996801376343
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: If I get one more of these loud ass amber alerts
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['if', 'i', 'get', 'one', 'more', 'of', 'these', 'loud', 'ass', 'amber', 'alerts']
cosine_similarity: 0.9897758960723877
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: One more Amber Alert today Im throwing a bitch fit
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['one', 'more', 'amber', 'alert', 'today', 'im', 'throwing', 'a', 'bitch', 'fit']
cosine_similarity: 0.9876754283905029
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: That amber alert just scared the beep outta me
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['that', 'amber', 'alert', 'just', 'scared', 'the', 'beep', 'outta', 'me']
cosine_similarity: 0.975342333316803
word_to_vector_cosine_similarity: sentence1: If I get one more damn amber alert, sentence2: This is the 4th time Ive gotten that amber alert
After tokenization, sentence1: ['if', 'i', 'get', 'one', 'more', 'damn', 'amber', 'alert'], sentence2: ['this', 'is', 'the', 'time', 'ive', 'gotten', 'that', 'amber', 'alert']
cosine_similarity: 0.9812307953834534
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller is a liability on defense
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'is', 'a', 'liability', 'on', 'defense']
cosine_similarity: 0.9412304162979126
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller is even slower in person
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'is', 'even', 'slower', 'in', 'person']
cosine_similarity: 0.9401905536651611
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller wit dat old man game
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'wit', 'dat', 'old', 'man', 'game']
cosine_similarity: 0.9437281489372253
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Andre miller you a bitch shut up
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'you', 'a', 'bitch', 'shut', 'up']
cosine_similarity: 0.9045161604881287
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: How does Andre miller still move this fast
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['how', 'does', 'andre', 'miller', 'still', 'move', 'this', 'fast']
cosine_similarity: 0.9428938627243042
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: The fact that Andre miller is consistent really pisses me off
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['the', 'fact', 'that', 'andre', 'miller', 'is', 'consistent', 'really', 'pisses', 'me', 'off']
cosine_similarity: 0.9284635782241821
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Warriors need a vet like Andre Miller on they team
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['warriors', 'need', 'a', 'vet', 'like', 'andre', 'miller', 'on', 'they', 'team']
cosine_similarity: 0.9564310908317566
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: Why is Andre Miller taking 3 s with his broke jumper
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['why', 'is', 'andre', 'miller', 'taking', 's', 'with', 'his', 'broke', 'jumper']
cosine_similarity: 0.9555771946907043
word_to_vector_cosine_similarity: sentence1: Andre miller best lobbing pg in the game, sentence2: andre Miller aint never had no jumper
After tokenization, sentence1: ['andre', 'miller', 'best', 'lobbing', 'pg', 'in', 'the', 'game'], sentence2: ['andre', 'miller', 'aint', 'never', 'had', 'no', 'jumper']
cosine_similarity: 0.915012776851654
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut about to die on the court
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'about', 'to', 'die', 'on', 'the', 'court']
cosine_similarity: 0.9724290370941162
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut bitch ass goin off
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'bitch', 'ass', 'goin', 'off']
cosine_similarity: 0.883537769317627
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut is playing like ah 1 Overall Draft Pick
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'is', 'playing', 'like', 'ah', 'overall', 'draft', 'pick']
cosine_similarity: 0.9759834408760071
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut just went behind his back
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'just', 'went', 'behind', 'his', 'back']
cosine_similarity: 0.9591800570487976
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut keeping Golden State in the game
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'keeping', 'golden', 'state', 'in', 'the', 'game']
cosine_similarity: 0.9735677242279053
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Andrew Bogut looking like he did at the University of Utah
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['andrew', 'bogut', 'looking', 'like', 'he', 'did', 'at', 'the', 'university', 'of', 'utah']
cosine_similarity: 0.9767675399780273
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Is Andrew Bogut rocking the stealth mullet
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['is', 'andrew', 'bogut', 'rocking', 'the', 'stealth', 'mullet']
cosine_similarity: 0.923993706703186
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Literally nobody on Earth likes Andrew Bogut
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['literally', 'nobody', 'on', 'earth', 'likes', 'andrew', 'bogut']
cosine_similarity: 0.9579567909240723
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: So is Andrew Bogut juicing or
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['so', 'is', 'andrew', 'bogut', 'juicing', 'or']
cosine_similarity: 0.9476170539855957
word_to_vector_cosine_similarity: sentence1: My player of the game so far is Andrew Bogut, sentence2: Steph curry looks sick but Andrew Bogut is playing like an all star quietly
After tokenization, sentence1: ['my', 'player', 'of', 'the', 'game', 'so', 'far', 'is', 'andrew', 'bogut'], sentence2: ['steph', 'curry', 'looks', 'sick', 'but', 'andrew', 'bogut', 'is', 'playing', 'like', 'an', 'all', 'star', 'quietly']
cosine_similarity: 0.9799492955207825
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Asik showed heart going to the line and knocking some of the FT s down
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['asik', 'showed', 'heart', 'going', 'to', 'the', 'line', 'and', 'knocking', 'some', 'of', 'the', 'ft', 's', 'down']
cosine_similarity: 0.911510705947876
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: But hacking asik andforcing awful shots
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['but', 'hacking', 'asik', 'awful', 'shots']
cosine_similarity: 0.8808949589729309
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Omer Asik came through big tonight
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['omer', 'asik', 'came', 'through', 'big', 'tonight']
cosine_similarity: 0.9354583621025085
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: Scott Brooks was worse with his hackAsik
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['scott', 'brooks', 'was', 'worse', 'with', 'his']
cosine_similarity: 0.8952993154525757
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: So much for HackaAsik
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['so', 'much', 'for']
cosine_similarity: 0.8215305209159851
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: That hack of Asik shit didnt work out
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['that', 'hack', 'of', 'asik', 'shit', 'didnt', 'work', 'out']
cosine_similarity: 0.8893646001815796
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: especially when Asik is out of the game
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['especially', 'when', 'asik', 'is', 'out', 'of', 'the', 'game']
cosine_similarity: 0.9254744648933411
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: they have been fouling asik since about the 6th min mark
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['they', 'have', 'been', 'fouling', 'asik', 'since', 'about', 'the', 'min', 'mark']
cosine_similarity: 0.9286020994186401
word_to_vector_cosine_similarity: sentence1: Rockets Asik showed the Thunder, sentence2: what to do with Asik then
After tokenization, sentence1: ['rockets', 'asik', 'showed', 'the', 'thunder'], sentence2: ['what', 'to', 'do', 'with', 'asik', 'then']
cosine_similarity: 0.8563092350959778
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom goes down in warmup and the wild turn to josh Harding
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'goes', 'down', 'in', 'warmup', 'and', 'the', 'wild', 'turn', 'to', 'josh', 'harding']
cosine_similarity: 0.9633684754371643
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom is out already good omen for the Hawks
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'is', 'out', 'already', 'good', 'omen', 'for', 'the', 'hawks']
cosine_similarity: 0.9522525668144226
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Backstrom would get hurt the day we start playoffs
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'would', 'get', 'hurt', 'the', 'day', 'we', 'start', 'playoffs']
cosine_similarity: 0.927696943283081
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Lol so Backstrom got hurt in warm ups
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['lol', 'so', 'backstrom', 'got', 'hurt', 'in', 'warm', 'ups']
cosine_similarity: 0.8936111330986023
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wishing Backstrom a speedy recovery
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['wishing', 'backstrom', 'a', 'speedy', 'recovery']
cosine_similarity: 0.8788540959358215
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: Wow backstrom would get hurt in warmups
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['wow', 'backstrom', 'would', 'get', 'hurt', 'in', 'warmups']
cosine_similarity: 0.9549112915992737
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: backstrom just got hurt in the warmup
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup']
cosine_similarity: 0.9698944091796875
word_to_vector_cosine_similarity: sentence1: Backstrom injured in warmup Harding gets the start, sentence2: lose Backstrom in the warm ups
After tokenization, sentence1: ['backstrom', 'injured', 'in', 'warmup', 'harding', 'gets', 'the', 'start'], sentence2: ['lose', 'backstrom', 'in', 'the', 'warm', 'ups']
cosine_similarity: 0.9232298731803894
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom Heatley and Pominville out for Minny
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['backstrom', 'heatley', 'and', 'pominville', 'out', 'for', 'minny']
cosine_similarity: 0.8747900724411011
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom apparently suffered an injury in warmup
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['backstrom', 'apparently', 'suffered', 'an', 'injury', 'in', 'warmup']
cosine_similarity: 0.9212742447853088
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Backstrom out in warm ups for the wild
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['backstrom', 'out', 'in', 'warm', 'ups', 'for', 'the', 'wild']
cosine_similarity: 0.9703841209411621
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: If Backstrom is out can I pick the Blackhawks to win in two
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['if', 'backstrom', 'is', 'out', 'can', 'i', 'pick', 'the', 'blackhawks', 'to', 'win', 'in', 'two']
cosine_similarity: 0.9747431874275208
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Looks like Backstrom hurt himself in warmup
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['looks', 'like', 'backstrom', 'hurt', 'himself', 'in', 'warmup']
cosine_similarity: 0.9841655492782593
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: Niklas Backstrom potentially got hurt during warmups
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['niklas', 'backstrom', 'potentially', 'got', 'hurt', 'during', 'warmups']
cosine_similarity: 0.9139617681503296
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: and now Backstrom is injured in the warmups timmywhitehead
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['and', 'now', 'backstrom', 'is', 'injured', 'in', 'the', 'warmups']
cosine_similarity: 0.9795131683349609
word_to_vector_cosine_similarity: sentence1: backstrom just got hurt in the warmup, sentence2: backstrom was just injured in warmups for the
After tokenization, sentence1: ['backstrom', 'just', 'got', 'hurt', 'in', 'the', 'warmup'], sentence2: ['backstrom', 'was', 'just', 'injured', 'in', 'warmups', 'for', 'the']
cosine_similarity: 0.9828523397445679
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom gets hurt in warm ups for the wild
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['backstrom', 'gets', 'hurt', 'in', 'warm', 'ups', 'for', 'the', 'wild']
cosine_similarity: 0.9307821393013
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom hurt and helped to the back during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['backstrom', 'hurt', 'and', 'helped', 'to', 'the', 'back', 'during', 'warmups']
cosine_similarity: 0.9595140218734741
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Backstrom s out for MIN
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['backstrom', 's', 'out', 'for', 'min']
cosine_similarity: 0.8041871786117554
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Harding in net for Backstrom
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['harding', 'in', 'net', 'for', 'backstrom']
cosine_similarity: 0.8532418012619019
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Lmao they had to help Backstrom off the ice
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['lmao', 'they', 'had', 'to', 'help', 'backstrom', 'off', 'the', 'ice']
cosine_similarity: 0.9083124399185181
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: WHAT THE HECK BACKSTROM GOT HURT IN WARMUPS
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['what', 'the', 'heck', 'backstrom', 'got', 'hurt', 'in', 'warmups']
cosine_similarity: 0.959818959236145
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: Yikes Backstrom might be injured
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['yikes', 'backstrom', 'might', 'be', 'injured']
cosine_similarity: 0.9508375525474548
word_to_vector_cosine_similarity: sentence1: backstrom hurt and left warmups, sentence2: you see backstrom is out
After tokenization, sentence1: ['backstrom', 'hurt', 'and', 'left', 'warmups'], sentence2: ['you', 'see', 'backstrom', 'is', 'out']
cosine_similarity: 0.9137299656867981
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Also hate that Backstrom went down in the warm up
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['also', 'hate', 'that', 'backstrom', 'went', 'down', 'in', 'the', 'warm', 'up']
cosine_similarity: 0.8365132212638855
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Backstrom is out we lose
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['backstrom', 'is', 'out', 'we', 'lose']
cosine_similarity: 0.8366538286209106
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Backstrom just went down during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['backstrom', 'just', 'went', 'down', 'during', 'warmups']
cosine_similarity: 0.9343372583389282
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Hopefully Backstrom is back soon
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['hopefully', 'backstrom', 'is', 'back', 'soon']
cosine_similarity: 0.8274986147880554
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Man if Backstrom is out that s huuuuge
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['man', 'if', 'backstrom', 'is', 'out', 'that', 's', 'huuuuge']
cosine_similarity: 0.8125864863395691
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Neve mind Niklas Backstrom s hurt
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['neve', 'mind', 'niklas', 'backstrom', 's', 'hurt']
cosine_similarity: 0.8112177848815918
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Omg backstrom injured in warmup
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['omg', 'backstrom', 'injured', 'in', 'warmup']
cosine_similarity: 0.9101243019104004
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wild Backstrom injured during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['wild', 'backstrom', 'injured', 'during', 'warmups']
cosine_similarity: 0.9425503015518188
word_to_vector_cosine_similarity: sentence1: Backstrom hurt himself during warmups, sentence2: Wild in playoffs Backstrom injured in pregame warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'himself', 'during', 'warmups'], sentence2: ['wild', 'in', 'playoffs', 'backstrom', 'injured', 'in', 'pregame', 'warmups']
cosine_similarity: 0.909921407699585
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom hurt in warn ups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['backstrom', 'hurt', 'in', 'warn', 'ups']
cosine_similarity: 0.9361975193023682
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom injured himself in the warmups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['backstrom', 'injured', 'himself', 'in', 'the', 'warmups']
cosine_similarity: 0.9359128475189209
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom was injured in warm up for Minnesota
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['backstrom', 'was', 'injured', 'in', 'warm', 'up', 'for', 'minnesota']
cosine_similarity: 0.9448740482330322
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Backstrom was injured warming up
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['backstrom', 'was', 'injured', 'warming', 'up']
cosine_similarity: 0.9198800325393677
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Hurt himself during warm ups tough break for backstrom
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['hurt', 'himself', 'during', 'warm', 'ups', 'tough', 'break', 'for', 'backstrom']
cosine_similarity: 0.9321033954620361
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: If you didnt know Backstrom inexplicably hurt himself in warmups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['if', 'you', 'didnt', 'know', 'backstrom', 'inexplicably', 'hurt', 'himself', 'in', 'warmups']
cosine_similarity: 0.9043194055557251
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: Minnesota Wild goalie Backstrom just got injured in warm ups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['minnesota', 'wild', 'goalie', 'backstrom', 'just', 'got', 'injured', 'in', 'warm', 'ups']
cosine_similarity: 0.933113694190979
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: OMG Backstrom is already injured
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['omg', 'backstrom', 'is', 'already', 'injured']
cosine_similarity: 0.9113175868988037
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: That sucks Nicklas Backstrom from the wild got hurt in warm ups
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['that', 'sucks', 'nicklas', 'backstrom', 'from', 'the', 'wild', 'got', 'hurt', 'in', 'warm', 'ups']
cosine_similarity: 0.9379292130470276
word_to_vector_cosine_similarity: sentence1: Backstrom s hurt in warmups, sentence2: When you hear Nicolas backstrom is injured
After tokenization, sentence1: ['backstrom', 's', 'hurt', 'in', 'warmups'], sentence2: ['when', 'you', 'hear', 'nicolas', 'backstrom', 'is', 'injured']
cosine_similarity: 0.8923681974411011
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Backstrom for Min is hurt in warmup
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'for', 'min', 'is', 'hurt', 'in', 'warmup']
cosine_similarity: 0.9696056246757507
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Looks like Backstrom injured in warmup
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['looks', 'like', 'backstrom', 'injured', 'in', 'warmup']
cosine_similarity: 0.9800604581832886
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Maybe with backstrom injured the D will step up
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['maybe', 'with', 'backstrom', 'injured', 'the', 'd', 'will', 'step', 'up']
cosine_similarity: 0.9462360739707947
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Minnesota goalie Backstrom hurt in warm ups
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['minnesota', 'goalie', 'backstrom', 'hurt', 'in', 'warm', 'ups']
cosine_similarity: 0.9571081399917603
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Nikalas backstrom hurt during warm ups
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['backstrom', 'hurt', 'during', 'warm', 'ups']
cosine_similarity: 0.9535923600196838
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Of course Backstrom would get injured during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['of', 'course', 'backstrom', 'would', 'get', 'injured', 'during', 'warmups']
cosine_similarity: 0.9818362593650818
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: Tell flynnkatie Backstrom got injured during warmups
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['tell', 'backstrom', 'got', 'injured', 'during', 'warmups']
cosine_similarity: 0.969366192817688
word_to_vector_cosine_similarity: sentence1: Backstrom Hurt in warmups Harding gets the start, sentence2: well first injury of the playoffs goes to backstrom in warmups of the first game
After tokenization, sentence1: ['backstrom', 'hurt', 'in', 'warmups', 'harding', 'gets', 'the', 'start'], sentence2: ['well', 'first', 'injury', 'of', 'the', 'playoffs', 'goes', 'to', 'backstrom', 'in', 'warmups', 'of', 'the', 'first', 'game']
cosine_similarity: 0.9620372653007507
word_to_vector_cosine_similarity: sentence1: Just lost Backstrom in warmup, sentence2: Backstrom gets hurt in warm ups for the wild
After tokenization, sentence1: ['just', 'lost', 'backstrom', 'in', 'warmup'], sentence2: ['backstrom', 'gets', 'hurt', 'in', 'warm', 'ups', 'for', 'the', 'wild']
cosine_similarity: 0.9632977843284607
word_to_vector_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: The last rap battle in 8 Mile nevr gets old ahah
After tokenization, sentence1: ['all', 'the', 'home', 'alones', 'watching', 'mile'], sentence2: ['the', 'last', 'rap', 'battle', 'in', 'mile', 'nevr', 'gets', 'old', 'ahah']
cosine_similarity: 0.9554800987243652
word_to_vector_cosine_similarity: sentence1: All the home alones watching 8 mile, sentence2: The rap battle at the end of 8 mile gets me so hype
After tokenization, sentence1: ['all', 'the', 'home', 'alones', 'watching', 'mile'], sentence2: ['the', 'rap', 'battle', 'at', 'the', 'end', 'of', 'mile', 'gets', 'me', 'so', 'hype']
cosine_similarity: 0.9636808037757874
word_to_vector_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: Rabbit on 8 mile out of place but determined to make it
After tokenization, sentence1: ['the', 'ending', 'to', 'mile', 'is', 'my', 'fav', 'part', 'of', 'the', 'whole', 'movie'], sentence2: ['rabbit', 'on', 'mile', 'out', 'of', 'place', 'but', 'determined', 'to', 'make', 'it']
cosine_similarity: 0.9746663570404053
word_to_vector_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: See 8 Mile is always on but it s the tv version so it s gay
After tokenization, sentence1: ['the', 'ending', 'to', 'mile', 'is', 'my', 'fav', 'part', 'of', 'the', 'whole', 'movie'], sentence2: ['see', 'mile', 'is', 'always', 'on', 'but', 'it', 's', 'the', 'tv', 'version', 'so', 'it', 's', 'gay']
cosine_similarity: 0.9744922518730164
word_to_vector_cosine_similarity: sentence1: The Ending to 8 Mile is my fav part of the whole movie, sentence2: Those last 3 battles in 8 Mile are THE shit
After tokenization, sentence1: ['the', 'ending', 'to', 'mile', 'is', 'my', 'fav', 'part', 'of', 'the', 'whole', 'movie'], sentence2: ['those', 'last', 'battles', 'in', 'mile', 'are', 'the', 'shit']
cosine_similarity: 0.9784131646156311
word_to_vector_cosine_similarity: sentence1: 0 and 8 mile on at the same time, sentence2: It s just rap lyrics from the movie 8 mile
After tokenization, sentence1: ['and', 'mile', 'on', 'at', 'the', 'same', 'time'], sentence2: ['it', 's', 'just', 'rap', 'lyrics', 'from', 'the', 'movie', 'mile']
cosine_similarity: 0.9730812907218933
word_to_vector_cosine_similarity: sentence1: 0 and 8 mile on at the same time, sentence2: I will never get tired of 8 Mile
After tokenization, sentence1: ['and', 'mile', 'on', 'at', 'the', 'same', 'time'], sentence2: ['i', 'will', 'never', 'get', 'tired', 'of', 'mile']
cosine_similarity: 0.9570022225379944
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: In 8 Mile in a scene the background music is Sweet Home Alabama
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['in', 'mile', 'in', 'a', 'scene', 'the', 'background', 'music', 'is', 'sweet', 'home', 'alabama']
cosine_similarity: 0.9774959087371826
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: 8 mile is on havent seen this movie in the longest
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['mile', 'is', 'on', 'havent', 'seen', 'this', 'movie', 'in', 'the', 'longest']
cosine_similarity: 0.9806771278381348
word_to_vector_cosine_similarity: sentence1: The last rap battle in 8 mile though, sentence2: But why were people watching the heat play when 8 mile is on
After tokenization, sentence1: ['the', 'last', 'rap', 'battle', 'in', 'mile', 'though'], sentence2: ['but', 'why', 'were', 'people', 'watching', 'the', 'heat', 'play', 'when', 'mile', 'is', 'on']
cosine_similarity: 0.9733127355575562
word_to_vector_cosine_similarity: sentence1: The last rap battle in 8 mile though, sentence2: I think everyone is watching 8 mile Rn
After tokenization, sentence1: ['the', 'last', 'rap', 'battle', 'in', 'mile', 'though'], sentence2: ['i', 'think', 'everyone', 'is', 'watching', 'mile', 'rn']
cosine_similarity: 0.9507029056549072
word_to_vector_cosine_similarity: sentence1: 8 mile is just a classic, sentence2: 8 mile that movie I love eminem in this movie
After tokenization, sentence1: ['mile', 'is', 'just', 'a', 'classic'], sentence2: ['mile', 'that', 'movie', 'i', 'love', 'eminem', 'in', 'this', 'movie']
cosine_similarity: 0.9681278467178345
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Listened to the final rap battle from 8 mile and now Im watching the whole movie
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['listened', 'to', 'the', 'final', 'rap', 'battle', 'from', 'mile', 'and', 'now', 'im', 'watching', 'the', 'whole', 'movie']
cosine_similarity: 0.9706138968467712
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: 8 mile has been that movie
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['mile', 'has', 'been', 'that', 'movie']
cosine_similarity: 0.9649457931518555
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: I really did miss my part on 8 Mile
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['i', 'really', 'did', 'miss', 'my', 'part', 'on', 'mile']
cosine_similarity: 0.9787110686302185
word_to_vector_cosine_similarity: sentence1: Well at least 8 Mile is on, sentence2: Lose Yourself is the perfect song to end 8 mile on
After tokenization, sentence1: ['well', 'at', 'least', 'mile', 'is', 'on'], sentence2: ['lose', 'yourself', 'is', 'the', 'perfect', 'song', 'to', 'end', 'mile', 'on']
cosine_similarity: 0.9753764867782593
word_to_vector_cosine_similarity: sentence1: 8 mile is on friday made, sentence2: I missed the best part of 8 mile
After tokenization, sentence1: ['mile', 'is', 'on', 'friday', 'made'], sentence2: ['i', 'missed', 'the', 'best', 'part', 'of', 'mile']
cosine_similarity: 0.9859362840652466
word_to_vector_cosine_similarity: sentence1: 8 mile is on friday made, sentence2: 8 mile is such an awesome movie
After tokenization, sentence1: ['mile', 'is', 'on', 'friday', 'made'], sentence2: ['mile', 'is', 'such', 'an', 'awesome', 'movie']
cosine_similarity: 0.9594147801399231
word_to_vector_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: Eminem s rap in the final battle of 8 Mile gets me pumped every time
After tokenization, sentence1: ['oh', 'shit', 'niggy', 'mile', 'is', 'on'], sentence2: ['eminem', 's', 'rap', 'in', 'the', 'final', 'battle', 'of', 'mile', 'gets', 'me', 'pumped', 'every', 'time']
cosine_similarity: 0.9236377477645874
word_to_vector_cosine_similarity: sentence1: Oh shit niggy 8 mile is on, sentence2: While yall argue about the game 8 mile is on MTV
After tokenization, sentence1: ['oh', 'shit', 'niggy', 'mile', 'is', 'on'], sentence2: ['while', 'yall', 'argue', 'about', 'the', 'game', 'mile', 'is', 'on', 'mtv']
cosine_similarity: 0.9547627568244934
word_to_vector_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: The end of 8 Mile makes me so happy
After tokenization, sentence1: ['ok', 'good', 'the', 'end', 'of', 'mile', 'is', 'on'], sentence2: ['the', 'end', 'of', 'mile', 'makes', 'me', 'so', 'happy']
cosine_similarity: 0.984442412853241
word_to_vector_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: The last 3 rap battles in 8 mile always get me hyped af
After tokenization, sentence1: ['ok', 'good', 'the', 'end', 'of', 'mile', 'is', 'on'], sentence2: ['the', 'last', 'rap', 'battles', 'in', 'mile', 'always', 'get', 'me', 'hyped', 'af']
cosine_similarity: 0.964591920375824
word_to_vector_cosine_similarity: sentence1: Ok good the end of 8 Mile is on, sentence2: I always get the movies 8 mile and green mile mixed up
After tokenization, sentence1: ['ok', 'good', 'the', 'end', 'of', 'mile', 'is', 'on'], sentence2: ['i', 'always', 'get', 'the', 'movies', 'mile', 'and', 'green', 'mile', 'mixed', 'up']
cosine_similarity: 0.9647327065467834
word_to_vector_cosine_similarity: sentence1: Let s go see after earth, sentence2: Did After Earth already come out
After tokenization, sentence1: ['let', 's', 'go', 'see', 'after', 'earth'], sentence2: ['did', 'after', 'earth', 'already', 'come', 'out']
cosine_similarity: 0.9674420356750488
word_to_vector_cosine_similarity: sentence1: Let s go see after earth, sentence2: idk I think we might go see after earth later if we do you wana go
After tokenization, sentence1: ['let', 's', 'go', 'see', 'after', 'earth'], sentence2: ['idk', 'i', 'think', 'we', 'might', 'go', 'see', 'after', 'earth', 'later', 'if', 'we', 'do', 'you', 'wana', 'go']
cosine_similarity: 0.976414680480957
word_to_vector_cosine_similarity: sentence1: Anyone trying to see After Earth sometime soon, sentence2: Me and my son went to see After Earth last night
After tokenization, sentence1: ['anyone', 'trying', 'to', 'see', 'after', 'earth', 'sometime', 'soon'], sentence2: ['me', 'and', 'my', 'son', 'went', 'to', 'see', 'after', 'earth', 'last', 'night']
cosine_similarity: 0.9673699140548706
word_to_vector_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I could of told Will Smith that After Earth was going 2 crash
After tokenization, sentence1: ['who', 'wants', 'to', 'see', 'after', 'earth', 'with', 'me'], sentence2: ['i', 'could', 'of', 'told', 'will', 'smith', 'that', 'after', 'earth', 'was', 'going', 'crash']
cosine_similarity: 0.971786379814148
word_to_vector_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: I knew After Earth would tank
After tokenization, sentence1: ['who', 'wants', 'to', 'see', 'after', 'earth', 'with', 'me'], sentence2: ['i', 'knew', 'after', 'earth', 'would', 'tank']
cosine_similarity: 0.9723041653633118
word_to_vector_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: After Earth finishes with 27 million
After tokenization, sentence1: ['who', 'wants', 'to', 'see', 'after', 'earth', 'with', 'me'], sentence2: ['after', 'earth', 'finishes', 'with', 'million']
cosine_similarity: 0.922920823097229
word_to_vector_cosine_similarity: sentence1: who wants to see after earth with me todayyyy, sentence2: even if After Earth is a good movie
After tokenization, sentence1: ['who', 'wants', 'to', 'see', 'after', 'earth', 'with', 'me'], sentence2: ['even', 'if', 'after', 'earth', 'is', 'a', 'good', 'movie']
cosine_similarity: 0.9761160016059875
word_to_vector_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: Ether to see after earth or fast 6
After tokenization, sentence1: ['i', 'see', 'the', 'after', 'earth', 'reviews', 'are', 'not', 'positive'], sentence2: ['ether', 'to', 'see', 'after', 'earth', 'or', 'fast']
cosine_similarity: 0.987482488155365
word_to_vector_cosine_similarity: sentence1: I see the After Earth reviews are not positive, sentence2: I need to watch after earth tho
After tokenization, sentence1: ['i', 'see', 'the', 'after', 'earth', 'reviews', 'are', 'not', 'positive'], sentence2: ['i', 'need', 'to', 'watch', 'after', 'earth', 'tho']
cosine_similarity: 0.976889431476593
word_to_vector_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: At the theater with the little about do watch After Earth
After tokenization, sentence1: ['i', 'need', 'to', 'watch', 'after', 'earth', 'asap'], sentence2: ['at', 'the', 'theater', 'with', 'the', 'little', 'about', 'do', 'watch', 'after', 'earth']
cosine_similarity: 0.9769679307937622
word_to_vector_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: Someone needs to come see after earth with me
After tokenization, sentence1: ['i', 'need', 'to', 'watch', 'after', 'earth', 'asap'], sentence2: ['someone', 'needs', 'to', 'come', 'see', 'after', 'earth', 'with', 'me']
cosine_similarity: 0.979856014251709
word_to_vector_cosine_similarity: sentence1: i need to watch after earth asap, sentence2: After Earth is a great ass movie
After tokenization, sentence1: ['i', 'need', 'to', 'watch', 'after', 'earth', 'asap'], sentence2: ['after', 'earth', 'is', 'a', 'great', 'ass', 'movie']
cosine_similarity: 0.9571712017059326
word_to_vector_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: Goin to see after earth with the fam
After tokenization, sentence1: ['heading', 'out', 'to', 'see', 'after', 'earth', 'in', 'a', 'bit'], sentence2: ['goin', 'to', 'see', 'after', 'earth', 'with', 'the', 'fam']
cosine_similarity: 0.9843347072601318
word_to_vector_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: will smith s speech in after earth is so relevant
After tokenization, sentence1: ['heading', 'out', 'to', 'see', 'after', 'earth', 'in', 'a', 'bit'], sentence2: ['will', 'smith', 's', 'speech', 'in', 'after', 'earth', 'is', 'so', 'relevant']
cosine_similarity: 0.9647049903869629
word_to_vector_cosine_similarity: sentence1: Heading out to see After Earth in a bit, sentence2: Just got done eating chinese with the fam now ganna go see after earth
After tokenization, sentence1: ['heading', 'out', 'to', 'see', 'after', 'earth', 'in', 'a', 'bit'], sentence2: ['just', 'got', 'done', 'eating', 'chinese', 'with', 'the', 'fam', 'now', 'ganna', 'go', 'see', 'after', 'earth']
cosine_similarity: 0.9673506617546082
word_to_vector_cosine_similarity: sentence1: Going to see after earth but, sentence2: After earth is out and I havent seen it yet
After tokenization, sentence1: ['going', 'to', 'see', 'after', 'earth', 'but'], sentence2: ['after', 'earth', 'is', 'out', 'and', 'i', 'havent', 'seen', 'it', 'yet']
cosine_similarity: 0.9810016751289368
word_to_vector_cosine_similarity: sentence1: Who wants to take me to see After Earth, sentence2: the hangover 3 and after earth are both really good
After tokenization, sentence1: ['who', 'wants', 'to', 'take', 'me', 'to', 'see', 'after', 'earth'], sentence2: ['the', 'hangover', 'and', 'after', 'earth', 'are', 'both', 'really', 'good']
cosine_similarity: 0.9573397636413574
word_to_vector_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: I kinda wanna see After Earth as well
After tokenization, sentence1: ['i', 'wanna', 'see', 'the', 'movie', 'after', 'earth'], sentence2: ['i', 'kinda', 'wanna', 'see', 'after', 'earth', 'as', 'well']
cosine_similarity: 0.9859218001365662
word_to_vector_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: NOW YOU SEE ME and AFTER EARTH Cant Outpace FAST FURIOUS 6
After tokenization, sentence1: ['i', 'wanna', 'see', 'the', 'movie', 'after', 'earth'], sentence2: ['now', 'you', 'see', 'me', 'and', 'after', 'earth', 'cant', 'outpace', 'fast', 'furious']
cosine_similarity: 0.9832678437232971
word_to_vector_cosine_similarity: sentence1: I wanna see the movie after earth, sentence2: After Earth 039 trumped by 039 Now You See Me 039 as 039 Fast
After tokenization, sentence1: ['i', 'wanna', 'see', 'the', 'movie', 'after', 'earth'], sentence2: ['after', 'earth', 'trumped', 'by', 'now', 'you', 'see', 'me', 'as', 'fast']
cosine_similarity: 0.9750257134437561
word_to_vector_cosine_similarity: sentence1: the US and belgium are tied at half, sentence2: Come on Romelu get some goals for Belgium
After tokenization, sentence1: ['the', 'us', 'and', 'belgium', 'are', 'tied', 'at', 'half'], sentence2: ['come', 'on', 'romelu', 'get', 'some', 'goals', 'for', 'belgium']
cosine_similarity: 0.9718703031539917
word_to_vector_cosine_similarity: sentence1: the US and belgium are tied at half, sentence2: Mirallas with a soft and cool finish off the rebound to put Belgium up 10
After tokenization, sentence1: ['the', 'us', 'and', 'belgium', 'are', 'tied', 'at', 'half'], sentence2: ['mirallas', 'with', 'a', 'soft', 'and', 'cool', 'finish', 'off', 'the', 'rebound', 'to', 'put', 'belgium', 'up']
cosine_similarity: 0.9653958678245544
word_to_vector_cosine_similarity: sentence1: Dude Belgium is freakin staked, sentence2: US vs Belgium or the wings
After tokenization, sentence1: ['dude', 'belgium', 'is', 'freakin', 'staked'], sentence2: ['us', 'vs', 'belgium', 'or', 'the', 'wings']
cosine_similarity: 0.8548768758773804
word_to_vector_cosine_similarity: sentence1: Dude Belgium is freakin staked, sentence2: Belgium almost take the lead in the 27th min
After tokenization, sentence1: ['dude', 'belgium', 'is', 'freakin', 'staked'], sentence2: ['belgium', 'almost', 'take', 'the', 'lead', 'in', 'the', 'min']
cosine_similarity: 0.8784986138343811
word_to_vector_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: What kind of formation Belgium playing there
After tokenization, sentence1: ['belgium', 's', 'gonna', 'rape', 'by', 'the', 'usa'], sentence2: ['what', 'kind', 'of', 'formation', 'belgium', 'playing', 'there']
cosine_similarity: 0.9525660276412964
word_to_vector_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: At half US 1 Belgium 1 Indians 5
After tokenization, sentence1: ['belgium', 's', 'gonna', 'rape', 'by', 'the', 'usa'], sentence2: ['at', 'half', 'us', 'belgium', 'indians']
cosine_similarity: 0.9350979328155518
word_to_vector_cosine_similarity: sentence1: Belgium s gonna rape by the USA, sentence2: Who s in goal for Belgium for the USMNT friendly
After tokenization, sentence1: ['belgium', 's', 'gonna', 'rape', 'by', 'the', 'usa'], sentence2: ['who', 's', 'in', 'goal', 'for', 'belgium', 'for', 'the', 'usmnt', 'friendly']
cosine_similarity: 0.9755635261535645
word_to_vector_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Vernaelen always gets injured for Belgium
After tokenization, sentence1: ['belgium', 'vs', 'usa', 'you', 'watching'], sentence2: ['always', 'gets', 'injured', 'for', 'belgium']
cosine_similarity: 0.9102368354797363
word_to_vector_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: 6th minute Belgium with the score
After tokenization, sentence1: ['belgium', 'vs', 'usa', 'you', 'watching'], sentence2: ['minute', 'belgium', 'with', 'the', 'score']
cosine_similarity: 0.9402766227722168
word_to_vector_cosine_similarity: sentence1: Belgium vs USA you watching, sentence2: Belgium in a friendly instead
After tokenization, sentence1: ['belgium', 'vs', 'usa', 'you', 'watching'], sentence2: ['belgium', 'in', 'a', 'friendly', 'instead']
cosine_similarity: 0.9446312785148621
word_to_vector_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: The Belgium GK wasnt trying to concede another goal
After tokenization, sentence1: ['watching', 'the', 'usmnt', 'vs', 'the', 'talented', 'belgium', 'team'], sentence2: ['the', 'belgium', 'gk', 'wasnt', 'trying', 'to', 'concede', 'another', 'goal']
cosine_similarity: 0.9051010608673096
word_to_vector_cosine_similarity: sentence1: Watching the USMNT vs the talented Belgium team, sentence2: that being said US 21 Belgium
After tokenization, sentence1: ['watching', 'the', 'usmnt', 'vs', 'the', 'talented', 'belgium', 'team'], sentence2: ['that', 'being', 'said', 'us', 'belgium']
cosine_similarity: 0.9022647142410278
word_to_vector_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: what s up with the nonHD main camera at the USBelgium soccer game
After tokenization, sentence1: ['yeah', 'belgium', 'is', 'definitely', 'good'], sentence2: ['what', 's', 'up', 'with', 'the', 'main', 'camera', 'at', 'the', 'soccer', 'game']
cosine_similarity: 0.9108554124832153
word_to_vector_cosine_similarity: sentence1: yeah Belgium is definitely good, sentence2: Belgium vs USA you watching
After tokenization, sentence1: ['yeah', 'belgium', 'is', 'definitely', 'good'], sentence2: ['belgium', 'vs', 'usa', 'you', 'watching']
cosine_similarity: 0.9155781865119934
word_to_vector_cosine_similarity: sentence1: Belgium has a great team, sentence2: a little late to the USABelgium game
After tokenization, sentence1: ['belgium', 'has', 'a', 'great', 'team'], sentence2: ['a', 'little', 'late', 'to', 'the', 'game']
cosine_similarity: 0.9370109438896179
word_to_vector_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: Belgium is playing 4 centerbacks and fellani and they concede on a set piece
After tokenization, sentence1: ['i', 'love', 'everyone', 'on', 'the', 'belgium', 'squad'], sentence2: ['belgium', 'is', 'playing', 'and', 'fellani', 'and', 'they', 'concede', 'on', 'a', 'set', 'piece']
cosine_similarity: 0.9611261487007141
word_to_vector_cosine_similarity: sentence1: I love everyone on the Belgium squad, sentence2: In other news this Belgium squad taking on USMNT is STACKED
After tokenization, sentence1: ['i', 'love', 'everyone', 'on', 'the', 'belgium', 'squad'], sentence2: ['in', 'other', 'news', 'this', 'belgium', 'squad', 'taking', 'on', 'usmnt', 'is', 'stacked']
cosine_similarity: 0.9652331471443176
word_to_vector_cosine_similarity: sentence1: Rafa Benitez is still a massive prick, sentence2: Mourinho to city Benitez to stay at Chelsea
After tokenization, sentence1: ['rafa', 'benitez', 'is', 'still', 'a', 'massive', 'prick'], sentence2: ['mourinho', 'to', 'city', 'benitez', 'to', 'stay', 'at', 'chelsea']
cosine_similarity: 0.9191781282424927
word_to_vector_cosine_similarity: sentence1: Rafa Benitez is still a massive prick, sentence2: Chelsea FC wouldnt get rid of Benitez now
After tokenization, sentence1: ['rafa', 'benitez', 'is', 'still', 'a', 'massive', 'prick'], sentence2: ['chelsea', 'fc', 'wouldnt', 'get', 'rid', 'of', 'benitez', 'now']
cosine_similarity: 0.9513035416603088
word_to_vector_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: I hope Chelsea fans are thoroughly embarrassed now with the way they treated Benitez
After tokenization, sentence1: ['rafa', 'benitez', 'he', 's', 'too', 'good', 'for', 'you'], sentence2: ['i', 'hope', 'chelsea', 'fans', 'are', 'thoroughly', 'embarrassed', 'now', 'with', 'the', 'way', 'they', 'treated', 'benitez']
cosine_similarity: 0.949142336845398
word_to_vector_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: Thank you for your tactics Benitez
After tokenization, sentence1: ['rafa', 'benitez', 'he', 's', 'too', 'good', 'for', 'you'], sentence2: ['thank', 'you', 'for', 'your', 'tactics', 'benitez']
cosine_similarity: 0.957180380821228
word_to_vector_cosine_similarity: sentence1: Rafa Benitez he s too good for you, sentence2: Got alot of time for rafa Benitez
After tokenization, sentence1: ['rafa', 'benitez', 'he', 's', 'too', 'good', 'for', 'you'], sentence2: ['got', 'alot', 'of', 'time', 'for', 'rafa', 'benitez']
cosine_similarity: 0.9788239598274231
word_to_vector_cosine_similarity: sentence1: Benitez is a sick manager, sentence2: Well done to Rafa Benitez a dignified man
After tokenization, sentence1: ['benitez', 'is', 'a', 'sick', 'manager'], sentence2: ['well', 'done', 'to', 'rafa', 'benitez', 'a', 'dignified', 'man']
cosine_similarity: 0.9568064212799072
word_to_vector_cosine_similarity: sentence1: congratulations to petr ech and rafa benitez, sentence2: Rafa Benitez a free agent
After tokenization, sentence1: ['congratulations', 'to', 'petr', 'ech', 'and', 'rafa', 'benitez'], sentence2: ['rafa', 'benitez', 'a', 'free', 'agent']
cosine_similarity: 0.908642590045929
word_to_vector_cosine_similarity: sentence1: congratulations to petr ech and rafa benitez, sentence2: Credit where credits due to Rafa Benitez
After tokenization, sentence1: ['congratulations', 'to', 'petr', 'ech', 'and', 'rafa', 'benitez'], sentence2: ['credit', 'where', 'credits', 'due', 'to', 'rafa', 'benitez']
cosine_similarity: 0.9204893112182617
word_to_vector_cosine_similarity: sentence1: God forbid lyknx Rafa Benitez, sentence2: How can chelsea fans still hate benitez
After tokenization, sentence1: ['god', 'forbid', 'rafa', 'benitez'], sentence2: ['how', 'can', 'chelsea', 'fans', 'still', 'hate', 'benitez']
cosine_similarity: 0.868623673915863
word_to_vector_cosine_similarity: sentence1: Thank you very much Rafa Benitez, sentence2: Why do liverpool fans love benitez so much
After tokenization, sentence1: ['thank', 'you', 'very', 'much', 'rafa', 'benitez'], sentence2: ['why', 'do', 'liverpool', 'fans', 'love', 'benitez', 'so', 'much']
cosine_similarity: 0.9646732807159424
word_to_vector_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: I am so happy for RAFA BENITEZ VictorMoses and Mikel
After tokenization, sentence1: ['rafa', 'benitez', 'i', 'must', 'thank', 'you'], sentence2: ['i', 'am', 'so', 'happy', 'for', 'rafa', 'benitez', 'and', 'mikel']
cosine_similarity: 0.9848617315292358
word_to_vector_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: Pleased for Benitez hasnt deserved the stick he s got
After tokenization, sentence1: ['rafa', 'benitez', 'i', 'must', 'thank', 'you'], sentence2: ['pleased', 'for', 'benitez', 'hasnt', 'deserved', 'the', 'stick', 'he', 's', 'got']
cosine_similarity: 0.9612414240837097
word_to_vector_cosine_similarity: sentence1: Rafa Benitez I must thank you, sentence2: Once a red always a blue rafa Benitez we want you
After tokenization, sentence1: ['rafa', 'benitez', 'i', 'must', 'thank', 'you'], sentence2: ['once', 'a', 'red', 'always', 'a', 'blue', 'rafa', 'benitez', 'we', 'want', 'you']
cosine_similarity: 0.9512678384780884
word_to_vector_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Benitez is alright tho man fuck chelsea fans they suck asshole
After tokenization, sentence1: ['thank', 'you', 'so', 'much', 'rafa', 'benitez'], sentence2: ['benitez', 'is', 'alright', 'tho', 'man', 'fuck', 'chelsea', 'fans', 'they', 'suck', 'asshole']
cosine_similarity: 0.9270437359809875
word_to_vector_cosine_similarity: sentence1: THANK YOU SO MUCH RAFA BENITEZ, sentence2: Credit where credits due to Rafa Benitez
After tokenization, sentence1: ['thank', 'you', 'so', 'much', 'rafa', 'benitez'], sentence2: ['credit', 'where', 'credits', 'due', 'to', 'rafa', 'benitez']
cosine_similarity: 0.9057286381721497
word_to_vector_cosine_similarity: sentence1: Rafa Benitez deserves a hell of a thank you, sentence2: Any praise for Benitez from my Chelsea followers lol
After tokenization, sentence1: ['rafa', 'benitez', 'deserves', 'a', 'hell', 'of', 'a', 'thank', 'you'], sentence2: ['any', 'praise', 'for', 'benitez', 'from', 'my', 'chelsea', 'followers', 'lol']
cosine_similarity: 0.9636006951332092
word_to_vector_cosine_similarity: sentence1: I dont understand the hatred for Rafa Benitez, sentence2: Top 4 and a trophy and still they dont give any respect for Benitez
After tokenization, sentence1: ['i', 'dont', 'understand', 'the', 'hatred', 'for', 'rafa', 'benitez'], sentence2: ['top', 'and', 'a', 'trophy', 'and', 'still', 'they', 'dont', 'give', 'any', 'respect', 'for', 'benitez']
cosine_similarity: 0.9840744137763977
word_to_vector_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Bill Self to the Big 12
After tokenization, sentence1: ['big', 'announce', 'basketball', 'challenge'], sentence2: ['bill', 'self', 'to', 'the', 'big']
cosine_similarity: 0.9071119427680969
word_to_vector_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: The Big 12 just got a whole lot more interesting
After tokenization, sentence1: ['big', 'announce', 'basketball', 'challenge'], sentence2: ['the', 'big', 'just', 'got', 'a', 'whole', 'lot', 'more', 'interesting']
cosine_similarity: 0.907336950302124
word_to_vector_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Just when you thought Bill Self wasnt going to own the Big 12 for another year
After tokenization, sentence1: ['big', 'announce', 'basketball', 'challenge'], sentence2: ['just', 'when', 'you', 'thought', 'bill', 'self', 'wasnt', 'going', 'to', 'own', 'the', 'big', 'for', 'another', 'year']
cosine_similarity: 0.9065315127372742
word_to_vector_cosine_similarity: sentence1: Big 12SEC announce basketball challenge, sentence2: Oklahoma in Houston also among the 10 SECBig 12 matchups
After tokenization, sentence1: ['big', 'announce', 'basketball', 'challenge'], sentence2: ['oklahoma', 'in', 'houston', 'also', 'among', 'the', 'matchups']
cosine_similarity: 0.9247687458992004
word_to_vector_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: Did kU win the Big 12 Quidditch Championship
After tokenization, sentence1: ['kansas', 'just', 'won', 'the', 'big', 'championship', 'again'], sentence2: ['did', 'ku', 'win', 'the', 'big', 'quidditch', 'championship']
cosine_similarity: 0.9759225249290466
word_to_vector_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: He can fuck up the Big 12 all he wants
After tokenization, sentence1: ['kansas', 'just', 'won', 'the', 'big', 'championship', 'again'], sentence2: ['he', 'can', 'fuck', 'up', 'the', 'big', 'all', 'he', 'wants']
cosine_similarity: 0.9395322203636169
word_to_vector_cosine_similarity: sentence1: Kansas just won the Big 12 championship again, sentence2: So what if the Big 12 had 14 teams
After tokenization, sentence1: ['kansas', 'just', 'won', 'the', 'big', 'championship', 'again'], sentence2: ['so', 'what', 'if', 'the', 'big', 'had', 'teams']
cosine_similarity: 0.9659425020217896
word_to_vector_cosine_similarity: sentence1: Sorry to the rest of the big 12, sentence2: And Kansas once again will win the Big 12
After tokenization, sentence1: ['sorry', 'to', 'the', 'rest', 'of', 'the', 'big'], sentence2: ['and', 'kansas', 'once', 'again', 'will', 'win', 'the', 'big']
cosine_similarity: 0.9794686436653137
word_to_vector_cosine_similarity: sentence1: The big 12 is about to be so stacked next year, sentence2: How many of those who were handing the Big 12 to Okla
After tokenization, sentence1: ['the', 'big', 'is', 'about', 'to', 'be', 'so', 'stacked', 'next', 'year'], sentence2: ['how', 'many', 'of', 'those', 'who', 'were', 'handing', 'the', 'big', 'to', 'okla']
cosine_similarity: 0.9783647060394287
word_to_vector_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: UKBig 12 challenge officially announced today
After tokenization, sentence1: ['will', 'prolly', 'win', 'the', 'big'], sentence2: ['challenge', 'officially', 'announced', 'today']
cosine_similarity: 0.9019830822944641
word_to_vector_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: 21 in Houston as part of Big 12SEC Challenge
After tokenization, sentence1: ['will', 'prolly', 'win', 'the', 'big'], sentence2: ['in', 'houston', 'as', 'part', 'of', 'big', 'challenge']
cosine_similarity: 0.9534963965415955
word_to_vector_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: No reason Kansas should lose a game in the big 12
After tokenization, sentence1: ['will', 'prolly', 'win', 'the', 'big'], sentence2: ['no', 'reason', 'kansas', 'should', 'lose', 'a', 'game', 'in', 'the', 'big']
cosine_similarity: 0.9620205760002136
word_to_vector_cosine_similarity: sentence1: will prolly win the Big 12, sentence2: The whole Big 12 but Okiestate in particular just lost their minds
After tokenization, sentence1: ['will', 'prolly', 'win', 'the', 'big'], sentence2: ['the', 'whole', 'big', 'but', 'in', 'particular', 'just', 'lost', 'their', 'minds']
cosine_similarity: 0.9629221558570862
word_to_vector_cosine_similarity: sentence1: Sorry to the rest of the big 12, sentence2: Texas Tech will play at Alabama in the SECBig 12 Basketball Challenge on Nov
After tokenization, sentence1: ['sorry', 'to', 'the', 'rest', 'of', 'the', 'big'], sentence2: ['texas', 'tech', 'will', 'play', 'at', 'alabama', 'in', 'the', 'basketball', 'challenge', 'on', 'nov']
cosine_similarity: 0.9469119906425476
word_to_vector_cosine_similarity: sentence1: UK part of the Big 12SEC Challenge, sentence2: There is NOOOO competition in Big 12 basketball
After tokenization, sentence1: ['uk', 'part', 'of', 'the', 'big', 'challenge'], sentence2: ['there', 'is', 'noooo', 'competition', 'in', 'big', 'basketball']
cosine_similarity: 0.9584144949913025
word_to_vector_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: to win the big 12 in all three major sports in the same year
After tokenization, sentence1: ['well', 'the', 'big', 'just', 'got', 'decided'], sentence2: ['to', 'win', 'the', 'big', 'in', 'all', 'three', 'major', 'sports', 'in', 'the', 'same', 'year']
cosine_similarity: 0.9616364240646362
word_to_vector_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: SECBig 12 Challenge in hoops has been announced to begin in 201314
After tokenization, sentence1: ['well', 'the', 'big', 'just', 'got', 'decided'], sentence2: ['challenge', 'in', 'hoops', 'has', 'been', 'announced', 'to', 'begin', 'in']
cosine_similarity: 0.9308721423149109
word_to_vector_cosine_similarity: sentence1: well the big 12 just got decided, sentence2: So Wiggins Is Settling For Playing In The Garbage Ass Big 12
After tokenization, sentence1: ['well', 'the', 'big', 'just', 'got', 'decided'], sentence2: ['so', 'wiggins', 'is', 'settling', 'for', 'playing', 'in', 'the', 'garbage', 'ass', 'big']
cosine_similarity: 0.9753780961036682
word_to_vector_cosine_similarity: sentence1: Just wish he wasnt in the Big 12, sentence2: Big 12 is gonna be exciting
After tokenization, sentence1: ['just', 'wish', 'he', 'wasnt', 'in', 'the', 'big'], sentence2: ['big', 'is', 'gonna', 'be', 'exciting']
cosine_similarity: 0.9783661365509033
word_to_vector_cosine_similarity: sentence1: Just wish he wasnt in the Big 12, sentence2: the BIG 12 goes through LAWRENCE
After tokenization, sentence1: ['just', 'wish', 'he', 'wasnt', 'in', 'the', 'big'], sentence2: ['the', 'big', 'goes', 'through', 'lawrence']
cosine_similarity: 0.9580591917037964
word_to_vector_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: I just found out Marilyn Monroe has a full bush
After tokenization, sentence1: ['were', 'in', 'reigate'], sentence2: ['i', 'just', 'found', 'out', 'marilyn', 'monroe', 'has', 'a', 'full', 'bush']
cosine_similarity: 0.8578868508338928
word_to_vector_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: Classic redhead with a natural bush
After tokenization, sentence1: ['were', 'in', 'reigate'], sentence2: ['classic', 'redhead', 'with', 'a', 'natural', 'bush']
cosine_similarity: 0.7725995182991028
word_to_vector_cosine_similarity: sentence1: hannahbush were in Reigate, sentence2: Hahah I heard your dumbass woke up in a bush
After tokenization, sentence1: ['were', 'in', 'reigate'], sentence2: ['hahah', 'i', 'heard', 'your', 'dumbass', 'woke', 'up', 'in', 'a', 'bush']
cosine_similarity: 0.8522698283195496
word_to_vector_cosine_similarity: sentence1: I dont have time for beating around the bush, sentence2: Honey has a brush with her non existent bush
After tokenization, sentence1: ['i', 'dont', 'have', 'time', 'for', 'beating', 'around', 'the', 'bush'], sentence2: ['honey', 'has', 'a', 'brush', 'with', 'her', 'non', 'existent', 'bush']
cosine_similarity: 0.9344807863235474
word_to_vector_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: How the hell could Obama kill more than Bush did in Iraq
After tokenization, sentence1: ['ctfu', 'the', 'man', 'in', 'the', 'bush'], sentence2: ['how', 'the', 'hell', 'could', 'obama', 'kill', 'more', 'than', 'bush', 'did', 'in', 'iraq']
cosine_similarity: 0.9758034348487854
word_to_vector_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: george bush is never a truther
After tokenization, sentence1: ['ctfu', 'the', 'man', 'in', 'the', 'bush'], sentence2: ['george', 'bush', 'is', 'never', 'a', 'truther']
cosine_similarity: 0.938325822353363
word_to_vector_cosine_similarity: sentence1: Ctfu the man in the bush, sentence2: Im a fan of Clintons Pretty much despise Bush
After tokenization, sentence1: ['ctfu', 'the', 'man', 'in', 'the', 'bush'], sentence2: ['im', 'a', 'fan', 'of', 'clintons', 'pretty', 'much', 'despise', 'bush']
cosine_similarity: 0.9562652707099915
word_to_vector_cosine_similarity: sentence1: He is worse than Bush, sentence2: It was under Bush it is now
After tokenization, sentence1: ['he', 'is', 'worse', 'than', 'bush'], sentence2: ['it', 'was', 'under', 'bush', 'it', 'is', 'now']
cosine_similarity: 0.9696451425552368
word_to_vector_cosine_similarity: sentence1: He is worse than Bush, sentence2: Did Obama and the Dems trust the Bush Government
After tokenization, sentence1: ['he', 'is', 'worse', 'than', 'bush'], sentence2: ['did', 'obama', 'and', 'the', 'dems', 'trust', 'the', 'bush', 'government']
cosine_similarity: 0.9384075403213501
word_to_vector_cosine_similarity: sentence1: He is worse than Bush, sentence2: that the Libs are going to say its Bush s fault and Im a racist
After tokenization, sentence1: ['he', 'is', 'worse', 'than', 'bush'], sentence2: ['that', 'the', 'libs', 'are', 'going', 'to', 'say', 'its', 'bush', 's', 'fault', 'and', 'im', 'a', 'racist']
cosine_similarity: 0.9586978554725647
word_to_vector_cosine_similarity: sentence1: Then time to trim the rose bush, sentence2: I dint like it under Bush either Obama has radically expanded it
After tokenization, sentence1: ['then', 'time', 'to', 'trim', 'the', 'rose', 'bush'], sentence2: ['i', 'dint', 'like', 'it', 'under', 'bush', 'either', 'obama', 'has', 'radically', 'expanded', 'it']
cosine_similarity: 0.9424854516983032
word_to_vector_cosine_similarity: sentence1: I want the Bush days back, sentence2: Dont beat around the bush just say it
After tokenization, sentence1: ['i', 'want', 'the', 'bush', 'days', 'back'], sentence2: ['dont', 'beat', 'around', 'the', 'bush', 'just', 'say', 'it']
cosine_similarity: 0.984318733215332
word_to_vector_cosine_similarity: sentence1: I want the Bush days back, sentence2: fell in a spiky bush and I have a prickly thing in my finger
After tokenization, sentence1: ['i', 'want', 'the', 'bush', 'days', 'back'], sentence2: ['fell', 'in', 'a', 'spiky', 'bush', 'and', 'i', 'have', 'a', 'prickly', 'thing', 'in', 'my', 'finger']
cosine_similarity: 0.9718008041381836
word_to_vector_cosine_similarity: sentence1: I want the Bush days back, sentence2: i once walked into a bush outside school and literally apologised to it
After tokenization, sentence1: ['i', 'want', 'the', 'bush', 'days', 'back'], sentence2: ['i', 'once', 'walked', 'into', 'a', 'bush', 'outside', 'school', 'and', 'literally', 'apologised', 'to', 'it']
cosine_similarity: 0.9737642407417297
word_to_vector_cosine_similarity: sentence1: I want the Bush days back, sentence2: Bush wiretapped without warrants Obama had them
After tokenization, sentence1: ['i', 'want', 'the', 'bush', 'days', 'back'], sentence2: ['bush', 'without', 'warrants', 'obama', 'had', 'them']
cosine_similarity: 0.9149393439292908
word_to_vector_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: started under bush and Im sure you were cool with it then
After tokenization, sentence1: ['footbridge', 'over', 'the', 'river', 'bush'], sentence2: ['started', 'under', 'bush', 'and', 'im', 'sure', 'you', 'were', 'cool', 'with', 'it', 'then']
cosine_similarity: 0.8623417615890503
word_to_vector_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: The new Bush tour merchandise is now available
After tokenization, sentence1: ['footbridge', 'over', 'the', 'river', 'bush'], sentence2: ['the', 'new', 'bush', 'tour', 'merchandise', 'is', 'now', 'available']
cosine_similarity: 0.8923953771591187
word_to_vector_cosine_similarity: sentence1: Footbridge over the River Bush, sentence2: we live near a bush reserve
After tokenization, sentence1: ['footbridge', 'over', 'the', 'river', 'bush'], sentence2: ['we', 'live', 'near', 'a', 'bush', 'reserve']
cosine_similarity: 0.9388440251350403
word_to_vector_cosine_similarity: sentence1: I did for Bush as well, sentence2: in my actual bush in a bush in Bushey
After tokenization, sentence1: ['i', 'did', 'for', 'bush', 'as', 'well'], sentence2: ['in', 'my', 'actual', 'bush', 'in', 'a', 'bush', 'in', 'bushey']
cosine_similarity: 0.9389052987098694
word_to_vector_cosine_similarity: sentence1: I did for Bush as well, sentence2: it started way before Bush Jr
After tokenization, sentence1: ['i', 'did', 'for', 'bush', 'as', 'well'], sentence2: ['it', 'started', 'way', 'before', 'bush', 'jr']
cosine_similarity: 0.9732149243354797
word_to_vector_cosine_similarity: sentence1: Darling stop beating around the bush, sentence2: They called Bush hitler too
After tokenization, sentence1: ['darling', 'stop', 'beating', 'around', 'the', 'bush'], sentence2: ['they', 'called', 'bush', 'hitler', 'too']
cosine_similarity: 0.9547279477119446
word_to_vector_cosine_similarity: sentence1: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR, sentence2: Why cant everyone just tell it how it is instead of beating around the bush
After tokenization, sentence1: ['fuck', 'the', 'government', 'fuck', 'the', 'obama', 'administration', 'fuck', 'bush', 'fuck', 'bush', 'sr'], sentence2: ['why', 'cant', 'everyone', 'just', 'tell', 'it', 'how', 'it', 'is', 'instead', 'of', 'beating', 'around', 'the', 'bush']
cosine_similarity: 0.9465409517288208
word_to_vector_cosine_similarity: sentence1: FUCK THE GOVERNMENT FUCK THE OBAMA ADMINISTRATION FUCK BUSH FUCK BUSH SR, sentence2: And his legs can have hair but no bush
After tokenization, sentence1: ['fuck', 'the', 'government', 'fuck', 'the', 'obama', 'administration', 'fuck', 'bush', 'fuck', 'bush', 'sr'], sentence2: ['and', 'his', 'legs', 'can', 'have', 'hair', 'but', 'no', 'bush']
cosine_similarity: 0.9225195646286011
word_to_vector_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Allen Bosh Chalmers James at Wade
After tokenization, sentence1: ['yeah', 'chalmers', 'that', 's', 'a', 'foul'], sentence2: ['allen', 'bosh', 'chalmers', 'james', 'at', 'wade']
cosine_similarity: 0.8636842966079712
word_to_vector_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Chalmers makes me mad low key
After tokenization, sentence1: ['yeah', 'chalmers', 'that', 's', 'a', 'foul'], sentence2: ['chalmers', 'makes', 'me', 'mad', 'low', 'key']
cosine_similarity: 0.9685675501823425
word_to_vector_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: Chalmers steady chasing on defense
After tokenization, sentence1: ['yeah', 'chalmers', 'that', 's', 'a', 'foul'], sentence2: ['chalmers', 'steady', 'chasing', 'on', 'defense']
cosine_similarity: 0.9011731147766113
word_to_vector_cosine_similarity: sentence1: Yeah CHALMERS that s a foul, sentence2: I like Cole s onball defense better than Chalmers
After tokenization, sentence1: ['yeah', 'chalmers', 'that', 's', 'a', 'foul'], sentence2: ['i', 'like', 'cole', 's', 'defense', 'better', 'than', 'chalmers']
cosine_similarity: 0.9816040992736816
word_to_vector_cosine_similarity: sentence1: And now chalmers with the scoop layup, sentence2: Anybody see David West Elbow the hell out of Chalmers bad shoulder
After tokenization, sentence1: ['and', 'now', 'chalmers', 'with', 'the', 'scoop', 'layup'], sentence2: ['anybody', 'see', 'david', 'west', 'elbow', 'the', 'hell', 'out', 'of', 'chalmers', 'bad', 'shoulder']
cosine_similarity: 0.9774250984191895
word_to_vector_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: MARIO CHALMERS JUST THREW IT TO HIS COACH HAHAHA
After tokenization, sentence1: ['mario', 'chalmers', 'pissin', 'me', 'off'], sentence2: ['mario', 'chalmers', 'just', 'threw', 'it', 'to', 'his', 'coach', 'hahaha']
cosine_similarity: 0.9034698009490967
word_to_vector_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Spo aint in the game chalmers
After tokenization, sentence1: ['mario', 'chalmers', 'pissin', 'me', 'off'], sentence2: ['spo', 'aint', 'in', 'the', 'game', 'chalmers']
cosine_similarity: 0.8749983906745911
word_to_vector_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Mario Chalmers needs to get punched in the face
After tokenization, sentence1: ['mario', 'chalmers', 'pissin', 'me', 'off'], sentence2: ['mario', 'chalmers', 'needs', 'to', 'get', 'punched', 'in', 'the', 'face']
cosine_similarity: 0.8946954011917114
word_to_vector_cosine_similarity: sentence1: Mario Chalmers pissin me off, sentence2: Chalmers throws it out of bounds
After tokenization, sentence1: ['mario', 'chalmers', 'pissin', 'me', 'off'], sentence2: ['chalmers', 'throws', 'it', 'out', 'of', 'bounds']
cosine_similarity: 0.8635400533676147
word_to_vector_cosine_similarity: sentence1: Chalmers just walked with the ball, sentence2: Uhm Chalmers didnt just travel
After tokenization, sentence1: ['chalmers', 'just', 'walked', 'with', 'the', 'ball'], sentence2: ['uhm', 'chalmers', 'didnt', 'just', 'travel']
cosine_similarity: 0.9450207948684692
word_to_vector_cosine_similarity: sentence1: Mario Chalmers looks like Mario, sentence2: Then Chalmers fucks up again lmao
After tokenization, sentence1: ['mario', 'chalmers', 'looks', 'like', 'mario'], sentence2: ['then', 'chalmers', 'fucks', 'up', 'again', 'lmao']
cosine_similarity: 0.8611149191856384
word_to_vector_cosine_similarity: sentence1: Mario Chalmers looks like Mario, sentence2: Lucky ass shxt by Chalmers
After tokenization, sentence1: ['mario', 'chalmers', 'looks', 'like', 'mario'], sentence2: ['lucky', 'ass', 'shxt', 'by', 'chalmers']
cosine_similarity: 0.860129177570343
word_to_vector_cosine_similarity: sentence1: I hate Mario Chalmersdont know why, sentence2: idc idc chalmers be making me mad
After tokenization, sentence1: ['i', 'hate', 'mario', 'know', 'why'], sentence2: ['idc', 'idc', 'chalmers', 'be', 'making', 'me', 'mad']
cosine_similarity: 0.9532608389854431
word_to_vector_cosine_similarity: sentence1: I hate Mario Chalmersdont know why, sentence2: Why is Mario Chalmers starting
After tokenization, sentence1: ['i', 'hate', 'mario', 'know', 'why'], sentence2: ['why', 'is', 'mario', 'chalmers', 'starting']
cosine_similarity: 0.9151886701583862
word_to_vector_cosine_similarity: sentence1: CHALMERS IS THE TURNOVER KING, sentence2: Didnt everyone love Mario Chalmers last year
After tokenization, sentence1: ['chalmers', 'is', 'the', 'turnover', 'king'], sentence2: ['didnt', 'everyone', 'love', 'mario', 'chalmers', 'last', 'year']
cosine_similarity: 0.9002931714057922
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: LMFAOOO who tf you throwing to Chalmers
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['lmfaooo', 'who', 'tf', 'you', 'throwing', 'to', 'chalmers']
cosine_similarity: 0.964851975440979
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: James with the block and then wade to James James to chalmers for the finish
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['james', 'with', 'the', 'block', 'and', 'then', 'wade', 'to', 'james', 'james', 'to', 'chalmers', 'for', 'the', 'finish']
cosine_similarity: 0.9590420126914978
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: This is like the 27th time chalmers fucked up
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['this', 'is', 'like', 'the', 'time', 'chalmers', 'fucked', 'up']
cosine_similarity: 0.9723256826400757
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: Chalmers would lock you down bubs
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['chalmers', 'would', 'lock', 'you', 'down', 'bubs']
cosine_similarity: 0.9606724381446838
word_to_vector_cosine_similarity: sentence1: Chalmers is always complaining to the refs, sentence2: Mario Chalmers is easily my worst favorite player in the NBA
After tokenization, sentence1: ['chalmers', 'is', 'always', 'complaining', 'to', 'the', 'refs'], sentence2: ['mario', 'chalmers', 'is', 'easily', 'my', 'worst', 'favorite', 'player', 'in', 'the', 'nba']
cosine_similarity: 0.9614307284355164
word_to_vector_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: The fuck Chalmers is doing
After tokenization, sentence1: ['the', 'fuck', 'chalmers', 'is', 'doing'], sentence2: ['the', 'fuck', 'chalmers', 'is', 'doing']
cosine_similarity: 1.0
word_to_vector_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: Do I watch the game or Chalmers face
After tokenization, sentence1: ['the', 'fuck', 'chalmers', 'is', 'doing'], sentence2: ['do', 'i', 'watch', 'the', 'game', 'or', 'chalmers', 'face']
cosine_similarity: 0.9621689915657043
word_to_vector_cosine_similarity: sentence1: The fuck Chalmers is doing, sentence2: Uhm Chalmers didnt just travel
After tokenization, sentence1: ['the', 'fuck', 'chalmers', 'is', 'doing'], sentence2: ['uhm', 'chalmers', 'didnt', 'just', 'travel']
cosine_similarity: 0.9516076445579529
word_to_vector_cosine_similarity: sentence1: Chara get in that box, sentence2: Chara is dirtier than a Fresno adult film star
After tokenization, sentence1: ['chara', 'get', 'in', 'that', 'box'], sentence2: ['chara', 'is', 'dirtier', 'than', 'a', 'fresno', 'adult', 'film', 'star']
cosine_similarity: 0.9363540410995483
word_to_vector_cosine_similarity: sentence1: Orr should beat the shit out of chara, sentence2: What s better than seeing Chara getting dropped by Orr
After tokenization, sentence1: ['orr', 'should', 'beat', 'the', 'shit', 'out', 'of', 'chara'], sentence2: ['what', 's', 'better', 'than', 'seeing', 'chara', 'getting', 'dropped', 'by', 'orr']
cosine_similarity: 0.9870923161506653
word_to_vector_cosine_similarity: sentence1: watching chara go down is the best feeling, sentence2: Chara s playing with hate in heart
After tokenization, sentence1: ['watching', 'chara', 'go', 'down', 'is', 'the', 'best', 'feeling'], sentence2: ['chara', 's', 'playing', 'with', 'hate', 'in', 'heart']
cosine_similarity: 0.9745883941650391
word_to_vector_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara is the most overrated player in the league
After tokenization, sentence1: ['orr', 'vs', 'chara', 'is', 'like', 'a', 'jack', 'russell', 'vs', 'a', 'wolfhound'], sentence2: ['chara', 'is', 'the', 'most', 'overrated', 'player', 'in', 'the', 'league']
cosine_similarity: 0.9133561253547668
word_to_vector_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: Chara move your feet along the boards
After tokenization, sentence1: ['orr', 'vs', 'chara', 'is', 'like', 'a', 'jack', 'russell', 'vs', 'a', 'wolfhound'], sentence2: ['chara', 'move', 'your', 'feet', 'along', 'the', 'boards']
cosine_similarity: 0.8382033705711365
word_to_vector_cosine_similarity: sentence1: Orr vs Chara is like a Jack Russell vs a Wolfhound, sentence2: FINALLY CHARA IS GIVEN A PENALTY
After tokenization, sentence1: ['orr', 'vs', 'chara', 'is', 'like', 'a', 'jack', 'russell', 'vs', 'a', 'wolfhound'], sentence2: ['finally', 'chara', 'is', 'given', 'a', 'penalty']
cosine_similarity: 0.9263771176338196
word_to_vector_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Orr wants a piece of chara I swear
After tokenization, sentence1: ['chara', 'is', 'holy', 'shit'], sentence2: ['orr', 'wants', 'a', 'piece', 'of', 'chara', 'i', 'swear']
cosine_similarity: 0.9525408148765564
word_to_vector_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Okay who else saw that beauty hit to chara by orr
After tokenization, sentence1: ['chara', 'is', 'holy', 'shit'], sentence2: ['okay', 'who', 'else', 'saw', 'that', 'beauty', 'hit', 'to', 'chara', 'by', 'orr']
cosine_similarity: 0.9455086588859558
word_to_vector_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: When anyone on the leafs knock down Chara I laugh so hard
After tokenization, sentence1: ['chara', 'is', 'holy', 'shit'], sentence2: ['when', 'anyone', 'on', 'the', 'leafs', 'knock', 'down', 'chara', 'i', 'laugh', 'so', 'hard']
cosine_similarity: 0.9463491439819336
word_to_vector_cosine_similarity: sentence1: Chara is 69 Holy shit, sentence2: Amazing how bad Chara looks when the speed of the game picks up
After tokenization, sentence1: ['chara', 'is', 'holy', 'shit'], sentence2: ['amazing', 'how', 'bad', 'chara', 'looks', 'when', 'the', 'speed', 'of', 'the', 'game', 'picks', 'up']
cosine_similarity: 0.9517599940299988
word_to_vector_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: Why is chara playing like a bitch
After tokenization, sentence1: ['colton', 'orr', 'lowers', 'the', 'boom', 'on', 'chara'], sentence2: ['why', 'is', 'chara', 'playing', 'like', 'a', 'bitch']
cosine_similarity: 0.9095085859298706
word_to_vector_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: BOOOOOMM down went chara again
After tokenization, sentence1: ['colton', 'orr', 'lowers', 'the', 'boom', 'on', 'chara'], sentence2: ['down', 'went', 'chara', 'again']
cosine_similarity: 0.9100167751312256
word_to_vector_cosine_similarity: sentence1: Colton Orr lowers the boom on Chara, sentence2: Zideno Chara is a brick wall on skates
After tokenization, sentence1: ['colton', 'orr', 'lowers', 'the', 'boom', 'on', 'chara'], sentence2: ['chara', 'is', 'a', 'brick', 'wall', 'on', 'skates']
cosine_similarity: 0.9233399033546448
word_to_vector_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: Chara penalty on the play
After tokenization, sentence1: ['orr', 'with', 'a', 'big', 'hit', 'on', 'chara'], sentence2: ['chara', 'penalty', 'on', 'the', 'play']
cosine_similarity: 0.9615659117698669
word_to_vector_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: Chara called for a high stick on the play too
After tokenization, sentence1: ['orr', 'with', 'a', 'big', 'hit', 'on', 'chara'], sentence2: ['chara', 'called', 'for', 'a', 'high', 'stick', 'on', 'the', 'play', 'too']
cosine_similarity: 0.9856194853782654
word_to_vector_cosine_similarity: sentence1: Orr with a big hit on Chara, sentence2: I keep waiting for the chara vs orr fight
After tokenization, sentence1: ['orr', 'with', 'a', 'big', 'hit', 'on', 'chara'], sentence2: ['i', 'keep', 'waiting', 'for', 'the', 'chara', 'vs', 'orr', 'fight']
cosine_similarity: 0.9659247398376465
word_to_vector_cosine_similarity: sentence1: Chara is a disgrace to the NHL, sentence2: Chara is just a big goon
After tokenization, sentence1: ['chara', 'is', 'a', 'disgrace', 'to', 'the', 'nhl'], sentence2: ['chara', 'is', 'just', 'a', 'big', 'goon']
cosine_similarity: 0.960723340511322
word_to_vector_cosine_similarity: sentence1: but orr keeps pushing chara, sentence2: Imma blame Chara for Lupul missing that one
After tokenization, sentence1: ['but', 'orr', 'keeps', 'pushing', 'chara'], sentence2: ['imma', 'blame', 'chara', 'for', 'lupul', 'missing', 'that', 'one']
cosine_similarity: 0.9350329041481018
word_to_vector_cosine_similarity: sentence1: but orr keeps pushing chara, sentence2: Why is Chara allowed to take down people without the puck
After tokenization, sentence1: ['but', 'orr', 'keeps', 'pushing', 'chara'], sentence2: ['why', 'is', 'chara', 'allowed', 'to', 'take', 'down', 'people', 'without', 'the', 'puck']
cosine_similarity: 0.9288696646690369
word_to_vector_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: When do you plan to be at 600 W Chicago next
After tokenization, sentence1: ['except', 'im', 'in', 'chicago', 'at', 'the', 'moment'], sentence2: ['when', 'do', 'you', 'plan', 'to', 'be', 'at', 'w', 'chicago', 'next']
cosine_similarity: 0.9618741869926453
word_to_vector_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: Big game 7 here in Chicago tonight
After tokenization, sentence1: ['except', 'im', 'in', 'chicago', 'at', 'the', 'moment'], sentence2: ['big', 'game', 'here', 'in', 'chicago', 'tonight']
cosine_similarity: 0.9665164351463318
word_to_vector_cosine_similarity: sentence1: except Im in Chicago at the moment, sentence2: I checked in at Chicago Park District
After tokenization, sentence1: ['except', 'im', 'in', 'chicago', 'at', 'the', 'moment'], sentence2: ['i', 'checked', 'in', 'at', 'chicago', 'park', 'district']
cosine_similarity: 0.964242696762085
word_to_vector_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: Nice new Guideshop in Chicago
After tokenization, sentence1: ['anyone', 'wanting', 'to', 'go', 'to', 'the', 'jt', 'concert', 'in', 'chicago'], sentence2: ['nice', 'new', 'in', 'chicago']
cosine_similarity: 0.9602895379066467
word_to_vector_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: The North side of Chicago is happy
After tokenization, sentence1: ['anyone', 'wanting', 'to', 'go', 'to', 'the', 'jt', 'concert', 'in', 'chicago'], sentence2: ['the', 'north', 'side', 'of', 'chicago', 'is', 'happy']
cosine_similarity: 0.9605454206466675
word_to_vector_cosine_similarity: sentence1: Anyone wanting to go to the JT concert 722 in Chicago, sentence2: just watched season finale of chicago fire and cried
After tokenization, sentence1: ['anyone', 'wanting', 'to', 'go', 'to', 'the', 'jt', 'concert', 'in', 'chicago'], sentence2: ['just', 'watched', 'season', 'finale', 'of', 'chicago', 'fire', 'and', 'cried']
cosine_similarity: 0.9426076412200928
word_to_vector_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: I really wanna do the color run in Chicago lol
After tokenization, sentence1: ['see', 'you', 'in', 'chicago', 'dierks'], sentence2: ['i', 'really', 'wanna', 'do', 'the', 'color', 'run', 'in', 'chicago', 'lol']
cosine_similarity: 0.9568437933921814
word_to_vector_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: The ungeekedeliteschicago Daily is out
After tokenization, sentence1: ['see', 'you', 'in', 'chicago', 'dierks'], sentence2: ['the', 'daily', 'is', 'out']
cosine_similarity: 0.9502812623977661
word_to_vector_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: I am watching Chicago Fire A Hell of a Ride
After tokenization, sentence1: ['see', 'you', 'in', 'chicago', 'dierks'], sentence2: ['i', 'am', 'watching', 'chicago', 'fire', 'a', 'hell', 'of', 'a', 'ride']
cosine_similarity: 0.9527386426925659
word_to_vector_cosine_similarity: sentence1: See you in Chicago Dierks, sentence2: panoramic shot of noKXL and stop deportations rallies at Obama fundraiser in Chicago
After tokenization, sentence1: ['see', 'you', 'in', 'chicago', 'dierks'], sentence2: ['panoramic', 'shot', 'of', 'nokxl', 'and', 'stop', 'deportations', 'rallies', 'at', 'obama', 'fundraiser', 'in', 'chicago']
cosine_similarity: 0.9118006825447083
word_to_vector_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: blogher is in chicago this year
After tokenization, sentence1: ['im', 'just', 'saying', 'game', 'in', 'chicago'], sentence2: ['blogher', 'is', 'in', 'chicago', 'this', 'year']
cosine_similarity: 0.9521575570106506
word_to_vector_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: Wings in a must win in Chicago
After tokenization, sentence1: ['im', 'just', 'saying', 'game', 'in', 'chicago'], sentence2: ['wings', 'in', 'a', 'must', 'win', 'in', 'chicago']
cosine_similarity: 0.945781409740448
word_to_vector_cosine_similarity: sentence1: Im just saying game 7 in Chicago, sentence2: Id definitely be happy to help but I wont be back in Chicago until Sunday
After tokenization, sentence1: ['im', 'just', 'saying', 'game', 'in', 'chicago'], sentence2: ['id', 'definitely', 'be', 'happy', 'to', 'help', 'but', 'i', 'wont', 'be', 'back', 'in', 'chicago', 'until', 'sunday']
cosine_similarity: 0.9689367413520813
word_to_vector_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: By Micheline Maynard Contributor Chicago has
After tokenization, sentence1: ['game', 'in', 'chicago', 'tonight'], sentence2: ['by', 'micheline', 'maynard', 'contributor', 'chicago', 'has']
cosine_similarity: 0.8050628304481506
word_to_vector_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: bro you in chicago widdit yet
After tokenization, sentence1: ['game', 'in', 'chicago', 'tonight'], sentence2: ['bro', 'you', 'in', 'chicago', 'widdit', 'yet']
cosine_similarity: 0.92421954870224
word_to_vector_cosine_similarity: sentence1: Game 7 in Chicago tonight, sentence2: is L still looking for a copy of Chicago
After tokenization, sentence1: ['game', 'in', 'chicago', 'tonight'], sentence2: ['is', 'l', 'still', 'looking', 'for', 'a', 'copy', 'of', 'chicago']
cosine_similarity: 0.9348651766777039
word_to_vector_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: hope you and Hunter made it to Chicago this time
After tokenization, sentence1: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl'], sentence2: ['hope', 'you', 'and', 'hunter', 'made', 'it', 'to', 'chicago', 'this', 'time']
cosine_similarity: 0.980858325958252
word_to_vector_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: Flying to chicago on the 14th
After tokenization, sentence1: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl'], sentence2: ['flying', 'to', 'chicago', 'on', 'the']
cosine_similarity: 0.9652056694030762
word_to_vector_cosine_similarity: sentence1: Good week to be a Northsiderrr in Chicago, sentence2: go to north coast in chicago its way cheaper and the same weekend
After tokenization, sentence1: ['good', 'week', 'to', 'be', 'a', 'in', 'chicago'], sentence2: ['go', 'to', 'north', 'coast', 'in', 'chicago', 'its', 'way', 'cheaper', 'and', 'the', 'same', 'weekend']
cosine_similarity: 0.9771391749382019
word_to_vector_cosine_similarity: sentence1: Good week to be a Northsiderrr in Chicago, sentence2: Leaving early tomorrow morning to go to the Hospital in Chicago
After tokenization, sentence1: ['good', 'week', 'to', 'be', 'a', 'in', 'chicago'], sentence2: ['leaving', 'early', 'tomorrow', 'morning', 'to', 'go', 'to', 'the', 'hospital', 'in', 'chicago']
cosine_similarity: 0.9800469279289246
word_to_vector_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: I live in South Ontario but have great friends in Chicago
After tokenization, sentence1: ['ahh', 'obama', 's', 'in', 'chicago'], sentence2: ['i', 'live', 'in', 'south', 'ontario', 'but', 'have', 'great', 'friends', 'in', 'chicago']
cosine_similarity: 0.9408639073371887
word_to_vector_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: did u really quit your job to go to edc chicago
After tokenization, sentence1: ['ahh', 'obama', 's', 'in', 'chicago'], sentence2: ['did', 'u', 'really', 'quit', 'your', 'job', 'to', 'go', 'to', 'edc', 'chicago']
cosine_similarity: 0.9221509695053101
word_to_vector_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: youre in Chicago and were here to say nokxl
After tokenization, sentence1: ['ahh', 'obama', 's', 'in', 'chicago'], sentence2: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl']
cosine_similarity: 0.9216969609260559
word_to_vector_cosine_similarity: sentence1: Ahh Obama s in Chicago, sentence2: Night in Chicago with my ladies
After tokenization, sentence1: ['ahh', 'obama', 's', 'in', 'chicago'], sentence2: ['night', 'in', 'chicago', 'with', 'my', 'ladies']
cosine_similarity: 0.9075974225997925
word_to_vector_cosine_similarity: sentence1: I cannot WAIT to go to Chicago, sentence2: Chicago is saying this todayEven nonhockey fans
After tokenization, sentence1: ['i', 'cannot', 'wait', 'to', 'go', 'to', 'chicago'], sentence2: ['chicago', 'is', 'saying', 'this', 'fans']
cosine_similarity: 0.9341691136360168
word_to_vector_cosine_similarity: sentence1: I cannot WAIT to go to Chicago, sentence2: It was a day of impulse buys in Chicago
After tokenization, sentence1: ['i', 'cannot', 'wait', 'to', 'go', 'to', 'chicago'], sentence2: ['it', 'was', 'a', 'day', 'of', 'impulse', 'buys', 'in', 'chicago']
cosine_similarity: 0.9383672475814819
word_to_vector_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: Just bought a ticket to Chicago for 2350 round trip
After tokenization, sentence1: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl'], sentence2: ['just', 'bought', 'a', 'ticket', 'to', 'chicago', 'for', 'round', 'trip']
cosine_similarity: 0.9569721221923828
word_to_vector_cosine_similarity: sentence1: youre in Chicago and were here to say nokxl, sentence2: If Chicago does I can officially turn off the TV for the season
After tokenization, sentence1: ['youre', 'in', 'chicago', 'and', 'were', 'here', 'to', 'say', 'nokxl'], sentence2: ['if', 'chicago', 'does', 'i', 'can', 'officially', 'turn', 'off', 'the', 'tv', 'for', 'the', 'season']
cosine_similarity: 0.9840550422668457
word_to_vector_cosine_similarity: sentence1: Yes yo CHRIS DAVIS IS BATS, sentence2: Is that Chris Davis out there
After tokenization, sentence1: ['yes', 'yo', 'chris', 'davis', 'is', 'bats'], sentence2: ['is', 'that', 'chris', 'davis', 'out', 'there']
cosine_similarity: 0.960939884185791
word_to_vector_cosine_similarity: sentence1: Wow Chris Davis is only 27, sentence2: Chris Davis actually reminds me of Hamilton at the plate
After tokenization, sentence1: ['wow', 'chris', 'davis', 'is', 'only'], sentence2: ['chris', 'davis', 'actually', 'reminds', 'me', 'of', 'hamilton', 'at', 'the', 'plate']
cosine_similarity: 0.966641902923584
word_to_vector_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: So um Chris Davis you guys
After tokenization, sentence1: ['chris', 'davis', 'rules', 'the', 'yard', 'tonight'], sentence2: ['so', 'um', 'chris', 'davis', 'you', 'guys']
cosine_similarity: 0.8951800465583801
word_to_vector_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: So happy Chris Davis is on my fantasy team
After tokenization, sentence1: ['chris', 'davis', 'rules', 'the', 'yard', 'tonight'], sentence2: ['so', 'happy', 'chris', 'davis', 'is', 'on', 'my', 'fantasy', 'team']
cosine_similarity: 0.9518905282020569
word_to_vector_cosine_similarity: sentence1: Chris Davis rules the Yard tonight, sentence2: No other words but shut the front door Chris Davis is my not so secret crush
After tokenization, sentence1: ['chris', 'davis', 'rules', 'the', 'yard', 'tonight'], sentence2: ['no', 'other', 'words', 'but', 'shut', 'the', 'front', 'door', 'chris', 'davis', 'is', 'my', 'not', 'so', 'secret', 'crush']
cosine_similarity: 0.9239528775215149
word_to_vector_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: Is Chris Davis a top 5 hitter in baseball right now
After tokenization, sentence1: ['chris', 'davis', 'is', 'way', 'to', 'nice'], sentence2: ['is', 'chris', 'davis', 'a', 'top', 'hitter', 'in', 'baseball', 'right', 'now']
cosine_similarity: 0.9775925874710083
word_to_vector_cosine_similarity: sentence1: Chris Davis is way to nice, sentence2: WHAT THE HELL DOES CHRIS DAVIS EAT FOR BREAKFAST
After tokenization, sentence1: ['chris', 'davis', 'is', 'way', 'to', 'nice'], sentence2: ['what', 'the', 'hell', 'does', 'chris', 'davis', 'eat', 'for', 'breakfast']
cosine_similarity: 0.9768643379211426
word_to_vector_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: Ayo smh Chris Davis TEACH ME
After tokenization, sentence1: ['chris', 'davis', 'is', 'on', 'the', 'roids', 'big', 'time'], sentence2: ['ayo', 'smh', 'chris', 'davis', 'teach', 'me']
cosine_similarity: 0.9262897372245789
word_to_vector_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: When is the Chris Davis ped suspension coming
After tokenization, sentence1: ['chris', 'davis', 'is', 'on', 'the', 'roids', 'big', 'time'], sentence2: ['when', 'is', 'the', 'chris', 'davis', 'ped', 'suspension', 'coming']
cosine_similarity: 0.9863466024398804
word_to_vector_cosine_similarity: sentence1: Chris Davis is on the roids BIG TIME, sentence2: Chris Davis is on pace for about 56 HRs
After tokenization, sentence1: ['chris', 'davis', 'is', 'on', 'the', 'roids', 'big', 'time'], sentence2: ['chris', 'davis', 'is', 'on', 'pace', 'for', 'about', 'hrs']
cosine_similarity: 0.982689380645752
[[0.9554801], [0.9636808], [0.97466636], [0.97449225], [0.97841316], [0.9730813], [0.9570022], [0.9774959], [0.9806771], [0.97331274], [0.9507029], [0.96812785], [0.9706139], [0.9649458], [0.97871107], [0.9753765], [0.9859363], [0.9594148], [0.92363775], [0.95476276], [0.9844424], [0.9645919], [0.9647327], [0.96744204], [0.9764147], [0.9673699], [0.9717864], [0.97230417], [0.9229208], [0.976116], [0.9874825], [0.97688943], [0.97696793], [0.979856], [0.9571712], [0.9843347], [0.964705], [0.96735066], [0.9810017], [0.95733976], [0.9859218], [0.98326784], [0.9750257], [0.9718703], [0.96539587], [0.8548769], [0.8784986], [0.952566], [0.93509793], [0.9755635], [0.91023684], [0.9402766], [0.9446313], [0.90510106], [0.9022647], [0.9108554], [0.9155782], [0.93701094], [0.96112615], [0.96523315], [0.9191781], [0.95130354], [0.94914234], [0.9571804], [0.97882396], [0.9568064], [0.9086426], [0.9204893], [0.8686237], [0.9646733], [0.98486173], [0.9612414], [0.95126784], [0.92704374], [0.90572864], [0.9636007], [0.9840744], [0.90711194], [0.90733695], [0.9065315], [0.92476875], [0.9759225], [0.9395322], [0.9659425], [0.97946864], [0.9783647], [0.9019831], [0.9534964], [0.9620206], [0.96292216], [0.946912], [0.9584145], [0.9616364], [0.93087214], [0.9753781], [0.97836614], [0.9580592], [0.85788685], [0.7725995], [0.8522698], [0.9344808], [0.97580343], [0.9383258], [0.9562653], [0.96964514], [0.93840754], [0.95869786], [0.94248545], [0.98431873], [0.9718008], [0.97376424], [0.91493934], [0.86234176], [0.8923954], [0.938844], [0.9389053], [0.9732149], [0.95472795], [0.94654095], [0.92251956], [0.8636843], [0.96856755], [0.9011731], [0.9816041], [0.9774251], [0.9034698], [0.8749984], [0.8946954], [0.86354005], [0.9450208], [0.8611149], [0.8601292], [0.95326084], [0.91518867], [0.9002932], [0.964852], [0.959042], [0.9723257], [0.96067244], [0.9614307], [1.0], [0.962169], [0.95160764], [0.93635404], [0.9870923], [0.9745884], [0.9133561], [0.8382034], [0.9263771], [0.9525408], [0.94550866], [0.94634914], [0.95176], [0.9095086], [0.9100168], [0.9233399], [0.9615659], [0.9856195], [0.96592474], [0.96072334], [0.9350329], [0.92886966], [0.9618742], [0.96651644], [0.9642427], [0.96028954], [0.9605454], [0.94260764], [0.9568438], [0.95028126], [0.95273864], [0.9118007], [0.95215756], [0.9457814], [0.96893674], [0.80506283], [0.92421955], [0.9348652], [0.9808583], [0.96520567], [0.9771392], [0.9800469], [0.9408639], [0.92215097], [0.92169696], [0.9075974], [0.9341691], [0.93836725], [0.9569721], [0.98405504], [0.9609399], [0.9666419], [0.89518005], [0.9518905], [0.9239529], [0.9775926], [0.97686434], [0.92628974], [0.9863466], [0.9826894]]
(CSI5138) demi@MS-7A71:~/CSI5180_A3$ [K(CSI5138) demi@MS-7A71:~/CSI5180_A3$ exit
exit

Script done on Sun 06 Mar 2022 08:05:44 PM EST
